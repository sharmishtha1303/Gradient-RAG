Information Science and Statistics Series Editors: M.
Jordan J.
Kleinberg B.
Scho¨lkopf Information Science and Statistics Akaike and Kitagawa: The Practice of Time Series Analysis.
Bishop: Pattern Recognition and Machine Learning.
Cowell, Dawid, Lauritzen, and Spiegelhalter: Probabilistic Networks and Expert Systems.
Doucet, de Freitas, and Gordon: Sequential Monte Carlo Methods in Practice.
Fine: Feedforward Neural Network Methodology.
Hawkins and Olwell: Cumulative Sum Charts and Charting for Quality Improvement.
Jensen: Bayesian Networks and Decision Graphs.
Marchette: Computer Intrusion Detection and Network Monitoring: A Statistical Viewpoint.
Rubinstein and Kroese: The Cross-Entropy Method: A Unified Approach to Combinatorial Optimization, Monte Carlo Simulation, and Machine Learning.
Studený: Probabilistic Conditional Independence Structures.
Vapnik: The Nature of Statistical Learning Theory, Second Edition.
Wallace: Statistical and Inductive Inference by Minimum Massage Length.
Christopher M.
Bishop Pattern Recognition and Machine Learning Christopher M.
Bishop F.
R.
Eng.
Assistant Director Microsoft Research Ltd Cambridge CB30FB, U.
K.
cmbishop@microsoft.
com http://research.
microsoft.
com/ cmbishop Series Editors Michael Jordan Professor Jon Kleinberg Bernhard Scho¨lkopf Departmentof Computer Departmentof Computer Max Planck Institutefor Scienceand Department Science Biological Cybernetics of Statistics Cornell University Spemannstrasse38 Universityof California, Ithaca, NY14853 72076Tu¨bingen Berkeley USA Germany Berkeley, CA94720 USA Libraryof Congress Control Number:2006922522 ISBN-10:0-387-31073-8 ISBN-13:978-0387-31073-2 Printedonacid-freepaper.
©2006Springer Science+Business Media, LLC Allrightsreserved.
Thisworkmaynotbetranslatedorcopiedinwholeorinpartwithoutthewrittenpermissionofthepublisher (Springer Science+Business Media, LLC,233Spring Street, New York, NY10013, USA), exceptforbriefexcerptsinconnection withreviewsorscholarlyanalysis.
Useinconnectionwithanyformofinformationstorageandretrieval, electronicadaptation, computersoftware, orbysimilarordissimilarmethodologynowknownorhereafterdevelopedisforbidden.
Theuseinthispublicationoftradenames, trademarks, servicemarks, andsimilarterms, eveniftheyarenotidentifiedassuch, isnottobetakenasanexpressionofopinionastowhetherornottheyaresubjecttoproprietaryrights.
Printedin Singapore.
(KYO) 9 8 7 6 5 4 3 2 1 springer.
com Thisbookisdedicatedtomyfamily: Jenna, Mark, and Hugh Totaleclipseofthesun, Antalya, Turkey,29March2006.
Preface Pattern recognition has its origins in engineering, whereas machine learning grew out of computer science.
However, these activities can be viewed as two facets of the same field, and together they have undergone substantial development over the pasttenyears.
Inparticular, Bayesianmethodshavegrownfromaspecialistnicheto becomemainstream, whilegraphicalmodelshaveemergedasageneralframework fordescribingandapplyingprobabilisticmodels.
Also, thepracticalapplicabilityof Bayesianmethodshasbeengreatlyenhancedthroughthedevelopmentofarangeof approximateinferencealgorithmssuchasvariational Bayesandexpectationpropa- gation.
Similarly, newmodelsbasedonkernelshavehadsignificantimpactonboth algorithmsandapplications.
Thisnewtextbookreflectstheserecentdevelopmentswhileprovidingacompre- hensive introduction to the fields of pattern recognition and machine learning.
It is aimedatadvancedundergraduatesorfirstyear Ph Dstudents, aswellasresearchers andpractitioners, andassumesnopreviousknowledgeofpatternrecognitionorma- chinelearningconcepts.
Knowledgeofmultivariatecalculusandbasiclinearalgebra isrequired, andsomefamiliaritywithprobabilitieswouldbehelpfulthoughnotes- sentialasthebookincludesaself-containedintroductiontobasicprobabilitytheory.
Becausethisbookhasbroadscope, itisimpossibletoprovideacompletelistof references, andinparticularnoattempthasbeenmadetoprovideaccuratehistorical attribution of ideas.
Instead, the aim has been to give references that offer greater detailthanispossiblehereandthathopefullyprovideentrypointsintowhat, insome cases, isaveryextensiveliterature.
Forthisreason, thereferencesareoftentomore recenttextbooksandreviewarticlesratherthantooriginalsources.
The book is supported by a great deal of additional material, including lecture slides as well as the complete set of figures used in the book, and the reader is encouragedtovisitthebookwebsiteforthelatestinformation: http://research.
microsoft.
com/∼cmbishop/PRML vii viii PREFACE Exercises The exercises that appear at the end of every chapter form an important com- ponent of the book.
Each exercise has been carefully chosen to reinforce concepts explainedinthetextortodevelopandgeneralizetheminsignificantways, andeach is graded according to difficulty ranging from ( ), which denotes a simple exercise taking a few minutes to complete, through to ( ), which denotes a significantly morecomplexexercise.
It has been difficult to know to what extent these solutions should be made widely available.
Those engaged in self study will find worked solutions very ben- eficial, whereas many course tutors request that solutions be available only via the publisher so that the exercises may be used in class.
In order to try to meet these conflictingrequirements, thoseexercisesthathelpamplifykeypointsinthetext, or thatfillinimportantdetails, havesolutionsthatareavailableasa PDFfilefromthe book web site.
Such exercises are denoted by www.
Solutions for the remaining exercises are available to course tutors by contacting the publisher (contact details are given on the book web site).
Readers are strongly encouraged to work through theexercisesunaided, andtoturntothesolutionsonlyasrequired.
Although this book focuses on concepts and principles, in a taught course the students should ideally have the opportunity to experiment with some of the key algorithms using appropriate data sets.
A companion volume (Bishop and Nabney, 2008) will deal with practical aspects of pattern recognition and machine learning, and will be accompanied by Matlab software implementing most of the algorithms discussedinthisbook.
Acknowledgements First of all I would like to express my sincere thanks to Markus Svense´n who has provided immense help with preparation of figures and with the typesetting of thebookin LATEX.
Hisassistancehasbeeninvaluable.
Iamverygratefulto Microsoft Researchforprovidingahighlystimulatingre- searchenvironmentandforgivingmethefreedomtowritethisbook(theviewsand opinions expressed in this book, however, are my own and are therefore not neces- sarilythesameasthoseof Microsoftoritsaffiliates).
Springer has provided excellent support throughout the final stages of prepara- tionofthisbook, and Iwouldliketothankmycommissioningeditor John Kimmel forhissupportandprofessionalism, aswellas Joseph Pilieroforhishelpindesign- ingthecoverandthetextformatand Mary Ann Bricknerforhernumerouscontribu- tionsduringtheproductionphase.
Theinspirationforthecoverdesigncamefroma discussionwith Antonio Criminisi.
I also wish to thank Oxford University Press for permission to reproduce ex- cerpts from an earlier textbook, Neural Networks for Pattern Recognition (Bishop, 1995a).
The images of the Mark 1 perceptron and of Frank Rosenblatt are repro- ducedwiththepermissionof Arvin Calspan Advanced Technology Center.
Iwould also like to thank Asela Gunawardana for plotting the spectrogram in Figure 13.1, and Bernhard Scho¨lkopf for permission to use his kernel PCA code to plot Fig- ure12.17.
PREFACE ix Many people have helped by proofreading draft material and providing com- mentsandsuggestions, including Shivani Agarwal, Ce´dric Archambeau, Arik Azran, Andrew Blake, Hakan Cevikalp, Michael Fourman, Brendan Frey, Zoubin Ghahra- mani, Thore Graepel, Katherine Heller, Ralf Herbrich, Geoffrey Hinton, Adam Jo- hansen, Matthew Johnson, Michael Jordan, Eva Kalyvianaki, Anitha Kannan, Julia Lasserre, David Liu, Tom Minka, Ian Nabney, Tonatiuh Pena, Yuan Qi, Sam Roweis, Balaji Sanjiya, Toby Sharp, Ana Costae Silva, David Spiegelhalter, Jay Stokes, Tara Symeonides, Martin Szummer, Marshall Tappen, Ilkay Ulusoy, Chris Williams, John Winn, and Andrew Zisserman.
Finally, I would like to thank my wife Jenna who has been hugely supportive throughouttheseveralyearsithastakentowritethisbook.
Chris Bishop Cambridge February2006 Mathematical notation I have tried to keep the mathematical content of the book to the minimum neces- sarytoachieveaproperunderstandingofthefield.
However, thisminimumlevelis nonzero, and it should be emphasized that a good grasp of calculus, linear algebra, andprobabilitytheoryisessentialforaclearunderstandingofmodernpatternrecog- nition and machine learning techniques.
Nevertheless, the emphasis in this book is onconveyingtheunderlyingconceptsratherthanonmathematicalrigour.
I havetried touseaconsistent notationthroughout thebook, although attimes this means departing from some of the conventions used in the corresponding re- search literature.
Vectors are denoted by lower case bold Roman letters such as x, and all vectors are assumed to be column vectors.
A superscript T denotes the transpose of a matrix or vector, so that x T will be a row vector.
Uppercase bold roman letters, such as M, denote matrices.
The notation (w 1 ,..., w M) denotes a row vector with M elements, while the corresponding column vector is written as w =(w 1 ,..., w M)T.
The notation [a, b] is used to denote the closed interval from a to b, that is the intervalincludingthevaluesaandbthemselves, while(a, b)denotesthecorrespond- ingopeninterval, thatistheintervalexcludingaandb.
Similarly, [a, b)denotesan interval that includes a but excludes b.
For the most part, however, there will be littleneedtodwellonsuchrefinementsaswhethertheendpointsofanintervalare includedornot.
The M × M identity matrix (also known as the unit matrix) is denoted IM, whichwillbeabbreviatedto Iwherethereisnoambiguityaboutitdimensionality.
Ithaselements Iij thatequal1ifi=j and0ifi =j.
A functional is denoted f[y] where y(x) is some function.
The concept of a functionalisdiscussedin Appendix D.
Thenotationg(x) = O(f(x))denotesthat|f(x)/g(x)|isboundedasx → ∞.
Forinstanceifg(x)=3x2+2, theng(x)=O(x2).
Theexpectationofafunctionf(x, y)withrespecttoarandomvariablexisde- notedby E x[f(x, y)].
Insituationswherethereisnoambiguityastowhichvariable is being averaged over, this will be simplified by omitting the suffix, for instance xi xii MATHEMATICALNOTATION E[x].
If the distribution of x is conditioned on another variable z, then the corre- spondingconditionalexpectationwillbewritten E x[f(x)|z].
Similarly, thevariance isdenotedvar[f(x)], andforvectorvariablesthecovarianceiswrittencov[x, y].
We shallalsousecov[x]asashorthandnotationforcov[x, x].
Theconceptsofexpecta- tionsandcovariancesareintroducedin Section1.2.2.
Ifwehave N valuesx 1 ,..., x N ofa D-dimensionalvectorx=(x 1 ,..., x D)T, we can combine the observations into a data matrix X in which the nth row of X corresponds to the row vector x T.
Thus the n, i element of X corresponds to the n ith elementofthenth observationxn.
Forthecaseofone-dimensionalvariableswe shalldenotesuchamatrixbyx, whichisacolumnvectorwhosenth elementisxn.
Notethatx(whichhasdimensionality N)usesadifferenttypefacetodistinguishit fromx(whichhasdimensionality D).
Contents Preface vii Mathematicalnotation xi Contents xiii 1 Introduction 1 xiii xiv CONTENTS 2 Probability Distributions 67 3 Linear Modelsfor Regression 137 CONTENTS xv 4 Linear Modelsfor Classification 179 5 Neural Networks 225 xvi CONTENTS 6 Kernel Methods 291 7 Sparse Kernel Machines 325 CONTENTS xvii 8 Graphical Models 359 9 Mixture Modelsand EM 423 10 Approximate Inference 461 xviii CONTENTS 11 Sampling Methods 523 12 Continuous Latent Variables 559 CONTENTS xix 13 Sequential Data 605 14 Combining Models 653 Appendix A Data Sets 677 Appendix B Probability Distributions 685 Appendix C Propertiesof Matrices 695 xx CONTENTS Appendix D Calculusof Variations 703 Appendix E Lagrange Multipliers 707 References 711 Index 729 1 Introduction Theproblemofsearchingforpatternsindataisafundamentaloneandhasalongand successful history.
For instance, the extensive astronomical observations of Tycho Braheinthe16th centuryallowed Johannes Keplertodiscovertheempiricallawsof planetarymotion, whichinturnprovidedaspringboardforthedevelopmentofclas- sical mechanics.
Similarly, the discovery of regularities in atomic spectra played a keyroleinthedevelopmentandverificationofquantumphysicsintheearlytwenti- ethcentury.
Thefieldofpatternrecognitionisconcernedwiththeautomaticdiscov- eryofregularitiesindatathroughtheuseofcomputeralgorithmsandwiththeuseof theseregularitiestotakeactionssuchasclassifyingthedataintodifferentcategories.
Considertheexampleofrecognizinghandwrittendigits, illustratedin Figure1.1.
Eachdigitcorrespondstoa28×28pixelimageandsocanberepresentedbyavector xcomprising784realnumbers.
Thegoalistobuildamachinethatwilltakesucha vectorxasinputandthatwillproducetheidentityofthedigit0,...,9astheoutput.
This is a nontrivial problem due to the wide variability of handwriting.
It could be 1 2 1.
INTRODUCTION Figure1.1 Examplesofhand-writtendig- itstakenfrom USzipcodes.
tackled using handcrafted rules or heuristics for distinguishing the digits based on theshapesofthestrokes, butinpracticesuchanapproachleadstoaproliferationof rulesandofexceptionstotherulesandsoon, andinvariablygivespoorresults.
Far better results can be obtained by adopting a machine learning approach in which a large set of N digits {x 1 ,..., x N } called a training set is used to tune the parameters of an adaptive model.
The categories of the digits in the training set areknowninadvance, typicallybyinspectingthemindividuallyandhand-labelling them.
Wecanexpressthecategoryofadigitusingtargetvectort, whichrepresents the identity of the corresponding digit.
Suitable techniques for representing cate- gories in terms of vectors will be discussed later.
Note that there is one such target vectortforeachdigitimagex.
The result of running the machine learning algorithm can be expressed as a functiony(x)whichtakesanewdigitimagexasinputandthatgeneratesanoutput vector y, encoded in the same way as the target vectors.
The precise form of the function y(x) is determined during the training phase, also known as the learning phase, on the basis of the training data.
Once the model is trained it can then de- terminetheidentityofnewdigitimages, whicharesaidtocompriseatestset.
The ability to categorize correctly new examples that differ from those used for train- ingisknownasgeneralization.
Inpracticalapplications, thevariabilityoftheinput vectors will be such that the training data can comprise only a tiny fraction of all possibleinputvectors, andsogeneralizationisacentralgoalinpatternrecognition.
Formostpracticalapplications, theoriginalinputvariablesaretypicallyprepro- cessed to transform them into some new space of variables where, it is hoped, the patternrecognitionproblemwillbeeasiertosolve.
Forinstance, inthedigitrecogni- tionproblem, theimagesofthedigitsaretypicallytranslatedandscaledsothateach digit is contained within a box of a fixed size.
This greatly reduces the variability within each digit class, because the location and scale of all the digits are now the same, which makes it much easier for a subsequent pattern recognition algorithm todistinguishbetweenthedifferentclasses.
Thispre-processingstageissometimes also called feature extraction.
Note that new test data must be pre-processed using thesamestepsasthetrainingdata.
Pre-processingmightalsobeperformedinordertospeedupcomputation.
For example, if the goal is real-time face detection in a high-resolution video stream, thecomputermusthandlehugenumbersofpixelspersecond, andpresentingthese directlytoacomplexpatternrecognitionalgorithmmaybecomputationallyinfeasi- ble.
Instead, the aim is to find useful features that are fast to compute, and yet that 1.
INTRODUCTION 3 also preserve useful discriminatory information enabling faces to be distinguished fromnon-faces.
Thesefeaturesarethenusedastheinputstothepatternrecognition algorithm.
Forinstance, theaveragevalueoftheimageintensityoverarectangular subregioncanbeevaluatedextremelyefficiently(Violaand Jones,2004), andasetof suchfeaturescanproveveryeffectiveinfastfacedetection.
Becausethenumberof suchfeaturesissmallerthanthenumberofpixels, thiskindofpre-processingrepre- sentsaformofdimensionalityreduction.
Caremustbetakenduringpre-processing because often information is discarded, and if this information is important to the solutionoftheproblemthentheoverallaccuracyofthesystemcansuffer.
Applicationsinwhichthetrainingdatacomprisesexamplesoftheinputvectors alongwiththeircorrespondingtargetvectorsareknownassupervisedlearningprob- lems.
Casessuchasthedigitrecognitionexample, inwhichtheaimistoassigneach inputvectortooneofafinitenumberofdiscretecategories, arecalledclassification problems.
If the desired output consists of one or more continuous variables, then thetaskiscalledregression.
Anexampleofaregressionproblemwouldbethepre- dictionoftheyieldinachemicalmanufacturingprocessinwhichtheinputsconsist oftheconcentrationsofreactants, thetemperature, andthepressure.
Inotherpatternrecognitionproblems, thetrainingdataconsistsofasetofinput vectors x without any corresponding target values.
The goal in such unsupervised learning problems may be to discover groups of similar examples within the data, whereitiscalledclustering, ortodeterminethedistributionofdatawithintheinput space, known as density estimation, or to project the data from a high-dimensional spacedowntotwoorthreedimensionsforthepurposeofvisualization.
Finally, thetechniqueofreinforcementlearning(Suttonand Barto,1998)iscon- cerned with the problem of finding suitable actions to take in a given situation in order to maximize a reward.
Here the learning algorithm is not given examples of optimal outputs, in contrast to supervised learning, but must instead discover them byaprocessoftrialanderror.
Typicallythereisasequenceofstatesandactionsin whichthelearningalgorithmisinteractingwithitsenvironment.
Inmanycases, the currentactionnotonlyaffectstheimmediaterewardbutalsohasanimpactonthere- wardatallsubsequenttimesteps.
Forexample, byusingappropriatereinforcement learningtechniquesaneuralnetworkcanlearntoplaythegameofbackgammontoa highstandard(Tesauro,1994).
Herethenetworkmustlearntotakeaboardposition as input, along with the result of a dice throw, and produce a strong move as the output.
Thisisdonebyhavingthenetworkplayagainstacopyofitselfforperhapsa milliongames.
Amajorchallengeisthatagameofbackgammoncaninvolvedozens of moves, and yet it is only at the end of the game that the reward, in the form of victory, is achieved.
The reward must then be attributed appropriately to all of the moves that led to it, even though some moves will have been good ones and others lessso.
Thisisanexampleofacreditassignmentproblem.
Ageneralfeatureofre- inforcementlearningisthetrade-offbetweenexploration, inwhichthesystemtries out new kinds of actions to see how effective they are, and exploitation, in which the system makes use of actions that are known to yield a high reward.
Too strong a focus on either exploration or exploitation will yield poor results.
Reinforcement learning continues to be an active area of machine learning research.
However, a 4 1.
INTRODUCTION Figure1.2 Plot of a training data set of N = 10 points, shown as blue circles, each comprising an observation 1 of the input variable x along with the corresponding target variable t t.
The green curve shows the function sin(2πx) used to gener- ate the data.
Our goal is to pre- 0 dict the value of t for some new value of x, without knowledge of thegreencurve.
−1 0 1 x detailedtreatmentliesbeyondthescopeofthisbook.
Although each of these tasks needs its own tools and techniques, many of the key ideas that underpin them are common to all such problems.
One of the main goalsofthischapteristointroduce, inarelativelyinformalway, severalofthemost important of these concepts and to illustrate them using simple examples.
Later in the book we shall see these same ideas re-emerge in the context of more sophisti- catedmodelsthatareapplicabletoreal-worldpatternrecognitionapplications.
This chapteralsoprovidesaself-containedintroductiontothreeimportanttoolsthatwill beusedthroughoutthebook, namelyprobabilitytheory, decisiontheory, andinfor- mation theory.
Although these might sound like daunting topics, they are in fact straightforward, and a clear understanding of them is essential if machine learning techniquesaretobeusedtobesteffectinpracticalapplications.
1.1.
Example: Polynomial Curve Fitting Webeginbyintroducingasimpleregressionproblem, whichweshalluseasarun- ning example throughout this chapter to motivate a number of key concepts.
Sup- poseweobserveareal-valuedinputvariablexandwewishtousethisobservationto predictthevalueofareal-valuedtargetvariablet.
Forthepresentpurposes, itisin- structivetoconsideranartificialexampleusingsyntheticallygenerateddatabecause wethenknowthepreciseprocessthatgeneratedthedataforcomparisonagainstany learned model.
The data for this example is generated from the function sin(2πx) withrandomnoiseincludedinthetargetvalues, asdescribedindetailin Appendix A.
Now suppose that we are given a training set comprising N observations of x, writtenx ≡ (x 1 ,..., x N)T, togetherwithcorrespondingobservationsofthevalues N = 10 data points.
The input data set x in Figure 1.2 was generated by choos- ingvaluesofxn, forn = 1,..., N, spaceduniformlyinrange[0,1], andthetarget data set t was obtained by first computing the corresponding values of the function 1.1.
Example: Polynomial Curve Fitting 5 sin(2πx) and then adding a small level of random noise having a Gaussian distri- bution(the Gaussiandistributionisdiscussedin Section1.2.4)toeachsuchpointin order to obtain the corresponding value tn.
By generating data in this way, we are capturingapropertyofmanyrealdatasets, namelythattheypossessanunderlying regularity, whichwewishtolearn, butthatindividualobservationsarecorruptedby randomnoise.
Thisnoisemightarisefromintrinsicallystochastic(i.
e.
random)pro- cessessuchasradioactivedecaybutmoretypicallyisduetotherebeingsourcesof variabilitythatarethemselvesunobserved.
Ourgoalistoexploitthistrainingsetinordertomakepredictionsofthevalue t of the target variable for some new value x of the input variable.
As we shall see later, this involves implicitly trying to discover the underlying function sin(2πx).
This is intrinsically a difficult problem as we have to generalize from a finite data set.
Furthermore the observed data are corrupted with noise, and so for a given x there is uncertainty as to the appropriate value for t.
Probability theory, discussed in Section 1.2, provides a framework for expressing such uncertainty in a precise andquantitativemanner, anddecisiontheory, discussedin Section1.5, allowsusto exploitthisprobabilisticrepresentationinordertomakepredictionsthatareoptimal accordingtoappropriatecriteria.
For the moment, however, we shall proceed rather informally and consider a simple approach based on curve fitting.
In particular, we shall fit the data using a polynomialfunctionoftheform M y(x, w)=w 0 +w 1 x+w 2 x2+...+w Mx M = wjx j (1.1) j=0 where M istheorder ofthepolynomial, andxj denotesxraisedtothepowerofj.
The polynomial coefficients w 0 ,..., w M are collectively denoted by the vector w.
Notethat, althoughthepolynomialfunctiony(x, w)isanonlinearfunctionofx, it isalinearfunctionofthecoefficientsw.
Functions, suchasthepolynomial, which arelinearintheunknownparametershaveimportantpropertiesandarecalledlinear modelsandwillbediscussedextensivelyin Chapters3and4.
Thevaluesofthecoefficientswillbedeterminedbyfittingthepolynomialtothe training data.
This can be done by minimizing an error function that measures the misfit between the function y(x, w), for any given value of w, and the training set datapoints.
Onesimplechoiceoferrorfunction, whichiswidelyused, isgivenby thesumofthesquaresoftheerrorsbetweenthepredictionsy(xn, w)foreachdata pointxn andthecorrespondingtargetvaluestn, sothatweminimize N 1 E(w)= {y(xn, w)−tn }2 (1.2) 2 n=1 wherethefactorof1/2isincludedforlaterconvenience.
Weshalldiscussthemo- tivation for this choice of error function later in this chapter.
For the moment we simply note that it is a nonnegative quantity that would be zero if, and only if, the 6 1.
INTRODUCTION Figure1.3 The error function (1.2) corre- sponds to (one half of) the sum of t tn the squares of the displacements (shown by the vertical green bars) ofeachdatapointfromthefunction y(x, w).
y(xn, w) xn x functiony(x, w)weretopassexactlythrougheachtrainingdatapoint.
Thegeomet- rical interpretation of the sum-of-squares error function is illustrated in Figure 1.3.
We can solve the curve fitting problem by choosing the value of w for which E(w) is as small as possible.
Because the error function is a quadratic function of thecoefficientsw, itsderivativeswithrespecttothecoefficientswillbelinearinthe elementsofw, andsotheminimizationoftheerrorfunctionhasauniquesolution, Exercise 1.1 denoted by w , which can be found in closed form.
The resulting polynomial is givenbythefunctiony(x, w ).
There remains the problem of choosing the order M of the polynomial, and as weshallseethiswillturnouttobeanexampleofanimportantconceptcalledmodel comparisonormodelselection.
In Figure1.4, weshowfourexamplesoftheresults of fitting polynomials having orders M = 0,1,3, and 9 to the data set shown in Figure1.2.
We notice that the constant (M = 0) and first order (M = 1) polynomials give rather poor fits to the data and consequently rather poor representations of the function sin(2πx).
The third order (M = 3) polynomial seems to give the best fit to the function sin(2πx) of the examples shown in Figure 1.4.
When we go to a much higher order polynomial (M = 9), we obtain an excellent fit to the training data.
Infact, thepolynomialpassesexactlythrougheachdatapointand E(w )=0.
However, the fitted curve oscillates wildly and gives a very poor representation of thefunctionsin(2πx).
Thislatterbehaviourisknownasover-fitting.
Aswehavenotedearlier, thegoalistoachievegoodgeneralizationbymaking accuratepredictions for newdata.
Wecanobtainsomequantitative insightinto the dependence of the generalization performance on M by considering a separate test set comprising 100 data points generated using exactly the same procedure used togeneratethetrainingsetpointsbutwithnewchoicesfortherandomnoisevalues includedinthetargetvalues.
Foreachchoiceof M, wecanthenevaluatetheresidual valueof E(w )givenby(1.2)forthetrainingdata, andwecanalsoevaluate E(w ) for the test data set.
It is sometimes more convenient to use the root-mean-square 1.1.
Example: Polynomial Curve Fitting 7 1 M =0 1 M =1 t t 0 0 −1 −1 0 1 0 1 x x 1 M =3 1 M =9 t t 0 0 −1 −1 0 1 0 1 x x Figure1.4 Plotsofpolynomialshavingvariousorders M, shownasredcurves, fittedtothedatasetshownin Figure1.2.
(RMS)errordefinedby E RMS = 2E(w )/N (1.3) in which the division by N allows us to compare different sizes of data sets on an equal footing, and the square root ensures that E RMS is measured on the same scale (and in the same units) as the target variable t.
Graphs of the training and test set RMS errors are shown, for various values of M, in Figure 1.5.
The test set error is a measure of how well we are doing in predicting the values of t for new data observations of x.
We note from Figure 1.5 that small values of M give relativelylargevaluesofthetestseterror, andthiscanbeattributedtothefactthat the corresponding polynomials are rather inflexible and are incapable of capturing the oscillations in the function sin(2πx).
Values of M in the range 3 M 8 givesmallvaluesforthetestseterror, andthesealsogivereasonablerepresentations of the generating function sin(2πx), as can be seen, for the case of M = 3, from Figure1.4.
8 1.
INTRODUCTION Figure1.5 Graphs of the root-mean-square error, defined by (1.3), evaluated onthetrainingsetandonaninde- pendenttestsetforvariousvalues of M.
M SMRE 1 Training Test 0.5 0 0 3 6 9 For M = 9, the training set error goes to zero, as we might expect because thispolynomialcontains10degreesoffreedomcorrespondingtothe10coefficients w 0 ,..., w 9, and so can be tuned exactly to the 10 data points in the training set.
However, thetestseterrorhasbecomeverylargeand, aswesawin Figure1.4, the correspondingfunctiony(x, w )exhibitswildoscillations.
This may seem paradoxical because a polynomial of given order contains all lowerorderpolynomialsasspecialcases.
The M =9polynomialisthereforecapa- bleofgeneratingresultsatleastasgoodasthe M =3polynomial.
Furthermore, we might suppose that the best predictor of new data would be the function sin(2πx) from which the data was generated (and we shall see later that this is indeed the case).
We know that a power series expansion of the function sin(2πx) contains termsofallorders, sowemightexpectthatresultsshouldimprovemonotonicallyas weincrease M.
We can gain some insight into the problem by examining the values of the co- efficients w obtained from polynomials of various order, as shown in Table 1.1.
Weseethat, as M increases, themagnitudeofthecoefficientstypicallygetslarger.
In particular for the M = 9 polynomial, the coefficients have become finely tuned tothedatabydevelopinglargepositiveandnegativevaluessothatthecorrespond- Table1.1 Table of the coefficients w for M =0 M =1 M =6 M =9 0 Observe how the typical mag- w -1.27 7.99 232.37 nitude of the coefficients in- 1 w -25.43 -5321.83 creases dramatically as the or- 2 derofthepolynomialincreases.
w 3 17.37 48568.31 w -231639.30 4 w 640042.26 5 w -1061800.52 6 w 1042400.18 7 w -557682.99 8 w 125201.43 9 1.1.
Example: Polynomial Curve Fitting 9 1 N =15 1 N =100 t t 0 0 −1 −1 0 1 0 1 x x Figure 1.6 Plots of the solutions obtained by minimizing the sum-of-squares error function using the M = 9 polynomial for N = 15 data points (left plot) and N = 100 data points (right plot).
We see that increasing the sizeofthedatasetreducestheover-fittingproblem.
ing polynomial function matches each of the data points exactly, but between data points(particularlyneartheendsoftherange)thefunctionexhibitsthelargeoscilla- tionsobservedin Figure1.4.
Intuitively, whatishappeningisthatthemoreflexible polynomialswithlargervaluesof M arebecomingincreasinglytunedtotherandom noiseonthetargetvalues.
Itisalsointerestingtoexaminethebehaviourofagivenmodelasthesizeofthe datasetisvaried, asshownin Figure1.6.
Weseethat, foragivenmodelcomplexity, the over-fitting problem become less severe as the size of the data set increases.
Another way to say this is that the larger the data set, the more complex (in other words more flexible) the model that we can afford to fit to the data.
One rough heuristic that is sometimes advocated is that the number of data points should be no less than some multiple (say 5 or 10) of the number of adaptive parameters in the model.
However, as we shall see in Chapter 3, the number of parameters is not necessarilythemostappropriatemeasureofmodelcomplexity.
Also, thereissomethingratherunsatisfyingabouthavingtolimitthenumberof parameters in a model according to the size of the available training set.
It would seemmorereasonabletochoosethecomplexityofthemodelaccordingtothecom- plexity of the problem being solved.
We shall see that the least squares approach to finding the model parameters represents a specific case of maximum likelihood (discussed in Section 1.2.5), and that the over-fitting problem can be understood as Section3.4 a general property of maximum likelihood.
By adopting a Bayesian approach, the over-fitting problem can be avoided.
We shall see that there is no difficulty from a Bayesian perspective in employing models for which the number of parameters greatlyexceedsthenumberofdatapoints.
Indeed, ina Bayesianmodeltheeffective numberofparametersadaptsautomaticallytothesizeofthedataset.
Forthemoment, however, itisinstructivetocontinuewiththecurrentapproach andtoconsiderhowinpracticewecanapplyittodatasetsoflimitedsizewherewe 10 1.
INTRODUCTION 1 lnλ=−18 1 lnλ=0 t t 0 0 −1 −1 0 1 0 1 x x Figure 1.7 Plots of M = 9 polynomials fitted to the data set shown in Figure 1.2 using the regularized error function(1.4)fortwovaluesoftheregularizationparameterλcorrespondingtolnλ = −18andlnλ = 0.
The caseofnoregularizer, i.
e.,λ=0, correspondingtolnλ=−∞, isshownatthebottomrightof Figure1.4.
maywishtouserelativelycomplexandflexiblemodels.
Onetechniquethatisoften used to control the over-fitting phenomenon in such cases is that of regularization, whichinvolvesaddingapenaltytermtotheerrorfunction(1.2)inordertodiscourage thecoefficientsfromreachinglargevalues.
Thesimplestsuchpenaltytermtakesthe formofasumofsquaresofallofthecoefficients, leadingtoamodifiederrorfunction oftheform N E (w)= 1 {y(xn, w)−tn }2 + λ w 2 (1.4) 2 2 n=1 where w 2 ≡w Tw =w2+w2+...+w2 , andthecoefficientλgovernstherel- 0 1 M ativeimportanceoftheregularizationtermcomparedwiththesum-of-squareserror term.
Note that often the coefficient w 0 is omitted from the regularizer because its inclusioncausestheresultstodependonthechoiceoforiginforthetargetvariable (Hastieetal.,2001), oritmaybeincludedbutwithitsownregularizationcoefficient (weshalldiscussthistopicinmoredetailin Section5.5.1).
Again, theerrorfunction Exercise 1.2 in(1.4)canbeminimizedexactlyinclosedform.
Techniquessuchasthisareknown inthestatisticsliteratureasshrinkagemethodsbecausetheyreducethevalueofthe coefficients.
The particular case of a quadratic regularizer is called ridge regres- sion(Hoerland Kennard,1970).
Inthecontextofneuralnetworks, thisapproachis knownasweightdecay.
Figure 1.7 shows the results of fitting the polynomial of order M = 9 to the samedatasetasbeforebutnowusingtheregularizederrorfunctiongivenby(1.4).
Weseethat, foravalueoflnλ = −18, theover-fittinghasbeensuppressedandwe now obtain a much closer representation of the underlying function sin(2πx).
If, however, weusetoolargeavalueforλthenweagainobtainapoorfit, asshownin Figure1.7forlnλ = 0.
Thecorrespondingcoefficientsfromthefittedpolynomials aregivenin Table1.2, showingthatregularizationhasthedesiredeffectofreducing 1.1.
Example: Polynomial Curve Fitting 11 Table1.2 Tableofthecoefficientsw for M = lnλ=−∞ lnλ=−18 lnλ=0 9polynomialswithvariousvaluesfor w 0.35 0.35 0.13 theregularizationparameterλ.
Note w 0 232.37 4.74 -0.05 that lnλ = −∞ corresponds to a 1 w -5321.83 -0.77 -0.06 modelwithnoregularization, i.
e., to 2 thegraphatthebottomrightin Fig- w 3 48568.31 -31.97 -0.05 4 λincreases, thetypicalmagnitudeof w 640042.26 55.28 -0.02 5 thecoefficientsgetssmaller.
w -1061800.52 41.32 -0.01 6 w 1042400.18 -45.95 -0.00 7 w -557682.99 -91.53 0.00 8 w 125201.43 72.68 0.01 9 themagnitudeofthecoefficients.
Theimpactoftheregularizationtermonthegeneralizationerrorcanbeseenby plottingthevalueofthe RMSerror(1.3)forbothtrainingandtestsetsagainstlnλ, asshownin Figure1.8.
Weseethatineffectλnowcontrolstheeffectivecomplexity ofthemodelandhencedeterminesthedegreeofover-fitting.
The issue of model complexity is an important one and will be discussed at lengthin Section1.3.
Herewesimplynotethat, ifweweretryingtosolveapractical application using this approach of minimizing an error function, we would have to findawaytodetermineasuitablevalueforthemodelcomplexity.
Theresultsabove suggest a simple way of achieving this, namely by taking the available data and partitioningitintoatrainingset, usedtodeterminethecoefficientsw, andaseparate validation set, also called a hold-out set, used to optimize the model complexity (either M or λ).
In many cases, however, this will prove to be too wasteful of Section1.3 valuabletrainingdata, andwehavetoseekmoresophisticatedapproaches.
So far our discussion of polynomial curve fitting has appealed largely to in- tuition.
We now seek a more principled approach to solving problems in pattern recognitionbyturningtoadiscussionofprobabilitytheory.
Aswellasprovidingthe foundation for nearly all of the subsequent developments in this book, it will also Figure1.8 Graphoftheroot-mean-squareer- ror(1.3)versuslnλforthe M = 9 polynomial.
SMRE 1 Training Test 0.5 0 −35 −30 −25 −20 lnλ 12 1.
INTRODUCTION give us some important insights into the concepts we have introduced in the con- text of polynomial curve fitting and will allow us to extend these to more complex situations.
1.2.
Probability Theory Akeyconceptinthefieldofpatternrecognitionisthatofuncertainty.
Itarisesboth throughnoiseonmeasurements, aswellasthroughthefinitesizeofdatasets.
Prob- abilitytheoryprovidesaconsistentframeworkforthequantificationandmanipula- tionofuncertaintyandformsoneofthecentralfoundationsforpatternrecognition.
Whencombinedwithdecisiontheory, discussedin Section1.5, itallowsustomake optimalpredictionsgivenalltheinformationavailabletous, eventhoughthatinfor- mationmaybeincompleteorambiguous.
Wewillintroducethebasicconceptsofprobabilitytheorybyconsideringasim- pleexample.
Imaginewehavetwoboxes, oneredandoneblue, andintheredbox wehave2applesand6oranges, andintheblueboxwehave3applesand1orange.
This is illustrated in Figure 1.9.
Now suppose we randomly pick one of the boxes and from that box we randomly select an item of fruit, and having observed which sort of fruit it is we replace it in the box from which it came.
We could imagine repeating this process many times.
Let us suppose that in so doing we pick the red box 40% of the time and we pick the blue box 60% of the time, and that when we removeanitemoffruitfromaboxweareequallylikelytoselectanyofthepieces offruitinthebox.
Inthisexample, theidentityoftheboxthatwillbechosenisarandomvariable, which we shall denote by B.
This random variable can take one of two possible values, namely r (corresponding to the red box) or b (corresponding to the blue box).
Similarly, theidentityofthefruitisalsoarandomvariableandwillbedenoted by F.
Itcantakeeitherofthevaluesa(forapple)oro(fororange).
To begin with, we shall define the probability of an event to be the fraction oftimesthateventoccursoutofthetotalnumberoftrials, inthelimitthatthetotal numberoftrialsgoestoinfinity.
Thustheprobabilityofselectingtheredboxis4/10 Figure1.9 We use a simple example of two colouredboxeseachcontainingfruit (apples shown in green and or- anges shown in orange) to intro- ducethebasicideasofprobability.
1.2.
Probability Theory 13 Figure1.10 Wecanderivethesumandproductrulesofprobabilityby consideringtworandomvariables, X, whichtakesthevalues{x i }where In this illustration we have M = 5 and L = 3.
If we consider a total number N of instances of these variables, then we denote the number } ofinstanceswhere X = x i and Y = y j byn ij, whichisthenumberof points in the corresponding cell of the array.
The number of points in columni, correspondingto X =x i, isdenotedbyc i, andthenumberof pointsinrowj, correspondingto Y =y j, isdenotedbyr j.
} ci yj nij rj xi and the probability of selecting the blue box is 6/10.
We write these probabilities asp(B = r) = 4/10andp(B = b) = 6/10.
Notethat, bydefinition, probabilities must lie in the interval [0,1].
Also, if the events are mutually exclusive and if they include all possible outcomes (for instance, in this example the box must be either redorblue), thenweseethattheprobabilitiesforthoseeventsmustsumtoone.
Wecannowaskquestionssuchas: “whatistheoverallprobabilitythatthese- lection procedure will pick an apple?”, or “given that we have chosen an orange, what is the probability that the box we chose was the blue one?”.
We can answer questions such as these, and indeed much more complex questions associated with problems in pattern recognition, once we have equipped ourselves with the two el- ementary rules of probability, known as the sum rule and the product rule.
Having obtainedtheserules, weshallthenreturntoourboxesoffruitexample.
Inordertoderivetherulesofprobability, considertheslightlymoregeneralex- ampleshownin Figure1.10involvingtworandomvariables X and Y (whichcould forinstancebethe Boxand Fruitvariablesconsideredabove).
Weshallsupposethat X cantakeanyofthevaluesxi wherei = 1,..., M, and Y cantakethevaluesyj where j = 1,..., L.
Consider a total of N trials in which we sample both of the variables X and Y, andletthenumberofsuchtrialsinwhich X = xi and Y = yj be nij.
Also, let the number of trials in which X takes the value xi (irrespective ofthevaluethat Y takes)bedenotedbyci, andsimilarlyletthenumberoftrialsin which Y takesthevalueyj bedenotedbyrj.
The probability that X will take the value xi and Y will take the value yj is written p(X = xi, Y = yj) and is called the joint probability of X = xi and Y = yj.
Itisgivenbythenumberofpointsfallinginthecelli, j asafractionofthe totalnumberofpoints, andhence nij p(X =xi, Y =yj)= .
(1.5) N Hereweareimplicitlyconsideringthelimit N →∞.
Similarly, theprobabilitythat X takes the value xi irrespective of the value of Y is written as p(X = xi) and is givenbythefractionofthetotalnumberofpointsthatfallincolumni, sothat ci p(X =xi)= .
(1.6) N Because the number of instances in column i in Figure 1.10 is just the sum of the numberofinstancesineachcellofthatcolumn, wehaveci = j nij andtherefore, 14 1.
INTRODUCTION from(1.5)and(1.6), wehave L p(X =xi)= p(X =xi, Y =yj) (1.7) j=1 which is the sum rule of probability.
Note that p(X = xi) is sometimes called the marginal probability, because it is obtained by marginalizing, or summing out, the othervariables(inthiscase Y).
If we consider only those instances for which X = xi, then the fraction of such instances for which Y = yj is written p(Y = yj |X = xi) and is called the conditional probability of Y = yj given X = xi.
It is obtained by finding the fractionofthosepointsincolumnithatfallincelli, j andhenceisgivenby p(Y =yj |X =xi)= nij .
(1.8) ci From(1.5),(1.6), and(1.8), wecanthenderivethefollowingrelationship p(X =xi, Y =yj) = nij = nij · ci N ci N = p(Y =yj |X =xi)p(X =xi) (1.9) whichistheproductruleofprobability.
Sofarwehavebeenquitecarefultomakeadistinctionbetweenarandomvari- able, suchasthebox B inthefruitexample, andthevaluesthattherandomvariable cantake, forexampleriftheboxweretheredone.
Thustheprobabilitythat Btakes the value r is denoted p(B = r).
Although this helps to avoid ambiguity, it leads to a rather cumbersome notation, and in many cases there will be no need for such pedantry.
Instead, we may simply writep(B) to denote a distribution over the ran- domvariable B, orp(r)todenotethedistributionevaluatedfortheparticularvalue r, providedthattheinterpretationisclearfromthecontext.
With this more compact notation, we can write the two fundamental rules of probabilitytheoryinthefollowingform.
The Rulesof Probability sumrule p(X)= p(X, Y) (1.10) Y productrule p(X, Y)=p(Y|X)p(X).
(1.11) Here p(X, Y) is a joint probability and is verbalized as “the probability of X and Y”.
Similarly, thequantityp(Y|X)isaconditionalprobabilityandisverbalizedas “theprobabilityof Y given X”, whereasthequantityp(X)isamarginalprobability 1.2.
Probability Theory 15 andissimply“theprobabilityof X”.
Thesetwosimplerulesformthebasisforall oftheprobabilisticmachinerythatweusethroughoutthisbook.
Fromtheproductrule, togetherwiththesymmetrypropertyp(X, Y)=p(Y, X), weimmediatelyobtainthefollowingrelationshipbetweenconditionalprobabilities p(X|Y)p(Y) p(Y|X)= (1.12) p(X) whichiscalled Bayes’theoremandwhichplaysacentralroleinpatternrecognition and machine learning.
Using the sum rule, the denominator in Bayes’ theorem can beexpressedintermsofthequantitiesappearinginthenumerator p(X)= p(X|Y)p(Y).
(1.13) Y Wecanviewthedenominatorin Bayes’theoremasbeingthenormalizationconstant requiredtoensurethatthesumoftheconditionalprobabilityontheleft-handsideof (1.12)overallvaluesof Y equalsone.
In Figure1.11, weshowasimpleexampleinvolvingajointdistributionovertwo variables to illustrate the concept of marginal and conditional distributions.
Here a finite sample of N = 60 data points has been drawn from the joint distribution and is shown in the top left.
In the top right is a histogram of the fractions of data pointshavingeachofthetwovaluesof Y.
Fromthedefinitionofprobability, these fractionswouldequalthecorrespondingprobabilitiesp(Y)inthelimit N →∞.
We canviewthehistogramasasimplewaytomodelaprobabilitydistributiongivenonly afinitenumberofpointsdrawnfromthatdistribution.
Modellingdistributionsfrom data lies at the heart of statistical pattern recognition and will be explored in great detailinthisbook.
Theremainingtwoplotsin Figure1.11showthecorresponding histogramestimatesofp(X)andp(X|Y =1).
Letusnowreturntoourexampleinvolvingboxesoffruit.
Forthemoment, we shall once again be explicit about distinguishing between the random variables and theirinstantiations.
Wehaveseenthattheprobabilitiesofselectingeithertheredor theblueboxesaregivenby p(B =r) = 4/10 (1.14) p(B =b) = 6/10 (1.15) respectively.
Notethatthesesatisfyp(B =r)+p(B =b)=1.
Nowsupposethatwepickaboxatrandom, anditturnsouttobethebluebox.
Then the probability of selecting an apple is just the fraction of apples in the blue boxwhichis3/4, andsop(F = a|B = b) = 3/4.
Infact, wecanwriteoutallfour conditionalprobabilitiesforthetypeoffruit, giventheselectedbox p(F =a|B =r) = 1/4 (1.16) p(F =o|B =r) = 3/4 (1.17) p(F =a|B =b) = 3/4 (1.18) p(F =o|B =b) = 1/4.
(1.19) 16 1.
INTRODUCTION p(X, Y) p(Y) Y =2 Y =1 X p(X) p(X|Y =1) X X Figure1.11 Anillustrationofadistributionovertwovariables, X, whichtakes9possiblevalues, and Y, which takestwopossiblevalues.
Thetopleftfigureshowsasampleof60pointsdrawnfromajointprobabilitydistri- butionoverthesevariables.
Theremainingfiguresshowhistogramestimatesofthemarginaldistributionsp(X) and p(Y), as well as the conditional distribution p(X|Y = 1) corresponding to the bottom row in the top left figure.
Again, notethattheseprobabilitiesarenormalizedsothat p(F =a|B =r)+p(F =o|B =r)=1 (1.20) andsimilarly p(F =a|B =b)+p(F =o|B =b)=1.
(1.21) Wecannowusethesumandproductrulesofprobabilitytoevaluatetheoverall probabilityofchoosinganapple p(F =a) = p(F =a|B =r)p(B =r)+p(F =a|B =b)p(B =b) 1 4 3 6 11 = × + × = (1.22) 4 10 4 10 20 fromwhichitfollows, usingthesumrule, thatp(F =o)=1−11/20=9/20.
1.2.
Probability Theory 17 Suppose instead we are told that a piece of fruit has been selected and it is an orange, and we would like to know which box it came from.
This requires that we evaluate the probability distribution over boxes conditioned on the identity of thefruit, whereastheprobabilitiesin(1.16)–(1.19)givetheprobability distribution over the fruit conditioned on the identity of the box.
We can solve the problem of reversingtheconditionalprobabilitybyusing Bayes’theoremtogive p(F =o|B =r)p(B =r) 3 4 20 2 p(B =r|F =o)= = × × = .
(1.23) p(F =o) 4 10 9 3 Fromthesumrule, itthenfollowsthatp(B =b|F =o)=1−2/3=1/3.
We can provide an important interpretation of Bayes’ theorem as follows.
If we had been asked which box had been chosen before being told the identity of the selected item of fruit, then the most complete information we have available is providedbytheprobabilityp(B).
Wecallthisthepriorprobabilitybecauseitisthe probabilityavailablebeforeweobservetheidentityofthefruit.
Oncewearetoldthat the fruit is an orange, we can then use Bayes’ theorem to compute the probability p(B|F), which we shall call the posterior probability because it is the probability obtainedafter wehaveobserved F.
Notethatinthisexample, thepriorprobability ofselectingtheredboxwas4/10, sothatweweremorelikelytoselectthebluebox thantheredone.
However, oncewehaveobservedthatthepieceofselectedfruitis an orange, we find that the posterior probability of the red box is now 2/3, so that it is now more likely that the box we selected was in fact the red one.
This result accordswithourintuition, astheproportionoforangesismuchhigherintheredbox thanitisinthebluebox, andsotheobservationthatthefruitwasanorangeprovides significantevidencefavouringtheredbox.
Infact, theevidenceissufficientlystrong that it outweighs the prior and makes it more likely that the red box was chosen ratherthantheblueone.
Finally, wenotethatifthejointdistributionoftwovariablesfactorizesintothe product of the marginals, so that p(X, Y) = p(X)p(Y), then X and Y are said to be independent.
From the product rule, we see that p(Y|X) = p(Y), and so the conditionaldistributionof Y given X isindeedindependentofthevalueof X.
For instance, in our boxes of fruit example, if each box contained the same fraction of applesandoranges, thenp(F|B) = P(F), sothattheprobabilityofselecting, say, anappleisindependentofwhichboxischosen.
1.2.1 Probability densities As well as considering probabilities defined over discrete sets of events, we also wish to consider probabilities with respect to continuous variables.
We shall limitourselvestoarelativelyinformaldiscussion.
Iftheprobabilityofareal-valued variable x falling in the interval (x, x+δx) is given by p(x)δx for δx → 0, then p(x)iscalledtheprobabilitydensityoverx.
Thisisillustratedin Figure1.12.
The probabilitythatxwilllieinaninterval(a, b)isthengivenby b p(x∈(a, b))= p(x)dx.
(1.24) a 18 1.
INTRODUCTION Figure1.12 The concept of probability for discrete variables can be ex- P(x) p(x) tended to that of a probability density p(x) over a continuous variable x and is such that the probabilityofxlyingintheinter- val(x, x+δx)isgivenbyp(x)δx for δx → 0.
The probability densitycanbeexpressedasthe derivativeofacumulativedistri- butionfunction P(x).
δx x Because probabilities are nonnegative, and because the value of x must lie some- whereontherealaxis, theprobabilitydensityp(x)mustsatisfythetwoconditions p(x) 0 (1.25) ∞ p(x)dx = 1.
(1.26) −∞ Under a nonlinear change of variable, a probability density transforms differently from a simple function, due to the Jacobian factor.
For instance, if we consider a change of variables x = g(y), then a function f(x) becomes f(y) = f(g(y)).
Now consider a probability density px(x) that corresponds to a density py(y) with respecttothenewvariabley, wherethesufficesdenotethefactthatpx(x)andpy(y) aredifferentdensities.
Observationsfallingintherange(x, x+δx)will, forsmall valuesofδx, betransformedintotherange(y, y+δy)wherepx(x)δx py(y)δy, andhence dx py(y) = px(x) dy = px(g(y))|g (y)|.
(1.27) Oneconsequenceofthispropertyisthattheconceptofthemaximumofaprobability Exercise 1.4 densityisdependentonthechoiceofvariable.
The probability that x lies in the interval (−∞, z) is given by the cumulative distributionfunctiondefinedby z P(z)= p(x)dx (1.28) −∞ whichsatisfies P (x)=p(x), asshownin Figure1.12.
Ifwehaveseveralcontinuousvariablesx 1 ,..., x D, denotedcollectivelybythe vectorx, thenwecandefineajointprobabilitydensityp(x) = p(x 1 ,..., x D)such 1.2.
Probability Theory 19 thattheprobabilityofxfallinginaninfinitesimalvolumeδxcontainingthepointx isgivenbyp(x)δx.
Thismultivariateprobabilitydensitymustsatisfy p(x) 0 (1.29) p(x)dx = 1 (1.30) inwhichtheintegralistakenoverthewholeofxspace.
Wecanalsoconsiderjoint probabilitydistributionsoveracombinationofdiscreteandcontinuousvariables.
Notethatifxisadiscretevariable, thenp(x)issometimescalledaprobability massfunctionbecauseitcanberegardedasasetof‘probabilitymasses’concentrated attheallowedvaluesofx.
The sum and product rules of probability, as well as Bayes’ theorem, apply equally to the case of probability densities, or to combinations of discrete and con- tinuous variables.
For instance, if x and y are two real variables, then the sum and productrulestaketheform p(x) = p(x, y)dy (1.31) p(x, y) = p(y|x)p(x).
(1.32) Aformaljustificationofthesumandproductrulesforcontinuousvariables(Feller, 1966) requires a branch of mathematics called measure theory and lies outside the scope of this book.
Its validity can be seen informally, however, by dividing each real variable into intervals of width ∆ and considering the discrete probability dis- tributionovertheseintervals.
Takingthelimit∆→0thenturnssumsintointegrals andgivesthedesiredresult.
1.2.2 Expectations and covariances One of the most important operations involving probabilities is that of finding weighted averages of functions.
The average value of some function f(x) under a probabilitydistributionp(x)iscalledtheexpectationoff(x)andwillbedenotedby E[f].
Foradiscretedistribution, itisgivenby E[f]= p(x)f(x) (1.33) x so that the average is weighted by the relative probabilities of the different values ofx.
Inthe caseof continuous variables, expectations areexpressed interms of an integrationwithrespecttothecorrespondingprobabilitydensity E[f]= p(x)f(x)dx.
(1.34) Ineithercase, ifwearegivenafinitenumber N ofpointsdrawnfromtheprobability distribution or probability density, then the expectation can be approximated as a 20 1.
INTRODUCTION finitesumoverthesepoints N 1 E[f] f(xn).
(1.35) N n=1 We shall make extensive use of this result when we discuss sampling methods in Chapter11.
Theapproximationin(1.35)becomesexactinthelimit N →∞.
Sometimeswewillbeconsideringexpectationsoffunctionsofseveralvariables, in which case we can use a subscript to indicate which variable is being averaged over, sothatforinstance E x[f(x, y)] (1.36) denotestheaverageofthefunctionf(x, y)withrespecttothedistributionofx.
Note that E x[f(x, y)]willbeafunctionofy.
We can also consider a conditional expectation with respect to a conditional distribution, sothat E x[f|y]= p(x|y)f(x) (1.37) x withananalogousdefinitionforcontinuousvariables.
Thevarianceoff(x)isdefinedby var[f]=E (f(x)−E[f(x)]) 2 (1.38) and provides a measure of how much variability there is in f(x) around its mean value E[f(x)].
Expandingoutthesquare, weseethatthevariancecanalsobewritten Exercise 1.5 intermsoftheexpectationsoff(x)andf(x)2 var[f]=E[f(x)2]−E[f(x)]2.
(1.39) Inparticular, wecanconsiderthevarianceofthevariablexitself, whichisgivenby var[x]=E[x2]−E[x]2.
(1.40) Fortworandomvariablesxandy, thecovarianceisdefinedby cov[x, y] = E x, y[{x−E[x]}{y−E[y]}] = E x, y[xy]−E[x]E[y] (1.41) which expresses the extent to which xand y vary together.
If xand y are indepen- Exercise 1.6 dent, thentheircovariancevanishes.
Inthecaseoftwovectorsofrandomvariablesxandy, thecovarianceisamatrix cov[x, y] = E x, y {x−E[x]}{y T−E[y T]} = E x, y [xy T]−E[x]E[y T].
(1.42) Ifweconsiderthecovarianceofthecomponentsofavectorxwitheachother, then weuseaslightlysimplernotationcov[x]≡cov[x, x].
1.2.
Probability Theory 21 1.2.3 Bayesian probabilities So far in this chapter, we have viewed probabilities in terms of the frequencies of random, repeatable events.
We shall refer to this as the classical or frequentist interpretation of probability.
Now we turn to the more general Bayesian view, in whichprobabilitiesprovideaquantificationofuncertainty.
Consideranuncertainevent, forexamplewhetherthemoonwasonceinitsown orbitaroundthesun, orwhetherthe Arcticicecapwillhavedisappearedbytheend of the century.
These are not events that can be repeated numerous times in order to define a notion of probability as we did earlier in the context of boxes of fruit.
Nevertheless, we will generally have some idea, for example, of how quickly we think thepolar ice ismelting.
If wenow obtain fresh evidence, for instance from a new Earthobservationsatellitegatheringnovelformsofdiagnosticinformation, we mayreviseouropinionontherateoficeloss.
Ourassessmentofsuchmatterswill affect the actions we take, for instance the extent to which we endeavour to reduce theemissionofgreenhousegasses.
Insuchcircumstances, wewouldliketobeable toquantifyourexpressionofuncertaintyandmakepreciserevisionsofuncertaintyin thelightofnewevidence, aswellassubsequentlytobeabletotakeoptimalactions ordecisionsasaconsequence.
Thiscanallbeachievedthroughtheelegant, andvery general, Bayesianinterpretationofprobability.
Theuseofprobabilitytorepresentuncertainty, however, isnotanad-hocchoice, butisinevitableif wearetorespectcommonsensewhilemakingrationalcoherent inferences.
For instance, Cox (1946) showed that if numerical values are used to represent degrees of belief, then a simple set of axioms encoding common sense propertiesofsuchbeliefsleadsuniquelytoasetofrulesformanipulatingdegreesof beliefthatareequivalenttothesumandproductrulesofprobability.
Thisprovided the first rigorous proof that probability theory could be regarded as an extension of Boolean logic to situations involving uncertainty (Jaynes, 2003).
Numerous other authors have proposed different sets of properties or axioms that such measures of uncertainty should satisfy (Ramsey, 1931; Good, 1950; Savage, 1961; de Finetti, 1970; Lindley, 1982).
In each case, the resulting numerical quantities behave pre- cisely according to the rules of probability.
It is therefore natural to refer to these quantitiesas(Bayesian)probabilities.
In the field of pattern recognition, too, it is helpful to have a more general no- Thomas Bayes gamblingandwiththenewconceptofinsurance.
One 1701–1761 particularlyimportantproblemconcernedso-calledin- verseprobability.
Asolutionwasproposedby Thomas Thomas Bayes was born in Tun- Bayes in his paper ‘Essay towards solving a problem bridge Wells and was a clergyman in the doctrine of chances’, which was published in aswellasanamateurscientistand 1764, some three years after his death, in the Philo- a mathematician.
He studied logic sophical Transactionsofthe Royal Society.
In fact, and theology at Edinburgh Univer- Bayesonlyformulatedhistheoryforthecaseofauni- sity and was elected Fellow of the formprior, anditwas Pierre-Simon Laplacewhoinde- Royal Society in 1742.
During the 18th century, is- pendentlyrediscoveredthetheoryingeneralformand sues regarding probability arose in connection with whodemonstrateditsbroadapplicability.
22 1.
INTRODUCTION tion of probability.
Consider the example of polynomial curve fitting discussed in Section1.1.
Itseemsreasonabletoapplythefrequentistnotionofprobabilitytothe randomvaluesoftheobservedvariablestn.
However, wewouldliketoaddressand quantifytheuncertaintythatsurroundstheappropriatechoiceforthemodelparam- eters w.
We shall see that, from a Bayesian perspective, we can use the machinery ofprobabilitytheorytodescribetheuncertaintyinmodelparameterssuchasw, or indeedinthechoiceofmodelitself.
Bayes’theoremnowacquiresanewsignificance.
Recallthatintheboxesoffruit example, the observation of the identity of the fruit provided relevant information that altered the probability that the chosen box was the red one.
In that example, Bayes’ theorem was used to convert a prior probability into a posterior probability by incorporating the evidence provided by the observed data.
As we shall see in detaillater, wecanadoptasimilarapproachwhenmakinginferencesaboutquantities such as the parameters w in the polynomial curve fitting example.
We capture our assumptions about w, before observing the data, in the form of a prior probability distribution p(w).
The effect of the observed data D = {t 1 ,..., t N } is expressed throughtheconditionalprobabilityp(D|w), andweshallseelater, in Section1.2.5, howthiscanberepresentedexplicitly.
Bayes’theorem, whichtakestheform p(D|w)p(w) p(w|D)= (1.43) p(D) thenallowsustoevaluatetheuncertaintyinwafterwehaveobserved Dintheform oftheposteriorprobabilityp(w|D).
Thequantityp(D|w)ontheright-handsideof Bayes’theoremisevaluatedfor the observed data set D and can be viewed as a function of the parameter vector w, in which case it is called the likelihood function.
It expresses how probable the observed data set is for different settings of the parameter vector w.
Note that the likelihoodisnotaprobabilitydistributionoverw, anditsintegralwithrespecttow doesnot(necessarily)equalone.
Giventhisdefinitionoflikelihood, wecanstate Bayes’theoreminwords posterior∝likelihood×prior (1.44) where all of these quantities are viewed as functions of w.
The denominator in (1.43) is the normalization constant, which ensures that the posterior distribution on the left-hand side is a valid probability density and integrates to one.
Indeed, integrating both sides of (1.43) with respect to w, we can express the denominator in Bayes’theoremintermsofthepriordistributionandthelikelihoodfunction p(D)= p(D|w)p(w)dw.
(1.45) Inboththe Bayesianandfrequentistparadigms, thelikelihoodfunctionp(D|w) plays a central role.
However, the manner in which it is used is fundamentally dif- ferent in the two approaches.
In a frequentist setting, w is considered to be a fixed parameter, whose value is determined by some form of ‘estimator’, and error bars 1.2.
Probability Theory 23 onthisestimateareobtainedbyconsideringthedistributionofpossibledatasets D.
By contrast, from the Bayesian viewpoint there is only a single data set D (namely theonethatisactuallyobserved), andtheuncertaintyintheparametersisexpressed throughaprobabilitydistributionoverw.
A widely used frequentist estimator is maximum likelihood, in which w is set to the value that maximizes the likelihood function p(D|w).
This corresponds to choosingthevalueofw forwhichtheprobabilityoftheobserveddatasetismaxi- mized.
Inthemachinelearningliterature, thenegativelogofthelikelihoodfunction is called an error function.
Because the negative logarithm is a monotonically de- creasingfunction, maximizingthelikelihoodisequivalenttominimizingtheerror.
Oneapproachtodeterminingfrequentisterrorbarsisthebootstrap(Efron,1979; Hastieetal.,2001), inwhichmultipledatasetsarecreatedasfollows.
Supposeour originaldatasetconsistsof N datapoints X = {x 1 ,..., x N }.
Wecancreateanew dataset X Bbydrawing N pointsatrandomfrom X, withreplacement, sothatsome pointsin Xmaybereplicatedin X B, whereasotherpointsin Xmaybeabsentfrom X B.
Thisprocesscanberepeated Ltimestogenerate Ldatasetseachofsize N and eachobtainedbysamplingfromtheoriginaldataset X.
Thestatisticalaccuracyof parameterestimatescanthenbeevaluatedbylookingatthevariabilityofpredictions betweenthedifferentbootstrapdatasets.
One advantage of the Bayesian viewpoint is that the inclusion of prior knowl- edge arises naturally.
Suppose, for instance, that a fair-looking coin is tossed three times and lands heads each time.
A classical maximum likelihood estimate of the Section2.1 probability of landing heads would give 1, implying that all future tosses will land heads! By contrast, a Bayesian approach with any reasonable prior will lead to a muchlessextremeconclusion.
There has been much controversy and debate associated with the relative mer- its of the frequentist and Bayesian paradigms, which have not been helped by the fact that there is no unique frequentist, or even Bayesian, viewpoint.
For instance, one common criticism of the Bayesian approach is that the prior distribution is of- ten selected on the basis of mathematical convenience rather than as a reflection of any prior beliefs.
Even the subjective nature of the conclusions through their de- pendenceonthechoiceofpriorisseenbysomeasasourceofdifficulty.
Reducing Section2.4.3 the dependence on the prior is one motivation for so-called noninformative priors.
However, these lead to difficulties when comparing different models, and indeed Bayesian methods based on poor choices of prior can give poor results with high confidence.
Frequentist evaluation methods offer some protection from such prob- Section1.3 lems, andtechniques suchascross-validation remain usefulinareas suchasmodel comparison.
This book places a strong emphasis on the Bayesian viewpoint, reflecting the hugegrowthinthepracticalimportanceof Bayesianmethodsinthepastfewyears, whilealsodiscussingusefulfrequentistconceptsasrequired.
Although the Bayesian framework has its origins in the 18th century, the prac- tical application of Bayesian methods was for a long time severely limited by the difficultiesincarryingthroughthefull Bayesianprocedure, particularlytheneedto marginalize(sumorintegrate)overthewholeofparameterspace, which, asweshall 24 1.
INTRODUCTION see, is required in order to make predictions or to compare different models.
The developmentofsamplingmethods, suchas Markovchain Monte Carlo(discussedin Chapter 11) along with dramatic improvements in the speed and memory capacity ofcomputers, openedthedoortothepracticaluseof Bayesiantechniquesinanim- pressiverangeofproblemdomains.
Monte Carlomethodsareveryflexibleandcan beappliedtoawiderangeofmodels.
However, theyarecomputationallyintensive andhavemainlybeenusedforsmall-scaleproblems.
More recently, highly efficient deterministic approximation schemes such as variational Bayesandexpectation propagation (discussed in Chapter 10) havebeen developed.
Theseofferacomplementaryalternativetosamplingmethodsandhave allowed Bayesiantechniquestobeusedinlarge-scaleapplications(Bleietal.,2003).
1.2.4 The Gaussian distribution We shall devote the whole of Chapter 2 to a study of various probability dis- tributions and their key properties.
It is convenient, however, to introduce here one of the most important probability distributions for continuous variables, called the normalor Gaussiandistribution.
Weshallmakeextensiveuseofthisdistributionin theremainderofthischapterandindeedthroughoutmuchofthebook.
For the case of a single real-valued variable x, the Gaussian distribution is de- finedby 1 1 N x|µ,σ2 = exp − (x−µ)2 (1.46) (2πσ2)1/2 2σ2 which is governed by two parameters: µ, called the mean, and σ2, called the vari- ance.
The square root of the variance, given by σ, is called the standard deviation, andthereciprocalofthevariance, writtenasβ = 1/σ2, iscalledtheprecision.
We shall see the motivation for these terms shortly.
Figure 1.13 shows a plot of the Gaussiandistribution.
Fromtheformof(1.46)weseethatthe Gaussiandistributionsatisfies N(x|µ,σ2)>0.
(1.47) Exercise 1.7 Alsoitisstraightforwardtoshowthatthe Gaussianisnormalized, sothat Pierre-Simon Laplace earth is thought to have formed from the condensa- 1749–1827 tion and cooling of a large rotating disk of gas and dust.
In1812hepublishedthefirsteditionof The´orie It is said that Laplace was seri- Analytiquedes Probabilite´s, in which Laplace states ouslylackinginmodestyandatone that “probability theory is nothing but common sense point declared himself to be the reducedtocalculation”.
Thisworkincludedadiscus- bestmathematicianin Franceatthe sionoftheinverseprobabilitycalculation(latertermed time, aclaimthatwasarguablytrue.
Bayes’theoremby Poincare´), whichheusedtosolve As well as being prolific in mathe- problems in life expectancy, jurisprudence, planetary matics, he also made numerous contributions to as- masses, triangulation, anderrorestimation.
tronomy, includingthenebularhypothesisbywhichthe 1.2.
Probability Theory 25 Figure1.13 Plot of the univariate Gaussian showing the mean µ and the N(x|µ,σ2) standarddeviationσ.
2σ µ x ∞ N x|µ,σ2 dx=1.
(1.48) −∞ Thus(1.46)satisfiesthetworequirementsforavalidprobabilitydensity.
Wecanreadilyfindexpectationsoffunctionsofxunderthe Gaussiandistribu- Exercise 1.8 tion.
Inparticular, theaveragevalueofxisgivenby ∞ E[x]= N x|µ,σ2 xdx=µ.
(1.49) −∞ Becausetheparameterµrepresentstheaveragevalueofxunderthedistribution, it isreferredtoasthemean.
Similarly, forthesecondordermoment ∞ E[x2]= N x|µ,σ2 x2dx=µ2+σ2.
(1.50) −∞ From(1.49)and(1.50), itfollowsthatthevarianceofxisgivenby var[x]=E[x2]−E[x]2 =σ2 (1.51) andhenceσ2isreferredtoasthevarianceparameter.
Themaximumofadistribution Exercise 1.9 isknownasitsmode.
Fora Gaussian, themodecoincideswiththemean.
Wearealsointerestedinthe Gaussiandistributiondefinedovera D-dimensional vectorxofcontinuousvariables, whichisgivenby 1 1 1 N(x|µ,Σ)= exp − (x−µ)TΣ −1(x−µ) (1.52) (2π)D/2|Σ|1/2 2 wherethe D-dimensionalvectorµiscalledthemean, the D×DmatrixΣiscalled the covariance, and |Σ| denotes the determinant of Σ.
We shall make use of the multivariate Gaussiandistributionbrieflyinthischapter, althoughitspropertieswill bestudiedindetailin Section2.3.
26 1.
INTRODUCTION Figure1.14 Illustrationofthelikelihoodfunctionfor a Gaussiandistribution, shownbythe red curve.
Here the black points de- p(x) note a data set of values {x n }, and the likelihood function given by (1.53) correspondstotheproductoftheblue N(xn |µ,σ2) values.
Maximizing the likelihood in- volves adjusting the mean and vari- ance of the Gaussian so as to maxi- mizethisproduct.
xn x Now suppose that we have a data set of observations x = (x 1 ,..., x N)T, rep- resenting N observations of the scalar variable x.
Note that we are using the type- face x to distinguish this from a single observation of the vector-valued variable (x 1 ,..., x D)T, which we denote by x.
We shall suppose that the observations are drawn independently from a Gaussian distribution whose mean µ and variance σ2 are unknown, and we would like to determine these parameters from the data set.
Data points that are drawn independently from the same distribution are said to be independentandidenticallydistributed, whichisoftenabbreviatedtoi.
i.
d.
Wehave seen that the joint probability of two independent events is given by the product of themarginalprobabilitiesforeacheventseparately.
Becauseourdatasetxisi.
i.
d., wecanthereforewritetheprobabilityofthedataset, givenµandσ2, intheform N p(x|µ,σ2)= N xn |µ,σ2 .
(1.53) n=1 Whenviewedasafunctionofµandσ2, thisisthelikelihoodfunctionforthe Gaus- sianandisinterpreteddiagrammaticallyin Figure1.14.
Onecommoncriterionfordeterminingtheparametersinaprobabilitydistribu- tion using an observed data set is to find the parameter values that maximize the likelihoodfunction.
Thismightseemlikeastrangecriterionbecause, fromourfore- goingdiscussionofprobabilitytheory, itwouldseemmorenaturaltomaximizethe probabilityoftheparametersgiventhedata, nottheprobabilityofthedatagiventhe parameters.
Infact, thesetwocriteriaarerelated, asweshalldiscussinthecontext Section1.2.5 ofcurvefitting.
For the moment, however, we shall determine values for the unknown parame- tersµandσ2 inthe Gaussianbymaximizingthelikelihoodfunction(1.53).
Inprac- tice, it is more convenient to maximize the log of the likelihood function.
Because thelogarithmisamonotonicallyincreasingfunctionofitsargument, maximization of the log of a function is equivalent to maximization of the function itself.
Taking the log not only simplifies the subsequent mathematical analysis, but it also helps numericallybecausetheproductofalargenumberofsmallprobabilitiescaneasily underflowthenumericalprecisionofthecomputer, andthisisresolvedbycomputing instead the sum of the log probabilities.
From (1.46) and (1.53), the log likelihood 1.2.
Probability Theory 27 functioncanbewrittenintheform N 1 N N lnp x|µ,σ2 =− 2σ2 (xn −µ)2− 2 lnσ2− 2 ln(2π).
(1.54) n=1 Maximizing (1.54) with respect to µ, we obtain the maximum likelihood solution Exercise 1.11 givenby N 1 µ ML = xn (1.55) N n=1 which is the sample mean, i.
e., the mean of the observed values {xn }.
Similarly, maximizing (1.54) with respect to σ2, we obtain the maximum likelihood solution forthevarianceintheform N 1 σ M 2 L = N (xn −µ ML )2 (1.56) n=1 whichisthesamplevariancemeasuredwithrespecttothesamplemeanµ ML.
Note thatweareperformingajointmaximizationof(1.54)withrespecttoµandσ2, but inthecaseofthe Gaussiandistributionthesolutionforµdecouplesfromthatforσ2 sothatwecanfirstevaluate(1.55)andthensubsequentlyusethisresulttoevaluate (1.56).
Laterinthischapter, andalsoinsubsequentchapters, weshallhighlightthesig- nificantlimitationsofthemaximumlikelihoodapproach.
Herewegiveanindication of the problem in the context of our solutions for the maximum likelihood param- eter settings for the univariate Gaussian distribution.
In particular, we shall show that the maximum likelihood approach systematically underestimates the variance of the distribution.
This is an example of a phenomenon called bias and is related Section1.1 totheproblemofover-fittingencounteredinthecontextofpolynomialcurvefitting.
We first note that the maximum likelihood solutionsµ ML andσ M 2 L are functions of the data set values x 1 ,..., x N.
Consider the expectations of these quantities with respect to the data set values, which themselves come from a Gaussian distribution Exercise 1.12 withparametersµandσ2.
Itisstraightforwardtoshowthat E[µ ML ] = µ (1.57) N −1 E[σ2 ] = σ2 (1.58) ML N sothatonaveragethemaximumlikelihoodestimatewillobtainthecorrectmeanbut will underestimate the true variance by a factor (N −1)/N.
The intuition behind thisresultisgivenby Figure1.15.
From(1.58)itfollowsthatthefollowingestimateforthevarianceparameteris unbiased N N 1 σ 2 = N −1 σ M 2 L = N −1 (xn −µ ML )2.
(1.59) n=1 28 1.
INTRODUCTION Figure1.15 Illustrationofhowbiasarisesinusingmax- imum likelihood to determine the variance of a Gaussian.
The green curve shows the true Gaussian distribution from which dataisgenerated, andthethreeredcurves show the Gaussian distributions obtained (a) by fitting to three data sets, each consist- ing of two data points shown in blue, us- ing the maximum likelihood results (1.55) and(1.56).
Averagedacrossthethreedata sets, the mean is correct, but the variance (b) is systematically under-estimated because it is measured relative to the sample mean andnotrelativetothetruemean.
(c) In Section10.1.3, weshallseehowthisresultarisesautomaticallywhenweadopta Bayesianapproach.
Notethatthebiasofthemaximumlikelihoodsolutionbecomeslesssignificant as the number N of data points increases, and in the limit N → ∞ the maximum likelihood solution for the variance equals the true variance of the distribution that generated the data.
In practice, for anything other than small N, this bias will not provetobeaseriousproblem.
However, throughoutthisbookweshallbeinterested in more complex models with many parameters, for which the bias problems asso- ciatedwithmaximumlikelihoodwillbemuchmoresevere.
Infact, asweshallsee, the issue of bias in maximum likelihood lies at the root of the over-fitting problem thatweencounteredearlierinthecontextofpolynomialcurvefitting.
1.2.5 Curve fitting re-visited Wehaveseenhowtheproblemofpolynomialcurvefittingcanbeexpressedin Section1.1 termsoferrorminimization.
Herewereturntothecurvefittingexampleandviewit from a probabilistic perspective, thereby gaining some insights into error functions andregularization, aswellastakingustowardsafull Bayesiantreatment.
The goal in the curve fitting problem is to be able to make predictions for the targetvariabletgivensomenewvalueoftheinputvariablexonthebasisofasetof trainingdatacomprising N inputvaluesx=(x 1 ,..., x N)Tandtheircorresponding target values t = (t 1 ,..., t N)T.
We can express our uncertainty over the value of thetargetvariableusingaprobabilitydistribution.
Forthispurpose, weshallassume that, given the value of x, the corresponding value of t has a Gaussian distribution withameanequaltothevaluey(x, w)ofthepolynomialcurvegivenby(1.1).
Thus wehave p(t|x, w,β)=N t|y(x, w),β −1 (1.60) where, for consistency with the notation in later chapters, we have defined a preci- sion parameter β corresponding to the inverse variance of the distribution.
This is illustratedschematicallyin Figure1.16.
1.2.
Probability Theory 29 Figure1.16 Schematic illustration of a Gaus- sianconditionaldistributionfortgivenxgivenby t y(x, w) (1.60), inwhichthemeanisgivenbythepolyno- mial function y(x, w), and the precision is given by the parameter β, which is related to the vari- ancebyβ−1 =σ2.
y(x0, w) 2σ p(t|x0, w,β) x0 x We now use the training data {x, t} to determine the values of the unknown parameters w and β by maximum likelihood.
If the data are assumed to be drawn independentlyfromthedistribution(1.60), thenthelikelihoodfunctionisgivenby N p(t|x, w,β)= N tn |y(xn, w),β −1 .
(1.61) n=1 Aswedidinthecaseofthesimple Gaussiandistributionearlier, itisconvenientto maximize the logarithm of the likelihood function.
Substituting for the form of the Gaussian distribution, given by (1.46), we obtain the log likelihood function in the form N β N N lnp(t|x, w,β)=− {y(xn, w)−tn }2 + lnβ− ln(2π).
(1.62) 2 2 2 n=1 Considerfirstthedeterminationofthemaximumlikelihoodsolutionforthepolyno- mial coefficients, which will be denoted by w ML.
These are determined by maxi- mizing (1.62) with respect to w.
For this purpose, we can omit the last two terms on the right-hand side of (1.62) because they do not depend on w.
Also, we note that scaling the log likelihood by a positive constant coefficient does not alter the location of the maximum with respect to w, and so we can replace the coefficient β/2with1/2.
Finally, insteadofmaximizingtheloglikelihood, wecanequivalently minimizethenegativeloglikelihood.
Wethereforeseethatmaximizinglikelihoodis equivalent, sofarasdeterminingwisconcerned, tominimizingthesum-of-squares errorfunctiondefinedby(1.2).
Thusthesum-of-squareserrorfunctionhasarisenas a consequence of maximizing likelihood under the assumption of a Gaussian noise distribution.
Wecanalsousemaximumlikelihoodtodeterminetheprecisionparameterβ of the Gaussianconditionaldistribution.
Maximizing(1.62)withrespecttoβ gives N 1 1 = {y(xn, w ML )−tn }2 .
(1.63) β N ML n=1 30 1.
INTRODUCTION Againwecanfirstdeterminetheparametervectorw MLgoverningthemeanandsub- sequentlyusethistofindtheprecisionβ ML aswasthecaseforthesimple Gaussian Section1.2.4 distribution.
Having determined the parameters w and β, we can now make predictions for new values of x.
Because we now have a probabilistic model, these are expressed in terms of the predictive distribution that gives the probability distribution over t, rather than simply a point estimate, and is obtained by substituting the maximum likelihoodparametersinto(1.60)togive p(t|x, w ML ,β ML )=N t|y(x, w ML ),β M −1 L .
(1.64) Nowletustakeasteptowardsamore Bayesianapproachandintroduceaprior distribution over the polynomial coefficients w.
For simplicity, let us consider a Gaussiandistributionoftheform α (M+1)/2 α p(w|α)=N(w|0,α −1I)= exp − w Tw (1.65) 2π 2 whereαistheprecisionofthedistribution, and M+1isthetotalnumberofelements in the vector w for an Mth order polynomial.
Variables such as α, which control the distribution of model parameters, are called hyperparameters.
Using Bayes’ theorem, the posterior distribution for w is proportional to the product of the prior distributionandthelikelihoodfunction p(w|x, t,α,β)∝p(t|x, w,β)p(w|α).
(1.66) We can now determine w by finding the most probable value of w given the data, in other words by maximizing the posterior distribution.
This technique is called maximum posterior, or simply MAP.
Taking the negative logarithm of (1.66) and combining with (1.62) and (1.65), we find that the maximum of the posterior is givenbytheminimumof N β α {y(xn, w)−tn }2+ w Tw.
(1.67) 2 2 n=1 Thus we see that maximizing the posterior distribution is equivalent to minimizing theregularizedsum-of-squares errorfunctionencounteredearlier intheform(1.4), witharegularizationparametergivenbyλ=α/β.
1.2.6 Bayesian curve fitting Althoughwehaveincludedapriordistributionp(w|α), wearesofarstillmak- ingapointestimateofwandsothisdoesnotyetamounttoa Bayesiantreatment.
In a fully Bayesian approach, we should consistently apply the sum and product rules ofprobability, whichrequires, asweshallseeshortly, thatweintegrateoverallval- ues of w.
Such marginalizations lie at the heart of Bayesian methods for pattern recognition.
1.2.
Probability Theory 31 In the curve fitting problem, we are given the training data x and t, along with a new test point x, and our goal is to predict the value of t.
We therefore wish to evaluate the predictive distribution p(t|x, x, t).
Here we shall assume that the parametersαandβarefixedandknowninadvance(inlaterchaptersweshalldiscuss howsuchparameterscanbeinferredfromdataina Bayesiansetting).
ABayesiantreatmentsimplycorrespondstoaconsistentapplicationofthesum andproductrulesofprobability, whichallowthepredictivedistributiontobewritten intheform p(t|x, x, t)= p(t|x, w)p(w|x, t)dw.
(1.68) Here p(t|x, w) is given by (1.60), and we have omitted the dependence on α and β to simplify the notation.
Here p(w|x, t) is the posterior distribution over param- eters, and can be found by normalizing the right-hand side of (1.66).
We shall see in Section 3.3 that, for problems such as the curve-fitting example, this posterior distribution is a Gaussian and can be evaluated analytically.
Similarly, the integra- tion in (1.68) can also be performed analytically with the result that the predictive distributionisgivenbya Gaussianoftheform p(t|x, x, t)=N t|m(x), s2(x) (1.69) wherethemeanandvariancearegivenby N m(x) = βφ(x)TS φ(xn)tn (1.70) n=1 s2(x) = β −1+φ(x)TSφ(x).
(1.71) Herethematrix Sisgivenby N S −1 =αI+β φ(xn)φ(x)T (1.72) n=1 where I is the unit matrix, and we have defined the vector φ(x) with elements φi(x)=xi fori=0,..., M.
We see that the variance, as well as the mean, of the predictive distribution in (1.69) is dependent on x.
The first term in (1.71) represents the uncertainty in the predictedvalueoftduetothenoiseonthetargetvariablesandwasexpressedalready inthemaximumlikelihoodpredictivedistribution(1.64)throughβ −1 .
However, the ML second term arises from the uncertainty in the parameters w and is a consequence of the Bayesian treatment.
The predictive distribution for the synthetic sinusoidal regressionproblemisillustratedin Figure1.17.
32 1.
INTRODUCTION Figure1.17 The predictive distribution result- ing from a Bayesian treatment of polynomial curve fitting using an 1 M = 9 polynomial, with the fixed parametersα=5×10−3andβ = t 11.1 (corresponding to the known noise variance), in which the red curve denotes the mean of the 0 predictive distribution and the red region corresponds to ±1 stan- darddeviationaroundthemean.
−1 0 1 x 1.3.
Model Selection Inourexampleofpolynomialcurvefittingusingleastsquares, wesawthattherewas an optimal order of polynomial that gave the best generalization.
The order of the polynomialcontrolsthenumberoffreeparametersinthemodelandtherebygoverns themodelcomplexity.
Withregularizedleastsquares, theregularizationcoefficient λ also controls the effective complexity of the model, whereas for more complex models, suchasmixturedistributionsorneuralnetworkstheremaybemultiplepa- rameters governing complexity.
In a practical application, we need to determine the values of such parameters, and the principal objective in doing so is usually to achieve the best predictive performance on new data.
Furthermore, as well as find- ingtheappropriatevaluesforcomplexityparameterswithinagivenmodel, wemay wishtoconsiderarangeofdifferenttypesofmodelinordertofindthebestonefor ourparticularapplication.
We have already seen that, in the maximum likelihood approach, the perfor- mance on the training set is not a good indicator of predictive performance on un- seendataduetotheproblemofover-fitting.
Ifdataisplentiful, thenoneapproachis simplytousesomeoftheavailabledatatotrainarangeofmodels, oragivenmodel with a range of values for its complexity parameters, and then to compare them on independent data, sometimes called a validation set, and select the one having the bestpredictiveperformance.
Ifthemodeldesignisiteratedmanytimesusingalim- itedsizedataset, thensomeover-fittingtothevalidationdatacanoccurandsoitmay benecessarytokeepasideathirdtestset onwhichtheperformanceoftheselected modelisfinallyevaluated.
Inmanyapplications, however, thesupplyofdatafortrainingandtestingwillbe limited, andinordertobuildgoodmodels, wewishtouseasmuchoftheavailable data as possible for training.
However, if the validation set is small, it will give a relativelynoisyestimateofpredictiveperformance.
Onesolutiontothisdilemmais tousecross-validation, whichisillustratedin Figure1.18.
Thisallowsaproportion (S−1)/S oftheavailabledatatobeusedfortrainingwhilemakinguseofallofthe 1.4.
The Curseof Dimensionality 33 Figure1.18 The technique of S-fold cross-validation, illus- run 1 trated here for the case of S = 4, involves tak- ing the available data and partitioning it into S run 2 groups (in the simplest case these are of equal size).
Then S−1ofthegroupsareusedtotrain asetofmodelsthatarethenevaluatedonthere- run 3 maining group.
This procedure is then repeated for all S possible choices for the held-out group, run 4 indicated here by the red blocks, and the perfor- mancescoresfromthe Srunsarethenaveraged.
datatoassessperformance.
Whendataisparticularlyscarce, itmaybeappropriate toconsiderthecase S =N, where N isthetotalnumberofdatapoints, whichgives theleave-one-outtechnique.
Onemajordrawbackofcross-validationisthatthenumberoftrainingrunsthat mustbeperformedisincreasedbyafactorof S, andthiscanproveproblematicfor modelsinwhichthetrainingisitselfcomputationallyexpensive.
Afurtherproblem withtechniquessuchascross-validationthatuseseparatedatatoassessperformance is that we might have multiple complexity parameters for a single model (for in- stance, there might be several regularization parameters).
Exploring combinations ofsettingsforsuchparameterscould, intheworstcase, requireanumberoftraining runs that is exponential in the number of parameters.
Clearly, we need a better ap- proach.
Ideally, thisshouldrelyonlyonthetrainingdataandshouldallowmultiple hyperparametersandmodeltypestobecomparedinasingletrainingrun.
Wethere- foreneedtofindameasureofperformancewhichdependsonlyonthetrainingdata andwhichdoesnotsufferfrombiasduetoover-fitting.
Historically various ‘information criteria’ have been proposed that attempt to correct for the bias of maximum likelihood by the addition of a penalty term to compensate for the over-fitting of more complex models.
For example, the Akaike informationcriterion, or AIC(Akaike,1974), choosesthemodelforwhichthequan- tity lnp(D|w ML )−M (1.73) is largest.
Here p(D|w ML ) is the best-fit log likelihood, and M is the number of adjustable parameters in the model.
A variant of this quantity, called the Bayesian information criterion, or BIC, will be discussed in Section 4.4.1.
Such criteria do nottakeaccountoftheuncertaintyinthemodelparameters, however, andinpractice theytendtofavouroverlysimplemodels.
Wethereforeturnin Section3.4toafully Bayesian approach where we shall see how complexity penalties arise in a natural andprincipledway.
1.4.
The Curse of Dimensionality Inthepolynomialcurvefittingexamplewehadjustoneinputvariablex.
Forprac- tical applications of pattern recognition, however, we will have to deal with spaces 34 1.
INTRODUCTION Figure1.19 Scatter plot of the oil flow data 2 for input variables x 6 and x 7, in which red denotes the ‘homoge- nous’ class, green denotes the ‘annular’class, andbluedenotes 1.5 the ‘laminar’ class.
Our goal is to classify the new test point de- notedby‘×’.
x7 1 0.5 0 0 0.25 0.5 0.75 1 x6 of high dimensionality comprising many input variables.
As we now discuss, this poses some serious challenges and is an important factor influencing the design of patternrecognitiontechniques.
Inordertoillustratetheproblemweconsiderasyntheticallygenerateddataset representing measurements taken from a pipeline containing a mixture of oil, wa- ter, andgas(Bishopand James, 1993).
Thesethreematerialscanbepresentinone ofthreedifferentgeometricalconfigurationsknownas‘homogenous’,‘annular’, and ‘laminar’, andthefractionsofthethreematerialscanalsovary.
Eachdatapointcom- prisesa12-dimensionalinputvectorconsistingofmeasurementstakenwithgamma ray densitometers that measure the attenuation of gamma rays passing along nar- row beams through the pipe.
This data set is described in detail in Appendix A.
Figure 1.19 shows 100 points from this data set on a plot showing two of the mea- surementsx 6 andx 7 (theremainingteninputvaluesareignoredforthepurposesof thisillustration).
Eachdatapointislabelledaccordingtowhichofthethreegeomet- ricalclassesitbelongsto, andourgoalistousethisdataasatrainingsetinorderto beabletoclassifyanewobservation(x 6 , x 7 ), suchastheonedenotedbythecross in Figure1.19.
Weobservethatthecrossissurroundedbynumerousredpoints, and sowemightsupposethatitbelongstotheredclass.
However, therearealsoplenty of green points nearby, so we might think that it could instead belong to the green class.
Itseemsunlikelythatitbelongstotheblueclass.
Theintuitionhereisthatthe identityofthecrossshouldbedeterminedmorestronglybynearbypointsfromthe trainingsetandlessstronglybymoredistantpoints.
Infact, thisintuitionturnsout tobereasonableandwillbediscussedmorefullyinlaterchapters.
How can we turn this intuition into a learning algorithm? One very simple ap- proach would be to divide the input space into regular cells, as indicated in Fig- ure 1.20.
When we are given a test point and we wish to predict its class, we first decide which cell it belongs to, and we then find all of the training data points that 1.4.
The Curseof Dimensionality 35 Figure1.20 Illustration of a simple approach 2 to the solution of a classification problem in which the input space is divided into cells and any new testpointisassignedtotheclass 1.5 thathasamajoritynumberofrep- resentatives in the same cell as the test point.
As we shall see shortly, this simplistic approach hassomesevereshortcomings.
x7 1 0.5 0 0 0.25 0.5 0.75 1 x6 fall in the same cell.
The identity of the test point is predicted as being the same as the class having the largest number of training points in the same cell as the test point(withtiesbeingbrokenatrandom).
Therearenumerousproblemswiththisnaiveapproach, butoneofthemostse- vere becomes apparent when we consider its extension to problems having larger numbersofinputvariables, correspondingtoinputspacesofhigherdimensionality.
Theoriginoftheproblemisillustratedin Figure1.21, whichshowsthat, ifwedivide aregionofaspaceintoregularcells, thenthenumberofsuchcellsgrowsexponen- tiallywiththedimensionalityofthespace.
Theproblemwithanexponentiallylarge numberofcellsisthatwewouldneedanexponentiallylargequantityoftrainingdata inordertoensurethatthecellsarenotempty.
Clearly, wehavenohopeofapplying such a technique in a space of more than a few variables, and so we need to find a moresophisticatedapproach.
We can gain further insight into the problems of high-dimensional spaces by Section1.1 returningtotheexampleofpolynomialcurvefittingandconsideringhowwewould Figure1.21 Illustration of the x2 curse of dimensionality, showing how the number of regions of a regular grid grows exponentially with the dimensionality D of the x2 space.
For clarity, only a subset of the cubical regions are shown for D=3.
x1 x1 x1 x3 D =1 D =2 D =3 36 1.
INTRODUCTION extend this approach to deal with input spaces having several variables.
If we have D input variables, then a general polynomial with coefficients up to order 3 would taketheform D D D D D D y(x, w)=w 0 + wixi+ wijxixj + wijkxixjxk.
(1.74) i=1 i=1 j=1 i=1 j=1 k=1 As Dincreases, sothenumberofindependentcoefficients(notallofthecoefficients areindependentduetointerchangesymmetriesamongstthexvariables)growspro- portionallyto D3.
Inpractice, tocapturecomplexdependenciesinthedata, wemay needtouseahigher-orderpolynomial.
Forapolynomialoforder M, thegrowthin Exercise 1.16 the number of coefficients is like DM.
Although this is now a power law growth, rather than an exponential growth, it still points to the method becoming rapidly unwieldyandoflimitedpracticalutility.
Our geometrical intuitions, formed through a life spent in a space of three di- mensions, can fail badly when we consider spaces of higher dimensionality.
As a simpleexample, considerasphereofradiusr = 1inaspaceof D dimensions, and askwhatisthefractionofthevolumeofthespherethatliesbetweenradiusr =1− and r = 1.
We can evaluate this fraction by noting that the volume of a sphere of radiusrin Ddimensionsmustscaleasr D, andsowewrite D VD(r)=KDr (1.75) Exercise 1.18 wheretheconstant KD dependsonlyon D.
Thustherequiredfractionisgivenby VD(1)−VD(1− ) =1−(1− ) D (1.76) VD(1) which is plotted as a function of for various values of D in Figure 1.22.
We see that, forlarge D, thisfractiontendsto1evenforsmallvaluesof .
Thus, inspaces ofhighdimensionality, mostofthevolumeofasphereisconcentratedinathinshell nearthesurface! As a further example, of direct relevance to pattern recognition, consider the behaviour of a Gaussian distribution in a high-dimensional space.
If we transform from Cartesiantopolarcoordinates, andthenintegrateoutthedirectionalvariables, Exercise 1.20 weobtainanexpressionforthedensityp(r)asafunctionofradiusrfromtheorigin.
Thus p(r)δr is the probability mass inside a thin shell of thickness δr located at radiusr.
Thisdistributionisplotted, forvariousvaluesof D, in Figure1.23, andwe see that for large D the probability mass of the Gaussian is concentrated in a thin shell.
Theseveredifficultythatcanariseinspacesofmanydimensionsissometimes calledthecurseofdimensionality(Bellman,1961).
Inthisbook, weshallmakeex- tensiveuseofillustrativeexamplesinvolvinginputspacesofoneortwodimensions, because this makes it particularly easy to illustrate the techniques graphically.
The readershouldbewarned, however, thatnotallintuitionsdevelopedinspacesoflow dimensionalitywillgeneralizetospacesofmanydimensions.
1.4.
The Curseof Dimensionality 37 Figure1.22 Plotofthefractionofthevolumeof aspherelyingintheranger=1− to r = 1 for various values of the dimensionality D.
noitcarf emulov 1 D=20 D=2 0.6 D=1 0.4 0.2 0 0 0.2 0.4 0.6 0.8 1 Although the curse of dimensionality certainly raises important issues for pat- ternrecognitionapplications, itdoesnotpreventusfromfindingeffectivetechniques applicable to high-dimensional spaces.
The reasons for this are twofold.
First, real datawilloftenbeconfinedtoaregionofthespacehavinglowereffectivedimension- ality, and in particular the directions over which important variations in the target variables occur may be so confined.
Second, real data will typically exhibit some smoothnessproperties(atleastlocally)sothatforthemostpartsmallchangesinthe inputvariableswillproducesmallchangesinthetargetvariables, andsowecanex- ploitlocalinterpolation-liketechniquestoallowustomakepredictionsofthetarget variablesfornewvaluesoftheinputvariables.
Successfulpatternrecognitiontech- niquesexploitoneorbothoftheseproperties.
Consider, forexample, anapplication inmanufacturinginwhichimagesarecapturedofidenticalplanarobjectsonacon- veyorbelt, inwhichthegoalistodeterminetheirorientation.
Eachimageisapoint Figure1.23 Plot of the probability density with respect to radius r of a Gaus- sian distribution for various values of the dimensionality D.
In a D=1 high-dimensionalspace, mostofthe probabilitymassofa Gaussianislo- D=2 catedwithinathinshellataspecific radius.
D=20 r )r(p 2 1 0 0 2 4 38 1.
INTRODUCTION in a high-dimensional space whose dimensionality is determined by the number of pixels.
Because the objects can occur at different positions within the image and in different orientations, there are three degrees of freedom of variability between images, and a set of images will live on a three dimensional manifold embedded within the high-dimensional space.
Due to the complex relationships between the object position or orientation and the pixel intensities, this manifold will be highly nonlinear.
Ifthegoalistolearnamodelthatcantakeaninputimageandoutputthe orientationoftheobjectirrespectiveofitsposition, thenthereisonlyonedegreeof freedomofvariabilitywithinthemanifoldthatissignificant.
1.5.
Decision Theory We have seen in Section 1.2 how probability theory provides us with a consistent mathematical framework for quantifying and manipulating uncertainty.
Here we turntoadiscussionofdecisiontheorythat, whencombinedwithprobabilitytheory, allowsustomakeoptimaldecisionsinsituationsinvolvinguncertaintysuchasthose encounteredinpatternrecognition.
Suppose we have an input vector x together with a corresponding vector t of targetvariables, andourgoalistopredicttgivenanewvalueforx.
Forregression problems, twillcomprisecontinuousvariables, whereasforclassificationproblems t will represent class labels.
The joint probability distribution p(x, t) provides a completesummaryoftheuncertaintyassociatedwiththesevariables.
Determination of p(x, t) from a set of training data is an example of inference and is typically a very difficult problem whose solution forms the subject of much of this book.
In a practical application, however, we must often make a specific prediction for the valueoft, ormoregenerallytakeaspecificactionbasedonourunderstandingofthe valuestislikelytotake, andthisaspectisthesubjectofdecisiontheory.
Consider, forexample, amedicaldiagnosisprobleminwhichwehavetakenan X-ray image of a patient, and we wish to determine whether the patient has cancer or not.
In this case, the input vector x is the set of pixel intensities in the image, andoutputvariabletwillrepresentthepresenceofcancer, whichwedenotebythe class C 1, or the absence of cancer, which we denote by the class C 2.
We might, for instance, choosettobeabinaryvariablesuchthatt=0correspondstoclass C 1and t = 1 corresponds to class C 2.
We shall see later that this choice of label values is particularlyconvenientforprobabilisticmodels.
Thegeneralinferenceproblemthen involves determining the joint distribution p(x, C k), or equivalently p(x, t), which gives us the most complete probabilistic description of the situation.
Although this can be a very useful and informative quantity, in the end we must decide either to give treatment to the patient or not, and we would like this choice to be optimal in some appropriate sense (Duda and Hart, 1973).
This is the decision step, and it is the subject of decision theory to tell us how to make optimal decisions given the appropriate probabilities.
We shall see that the decision stage is generally very simple, eventrivial, oncewehavesolvedtheinferenceproblem.
Herewegiveanintroductiontothekeyideasofdecisiontheoryasrequiredfor 1.5.
Decision Theory 39 therestofthebook.
Furtherbackground, aswellasmoredetailedaccounts, canbe foundin Berger(1985)and Bather(2000).
Before giving a more detailed analysis, let us first consider informally how we might expect probabilities to play a role in making decisions.
When we obtain the X-ray image x for a new patient, our goal is to decide which of the two classes to assign to the image.
We are interested in the probabilities of the two classes given the image, which are given by p(C k |x).
Using Bayes’ theorem, these probabilities canbeexpressedintheform p(C k |x)= p(x|C k)p(C k) .
(1.77) p(x) Note that any of the quantities appearing in Bayes’ theorem can be obtained from thejointdistributionp(x, C k)byeithermarginalizingorconditioningwithrespectto theappropriatevariables.
Wecannowinterpretp(C k)asthepriorprobabilityforthe class C k, andp(C k |x)asthecorrespondingposteriorprobability.
Thusp(C 1 )repre- sentstheprobabilitythatapersonhascancer, beforewetakethe X-raymeasurement.
Similarly, p(C 1 |x)isthecorrespondingprobability, revisedusing Bayes’theoremin lightoftheinformationcontainedinthe X-ray.
Ifouraimistominimizethechance ofassigningxtothewrongclass, thenintuitivelywewouldchoosetheclasshaving the higher posterior probability.
We now show that this intuition is correct, and we alsodiscussmoregeneralcriteriaformakingdecisions.
1.5.1 Minimizing the misclassification rate Suppose that our goal is simply to make as few misclassifications as possible.
We need a rule that assigns each value of x to one of the available classes.
Such a rulewilldividetheinputspaceintoregions R k calleddecisionregions, oneforeach class, such that all points in R k are assigned to class C k.
The boundaries between decisionregionsarecalleddecisionboundariesordecisionsurfaces.
Notethateach decisionregionneednotbecontiguousbutcouldcomprisesomenumberofdisjoint regions.
Weshallencounterexamplesofdecisionboundariesanddecisionregionsin laterchapters.
Inordertofindtheoptimaldecisionrule, considerfirstofallthecase oftwoclasses, asinthecancerproblemforinstance.
Amistakeoccurswhenaninput vectorbelongingtoclass C 1 isassignedtoclass C 2 orviceversa.
Theprobabilityof thisoccurringisgivenby p(mistake) = p(x∈R , C )+p(x∈R , C ) 1 2 2 1 = p(x, C 2 )dx+ p(x, C 1 )dx.
(1.78) R R 1 2 We are free to choose the decision rule that assigns each point x to one of the two classes.
Clearlytominimizep(mistake)weshouldarrangethateachxisassignedto whicheverclasshasthesmallervalueoftheintegrandin(1.78).
Thus, ifp(x, C 1 )> p(x, C 2 ) for a given value of x, then we should assign that x to class C 1.
From the product rule of probability we have p(x, C k) = p(C k |x)p(x).
Because the factor p(x)iscommontobothterms, wecanrestatethisresultassayingthattheminimum 40 1.
INTRODUCTION x0 x p(x, C 1) p(x, C 2) x R R 1 2 Figure 1.24 Schematic illustration of the joint probabilities p(x, C k ) for each of two classes plotted againstx, togetherwiththedecisionboundaryx = xb.
Valuesofx xbareclassifiedas class C 2 and hence belong to decision region R 2, whereas points x < xb are classified as C 1 and belong to R 1.
Errors arise from the blue, green, and red regions, so that for x<xbtheerrorsareduetopointsfromclass C 2beingmisclassifiedas C 1(representedby thesumoftheredandgreenregions), andconverselyforpointsintheregionx xbthe errorsareduetopointsfromclass C 1 beingmisclassifiedas C 2 (representedbytheblue region).
As we vary the location xb of the decision boundary, the combined areas of the blueandgreenregionsremainsconstant, whereasthesizeoftheredregionvaries.
The optimalchoiceforxbiswherethecurvesforp(x, C 1 )andp(x, C 2 )cross, correspondingto xb=x 0, becauseinthiscasetheredregiondisappears.
Thisisequivalenttotheminimum misclassificationratedecisionrule, whichassignseachvalueofxtotheclasshavingthe higherposteriorprobabilityp(C k |x).
probabilityofmakingamistakeisobtainedifeachvalueofxisassignedtotheclass for which the posterior probability p(C k |x) is largest.
This result is illustrated for twoclasses, andasingleinputvariablex, in Figure1.24.
For the more general case of K classes, it is slightly easier to maximize the probabilityofbeingcorrect, whichisgivenby K p(correct) = p(x∈R k, C k) k=1 K = p(x, C k)dx (1.79) k=1 R k which is maximized when the regions R k are chosen such that each x is assigned to the class for which p(x, C k) is largest.
Again, using the product rulep(x, C k) = p(C k |x)p(x), and noting that the factor of p(x) is common to all terms, we see that each x should be assigned to the class having the largest posterior probability p(C k |x).
1.5.
Decision Theory 41 Figure1.25 An example of a loss matrix with ele- cancer normal ments L kj forthecancertreatmentproblem.
Therows cancer 0 1000 correspondtothetrueclass, whereasthecolumnscor- respond to the assignment of class made by our deci- normal 1 0 sioncriterion.
1.5.2 Minimizing the expected loss For many applications, our objective will be more complex than simply mini- mizingthenumberofmisclassifications.
Letusconsideragainthemedicaldiagnosis problem.
Wenotethat, ifapatientwhodoesnothavecancerisincorrectlydiagnosed as having cancer, the consequences may be some patient distress plus the need for further investigations.
Conversely, if a patient with cancer is diagnosed as healthy, theresultmaybeprematuredeathduetolackoftreatment.
Thustheconsequences ofthesetwotypesofmistakecanbedramaticallydifferent.
Itwouldclearlybebetter tomakefewermistakesofthesecondkind, evenifthiswasattheexpenseofmaking moremistakesofthefirstkind.
We can formalize such issues through the introduction of a loss function, also called a cost function, which is a single, overall measure of loss incurred in taking anyoftheavailabledecisionsoractions.
Ourgoalisthentominimizethetotalloss incurred.
Note that some authors consider instead a utility function, whose value they aim to maximize.
These are equivalent concepts if we take the utility to be simplythenegativeoftheloss, andthroughoutthistextweshallusethelossfunction convention.
Supposethat, foranewvalueofx, thetrueclassis C kandthatweassign x to class C j (where j may or may not be equal to k).
In so doing, we incur some leveloflossthatwedenoteby Lkj, whichwecanviewasthek, j elementofaloss matrix.
Forinstance, inourcancerexample, wemighthavealossmatrixoftheform shownin Figure1.25.
Thisparticularlossmatrixsaysthatthereisnolossincurred ifthecorrectdecisionismade, thereisalossof1ifahealthypatientisdiagnosedas havingcancer, whereasthereisalossof1000ifapatienthavingcancerisdiagnosed ashealthy.
The optimal solution is the one which minimizes the loss function.
However, the loss function depends on the true class, which is unknown.
For a given input vectorx, our uncertainty inthetrue classisexpressedthrough thejoint probability distributionp(x, C k)andsoweseekinsteadtominimizetheaverageloss, wherethe averageiscomputedwithrespecttothisdistribution, whichisgivenby E[L]= Lkjp(x, C k)dx.
(1.80) k j R j Each x can be assigned independently to one of the decision regions R j.
Our goal is to choose the regions R j in order to mi nimize the expected loss (1.80), which impliesthatforeachxweshouldminimize k Lkjp(x, C k).
Asbefore, wecanuse the product rule p(x, C k) = p(C k |x)p(x) to eliminate the common factor of p(x).
Thusthedecisionrulethatminimizestheexpectedlossistheonethatassignseach 42 1.
INTRODUCTION Figure1.26 I x llu s s u t c ra h ti t o h n at o t f he th l e ar r g e e je r c o t f o th p e tio tw n.
op In o p s u te ts - 1.0 p(C 1 |x) p(C 2 |x) riorprobabilitiesislessthanorequalto θ somethresholdθwillberejected.
0.0 x reject region newxtotheclassj forwhichthequantity Lkjp(C k |x) (1.81) k isaminimum.
Thisisclearlytrivialtodo, onceweknowtheposteriorclassproba- bilitiesp(C k |x).
1.5.3 The reject option We have seen that classification errors arise from the regions of input space wherethelargestoftheposteriorprobabilitiesp(C k |x)issignificantlylessthanunity, orequivalentlywherethejointdistributionsp(x, C k)havecomparablevalues.
These are the regions where we are relatively uncertain about class membership.
In some applications, it will be appropriate to avoid making decisions on the difficult cases inanticipationofalowererrorrateonthoseexamplesforwhichaclassificationde- cisionismade.
Thisisknownastherejectoption.
Forexample, inourhypothetical medical illustration, it may be appropriate to use an automatic system to classify those X-rayimagesforwhichthereislittledoubtastothecorrectclass, whileleav- ing a human expert to classify the more ambiguous cases.
We can achieve this by introducing a threshold θ and rejecting those inputs x for which the largest of the posterior probabilities p(C k |x) is less than or equal to θ.
This is illustrated for the case of two classes, and a single continuous input variable x, in Figure 1.26.
Note thatsettingθ = 1willensurethatallexamplesarerejected, whereasifthereare K classes then setting θ < 1/K will ensure that no examples are rejected.
Thus the fractionofexamplesthatgetrejectediscontrolledbythevalueofθ.
We can easily extend the reject criterion to minimize the expected loss, when a loss matrix is given, taking account of the loss incurred when a reject decision is Exercise 1.24 made.
1.5.4 Inference and decision We have broken the classification problem down into two separate stages, the inferencestageinwhichweusetrainingdatatolearnamodelforp(C k |x), andthe 1.5.
Decision Theory 43 subsequentdecisionstageinwhichweusetheseposteriorprobabilitiestomakeop- timalclassassignments.
Analternativepossibilitywouldbetosolvebothproblems togetherandsimplylearnafunctionthatmapsinputsxdirectlyintodecisions.
Such afunctioniscalledadiscriminantfunction.
Infact, wecanidentifythreedistinctapproachestosolvingdecisionproblems, allofwhichhavebeenusedinpracticalapplications.
Thesearegiven, indecreasing orderofcomplexity, by: (a) Firstsolvetheinferenceproblemofdeterminingtheclass-conditionaldensities p(x|C k) for each class C k individually.
Also separately infer the prior class probabilitiesp(C k).
Thenuse Bayes’theoremintheform p(C k |x)= p(x|C k)p(C k) (1.82) p(x) to find the posterior class probabilities p(C k |x).
As usual, the denominator in Bayes’ theorem can be found in terms of the quantities appearing in the numerator, because p(x)= p(x|C k)p(C k).
(1.83) k Equivalently, we can model the joint distribution p(x, C k) directly and then normalize to obtain the posterior probabilities.
Having found the posterior probabilities, weusedecision theory todetermine classmembership for each newinputx.
Approachesthatexplicitlyorimplicitlymodelthedistributionof inputsaswellasoutputsareknownasgenerativemodels, becausebysampling fromthemitispossibletogeneratesyntheticdatapointsintheinputspace.
(b) Firstsolvetheinferenceproblemofdeterminingtheposteriorclassprobabilities p(C k |x), and then subsequently use decision theory to assign each new x to one of the classes.
Approaches that model the posterior probabilities directly arecalleddiscriminativemodels.
(c) Find a function f(x), called a discriminant function, which maps each input x directly onto a class label.
For instance, in the case of two-class problems, f(·)mightbebinaryvaluedandsuchthatf =0representsclass C 1andf =1 representsclass C 2.
Inthiscase, probabilitiesplaynorole.
Let us consider the relative merits of these three alternatives.
Approach (a) is the most demanding because it involves finding the joint distribution over both x and C k.
For many applications, x will have high dimensionality, and consequently we may need a large training set in order to be able to determine the class-conditional densities to reasonable accuracy.
Note that the class priorsp(C k) can often be esti- matedsimplyfromthefractionsofthetrainingsetdatapointsineachoftheclasses.
One advantage of approach (a), however, is that it also allows the marginal density ofdatap(x)tobedeterminedfrom(1.83).
Thiscanbeusefulfordetectingnewdata pointsthathavelowprobabilityunderthemodelandforwhichthepredictionsmay 44 1.
INTRODUCTION p(x|C 2) p(x|C 1) x seitisned ssalc 5 1.2 p(C 1 |x) p(C 2 |x) 1 4 0.8 3 0.6 2 0.4 1 0.2 0 0 x Figure 1.27 Example of the class-conditional densities for two classes having a single input variable x (left plot) together with the corresponding posterior probabilities (right plot).
Note that the left-hand mode of the class-conditionaldensityp(x|C 1 ), showninblueontheleftplot, hasnoeffectontheposteriorprobabilities.
The vertical green line in the right plot shows the decision boundary in x that gives the minimum misclassification rate.
beoflowaccuracy, whichisknownasoutlierdetectionornoveltydetection(Bishop, 1994; Tarassenko,1995).
However, ifweonlywishtomakeclassificationdecisions, thenitcanbewaste- fulofcomputationalresources, andexcessivelydemandingofdata, tofindthejoint distribution p(x, C k) when in fact we only really need the posterior probabilities p(C k |x), which can be obtained directly through approach (b).
Indeed, the class- conditional densities may contain a lot of structure that has little effect on the pos- terior probabilities, as illustrated in Figure 1.27.
There has been much interest in exploringtherelativemeritsofgenerativeanddiscriminativeapproachestomachine learning, andinfindingwaystocombinethem(Jebara,2004; Lasserreetal.,2006).
An even simpler approach is (c) in which we use the training data to find a discriminant function f(x) that maps each x directly onto a class label, thereby combining the inference and decision stages into a single learning problem.
In the example of Figure 1.27, this would correspond to finding the value of x shown by the vertical green line, because this is the decision boundary giving the minimum probabilityofmisclassification.
Withoption(c), however, wenolongerhaveaccesstotheposteriorprobabilities p(C k |x).
There are many powerful reasons for wanting to compute the posterior probabilities, evenifwesubsequentlyusethemtomakedecisions.
Theseinclude: Minimizingrisk.
Consideraprobleminwhichtheelementsofthelossmatrixare subjected to revision from time to time (such as might occur in a financial 1.5.
Decision Theory 45 application).
Ifweknowtheposteriorprobabilities, wecantriviallyrevisethe minimumriskdecisioncriterionbymodifying(1.81)appropriately.
Ifwehave onlyadiscriminantfunction, thenanychangetothelossmatrixwouldrequire thatwereturntothetrainingdataandsolvetheclassificationproblemafresh.
Rejectoption.
Posteriorprobabilitiesallowustodeterminearejectioncriterionthat will minimize the misclassification rate, or more generally the expected loss, foragivenfractionofrejecteddatapoints.
Compensatingforclasspriors.
Consider our medical X-ray problem again, and supposethatwehavecollectedalargenumberof X-rayimagesfromthegen- eralpopulationforuseastrainingdatainordertobuildanautomatedscreening system.
Becausecancerisrareamongstthegeneralpopulation, wemightfind that, say, only 1 in every 1,000 examples corresponds to the presence of can- cer.
If we used such a data set to train an adaptive model, we could run into severedifficultiesduetothesmallproportionofthecancerclass.
Forinstance, aclassifierthatassignedeverypointtothenormalclasswouldalreadyachieve 99.9% accuracy and it would be difficult to avoid this trivial solution.
Also, even a large data set will contain very few examples of X-ray images corre- sponding to cancer, and so the learning algorithm will not be exposed to a broadrangeofexamplesofsuchimagesandhenceisnotlikelytogeneralize well.
Abalanceddatasetinwhichwehaveselectedequalnumbersofexam- ples from each of the classes would allow us to find a more accurate model.
However, we then have to compensate for the effects of our modifications to the training data.
Suppose we have used such a modified data set and found modelsfortheposteriorprobabilities.
From Bayes’theorem(1.82), weseethat theposteriorprobabilitiesareproportionaltothepriorprobabilities, whichwe caninterpretasthefractionsofpointsineachclass.
Wecanthereforesimply taketheposteriorprobabilitiesobtainedfromourartificiallybalanceddataset andfirstdividebytheclassfractionsinthatdatasetandthenmultiplybythe classfractionsinthepopulationtowhichwewishtoapplythemodel.
Finally, weneedtonormalizetoensurethatthenewposteriorprobabilitiessumtoone.
Note that this procedure cannot be applied if we have learned a discriminant functiondirectlyinsteadofdeterminingposteriorprobabilities.
Combiningmodels.
Forcomplexapplications, wemaywishtobreaktheproblem intoanumberofsmallersubproblemseachofwhichcanbetackledbyasep- arate module.
For example, in our hypothetical medical diagnosis problem, wemayhaveinformationavailablefrom, say, bloodtestsaswellas X-rayim- ages.
Ratherthancombineallofthisheterogeneousinformationintoonehuge input space, it may be more effective to build one system to interpret the X- rayimagesandadifferentonetointerprettheblooddata.
Aslongaseachof the two models gives posterior probabilities for the classes, we can combine the outputs systematically using the rules of probability.
One simple way to do this is to assume that, for each class separately, the distributions of inputs for the X-ray images, denoted by x I, and the blood data, denoted by x B, are 46 1.
INTRODUCTION independent, sothat p(x I , x B |C k)=p(x I |C k)p(x B |C k).
(1.84) Section8.2 Thisisanexampleofconditionalindependenceproperty, becausetheindepen- denceholdswhenthedistributionisconditionedontheclass C k.
Theposterior probability, givenboththe X-rayandblooddata, isthengivenby p(C k |x I , x B ) ∝ p(x I , x B |C k)p(C k) ∝ p(x I |C k)p(x B |C k)p(C k) ∝ p(C k |x I )p(C k |x B ) (1.85) p(C k) Thusweneedtheclasspriorprobabilitiesp(C k), whichwecaneasilyestimate fromthefractionsofdatapointsineachclass, andthenweneedtonormalize theresultingposteriorprobabilitiessotheysumtoone.
Theparticularcondi- Section8.2.2 tionalindependenceassumption(1.84)isanexampleofthenaive Bayesmodel.
Note that the joint marginal distribution p(x I , x B ) will typically not factorize under this model.
We shall see in later chapters how to construct models for combining data that do not require the conditional independence assumption (1.84).
1.5.5 Loss functions for regression So far, we have discussed decision theory in the context of classification prob- lems.
We now turn to the case of regression problems, such as the curve fitting Section1.1 example discussed earlier.
The decision stage consists of choosing a specific esti- mate y(x) of the value of t for each input x.
Suppose that in doing so, we incur a loss L(t, y(x)).
Theaverage, orexpected, lossisthengivenby E[L]= L(t, y(x))p(x, t)dxdt.
(1.86) Acommonchoiceoflossfunctioninregressionproblemsisthesquaredlossgiven by L(t, y(x))={y(x)−t}2.
Inthiscase, theexpectedlosscanbewritten E[L]= {y(x)−t}2p(x, t)dxdt.
(1.87) Our goal is to choose y(x) so as to minimize E[L].
If we assume a completely Appendix D flexible function y(x), we can do this formally using the calculus of variations to give δE[L] =2 {y(x)−t}p(x, t)dt=0.
(1.88) δy(x) Solvingfory(x), andusingthesumandproductrulesofprobability, weobtain tp(x, t)dt y(x)= = tp(t|x)dt=E t[t|x] (1.89) p(x) 1.5.
Decision Theory 47 Figure1.28 The regression function y(x), whichminimizestheexpected t squared loss, is given by the meanoftheconditionaldistri- y(x) butionp(t|x).
y(x0) p(t|x0) x0 x whichistheconditionalaverageoftconditionedonxandisknownastheregression function.
Thisresultisillustratedin Figure1.28.
Itcanreadilybeextendedtomul- tipletargetvariablesrepresentedbythevectort, inwhichcasetheoptimalsolution Exercise 1.25 istheconditionalaveragey(x)=E t[t|x].
We can also derive this result in a slightly different way, which will also shed light on the nature of the regression problem.
Armed with the knowledge that the optimal solution is the conditional expectation, we can expand the square term as follows {y(x)−t}2 ={y(x)−E[t|x]+E[t|x]−t}2 = {y(x)−E[t|x]}2+2{y(x)−E[t|x]}{E[t|x]−t}+{E[t|x]−t}2 where, tokeepthenotationuncluttered, weuse E[t|x]todenote E t[t|x].
Substituting intothelossfunctionandperformingtheintegralovert, weseethatthecross-term vanishesandweobtainanexpressionforthelossfunctionintheform E[L]= {y(x)−E[t|x]}2 p(x)dx+ {E[t|x]−t}2p(x)dx.
(1.90) Thefunctiony(x)weseektodetermineentersonlyinthefirstterm, whichwillbe minimized when y(x) is equal to E[t|x], in which case this term will vanish.
This issimplytheresultthatwederivedpreviouslyandthatshowsthattheoptimalleast squarespredictorisgivenbytheconditionalmean.
Thesecondtermisthevariance of the distribution of t, averaged over x.
It represents the intrinsic variability of the target data and can be regarded as noise.
Because it is independent of y(x), it representstheirreducibleminimumvalueofthelossfunction.
Aswiththeclassificationproblem, wecaneitherdeterminetheappropriateprob- abilities and then use these to make optimal decisions, or we can build models that makedecisionsdirectly.
Indeed, wecanidentifythreedistinctapproachestosolving regressionproblemsgiven, inorderofdecreasingcomplexity, by: (a) Firstsolvetheinferenceproblemofdeterminingthejointdensityp(x, t).
Then normalizetofindtheconditionaldensityp(t|x), andfinallymarginalizetofind theconditionalmeangivenby(1.89).
48 1.
INTRODUCTION (b) Firstsolvetheinferenceproblemofdeterminingtheconditionaldensityp(t|x), andthensubsequentlymarginalizetofindtheconditionalmeangivenby(1.89).
(c) Findaregressionfunctiony(x)directlyfromthetrainingdata.
Therelativemeritsofthesethreeapproachesfollowthesamelinesasforclassifica- tionproblemsabove.
Thesquaredlossisnottheonlypossiblechoiceoflossfunctionforregression.
Indeed, there are situations in which squared loss can lead to very poor results and where we need to develop more sophisticated approaches.
An important example concerns situations in which the conditional distribution p(t|x) is multimodal, as Section5.6 oftenarisesinthesolutionofinverseproblems.
Hereweconsiderbrieflyonesimple generalization of the squared loss, called the Minkowski loss, whose expectation is givenby E[Lq]= |y(x)−t|q p(x, t)dxdt (1.91) which reduces to the expected squared loss for q = 2.
The function |y − t|q is plottedagainsty−tforvariousvaluesofq in Figure1.29.
Theminimumof E[Lq] is given by the conditional mean for q = 2, the conditional median for q = 1, and Exercise 1.27 theconditionalmodeforq →0.
1.6.
Information Theory Inthischapter, wehavediscussedavarietyofconceptsfromprobabilitytheoryand decisiontheorythatwillformthefoundationsformuchofthesubsequentdiscussion in this book.
We close this chapter by introducing some additional concepts from the field of information theory, which will also prove useful in our development of patternrecognitionandmachinelearningtechniques.
Again, weshallfocusonlyon the key concepts, and we refer the reader elsewhere for more detailed discussions (Viterbiand Omura,1979; Coverand Thomas,1991; Mac Kay,2003).
We begin by considering a discrete random variable x and we ask how much information is received when we observe a specific value for this variable.
The amount of information can be viewed as the ‘degree of surprise’ on learning the valueofx.
If wearetoldthatahighlyimprobable eventhasjustoccurred, wewill have received more information than if we were told that some very likely event has just occurred, and if we knew that the event was certain to happen we would receive no information.
Our measure of information content will therefore depend on the probability distribution p(x), and we therefore look for a quantity h(x) that is a monotonic function of the probability p(x) and that expresses the information content.
The form of h(·) can be found by noting that if we have two events x and y that are unrelated, then the information gain from observing both of them should be the sum of the information gained from each of them separately, so that h(x, y) = h(x)+h(y).
Two unrelated events will be statistically independent and so p(x, y) = p(x)p(y).
From these two relationships, it is easily shown that h(x) Exercise 1.28 mustbegivenbythelogarithmofp(x)andsowehave 1.6.
Information Theory 49 y−t q|t−y| 2 q=0.3 1 0 −2 −1 0 1 2 y−t q|t−y| 2 q=1 1 0 −2 −1 0 1 2 y−t q|t−y| 2 q=2 1 0 −2 −1 0 1 2 y−t q|t−y| 2 q=10 1 0 −2 −1 0 1 2 Figure1.29 Plotsofthequantity L q =|y−t|q forvariousvaluesofq.
h(x)=−log p(x) (1.92) 2 where the negative sign ensures that information is positive or zero.
Note that low probability events x correspond to high information content.
The choice of basis for the logarithm is arbitrary, and for the moment we shall adopt the convention prevalentininformationtheoryofusinglogarithmstothebaseof2.
Inthiscase, as weshallseeshortly, theunitsofh(x)arebits(‘binarydigits’).
Nowsupposethatasenderwishestotransmitthevalueofarandomvariableto a receiver.
The average amount of information that they transmit in the process is obtainedbytakingtheexpectationof(1.92)withrespecttothedistributionp(x)and isgivenby H[x]=− p(x)log p(x).
(1.93) 2 x This important quantity is called the entropy of the random variable x.
Note that limp→0 plnp = 0 and so we shall take p(x)lnp(x) = 0 whenever we encounter a valueforxsuchthatp(x)=0.
Sofarwehavegivenaratherheuristicmotivationforthedefinitionofinforma- 50 1.
INTRODUCTION tion(1.92)andthecorrespondingentropy(1.93).
Wenowshowthatthesedefinitions indeed possess useful properties.
Consider a random variable x having 8 possible states, each of which is equally likely.
In order to communicate the value of x to a receiver, we would need to transmit a message of length 3 bits.
Notice that the entropyofthisvariableisgivenby 1 1 H[x]=−8× log =3bits.
8 2 8 Now consider an example (Cover and Thomas, 1991) of a variable having 8 pos- sible states {a, b, c, d, e, f, g, h} for which the respective probabilities are given by (1,1,1, 1 , 1 , 1 , 1 , 1 ).
Theentropyinthiscaseisgivenby 2 4 8 16 64 64 64 64 1 1 1 1 1 1 1 1 4 1 H[x]=− log − log − log − log − log =2bits.
2 2 2 2 2 2 2 4 4 8 8 16 16 64 64 Weseethatthenonuniformdistributionhasasmallerentropythantheuniformone, andweshallgainsomeinsightintothisshortlywhenwediscusstheinterpretationof entropyintermsofdisorder.
Forthemoment, letusconsiderhowwewouldtransmit the identity of the variable’s state to a receiver.
We could do this, as before, using a3-bitnumber.
However, wecantakeadvantageofthenonuniformdistributionby usingshortercodesforthemoreprobableevents, attheexpenseoflongercodesfor the less probable events, in the hope of getting a shorter average code length.
This can be done by representing the states {a, b, c, d, e, f, g, h} using, for instance, the following set of code strings: 0, 10, 110, 1110, 111100, 111101, 111110, 111111.
Theaveragelengthofthecodethathastobetransmittedisthen 1 1 1 1 1 averagecodelength= ×1+ ×2+ ×3+ ×4+4× ×6=2bits 2 4 8 16 64 whichagainisthesameastheentropyoftherandomvariable.
Notethatshortercode strings cannot be used because it must be possible to disambiguate aconcatenation of such strings into its component parts.
For instance, 11001110 decodes uniquely intothestatesequencec, a, d.
Thisrelationbetweenentropyandshortestcodinglengthisageneralone.
The noiseless coding theorem (Shannon, 1948) states that the entropy is a lower bound onthenumberofbitsneededtotransmitthestateofarandomvariable.
From now on, we shall switch to the use of natural logarithms in defining en- tropy, asthiswillprovideamoreconvenientlinkwithideaselsewhereinthisbook.
In this case, the entropy is measured in units of ‘nats’ instead of bits, which differ simplybyafactorofln2.
We have introduced the concept of entropy in terms of the average amount of informationneededtospecifythestateofarandomvariable.
Infact, theconceptof entropy has much earlier origins in physics where it was introduced in the context ofequilibriumthermodynamicsandlatergivenadeeperinterpretationasameasure of disorder through developments in statistical mechanics.
We can understand this alternativeviewofentropybyconsideringasetof N identicalobjectsthataretobe dividedamongstasetofbins, suchthatthereareni objectsintheith bin.
Consider 1.6.
Information Theory 51 the number of different ways of allocating the objects to the bins.
There are N ways to choose the first object, (N − 1) ways to choose the second object, and so on, leading to a total of N! ways to allocate all N objects to the bins, where N! (pronounced‘factorial N’)denotestheproduct N×(N−1)×···×2×1.
However, wedon’twishtodistinguishbetweenrearrangementsofobjectswithineachbin.
In the ith bin there are ni! ways of reordering the objects, and so the total number of waysofallocatingthe N objectstothebinsisgivenby N! W = (1.94) i ni! whichiscalledthemultiplicity.
Theentropyisthendefinedasthelogarithmofthe multiplicityscaledbyanappropriateconstant 1 1 1 H= ln W = ln N!− lnni!.
(1.95) N N N i Wenowconsiderthelimit N →∞, inwhichthefractionsni/N areheldfixed, and apply Stirling’sapproximation ln N! Nln N −N (1.96) whichgives H=− lim ni ln ni =− pilnpi (1.97) N→∞ N N i i where we have used i ni = N.
Here pi = lim N→∞(ni/N) is the probability of an object being assigned to the ith bin.
In physics terminology, the specific ar- rangementsofobjectsinthebinsiscalledamicrostate, andtheoveralldistribution of occupation numbers, expressed through the ratios ni/N, is called a macrostate.
Themultiplicity W isalsoknownastheweightofthemacrostate.
Wecaninterpretthebinsasthestatesxiofadiscreterandomvariable X, where p(X =xi)=pi.
Theentropyoftherandomvariable X isthen H[p]=− p(xi)lnp(xi).
(1.98) i Distributionsp(xi)thataresharplypeakedaroundafewvalueswillhavearelatively low entropy, whereas those that are spread more evenly across many values will havehigherentropy, asillustratedin Figure1.30.
Because0 pi 1, theentropy is nonnegative, and it will equal its minimum value of 0 when one of the pi = 1 and all other pj= i = 0.
The maximum entropy configuration can be found by Appendix E maximizing H using a Lagrange multiplier to enforce the normalization constraint ontheprobabilities.
Thuswemaximize H =− p(xi)lnp(xi)+λ p(xi)−1 (1.99) i i 52 1.
INTRODUCTION seitilibaborp 0.5 H=1.77 0.25 0 seitilibaborp 0.5 H=3.09 0.25 0 Figure1.30 Histogramsoftwoprobabilitydistributionsover30binsillustratingthehighervalueoftheentropy Hforthebroaderdistribution.
Thelargestentropywouldarisefromauniformdistributionthatwouldgive H = −ln(1/30)=3.40.
from which we find that all of the p(xi) are equal and are given by p(xi) = 1/M where M is the total number of states xi.
The corresponding value of the entropy is then H = ln M.
This result can also be derived from Jensen’s inequality (to be Exercise 1.29 discussedshortly).
Toverifythatthestationarypointisindeedamaximum, wecan evaluatethesecondderivativeoftheentropy, whichgives ∂H 1 =−Iij (1.100) ∂p(xi)∂p(xj) pi where Iij aretheelementsoftheidentitymatrix.
We can extend the definition of entropy to include distributionsp(x) over con- tinuousvariablesxasfollows.
Firstdividexintobinsofwidth∆.
Then, assuming p(x)iscontinuous, themeanvaluetheorem(Weisstein,1999)tellsusthat, foreach suchbin, theremustexistavaluexi suchthat (i+1)∆ p(x)dx=p(xi)∆.
(1.101) i∆ Wecannowquantizethecontinuousvariablexbyassigninganyvaluextothevalue xi wheneverxfallsintheith bin.
Theprobabilityofobservingthevaluexi isthen p(xi)∆.
Thisgivesadiscretedistributionforwhichtheentropytakestheform H ∆ =− p(xi)∆ln(p(xi)∆)=− p(xi)∆lnp(xi)−ln∆ (1.102) i i where we have used i p(xi)∆ = 1, which follows from (1.101).
We now omit thesecondterm−ln∆ontheright-handsideof(1.102)andthenconsiderthelimit 1.6.
Information Theory 53 ∆→0.
Thefirsttermontheright-handsideof(1.102)willapproachtheintegralof p(x)lnp(x)inthislimitsothat lim p(xi)∆lnp(xi) =− p(x)lnp(x)dx (1.103) ∆→0 i where the quantity on the right-hand side is called the differential entropy.
We see thatthediscreteandcontinuousformsoftheentropydifferbyaquantityln∆, which diverges in the limit ∆ → 0.
This reflects the fact that to specify a continuous variable very precisely requires a large number of bits.
For a density defined over multiple continuous variables, denoted collectively by the vector x, the differential entropyisgivenby H[x]=− p(x)lnp(x)dx.
(1.104) In the case of discrete distributions, we saw that the maximum entropy con- figuration corresponded to an equal distribution of probabilities across the possible states of the variable.
Let us now consider the maximum entropy configuration for acontinuousvariable.
Inorderforthismaximumtobewelldefined, itwillbenec- essary to constrain the first and second moments of p(x) as well as preserving the normalization constraint.
We therefore maximize the differential entropy with the Ludwig Boltzmann dynamics, which states that the entropy of a closed 1844–1906 system tends to increase with time.
By contrast, at the microscopic level the classical Newtonian equa- Ludwig Eduard Boltzmann was an tions of physics are reversible, and so they found it Austrian physicist who created the difficult to see how the latter could explain the for- field of statistical mechanics.
Prior mer.
They didn’t fully appreciate Boltzmann’s argu- to Boltzmann, the concept of en- ments, whichwerestatisticalinnatureandwhichcon- tropy was already known from cluded not that entropy could never decrease over classical thermodynamics where it time but simply that with overwhelming probability it quantifies the fact that when we take energy from a wouldgenerallyincrease.
Boltzmannevenhadalong- system, not all of that energy is typically available runningdisputewiththeeditoroftheleading German to do useful work.
Boltzmann showed that the ther- physics journal who refused to let him refer to atoms modynamic entropy S, a macroscopic quantity, could andmoleculesasanythingotherthanconvenientthe- be related to the statistical properties at the micro- oreticalconstructs.
Thecontinuedattacksonhiswork scopic level.
This is expressed through the famous lead to bouts of depression, and eventually he com- equation S = kln W in which W represents the mitted suicide.
Shortly after Boltzmann’s death, new number of possible microstates in a macrostate, and experiments by Perrin on colloidal suspensions veri- k 1.38 × 10−23 (in units of Joules per Kelvin) is fiedhistheoriesandconfirmedthevalueofthe Boltz- known as Boltzmann’s constant.
Boltzmann’s ideas mannconstant.
Theequation S =kln W iscarvedon weredisputedbymanyscientistsoftheyday.
Onedif- Boltzmann’stombstone.
ficultytheysawarosefromthesecondlawofthermo- 54 1.
INTRODUCTION threeconstraints ∞ p(x)dx = 1 (1.105) −∞ ∞ xp(x)dx = µ (1.106) −∞ ∞ (x−µ)2p(x)dx = σ2.
(1.107) −∞ Appendix E Theconstrainedmaximizationcanbeperformedusing Lagrangemultiplierssothat wemaximizethefollowingfunctionalwithrespecttop(x) ∞ ∞ − p(x)lnp(x)dx+λ p(x)dx−1 1 − ∞ − ∞ ∞ ∞ +λ xp(x)dx−µ +λ (x−µ)2p(x)dx−σ2 .
2 3 −∞ −∞ Appendix D Usingthecalculusofvariations, wesetthederivativeofthisfunctionaltozerogiving p(x)=exp −1+λ 1 +λ 2 x+λ 3 (x−µ)2 .
(1.108) The Lagrange multipliers can be found by back substitution of this result into the Exercise 1.34 threeconstraintequations, leadingfinallytotheresult 1 (x−µ)2 p(x)= exp − (1.109) (2πσ2)1/2 2σ2 andsothedistributionthatmaximizesthedifferentialentropyisthe Gaussian.
Note thatwedidnotconstrainthedistributiontobenonnegativewhenwemaximizedthe entropy.
However, because the resulting distribution is indeed nonnegative, we see withhindsightthatsuchaconstraintisnotnecessary.
Exercise 1.35 Ifweevaluatethedifferentialentropyofthe Gaussian, weobtain 1 H[x]= 1+ln(2πσ2) .
(1.110) 2 Thus we see again that the entropy increases as the distribution becomes broader, i.
e., as σ2 increases.
This result also shows that the differential entropy, unlike the discreteentropy, canbenegative, because H(x)<0in(1.110)forσ2 <1/(2πe).
Supposewehaveajointdistributionp(x, y)fromwhichwedrawpairsofvalues ofxandy.
Ifavalueofxisalreadyknown, thentheadditionalinformationneeded to specify the corresponding value of y is given by −lnp(y|x).
Thus the average additionalinformationneededtospecifyycanbewrittenas H[y|x]=− p(y, x)lnp(y|x)dydx (1.111) 1.6.
Information Theory 55 which is called the conditional entropy of y given x.
It is easily seen, using the Exercise 1.37 productrule, thattheconditionalentropysatisfiestherelation H[x, y]=H[y|x]+H[x] (1.112) where H[x, y] is the differential entropy of p(x, y) and H[x] is the differential en- tropy of the marginal distribution p(x).
Thus the information needed to describe x and y is given by the sum of the information needed to describe x alone plus the additionalinformationrequiredtospecifyygivenx.
1.6.1 Relative entropy and mutual information Sofarinthissection, wehaveintroducedanumberofconceptsfrominformation theory, including the key notion of entropy.
We now start to relate these ideas to pattern recognition.
Consider some unknown distribution p(x), and suppose that we have modelled this using an approximating distributionq(x).
If we use q(x) to constructacodingschemeforthepurposeoftransmittingvaluesofxtoareceiver, then the average additional amount of information (in nats) required to specify the valueofx(assumingwechooseanefficientcodingscheme)asaresultofusingq(x) insteadofthetruedistributionp(x)isgivenby KL(p q) = − p(x)lnq(x)dx− − p(x)lnp(x)dx q(x) = − p(x)ln dx.
(1.113) p(x) This is known as the relative entropy or Kullback-Leibler divergence, or KL diver- gence(Kullbackand Leibler,1951), betweenthedistributionsp(x)andq(x).
Note thatitisnotasymmetricalquantity, thatistosay KL(p q) ≡KL(q p).
Wenowshowthatthe Kullback-Leiblerdivergencesatisfies KL(p q) 0with equality if, and only if, p(x) = q(x).
To do this we first introduce the concept of convex functions.
A function f(x) is said to be convex if it has the property that everychordliesonorabovethefunction, asshownin Figure1.31.
Anyvalueofx intheintervalfromx = atox = bcanbewrittenintheformλa+(1−λ)bwhere 0 λ 1.
Thecorrespondingpointonthechordisgivenbyλf(a)+(1−λ)f(b), Claude Shannon ory.
Thispaperintroducedtheword‘bit’, andhiscon- 1916–2001 ceptthatinformationcouldbesentasastreamof1s and 0s paved the way for the communications revo- Aftergraduatingfrom Michiganand lution.
It is said that von Neumann recommended to MIT, Shannonjoinedthe AT&TBell Shannon that he use the term entropy, not only be- Telephonelaboratoriesin1941.
His cause of its similarity to the quantity used in physics, paper ‘A Mathematical Theory of but also because “nobody knows what entropy really Communication’ published in the is, soinanydiscussionyouwillalwayshaveanadvan- Bell System Technical Journal in tage”.
1948laidthefoundationsformoderninformationthe- 56 1.
INTRODUCTION Figure1.31 Aconvexfunctionf(x)isoneforwhichev- erychord(showninblue)liesonorabove f(x) thefunction(showninred).
chord a xxλλ b x and the corresponding value of the function is f(λa+(1−λ)b).
Convexity then implies f(λa+(1−λ)b) λf(a)+(1−λ)f(b).
(1.114) This is equivalent to the requirement that the second derivative of the function be Exercise 1.36 everywherepositive.
Examplesofconvexfunctionsarexlnx(forx>0)andx2.
A functioniscalledstrictlyconvexiftheequalityissatisfiedonlyforλ=0andλ=1.
Ifafunctionhastheoppositeproperty, namelythateverychordliesonorbelowthe function, itiscalledconcave, withacorrespondingdefinitionforstrictlyconcave.
If afunctionf(x)isconvex, then−f(x)willbeconcave.
Exercise 1.38 Using the technique of proof by induction, we can show from (1.114) that a convexfunctionf(x)satisfies M M f λixi λif(xi) (1.115) i=1 i=1 where λi 0 and i λi = 1, for any set of points {xi }.
The result (1.115) is known as Jensen’s inequality.
If we interpret the λi as the probability distribution overadiscretevariablextakingthevalues{xi }, then(1.115)canbewritten f(E[x]) E[f(x)] (1.116) where E[·] denotes the expectation.
For continuous variables, Jensen’s inequality takestheform f xp(x)dx f(x)p(x)dx.
(1.117) We can apply Jensen’s inequality in the form (1.117) to the Kullback-Leibler divergence(1.113)togive q(x) KL(p q)=− p(x)ln dx −ln q(x)dx=0 (1.118) p(x) 1.6.
Information Theory 57 wherewehaveusedth efactthat−lnxisaconvexfunction, togetherwiththenor- malization condition q(x)dx = 1.
In fact, −lnx is a strictly convex function, so the equality will hold if, and only if, q(x) = p(x) for all x.
Thus we can in- terpretthe Kullback-Leiblerdivergenceasameasureofthedissimilarityofthetwo distributionsp(x)andq(x).
Weseethatthereisanintimaterelationshipbetweendatacompressionandden- sityestimation(i.
e., theproblemofmodellinganunknownprobabilitydistribution) because the most efficient compression is achieved when we know the true distri- bution.
If we use a distribution that is different from the true one, then we must necessarily have a less efficient coding, and on average the additional information that must be transmitted is (at least) equal to the Kullback-Leibler divergence be- tweenthetwodistributions.
Supposethatdataisbeinggeneratedfromanunknowndistributionp(x)thatwe wish to model.
We can try to approximate this distribution using some parametric distribution q(x|θ), governed by a set of adjustable parameters θ, for example a multivariate Gaussian.
Onewaytodetermineθistominimizethe Kullback-Leibler divergence between p(x) and q(x|θ) with respect to θ.
We cannot do this directly becausewedon’tknowp(x).
Suppose, however, thatwehaveobservedafiniteset of training points xn, for n = 1,..., N, drawn from p(x).
Then the expectation with respect to p(x) can be approximated by a finite sum over these points, using (1.35), sothat N KL(p q) {−lnq(xn |θ)+lnp(xn)}.
(1.119) n=1 Thesecondtermontheright-handsideof(1.119)isindependentofθ, andthefirst termisthenegativeloglikelihoodfunctionforθunderthedistributionq(x|θ)eval- uated using the training set.
Thus we see that minimizing this Kullback-Leibler divergenceisequivalenttomaximizingthelikelihoodfunction.
Nowconsiderthejointdistributionbetweentwosetsofvariablesxandygiven byp(x, y).
Ifthesetsofvariablesareindependent, thentheirjointdistributionwill factorizeintotheproductoftheirmarginalsp(x, y)=p(x)p(y).
Ifthevariablesare notindependent, wecangainsomeideaofwhethertheyare‘close’tobeingindepen- dent by considering the Kullback-Leibler divergence between the joint distribution andtheproductofthemarginals, givenby I[x, y] ≡ KL(p(x, y) p(x)p(y)) p(x)p(y) = − p(x, y)ln dxdy (1.120) p(x, y) which is called the mutual information between the variables x and y.
From the properties of the Kullback-Leibler divergence, we see that I(x, y) 0 with equal- ity if, and only if, x and y are independent.
Using the sum and product rules of probability, weseethatthemutualinformationisrelatedtotheconditionalentropy Exercise 1.41 through I[x, y]=H[x]−H[x|y]=H[y]−H[y|x].
(1.121) 58 1.
INTRODUCTION Thuswecanviewthemutualinformationasthereductionintheuncertaintyaboutx byvirtueofbeingtoldthevalueofy (orviceversa).
Froma Bayesianperspective, wecanviewp(x)asthepriordistributionforxandp(x|y)astheposteriordistribu- tionafterwehaveobservednewdatay.
Themutualinformationthereforerepresents thereductioninuncertaintyaboutxasaconsequenceofthenewobservationy.
Exercises 1.1 ( ) www Consider the sum-of-squares error function given by (1.2) in which the function y(x, w) is given by the polynomial (1.1).
Show that the coefficients w ={wi }thatminimizethiserrorfunctionaregivenbythesolutiontothefollowing setoflinearequations M Aijwj =Ti (1.122) j=0 where N N Aij = (xn) i+j , Ti = (xn) i tn.
(1.123) n=1 n=1 Hereasuffixiorj denotestheindexofacomponent, whereas(x)i denotesxraised tothepowerofi.
1.2 ( ) Writedownthesetofcoupledlinearequations, analogousto(1.122), satisfied bythecoefficientswiwhichminimizetheregularizedsum-of-squareserrorfunction givenby(1.4).
1.3 ( ) Suppose that we have three coloured boxes r (red), b (blue), and g (green).
Box r contains 3 apples, 4 oranges, and 3 limes, box b contains 1 apple, 1 orange, and0limes, andboxg contains3apples,3oranges, and4limes.
Ifaboxischosen at random with probabilities p(r) = 0.2, p(b) = 0.2, p(g) = 0.6, and a piece of fruitisremovedfromthebox(withequalprobabilityofselectinganyoftheitemsin the box), then what is the probability of selecting an apple? If we observe that the selectedfruitisinfactanorange, whatistheprobabilitythatitcamefromthegreen box? 1.4 ( ) www Consider a probability densitypx(x) defined over a continuous vari- able x, and suppose that we make a nonlinear change of variable using x = g(y), so that the density transforms according to (1.27).
By differentiating (1.27), show thatthelocation yofthemaximumofthedensityinyisnotingeneralrelatedtothe location x of the maximum of the density over x by the simple functional relation x = g( y) as a consequence of the Jacobian factor.
This shows that the maximum ofaprobabilitydensity(incontrasttoasimplefunction)isdependentonthechoice of variable.
Verify that, in the case of a linear transformation, the location of the maximumtransformsinthesamewayasthevariableitself.
1.5 ( ) Usingthedefinition(1.38)showthatvar[f(x)]satisfies(1.39).
Exercises 59 1.6 ( ) Show that if two variables x and y are independent, then their covariance is zero.
1.7 ( ) www In this exercise, we prove the normalization condition (1.48) for the univariate Gaussian.
Todothisconsider, theintegral ∞ 1 I = exp − x2 dx (1.124) 2σ2 −∞ whichwecanevaluatebyfirstwritingitssquareintheform ∞ ∞ 1 1 I2 = exp − x2− y2 dxdy.
(1.125) 2σ2 2σ2 −∞ −∞ Nowmakethetransformationfrom Cartesiancoordinates(x, y)topolarcoordinates (r,θ)andthensubstituteu = r2.
Showthat, byperformingtheintegralsoverθ and u, andthentakingthesquarerootofbothsides, weobtain I = 2πσ2 1/2 .
(1.126) Finally, usethisresulttoshowthatthe Gaussiandistribution N(x|µ,σ2)isnormal- ized.
1.8 ( ) www By using a change of variables, verify that the univariate Gaussian distributiongivenby(1.46)satisfies(1.49).
Next, bydifferentiatingbothsidesofthe normalizationcondition ∞ N x|µ,σ2 dx=1 (1.127) −∞ withrespecttoσ2, verifythatthe Gaussiansatisfies(1.50).
Finally, showthat(1.51) holds.
1.9 ( ) www Show that the mode (i.
e.
the maximum) of the Gaussian distribution (1.46) is given by µ.
Similarly, show that the mode of the multivariate Gaussian (1.52)isgivenbyµ.
1.10 ( ) www Suppose that the two variables x and z are statistically independent.
Showthatthemeanandvarianceoftheirsumsatisfies E[x+z] = E[x]+E[z] (1.128) var[x+z] = var[x]+var[z].
(1.129) 1.11 ( ) Bysettingthederivativesoftheloglikelihoodfunction(1.54)withrespecttoµ andσ2 equaltozero, verifytheresults(1.55)and(1.56).
60 1.
INTRODUCTION 1.12 ( ) www Usingtheresults(1.49)and(1.50), showthat E[xnxm]=µ2+Inmσ2 (1.130) wherexnandxmdenotedatapointssampledfroma Gaussiandistributionwithmean µ and variance σ2, and Inm satisfies Inm = 1 if n = m and Inm = 0 otherwise.
Henceprovetheresults(1.57)and(1.58).
1.13 ( ) Supposethatthevarianceofa Gaussianisestimatedusingtheresult(1.56)but with the maximum likelihood estimate µ ML replaced with the true value µ of the mean.
Show that this estimator has the property that its expectation is given by the truevarianceσ2.
1.14 ( ) Show that an arbitrary square matrix with elements wij can be written in the form wij = w i S j + w i A j where w i S j and w i A j are symmetric and anti-symmetric matrices, respectively, satisfying w S = w S and w A = −w A for all i and j.
Now ij ji ij ji considerthesecondorderterminahigherorderpolynomialin Ddimensions, given by D D wijxixj.
(1.131) i=1 j=1 Showthat D D D D wijxixj = w i S j xixj (1.132) i=1 j=1 i=1 j=1 so that the contribution from the anti-symmetric matrix vanishes.
We therefore see that, without loss of generality, the matrix of coefficients wij can be chosen to be symmetric, andsonotallofthe D2 elementsofthismatrixcanbechosenindepen- dently.
Show thatthenumber of independent parameters inthematrixw S isgiven ij by D(D+1)/2.
1.15 ( ) www Inthisexerciseandthenext, weexplorehowthenumberofindepen- dentparametersinapolynomialgrowswiththeorder M ofthepolynomialandwith the dimensionality D of the input space.
We start by writing down the Mth order termforapolynomialin Ddimensionsintheform D D D ··· wi i ···i xi xi ···xi .
(1.133) 1 2 M 1 2 M i 1 =1i 2 =1 i M=1 The coefficients wi i ···i comprise DM elements, but the number of independent 1 2 M parameters is significantly fewer due to the many interchange symmetries of the factorxi xi ···xi .
Beginbyshowingthattheredundancyinthecoefficientscan 1 2 M beremovedbyrewritingthis Mth ordertermintheform D i 1 i M−1 ··· w i i ···i xi xi ···xi .
(1.134) 1 2 M 1 2 M i 1 =1i 2 =1 i M=1 Exercises 61 Notethatthepreciserelationshipbetweenthew coefficientsandwcoefficientsneed notbemadeexplicit.
Usethisresulttoshowthatthenumberofindependentparam- etersn(D, M), whichappearatorder M, satisfiesthefollowingrecursionrelation D n(D, M)= n(i, M −1).
(1.135) i=1 Nextuseproofbyinductiontoshowthatthefollowingresultholds D (i+M −2)! (D+M −1)! = (1.136) (i−1)!(M −1)! (D−1)! M! i=1 whichcanbedonebyfirstprovingtheresultfor D = 1andarbitrary M bymaking use of the result 0! = 1, then assuming it is correct for dimension D and verifying thatitiscorrectfordimension D+1.
Finally, usethetwopreviousresults, together withproofbyinduction, toshow (D+M −1)! n(D, M)= .
(1.137) (D−1)! M! To do this, first show that the result is true for M = 2, and any value of D 1, bycomparisonwiththeresultof Exercise 1.14.
Thenmakeuseof(1.135), together with(1.136), toshowthat, iftheresultholdsatorder M−1, thenitwillalsoholdat order M 1.16 ( ) In Exercise 1.15, weprovedtheresult(1.135)forthenumberofindependent parametersinthe Mth ordertermofa D-dimensionalpolynomial.
Wenowfindan expression for the total number N(D, M) of independent parameters in all of the termsuptoandincludingthe M6thorder.
Firstshowthat N(D, M)satisfies M N(D, M)= n(D, m) (1.138) m=0 where n(D, m) is the number of independent parameters in the term of order m.
Nowmakeuseoftheresult(1.137), togetherwithproofbyinduction, toshowthat (D+M)! N(d, M)= .
(1.139) D! M! This can be done by first proving that the result holds for M = 0 and arbitrary D 1, then assuming that it holds at order M, and hence showing that it holds at order M +1.
Finally, makeuseof Stirling’sapproximationintheform n! n n e −n (1.140) for large n to show that, for D M, the quantity N(D, M) grows like DM, and for M D it grows like MD.
Consider a cubic (M = 3) polynomial in D dimensions, and evaluate numerically the total number of independent parameters for (i) D = 10 and (ii) D = 100, which correspond to typical small-scale and medium-scalemachinelearningapplications.
62 1.
INTRODUCTION 1.17 ( ) www Thegammafunctionisdefinedby ∞ Γ(x)≡ u x−1e −u du.
(1.141) 0 Using integration by parts, prove the relation Γ(x+1) = xΓ(x).
Show also that Γ(1)=1andhencethatΓ(x+1)=x! whenxisaninteger.
1.18 ( ) www We can use the result (1.126) to derive an expression for the surface area SD, andthevolume VD, ofasphereofunitradiusin Ddimensions.
Todothis, consider the following result, which is obtained by transforming from Cartesian to polarcoordinates D ∞ ∞ e −x2 i dxi =SD e −r2 r D−1dr.
(1.142) i=1 −∞ 0 Usingthedefinition(1.141)ofthe Gammafunction, togetherwith(1.126), evaluate bothsidesofthisequation, andhenceshowthat 2πD/2 SD = .
(1.143) Γ(D/2) Next, byintegratingwithrespecttoradiusfrom0to1, showthatthevolumeofthe unitspherein Ddimensionsisgivenby SD VD = .
(1.144) D √ Finally, use the results Γ(1) = 1 and Γ(3/2) = π/2 to show that (1.143) and (1.144)reducetotheusualexpressionsfor D =2and D =3.
1.19 ( ) Consider a sphere of radius a in D-dimensions together with the concentric hypercubeofside2a, sothatthespheretouchesthehypercubeatthecentresofeach ofitssides.
Byusingtheresultsof Exercise 1.18, showthattheratioofthevolume ofthespheretothevolumeofthecubeisgivenby volumeofsphere πD/2 = .
(1.145) volumeofcube D2D−1Γ(D/2) Nowmakeuseof Stirling’sformulaintheform Γ(x+1) (2π)1/2e −x x x+1/2 (1.146) which is valid for x 1, to show that, as D → ∞, the ratio (1.145) goes to zero.
Show also that the ratio of the distance from the centre of the hypercub√e to one of thecorners, dividedbytheperpendiculardistancetooneofthesides, is D, which thereforegoesto∞as D → ∞.
Fromtheseresultsweseethat, inaspaceofhigh dimensionality, mostofthevolumeofacubeisconcentratedinthelargenumberof corners, whichthemselvesbecomeverylong‘spikes’! Exercises 63 1.20 ( ) www Inthisexercise, weexplorethebehaviourofthe Gaussiandistribution inhigh-dimensionalspaces.
Considera Gaussiandistributionin Ddimensionsgiven by 1 x 2 p(x)= exp − .
(1.147) (2πσ2)D/2 2σ2 Wewishtofindthedensitywithrespecttoradiusinpolarcoordinatesinwhichthe direction variables have been integrated out.
To do this, show that the integral of theprobabilitydensityoverathinshellofradiusr andthickness , where 1, is givenbyp(r) where p(r)= SDr D−1 exp − r2 (1.148) (2πσ2)D/2 2σ2 where SDisthesurfaceareaofaunitspherein Ddimensions.√Showthatthefunction p(r)hasasinglestationarypointlocated, forlarge D, at r Dσ.
Byconsidering p( r+ )where r, showthatforlarge D, 3 2 p( r+ )=p( r)exp − (1.149) 2σ2 whichshowsthat risamaximumoftheradialprobabilitydensityandalsothatp(r) decays exponentially away from its maximum at r with length scale σ.
We have already seen that σ r for large D, and so we see that most of the probability massisconcentratedinathinshellatlargeradius.
Finally, showthattheprobability density p(x) is larger at the origin than at the radius r by a factor of exp(D/2).
We therefore see that most of the probability mass in a high-dimensional Gaussian distributionislocatedatadifferentradiusfromtheregionofhighprobabilitydensity.
This property of distributions in spaces of high dimensionality will have important consequences when we consider Bayesian inference of model parameters in later chapters.
1.21 ( ) Consider two nonnegative numbers a and b, and show that, if a b, then a (ab)1/2.
Use this result to show that, if the decision regions of a two-class classification problem are chosen to minimize the probability of misclassification, thisprobabilitywillsatisfy p(mistake) {p(x, C 1 )p(x, C 2 )}1/2 dx.
(1.150) 1.22 ( ) www Givenalossmatrixwithelements Lkj, theexpectedriskisminimized if, for each x, we choose the class that minimizes (1.81).
Verify that, when the loss matrix is given by Lkj = 1−Ikj, where Ikj are the elements of the identity matrix, thisreducestothecriterionofchoosingtheclasshavingthelargestposterior probability.
Whatistheinterpretationofthisformoflossmatrix? 1.23 ( ) Derive the criterion for minimizing the expected loss when there is a general lossmatrixandgeneralpriorprobabilitiesfortheclasses.
64 1.
INTRODUCTION 1.24 ( ) www Consider a classification problem in which the loss incurred when an input vector from class C k is classified as belonging to class C j is given by the loss matrix Lkj, and for which the loss incurred in selecting the reject option is λ.
Findthedecisioncriterionthatwillgivetheminimumexpectedloss.
Verifythatthis reducestotherejectcriteriondiscussedin Section1.5.3whenthelossmatrixisgiven by Lkj =1−Ikj.
Whatistherelationshipbetweenλandtherejectionthresholdθ? 1.25 ( ) www Consider the generalization of the squared loss function (1.87) for a singletargetvariablettothecaseofmultipletargetvariablesdescribedbythevector tgivenby E[L(t, y(x))]= y(x)−t 2p(x, t)dxdt.
(1.151) Usingthecalculusofvariations, showthatthefunctiony(x)forwhichthisexpected lossisminimizedisgivenbyy(x)=E t [t|x].
Showthatthisresultreducesto(1.89) forthecaseofasingletargetvariablet.
1.26 ( ) By expansion of the square in (1.151), derive a result analogous to (1.90) and henceshowthatthefunctiony(x)thatminimizestheexpectedsquaredlossforthe caseofavectortoftargetvariablesisagaingivenbytheconditionalexpectationof t.
1.27 ( ) www Considertheexpectedlossforregressionproblemsunderthe Lq loss function given by (1.91).
Write down the condition that y(x) must satisfy in order to minimize E[Lq].
Show that, for q = 1, this solution represents the conditional median, i.
e., the function y(x) such that the probability mass for t < y(x) is the same as for t y(x).
Also show that the minimum expected Lq loss for q → 0 is givenbytheconditionalmode, i.
e., bythefunctiony(x)equaltothevalueoftthat maximizesp(t|x)foreachx.
1.28 ( ) In Section1.6, weintroducedtheideaofentropyh(x)astheinformationgained on observing the value of a random variable x having distribution p(x).
We saw that, for independent variables x and y for which p(x, y) = p(x)p(y), the entropy functionsareadditive, sothath(x, y)=h(x)+h(y).
Inthisexercise, wederivethe relation between h and p in the form of a function h(p).
First show that h(p2) = 2h(p), and hence by induction that h(pn) = nh(p) where n is a positive integer.
Hence show that h(pn/m) = (n/m)h(p) where m is also a positive integer.
This implies that h(px) = xh(p) where x is a positive rational number, and hence by continuity when it is a positive real number.
Finally, show that this implies h(p) musttaketheformh(p)∝lnp.
1.29 ( ) www Consider an M-state discrete random variable x, and use Jensen’s in- equalityintheform(1.115)toshowthattheentropyofitsdistributionp(x)satisfies H[x] ln M.
1.30 ( ) Evaluate the Kullback-Leibler divergence (1.113) between two Gaussians p(x)=N(x|µ,σ2)andq(x)=N(x|m, s2).
Exercises 65 Table1.3 Thejointdistributionp(x, y)fortwobinaryvariables y xandyusedin Exercise 1.39.
0 1 0 1/3 1/3 x 1 0 1/3 1.31 ( ) www Considertwovariablesxandyhavingjointdistributionp(x, y).
Show thatthedifferentialentropyofthispairofvariablessatisfies H[x, y] H[x]+H[y] (1.152) withequalityif, andonlyif, xandyarestatisticallyindependent.
1.32 ( ) Consider a vector x of continuous variables with distribution p(x) and corre- sponding entropy H[x].
Suppose that we make a nonsingular linear transformation ofxtoobtainanewvariabley =Ax.
Showthatthecorrespondingentropyisgiven by H[y]=H[x]+ln|A|where|A|denotesthedeterminantof A.
1.33 ( ) Suppose that the conditional entropy H[y|x] between two discrete random variables x and y is zero.
Show that, for all values of x such that p(x) > 0, the variabley mustbeafunctionofx, inotherwordsforeachxthereisonlyonevalue ofysuchthatp(y|x) =0.
1.34 ( ) www Usethecalculusofvariationstoshowthatthestationarypointofthe and(1.107)toeliminatethe Lagrangemultipliersandhenceshowthatthemaximum entropysolutionisgivenbythe Gaussian(1.109).
1.35 ( ) www Use the results (1.106) and (1.107) to show that the entropy of the univariate Gaussian(1.109)isgivenby(1.110).
1.36 ( ) A strictly convex function is defined as one for which every chord lies above thefunction.
Showthatthisisequivalenttotheconditionthatthesecondderivative ofthefunctionbepositive.
1.37 ( ) Usingthedefinition(1.111)togetherwiththeproductruleofprobability, prove theresult(1.112).
1.38 ( ) www Usingproofbyinduction, showthattheinequality(1.114)forconvex functionsimpliestheresult(1.115).
1.39 ( ) Considertwobinaryvariablesxandy havingthejointdistributiongivenin Table1.3.
Evaluatethefollowingquantities (a) H[x] (c) H[y|x] (e) H[x, y] (b) H[y] (d) H[x|y] (f) I[x, y].
Drawadiagramtoshowtherelationshipbetweenthesevariousquantities.
66 1.
INTRODUCTION 1.40 ( ) Byapplying Jensen’sinequality(1.115)withf(x) = lnx, showthatthearith- meticmeanofasetofrealnumbersisneverlessthantheirgeometricalmean.
1.41 ( ) www Using the sum and product rules of probability, show that the mutual information I(x, y)satisfiestherelation(1.121).
2 Probability Distributions In Chapter 1, we emphasized the central role played by probability theory in the solution of pattern recognition problems.
We turn now to an exploration of some particular examples of probability distributions and their properties.
Aswell as be- ing of great interest in their own right, these distributions can form building blocks for more complex models and will be used extensively throughout the book.
The distributions introduced in this chapter will also serve another important purpose, namely to provide us with the opportunity to discuss some key statistical concepts, such as Bayesian inference, in the context of simple models before we encounter theminmorecomplexsituationsinlaterchapters.
One role for the distributions discussed in this chapter is to model the prob- ability distribution p(x) of a random variable x, given a finite set x 1 ,..., x N of observations.
This problem is known as density estimation.
For the purposes of this chapter, we shall assume that the data points are independent and identically distributed.
It should be emphasized that the problem of density estimation is fun- 67 68 2.
PROBABILITYDISTRIBUTIONS damentallyill-posed, becausethereareinfinitelymanyprobabilitydistributionsthat could have given rise to the observed finite data set.
Indeed, any distribution p(x) that is nonzero at each of the data points x 1 ,..., x N is a potential candidate.
The issueofchoosinganappropriatedistributionrelatestotheproblemofmodelselec- tion that has already been encountered in the context of polynomial curve fitting in Chapter1andthatisacentralissueinpatternrecognition.
Webeginbyconsideringthebinomialandmultinomialdistributionsfordiscrete random variables and the Gaussian distribution for continuous random variables.
These are specific examples of parametric distributions, so-called because they are governedbyasmallnumberofadaptiveparameters, suchasthemeanandvariancein thecaseofa Gaussianforexample.
Toapplysuchmodelstotheproblemofdensity estimation, weneedaprocedurefordeterminingsuitablevaluesfortheparameters, given an observed data set.
In a frequentist treatment, we choose specific values fortheparametersbyoptimizingsomecriterion, suchasthelikelihoodfunction.
By contrast, ina Bayesiantreatmentweintroducepriordistributionsovertheparameters and then use Bayes’ theorem to compute the corresponding posterior distribution giventheobserveddata.
We shall see that an important role is played by conjugate priors, that lead to posterior distributions having the same functional form as the prior, and that there- foreleadtoagreatlysimplified Bayesiananalysis.
Forexample, theconjugateprior fortheparametersofthemultinomialdistributioniscalledthe Dirichletdistribution, whiletheconjugatepriorforthemeanofa Gaussianisanother Gaussian.
Allofthese distributions are examples of the exponential family of distributions, which possess anumberofimportantproperties, andwhichwillbediscussedinsomedetail.
Onelimitationoftheparametricapproachisthatitassumesaspecificfunctional form for the distribution, which may turn out to be inappropriate for a particular application.
An alternative approach is given by nonparametric density estimation methodsinwhichtheformofthedistributiontypicallydependsonthesizeofthedata set.
Such models still contain parameters, but these control the model complexity rather than the form of the distribution.
We end this chapter by considering three nonparametric methods based respectively on histograms, nearest-neighbours, and kernels.
2.1.
Binary Variables Webeginbyconsideringasinglebinaryrandomvariablex ∈ {0,1}.
Forexample, x might describe the outcome of flipping a coin, with x = 1 representing ‘heads’, and x = 0 representing ‘tails’.
We can imagine that this is a damaged coin so that the probability of landing heads is not necessarily the same as that of landing tails.
Theprobabilityofx=1willbedenotedbytheparameterµsothat p(x=1|µ)=µ (2.1) 2.1.
Binary Variables 69 where0 µ 1, fromwhichitfollowsthatp(x = 0|µ) = 1−µ.
Theprobability distributionoverxcanthereforebewrittenintheform Bern(x|µ)=µ x (1−µ)1−x (2.2) Exercise 2.1 whichisknownasthe Bernoullidistribution.
Itiseasilyverifiedthatthisdistribution isnormalizedandthatithasmeanandvariancegivenby E[x] = µ (2.3) var[x] = µ(1−µ).
(2.4) Now suppose we have a data set D = {x 1 ,..., x N } of observed values of x.
Wecanconstructthelikelihoodfunction, whichisafunctionofµ, ontheassumption thattheobservationsaredrawnindependentlyfromp(x|µ), sothat N N p(D|µ)= p(xn |µ)= µ x n(1−µ)1−x n.
(2.5) n=1 n=1 Inafrequentistsetting, wecanestimateavalueforµbymaximizingthelikelihood function, orequivalentlybymaximizingthelogarithmofthelikelihood.
Inthecase ofthe Bernoullidistribution, theloglikelihoodfunctionisgivenby N N lnp(D|µ)= lnp(xn |µ)= {xnlnµ+(1−xn)ln(1−µ)}.
(2.6) n=1 n=1 At this point, it is worth noting that the log likelihood function depends on the N observationsxn onlythroughtheirsum n xn.
Thissumprovidesanexampleofa sufficientstatisticforthedataunderthisdistribution, andweshallstudytheimpor- Section2.4 tant role of sufficient statistics in some detail.
If we set the derivative of lnp(D|µ) withrespecttoµequaltozero, weobtainthemaximumlikelihoodestimator N 1 µ ML = xn (2.7) N n=1 Jacob Bernoulli histime, including Boyleand Hookein England.
When 1654–1705 hereturnedto Switzerland, hetaughtmechanicsand became Professor of Mathematics at Basel in 1687.
Jacob Bernoulli, also known as Unfortunately, rivalrybetween Jacobandhisyounger Jacquesor James Bernoulli, wasa brother Johannturnedaninitiallyproductivecollabora- Swiss mathematician and was the tionintoabitterandpublicdispute.
Jacob’smostsig- first of many in the Bernoulli family nificantcontributionstomathematicsappearedin The to pursue a career in science and Artof Conjecturepublishedin1713, eightyearsafter mathematics.
Although compelled his death, which deals with topics in probability the- to study philosophy and theology against his will by oryincludingwhathasbecomeknownasthe Bernoulli his parents, he travelled extensively after graduating distribution.
inordertomeetwithmanyoftheleadingscientistsof 70 2.
PROBABILITYDISTRIBUTIONS Figure2.1 Histogram plot of the binomial dis- 0.3 tribution(2.9)asafunctionofmfor N =10andµ=0.25.
0.2 0.1 0 0 1 2 3 4 5 6 7 8 9 10 m which is also known as the sample mean.
If we denote the number of observations ofx=1(heads)withinthisdatasetbym, thenwecanwrite(2.7)intheform m µ ML = (2.8) N sothattheprobabilityoflandingheadsisgiven, inthismaximumlikelihoodframe- work, bythefractionofobservationsofheadsinthedataset.
Nowsupposeweflipacoin, say,3timesandhappentoobserve3heads.
Then N = m = 3 and µ ML = 1.
In this case, the maximum likelihood result would predict that all future observations should give heads.
Common sense tells us that thisisunreasonable, andinfactthisisanextremeexampleoftheover-fittingassoci- atedwithmaximumlikelihood.
Weshallseeshortlyhowtoarriveatmoresensible conclusionsthroughtheintroductionofapriordistributionoverµ.
Wecanalsoworkoutthedistributionofthenumbermofobservationsofx=1, given that the data set has size N.
This is called the binomial distribution, and from (2.5) we see that it is proportional to µm(1−µ)N−m.
In order to obtain the normalization coefficient we note that out of N coin flips, we have to add up all of the possible ways of obtaining m heads, so that the binomial distribution can be written N Bin(m|N,µ)= µ m (1−µ) N−m (2.9) m where N N! ≡ (2.10) m (N −m)! m! Exercise 2.3 is the number of ways of choosing m objects out of a total of N identical objects.
Figure2.1showsaplotofthebinomialdistributionfor N =10andµ=0.25.
The mean and variance of the binomial distribution can be found by using the result of Exercise 1.10, which shows that for independent events the mean of the sumisthesumofthemeans, andthevarianceofthesumisthesumofthevariances.
Because m = x 1 +...+x N, and for each observation the mean and variance are 2.1.
Binary Variables 71 givenby(2.3)and(2.4), respectively, wehave N E[m]≡ m Bin(m|N,µ) = Nµ (2.11) m=0 N var[m]≡ (m−E[m]) 2 Bin(m|N,µ) = Nµ(1−µ).
(2.12) m=0 Exercise 2.4 Theseresultscanalsobeproveddirectlyusingcalculus.
2.1.1 The beta distribution Wehaveseen in(2.8)that themaximum likelihood setting for theparameterµ inthe Bernoullidistribution, andhenceinthebinomialdistribution, isgivenbythe fractionoftheobservationsinthedatasethavingx = 1.
Aswehavealreadynoted, this can give severely over-fitted results for small data sets.
In order to develop a Bayesian treatmentfor this problem, weneedtointroduce aprior distributionp(µ) overtheparameterµ.
Hereweconsideraformofpriordistributionthathasasimple interpretation as well as some useful analytical properties.
To motivate this prior, we note that the likelihood function takes the form of the product of factors of the form µx(1 − µ)1−x.
If we choose a prior to be proportional to powers of µ and (1−µ), then the posterior distribution, which is proportional to the product of the prior and the likelihood function, will have the same functional form as the prior.
Thispropertyiscalledconjugacyandwewillseeseveralexamplesofitlaterinthis chapter.
Wethereforechooseaprior, calledthebetadistribution, givenby Γ(a+b) Beta(µ|a, b)= µ a−1(1−µ) b−1 (2.13) Γ(a)Γ(b) where Γ(x) is the gamma function defined by (1.141), and the coefficient in (2.13) Exercise 2.5 ensuresthatthebetadistributionisnormalized, sothat 1 Beta(µ|a, b)dµ=1.
(2.14) 0 Exercise 2.6 Themeanandvarianceofthebetadistributionaregivenby a E[µ] = (2.15) a+b ab var[µ] = .
(2.16) (a+b)2(a+b+1) The parameters a and b are often called hyperparameters because they control the distribution of the parameter µ.
Figure 2.2 shows plots of the beta distribution for variousvaluesofthehyperparameters.
The posterior distribution of µ is now obtained by multiplying the beta prior (2.13)bythebinomiallikelihoodfunction(2.9)andnormalizing.
Keepingonlythe factorsthatdependonµ, weseethatthisposteriordistributionhastheform p(µ|m, l, a, b)∝µ m+a−1(1−µ) l+b−1 (2.17) 72 2.
PROBABILITYDISTRIBUTIONS 3 3 a=0.1 a=1 b=0.1 b=1 2 2 1 1 0 0 0 0.5 µ 1 0 0.5 µ 1 3 3 a=2 a=8 b=3 b=4 2 2 1 1 0 0 0 0.5 µ 1 0 0.5 µ 1 Figure2.2 Plotsofthebetadistribution Beta(µ|a, b)givenby(2.13)asafunctionofµforvariousvaluesofthe hyperparametersaandb.
where l = N −m, and therefore corresponds to the number of ‘tails’ in the coin example.
We see that (2.17) has the same functional dependence on µ as the prior distribution, reflectingtheconjugacypropertiesofthepriorwithrespecttothelike- lihoodfunction.
Indeed, itissimplyanotherbetadistribution, anditsnormalization coefficientcanthereforebeobtainedbycomparisonwith(2.13)togive Γ(m+a+l+b) p(µ|m, l, a, b)= µ m+a−1(1−µ) l+b−1.
(2.18) Γ(m+a)Γ(l+b) We see that the effect of observing a data set of m observations of x = 1 and l observations of x = 0 has been to increase the value of a by m, and the value of bbyl, ingoingfromthepriordistributiontotheposteriordistribution.
Thisallows us to provide a simple interpretation of the hyperparameters a and b in the prior as an effective number of observations of x = 1 and x = 0, respectively.
Note that a and b need not be integers.
Furthermore, the posterior distribution can act as the priorifwesubsequentlyobserveadditionaldata.
Toseethis, wecanimaginetaking observationsoneatatimeandaftereachobservationupdatingthecurrentposterior 2.1.
Binary Variables 73 2 2 2 prior likelihood function posterior 1 1 1 0 0 0 0 0.5 1 0 0.5 1 0 0.5 1 µ µ µ Figure 2.3 Illustration of one step of sequential Bayesian inference.
The prior is given by a beta distribution with parameters a = 2, b = 2, and the likelihood function, given by (2.9) with N = m = 1, corresponds to a singleobservationofx=1, sothattheposteriorisgivenbyabetadistributionwithparametersa=3, b=2.
distribution by multiplying by the likelihood function for the new observation and thennormalizingtoobtainthenew, revisedposteriordistribution.
Ateachstage, the posteriorisabetadistributionwithsometotalnumberof(priorandactual)observed values for x = 1 and x = 0 given by the parameters a and b.
Incorporation of an additional observation of x = 1 simply corresponds to incrementing the value ofa by1, whereasforanobservationofx=0weincrementbby1.
Figure2.3illustrates onestepinthisprocess.
Weseethatthissequentialapproachtolearningarisesnaturallywhenweadopt a Bayesianviewpoint.
Itisindependentofthechoiceofpriorandofthelikelihood functionanddependsonlyontheassumptionofi.
i.
d.
data.
Sequentialmethodsmake useofobservationsoneatatime, orinsmallbatches, andthendiscardthembefore thenextobservationsareused.
Theycanbeused, forexample, inreal-timelearning scenarios where a steady stream of data is arriving, and predictions must be made before all of the data is seen.
Because they do not require the whole data set to be storedorloadedintomemory, sequentialmethodsarealsousefulforlargedatasets.
Section2.3.5 Maximumlikelihoodmethodscanalsobecastintoasequentialframework.
If our goal is to predict, as best we can, the outcome of the next trial, then we must evaluate the predictive distribution ofx, given the observed data set D.
From thesumandproductrulesofprobability, thistakestheform 1 1 p(x=1|D)= p(x=1|µ)p(µ|D)dµ= µp(µ|D)dµ=E[µ|D].
(2.19) 0 0 Usingtheresult(2.18)fortheposteriordistributionp(µ|D), togetherwiththeresult (2.15)forthemeanofthebetadistribution, weobtain m+a p(x=1|D)= (2.20) m+a+l+b whichhasasimpleinterpretationasthetotalfractionofobservations(bothrealob- servations and fictitious prior observations) that correspond to x = 1.
Note that in the limit of an infinitely large data set m, l → ∞ the result (2.20) reduces to the maximumlikelihoodresult(2.8).
Asweshallsee, itisaverygeneralpropertythat the Bayesianandmaximumlikelihoodresultswillagreeinthelimitofaninfinitely 74 2.
PROBABILITYDISTRIBUTIONS largedataset.
Forafinitedataset, theposteriormeanforµalwaysliesbetweenthe priormeanandthemaximumlikelihoodestimateforµcorrespondingtotherelative Exercise 2.7 frequenciesofeventsgivenby(2.7).
From Figure 2.2, we see that as the number of observations increases, so the posterior distribution becomes more sharply peaked.
This can also be seen from the result (2.16) for the variance of the beta distribution, in which we see that the variancegoestozerofora → ∞orb → ∞.
Infact, wemightwonderwhetheritis ageneralpropertyof Bayesianlearningthat, asweobservemoreandmoredata, the uncertaintyrepresentedbytheposteriordistributionwillsteadilydecrease.
To address this, we can take a frequentist view of Bayesian learning and show that, on average, such a property does indeed hold.
Consider a general Bayesian inference problem for a parameter θ for which we have observed a data set D, de- Exercise 2.8 scribedbythejointdistributionp(θ, D).
Thefollowingresult E θ[θ]=E D[E θ[θ|D]] (2.21) where E θ[θ] ≡ p(θ)θdθ (2.22) E D[E θ[θ|D]] ≡ θp(θ|D)dθ p(D)d D (2.23) saysthattheposteriormeanofθ, averagedoverthedistributiongeneratingthedata, isequaltothepriormeanofθ.
Similarly, wecanshowthat varθ[θ]=E D[varθ[θ|D]]+var D[E θ[θ|D]].
(2.24) The term on the left-hand side of (2.24) is the prior variance of θ.
On the right- handside, thefirsttermistheaverageposterior varianceofθ, andthesecondterm measuresthevarianceintheposteriormeanofθ.
Becausethisvarianceisapositive quantity, thisresultshowsthat, onaverage, theposteriorvarianceofθissmallerthan thepriorvariance.
Thereductioninvarianceisgreaterifthevarianceintheposterior meanisgreater.
Note, however, thatthisresultonlyholdsonaverage, andthatfora particularobserveddatasetitispossiblefortheposteriorvariancetobelargerthan thepriorvariance.
2.2.
Multinomial Variables Binaryvariablescanbeusedtodescribequantitiesthatcantakeoneoftwopossible values.
Often, however, we encounter discrete variables that can take on one of K possible mutually exclusive states.
Although there are various alternative ways to express such variables, we shall see shortly that a particularly convenient represen- tationisthe1-of-K schemeinwhichthevariableisrepresentedbya K-dimensional vectorxinwhichoneoftheelementsxk equals1, andallremainingelementsequal 2.2.
Multinomial Variables 75 0.
So, forinstanceifwehaveavariablethatcantake K = 6statesandaparticular observationofthevariablehappenstocorrespondtothestatewherex 3 = 1, thenx willberepresentedby x=(0,0,1,0,0,0)T.
(2.25) K Notethatsuchvectorssatisfy k=1 xk =1.
Ifwedenotetheprobabilityofxk =1 bytheparameterµk, thenthedistributionofxisgiven K p(x|µ)= µ x k (2.26) k k=1 wher eµ=(µ 1 ,...,µK)T, andtheparametersµk areconstrainedtosatisfyµk 0 and k µk =1, becausetheyrepresentprobabilities.
Thedistribution(2.26)canbe regardedasageneralizationofthe Bernoullidistributiontomorethantwooutcomes.
Itiseasilyseenthatthedistributionisnormalized K p(x|µ)= µk =1 (2.27) x k=1 andthat x Now consider a data set D of N independent observations x 1 ,..., x N.
The correspondinglikelihoodfunctiontakestheform N K K P K p(D|µ)= µ x nk = µ ( n x nk ) = µ m k.
(2.29) k k k n=1k=1 k=1 k=1 We see that the likelihood function depends on the N data points only through the K quantities mk = xnk (2.30) n whichrepresentthenumberofobservationsofxk =1.
Thesearecalledthesufficient Section2.4 statisticsforthisdistribution.
In order to find the maximum likelihood solution for µ, we need to maximize lnp(D|µ)withrespecttoµk takingaccountoftheconstraintthattheµk mustsum Appendix E toone.
Thiscanbeachievedusinga Lagrangemultiplierλandmaximizing K K mklnµk +λ µk −1 .
(2.31) k=1 k=1 Settingthederivativeof(2.31)withrespecttoµk tozero, weobtain µk =−mk/λ.
(2.32) 76 2.
PROBABILITYDISTRIBUTIONS W ecansolveforthe Lagrangemultiplierλbysubstituting(2.32)intotheconstraint k µk = 1 to give λ = −N.
Thus we obtain the maximum likelihood solution in theform µML = mk (2.33) k N whichisthefractionofthe N observationsforwhichxk =1.
Wecanconsiderthejointdistributionofthequantitiesm 1 ,..., m K, conditioned on the parameters µ and on the total number N of observations.
From (2.29) this takestheform K N Mult(m 1 , m 2 ,..., m K |µ, N)= m 1 m 2 ...
m K µ m k k (2.34) k=1 whichisknownasthemultinomialdistribution.
Thenormalizationcoefficientisthe numberofwaysofpartitioning N objectsinto K groupsofsizem 1 ,..., m K andis givenby N N! = .
(2.35) m 1 m 2 ...
m K m 1 ! m 2 !...
m K! Notethatthevariablesmk aresubjecttotheconstraint K mk =N.
(2.36) k=1 2.2.1 The Dirichlet distribution We now introduce a family of prior distributions for the parameters {µk } of the multinomial distribution (2.34).
By inspection of the form of the multinomial distribution, weseethattheconjugatepriorisgivenby K p(µ|α)∝ µ α k −1 (2.37) k k=1 where 0 µk 1 and k µk = 1.
Here α 1 ,...,αK are the parameters of the distribution, and α denotes (α 1 ,...,αK)T.
Note that, because of the summation constraint, the distribution over the space of the {µk } is confined to a simplex of dimensionality K −1, asillustratedfor K =3in Figure2.4.
Exercise 2.9 Thenormalizedformforthisdistributionisby K Γ(α ) Dir(µ|α)= 0 µ α k −1 (2.38) Γ(α 1 )···Γ(αK) k k=1 whichiscalledthe Dirichlet distribution.
HereΓ(x)isthegammafunctiondefined by(1.141)while K α 0 = αk.
(2.39) k=1 2.2.
Multinomial Variables 77 Figure2.4 The Dirichlet distribution over three variables µ 1 ,µ 2 ,µ 3 µ2 is confined to a simplex (a bounded linear manifold) of the form shown P, as a consequence of the constraints 0 µ k 1and k µ k =1.
µ1 µ3 Plotsofthe Dirichletdistributionoverthesimplex, forvarioussettingsoftheparam- etersαk, areshownin Figure2.5.
Multiplying the prior (2.38) by the likelihood function (2.34), we obtain the posteriordistributionfortheparameters{µk }intheform K p(µ|D,α)∝p(D|µ)p(µ|α)∝ µ α k+m k −1 .
(2.40) k k=1 Weseethattheposteriordistributionagaintakestheformofa Dirichletdistribution, confirming that the Dirichlet is indeed a conjugate prior for the multinomial.
This allows us to determine the normalization coefficient by comparison with (2.38) so that p(µ|D,α) = Dir(µ|α+m) K Γ(α +N) = 0 µ α k+m k −1 (2.41) Γ(α 1 +m 1 )···Γ(αK +m K) k k=1 where we have denoted m = (m 1 ,..., m K)T.
As for the case of the binomial distribution with its beta prior, we can interpret the parameters αk of the Dirichlet priorasaneffectivenumberofobservationsofxk =1.
Note that two-state quantities can either be represented as binary variables and Lejeune Dirichlet from ‘le jeune de Richelet’ (the young person from 1805–1859 Richelet).
Dirichlet’s first paper, which was published in 1825, brought him instant fame.
It concerned Fer- Johann Peter Gustav Lejeune mat’s last theorem, which claims that there are no Dirichlet was a modest and re- positive integer solutions to xn+yn = zn for n > 2.
served mathematician who made Dirichletgaveapartialproofforthecasen=5, which contributionsinnumbertheory, me- wassentto Legendreforreviewandwhointurncom- chanics, and astronomy, and who pletedtheproof.
Later, Dirichletgaveacompleteproof gave the first rigorous analysis of forn=14, althoughafullproofof Fermat’slasttheo- Fourier series.
His family originated from Richelet remforarbitrarynhadtowaituntiltheworkof Andrew in Belgium, and the name Lejeune Dirichlet comes Wilesintheclosingyearsofthe20thcentury.
78 2.
PROBABILITYDISTRIBUTIONS Figure2.5 Plotsofthe Dirichletdistributionoverthreevariables, wherethetwohorizontalaxesarecoordinates intheplaneofthesimplexandtheverticalaxiscorrespondstothevalueofthedensity.
Here{α k }=0.1onthe leftplot,{α k }=1inthecentreplot, and{α k }=10intherightplot.
modelled using the binomial distribution (2.9) or as 1-of-2 variables and modelled usingthemultinomialdistribution(2.34)with K =2.
2.3.
The Gaussian Distribution The Gaussian, alsoknownasthenormaldistribution, isawidelyusedmodelforthe distributionofcontinuousvariables.
Inthecaseofasinglevariablex, the Gaussian distributioncanbewrittenintheform 1 1 N(x|µ,σ2)= exp − (x−µ)2 (2.42) (2πσ2) 1/2 2σ2 where µ is the mean and σ2 is the variance.
For a D-dimensional vector x, the multivariate Gaussiandistributiontakestheform 1 1 1 N(x|µ,Σ)= exp − (x−µ)TΣ−1(x−µ) (2.43) (2π)D/2|Σ|1/2 2 whereµisa D-dimensionalmeanvector,Σisa D×Dcovariancematrix, and|Σ| denotesthedeterminantofΣ.
The Gaussiandistributionarisesinmanydifferentcontextsandcanbemotivated Section1.6 fromavarietyofdifferentperspectives.
Forexample, wehavealreadyseenthatfor a single real variable, the distribution that maximizes the entropy is the Gaussian.
Exercise 2.14 Thispropertyappliesalsotothemultivariate Gaussian.
Anothersituationinwhichthe Gaussiandistributionarisesiswhenweconsider the sum of multiple random variables.
The central limit theorem (due to Laplace) tellsusthat, subjecttocertainmildconditions, thesumofasetofrandomvariables, whichisofcourseitselfarandomvariable, hasadistributionthatbecomesincreas- ingly Gaussianasthenumberoftermsinthesumincreases(Walker,1969).
Wecan 2.3.
The Gaussian Distribution 79 3 3 3 N =1 N =2 N =10 2 2 2 1 1 1 0 0 0 0 0.5 1 0 0.5 1 0 0.5 1 Figure 2.6 Histogram plots of the mean of N uniformly distributed numbers for various values of N.
We observethatas N increases, thedistributiontendstowardsa Gaussian.
illustrate this by considering N variables x 1 ,..., x N each of which has a uniform distributionovertheinterval[0,1]andthenconsideringthedistributionofthemean (x 1 +···+x N)/N.
Forlarge N, thisdistributiontendstoa Gaussian, asillustrated in Figure 2.6.
In practice, the convergence to a Gaussian as N increases can be very rapid.
One consequence of this result is that the binomial distribution (2.9), whichisadistributionovermdefinedbythesumof N observationsoftherandom binaryvariablex, willtendtoa Gaussianas N → ∞(see Figure2.1forthecaseof N =10).
The Gaussiandistributionhasmanyimportantanalyticalproperties, andweshall considerseveraloftheseindetail.
Asaresult, thissectionwillberathermoretech- nically involved than some of the earlier sections, and will require familiarity with Appendix C variousmatrixidentities.
However, westronglyencouragethereadertobecomepro- ficientinmanipulating Gaussiandistributionsusingthetechniquespresentedhereas this will prove invaluable in understanding the more complex models presented in laterchapters.
Webeginbyconsideringthegeometricalformofthe Gaussiandistribution.
The Carl Friedrich Gauss ematician and scientist with a reputation for being a 1777–1855 hard-workingperfectionist.
Oneofhismanycontribu- tions was to show that least squares can be derived It is said that when Gauss went under the assumption of normally distributed errors.
to elementary school at age 7, his Healsocreatedanearlyformulationofnon-Euclidean teacher Bu¨ttner, trying to keep the geometry(aself-consistentgeometricaltheorythatvi- classoccupied, askedthepupilsto olates the axioms of Euclid) but was reluctant to dis- sumtheintegersfrom1to100.
To cuss it openly for fear that his reputation might suffer the teacher’s amazement, Gauss if it were seen that he believed in such a geometry.
arrivedattheanswerinamatterofmomentsbynoting Atonepoint, Gausswasaskedtoconductageodetic thatthesumcanberepresentedas50pairs(1+100, survey of the state of Hanover, which led to his for- 2+99, etc.)eachofwhichaddedto101, givingthean- mulation of the normal distribution, now also known swer5,050.
Itisnowbelievedthattheproblemwhich as the Gaussian.
After his death, a study of his di- wasactuallysetwasofthesameformbutsomewhat aries revealed that he had discovered several impor- harderinthatthesequencehadalargerstartingvalue tant mathematical results years or even decades be- andalargerincrement.
Gausswas a Germanmath- foretheywerepublishedbyothers.
80 2.
PROBABILITYDISTRIBUTIONS functionaldependenceofthe Gaussianonxisthroughthequadraticform ∆2 =(x−µ)TΣ −1(x−µ) (2.44) which appears in the exponent.
The quantity ∆ is called the Mahalanobis distance fromµtoxandreducestothe EuclideandistancewhenΣistheidentitymatrix.
The Gaussiandistributionwillbeconstantonsurfacesinx-spaceforwhichthisquadratic formisconstant.
First of all, we note that the matrix Σ can be taken to be symmetric, without lossofgenerality, becauseanyantisymmetriccomponentwoulddisappearfromthe Exercise 2.17 exponent.
Nowconsidertheeigenvectorequationforthecovariancematrix Σui =λiui (2.45) wherei = 1,..., D.
BecauseΣisareal, symmetricmatrixitseigenvalueswillbe Exercise 2.18 real, anditseigenvectorscanbechosentoformanorthonormalset, sothat u T i uj =Iij (2.46) where Iij isthei, j elementoftheidentitymatrixandsatisfies 1, ifi=j Iij = 0, otherwise.
(2.47) ThecovariancematrixΣcanbeexpressedasanexpansionintermsofitseigenvec- Exercise 2.19 torsintheform D Σ= λiuiu T i (2.48) i=1 andsimilarlytheinversecovariancematrixΣ −1 canbeexpressedas D 1 Σ −1 = uiu T i .
(2.49) λi i=1 Substituting(2.49)into(2.44), thequadraticformbecomes D y2 ∆2 = i (2.50) λi i=1 wherewehavedefined yi =u T i (x−µ).
(2.51) Wecaninterpret{yi }asanewcoordinatesystemdefinedbytheorthonormalvectors ui that are shifted and rotated with respect to the original xi coordinates.
Forming thevectory =(y 1 ,..., y D)T, wehave y =U(x−µ) (2.52) 2.3.
The Gaussian Distribution 81 Figure2.7 Theredcurveshowstheellip- x2 u2 ticalsurfaceofconstantproba- bilitydensityfora Gaussianin a two-dimensional space x = u1 (x 1 , x 2 ) on which the density is exp(−1/2) of its value at y2 x = µ.
The major axes of the ellipse are defined by the y1 eigenvectors u i of the covari- µ ance matrix, with correspond- ingeigenvaluesλ i.
λ 1/2 2 1/2 λ 1 x1 where Uisamatrixwhoserowsaregivenbyu T.
From(2.46)itfollowsthat Uis i Appendix C anorthogonalmatrix, i.
e., itsatisfies UUT = I, andhencealso UTU = I, where I istheidentitymatrix.
Thequadraticform, andhencethe Gaussiandensity, willbeconstantonsurfaces for which (2.51) is constant.
If all of the eigenvalues λi are positive, then these surfacesrepresentellipsoids, withtheircentresatµandtheiraxesorientedalongui, 1/2 andwithscalingfactorsinthedirectionsoftheaxesgivenbyλ , asillustratedin i Figure2.7.
For the Gaussian distribution to be well defined, it is necessary for all of the eigenvalues λi of the covariance matrix to be strictly positive, otherwise the dis- tribution cannot be properly normalized.
A matrix whose eigenvalues are strictly positive is said to be positive definite.
In Chapter 12, we will encounter Gaussian distributions for which one or more of the eigenvalues are zero, in which case the distributionissingularandisconfinedtoasubspaceoflowerdimensionality.
Ifall oftheeigenvaluesarenonnegative, thenthecovariancematrixissaidtobepositive semidefinite.
Nowconsidertheformofthe Gaussiandistributioninthenewcoordinatesystem definedbytheyi.
Ingoingfromthextotheycoordinatesystem, wehavea Jacobian matrix Jwithelementsgivenby ∂xi Jij = =Uji (2.53) ∂yj where Uji aretheelementsofthematrix UT.
Usingtheorthonormalitypropertyof thematrix U, weseethatthesquareofthedeterminantofthe Jacobianmatrixis |J|2 = UT 2 = UT |U|= UTU =|I|=1 (2.54) andhence|J|=1.
Also, thedeterminant|Σ|ofthecovariancematrixcanbewritten 82 2.
PROBABILITYDISTRIBUTIONS astheproductofitseigenvalues, andhence D |Σ|1/2 = λ 1/2 .
(2.55) j j=1 Thusintheyj coordinatesystem, the Gaussiandistributiontakestheform p(y)=p(x)|J|= D 1 exp − y j 2 (2.56) (2πλj)1/2 2λj j=1 whichistheproductof Dindependentunivariate Gaussiandistributions.
Theeigen- vectors therefore define a new set of shifted and rotated coordinates with respect to which the joint probability distribution factorizes into a product of independent distributions.
Theintegralofthedistributionintheycoordinatesystemisthen p(y)dy = j D =1 − ∞ ∞ (2πλ 1 j)1/2 exp − 2 y λ j 2 j dyj =1 (2.57) wherewehaveusedtheresult(1.48)forthenormalizationoftheunivariate Gaussian.
Thisconfirmsthatthemultivariate Gaussian(2.43)isindeednormalized.
Wenowlookatthemomentsofthe Gaussiandistributionandtherebyprovidean interpretationoftheparametersµandΣ.
Theexpectationofxunderthe Gaussian distributionisgivenby 1 1 1 E[x] = exp − (x−µ)TΣ −1(x−µ) xdx (2π)D/2|Σ|1/2 2 1 1 1 = exp − z TΣ −1z (z+µ)dz (2.58) (2π)D/2|Σ|1/2 2 wherewehavechangedvariablesusingz = x−µ.
Wenownotethattheexponent isanevenfunctionofthecomponentsofzand, becausetheintegralsovertheseare taken over the range (−∞,∞), the term in z in the factor (z+µ) will vanish by symmetry.
Thus E[x]=µ (2.59) andsowerefertoµasthemeanofthe Gaussiandistribution.
Wenowconsidersecondordermomentsofthe Gaussian.
Intheunivariatecase, weconsideredthesecondordermomentgivenby E[x2].
Forthemultivariate Gaus- sian, there are D2 second order moments given by E[xixj], which we can group togethertoformthematrix E[xx T].
Thismatrixcanbewrittenas 1 1 1 E[xx T]= exp − (x−µ)TΣ −1(x−µ) xx Tdx (2π)D/2|Σ|1/2 2 1 1 1 = exp − z TΣ −1z (z+µ)(z+µ)Tdz (2π)D/2|Σ|1/2 2 2.3.
The Gaussian Distribution 83 whereagainwehavechangedvariablesusingz = x−µ.
Notethatthecross-terms involvingµz T andµTzwillagainvanishbysymmetry.
ThetermµµT isconstant and can be taken outside the integral, which itself is unity because the Gaussian distribution is normalized.
Consider the term involving zz T.
Again, we can make useoftheeigenvectorexpansionofthecovariancematrixgivenby(2.45), together withthecompletenessofthesetofeigenvectors, towrite D z= yjuj (2.60) j=1 whereyj =u T j z, whichgives 1 1 1 exp − z TΣ −1z zz Tdz (2π)D/2|Σ|1/2 2 1 1 D D D y2 = (2π)D/2|Σ|1/2 uiu T j exp − 2λ k k yiyjdy i=1 j=1 k=1 D = uiu T i λi =Σ (2.61) i=1 where we have made use of the eigenvector equation (2.45), together with the fact that the integral on the right-hand side of the middle line vanishes by symmetry unlessi = j, andinthefinallinewehavemadeuseoftheresults(1.50)and(2.55), togetherwith(2.48).
Thuswehave E[xx T]=µµT+Σ.
(2.62) Forsinglerandomvariables, wesubtractedthemeanbeforetakingsecondmo- ments in order to define a variance.
Similarly, in the multivariate case it is again convenienttosubtractoffthemean, givingrisetothecovarianceofarandomvector xdefinedby cov[x]=E (x−E[x])(x−E[x])T .
(2.63) For the specific case of a Gaussian distribution, we can make use of E[x] = µ, togetherwiththeresult(2.62), togive cov[x]=Σ.
(2.64) Because the parameter matrix Σ governs the covariance of x under the Gaussian distribution, itiscalledthecovariancematrix.
Although the Gaussian distribution (2.43) is widely used as a density model, it suffersfromsomesignificantlimitations.
Considerthenumberoffreeparametersin the distribution.
A general symmetric covariance matrix Σ will have D(D +1)/2 Exercise 2.21 independentparameters, andthereareanother D independentparametersinµ, giv- ing D(D +3)/2 parameters in total.
For large D, the total number of parameters 84 2.
PROBABILITYDISTRIBUTIONS Figure2.8 Contours of constant x2 x2 x2 probability density for a Gaussian distribution in two dimensions in whichthecovariancematrixis(a)of general form, (b) diagonal, in which the elliptical contours are aligned with the coordinate axes, and (c) x1 x1 x1 proportionaltotheidentitymatrix, in (a) (b) (c) which the contours are concentric circles.
therefore grows quadratically with D, and the computational task of manipulating andinvertinglargematricescanbecomeprohibitive.
Onewaytoaddressthisprob- lem is to use restricted forms of the covariance matrix.
If we consider covariance matricesthatarediagonal, sothatΣ = diag(σ2), wethenhaveatotalof2D inde- i pendent parameters in the density model.
The corresponding contours of constant densityaregivenbyaxis-alignedellipsoids.
Wecouldfurtherrestrictthecovariance matrixtobeproportionaltotheidentitymatrix,Σ=σ2I, knownasanisotropicco- variance, giving D+1independentparametersinthemodelandsphericalsurfaces ofconstantdensity.
Thethreepossibilitiesofgeneral, diagonal, andisotropiccovari- ancematricesareillustratedin Figure2.8.
Unfortunately, whereassuchapproaches limitthenumberofdegreesoffreedominthedistributionandmakeinversionofthe covariancematrixamuchfasteroperation, theyalsogreatlyrestricttheformofthe probabilitydensityandlimititsabilitytocaptureinterestingcorrelationsinthedata.
A further limitation of the Gaussian distribution is that it is intrinsically uni- modal(i.
e., hasasinglemaximum)andsoisunabletoprovideagoodapproximation tomultimodaldistributions.
Thusthe Gaussiandistributioncanbebothtooflexible, inthesenseofhavingtoomanyparameters, whilealsobeingtoolimitedintherange ofdistributionsthatitcanadequatelyrepresent.
Wewillseelaterthattheintroduc- tionoflatentvariables, alsocalledhiddenvariablesorunobservedvariables, allows both of these problems to be addressed.
In particular, a rich family of multimodal distributionsisobtainedbyintroducingdiscretelatentvariablesleadingtomixtures of Gaussians, asdiscussedin Section2.3.9.
Similarly, theintroductionofcontinuous latentvariables, asdescribedin Chapter12, leadstomodelsinwhichthenumberof freeparameterscanbecontrolledindependentlyofthedimensionality Dofthedata spacewhilestillallowingthemodeltocapturethedominantcorrelationsinthedata set.
Indeed, these two approaches can be combined and further extended to derive a very rich set of hierarchical models that can be adapted to a broad range of prac- Section8.3 tical applications.
For instance, the Gaussian version of the Markov random field, which is widely used as a probabilistic model of images, is a Gaussian distribution overthejointspaceofpixelintensitiesbutrenderedtractablethroughtheimposition of considerable structurereflecting thespatial organization of thepixels.
Similarly, Section13.3 the linear dynamical system, used to model time series data for applications such as tracking, is also a joint Gaussian distribution over a potentially large number of observedandlatentvariablesandagainistractableduetothestructureimposedon the distribution.
A powerful framework for expressing the form and properties of 2.3.
The Gaussian Distribution 85 suchcomplexdistributionsisthatofprobabilisticgraphicalmodels, whichwillform thesubjectof Chapter8.
2.3.1 Conditional Gaussian distributions An important property of the multivariate Gaussian distribution is that if two sets of variables are jointly Gaussian, then the conditional distribution of one set conditioned on the other is again Gaussian.
Similarly, the marginal distribution of eithersetisalso Gaussian.
Considerfirstthecaseofconditionaldistributions.
Supposexisa D-dimensional vector with Gaussian distribution N(x|µ,Σ) and that we partition x into two dis- jointsubsetsxa andxb.
Withoutlossofgenerality, wecantakexa toformthefirst M componentsofx, withxbcomprisingtheremaining D−M components, sothat xa x= .
(2.65) xb Wealsodefinecorrespondingpartitionsofthemeanvectorµgivenby µ µ= a (2.66) µ b andofthecovariancematrixΣgivenby Σaa Σab Σ= .
(2.67) Σba Σbb NotethatthesymmetryΣT =ΣofthecovariancematriximpliesthatΣaa andΣbb aresymmetric, whileΣba =ΣT ab .
Inmanysituations, itwillbeconvenienttoworkwiththeinverseofthecovari- ancematrix Λ≡Σ −1 (2.68) which is known as the precision matrix.
In fact, we shall see that some properties of Gaussian distributions are most naturally expressed in terms of the covariance, whereas others take a simpler form when viewed in terms of the precision.
We thereforealsointroducethepartitionedformoftheprecisionmatrix Λaa Λab Λ= (2.69) Λba Λbb corresponding to the partitioning (2.65) of the vector x.
Because the inverse of a Exercise 2.22 symmetricmatrixisalsosymmetric, weseethatΛaa andΛbb aresymmetric, while ΛT ab = Λba.
Itshouldbestressedatthispointthat, forinstance, Λaa isnotsimply givenbytheinverseofΣaa.
Infact, weshallshortlyexaminetherelationbetween theinverseofapartitionedmatrixandtheinversesofitspartitions.
Letusbeginbyfindinganexpressionfortheconditionaldistributionp(xa |xb).
Fromtheproductruleofprobability, weseethatthisconditionaldistributioncanbe 86 2.
PROBABILITYDISTRIBUTIONS evaluated from the joint distribution p(x) = p(xa, xb) simply by fixing xb to the observedvalueandnormalizingtheresultingexpressiontoobtainavalidprobability distribution over xa.
Instead of performing this normalization explicitly, we can obtainthesolutionmoreefficientlybyconsideringthequadraticformintheexponent of the Gaussian distribution given by (2.44) and then reinstating the normalization coefficient at the end of the calculation.
If we make use of the partitioning (2.65), (2.66), and(2.69), weobtain 1 − (x−µ)TΣ −1(x−µ)= 2 1 1 − (xa −µ a )TΛaa(xa −µ a )− (xa −µ a )TΛab(xb −µ b ) 2 2 1 1 − (xb −µ b )TΛba(xa −µ a )− (xb −µ b )TΛbb(xb −µ b ).
(2.70) 2 2 We see that as a function of xa, this is again a quadratic form, and hence the cor- responding conditional distribution p(xa |xb) will be Gaussian.
Because this distri- bution is completely characterized by its mean and its covariance, our goal will be to identify expressions for the mean and covariance of p(xa |xb) by inspection of (2.70).
This is an example of a rather common operation associated with Gaussian distributions, sometimes called ‘completing the square’, in which we are given a quadraticformdefiningtheexponenttermsina Gaussiandistribution, andweneed todeterminethecorrespondingmeanandcovariance.
Suchproblemscanbesolved straightforwardly by noting that the exponent in a general Gaussian distribution N(x|µ,Σ)canbewritten 1 1 − (x−µ)TΣ −1(x−µ)=− x TΣ −1x+x TΣ −1µ+const (2.71) 2 2 where‘const’ denotes termswhich areindependent ofx, andwehavemadeuse of the symmetry of Σ.
Thus if we take our general quadratic form and express it in theformgivenbytheright-handsideof(2.71), thenwecanimmediatelyequatethe matrix of coefficients entering the second order term in x to the inverse covariance matrixΣ −1 andthecoefficientofthelinearterminxtoΣ −1µ, fromwhichwecan obtainµ.
Nowletusapplythisproceduretotheconditional Gaussiandistributionp(xa |xb) forwhichthequadraticformintheexponentisgivenby(2.70).
Wewilldenotethe mean and covariance of this distribution by µ a|b and Σ a|b, respectively.
Consider thefunctionaldependenceof(2.70)onxa inwhichxb isregardedasaconstant.
If wepickoutalltermsthataresecondorderinxa, wehave 1 − x T a Λaaxa (2.72) 2 fromwhichwecanimmediatelyconcludethatthecovariance(inverseprecision)of p(xa |xb)isgivenby Σ a|b =Λ − aa 1.
(2.73) 2.3.
The Gaussian Distribution 87 Nowconsiderallofthetermsin(2.70)thatarelinearinxa x T a {Λaa µ a −Λab(xb −µ b )} (2.74) where we have used ΛT ba = Λab.
From our discussion of the general form (2.71), thecoefficientofxa inthisexpressionmustequalΣ − a| 1 b µ a|b andhence µ a|b = Σ a|b {Λaa µ a −Λab(xb −µ b )} = µ a −Λ − aa 1Λab(xb −µ b ) (2.75) wherewehavemadeuseof(2.73).
Theresults(2.73)and(2.75)areexpressedintermsofthepartitionedprecision matrixoftheoriginaljointdistributionp(xa, xb).
Wecanalsoexpresstheseresults intermsofthecorrespondingpartitionedcovariancematrix.
Todothis, wemakeuse Exercise 2.24 ofthefollowingidentityfortheinverseofapartitionedmatrix A B −1 M −MBD−1 C D = −D−1CM D−1+D−1CMBD−1 (2.76) wherewehavedefined M=(A−BD −1C) −1.
(2.77) Thequantity M−1 isknownasthe Schurcomplementofthematrixontheleft-hand sideof(2.76)withrespecttothesubmatrix D.
Usingthedefinition −1 Σaa Σab Λaa Λab = (2.78) Σba Σbb Λba Λbb andmakinguseof(2.76), wehave Λaa = (Σaa −ΣabΣ − bb 1Σba) −1 (2.79) Λab = −(Σaa −ΣabΣ − bb 1Σba) −1ΣabΣ − bb 1.
(2.80) Fromtheseweobtainthefollowingexpressionsforthemeanandcovarianceofthe conditionaldistributionp(xa |xb) µ a|b = µ a +ΣabΣ − bb 1(xb −µ b ) (2.81) Σ a|b = Σaa −ΣabΣ − bb 1Σba.
(2.82) Comparing(2.73)and(2.82), weseethattheconditionaldistributionp(xa |xb)takes a simpler form when expressed in terms of the partitioned precision matrix than when it is expressed in terms of the partitioned covariance matrix.
Note that the meanoftheconditionaldistributionp(xa |xb), givenby(2.81), isalinearfunctionof xb andthatthecovariance, givenby(2.82), isindependentofxa.
Thisrepresentsan Section8.1.4 exampleofalinear-Gaussianmodel.
88 2.
PROBABILITYDISTRIBUTIONS 2.3.2 Marginal Gaussian distributions We have seen that if a joint distribution p(xa, xb) is Gaussian, then the condi- tionaldistributionp(xa |xb)willagainbe Gaussian.
Nowweturntoadiscussionof themarginaldistributiongivenby p(xa)= p(xa, xb)dxb (2.83) which, asweshallsee, isalso Gaussian.
Onceagain, ourstrategyforevaluatingthis distributionefficientlywillbetofocusonthequadraticformintheexponentofthe joint distribution and thereby to identify the mean and covariance of the marginal distributionp(xa).
The quadratic form for the joint distribution can be expressed, using the par- titioned precision matrix, in the form (2.70).
Because our goal is to integrate out xb, thisismosteasilyachievedbyfirstconsideringthetermsinvolvingxb andthen completingthesquareinordertofacilitateintegration.
Pickingoutjustthoseterms thatinvolvexb, wehave 1 1 1 − 2 x T b Λbbxb+x T b m=− 2 (xb −Λ − bb 1m)TΛbb(xb −Λ − bb 1m)+ 2 m TΛ − bb 1m (2.84) wherewehavedefined m=Λbb µ b −Λba(xa −µ a ).
(2.85) Weseethatthedependenceonxbhasbeencastintothestandardquadraticformofa Gaussiandistributioncorrespondingtothefirsttermontheright-handsideof(2.84), plus a term that does not depend on xb (but that does depend on xa).
Thus, when we take the exponential of this quadratic form, we see that the integration over xb requiredby(2.83)willtaketheform 1 exp − 2 (xb −Λ − bb 1m)TΛbb(xb −Λ − bb 1m) dxb.
(2.86) This integration is easily performed by noting that it is the integral over an unnor- malized Gaussian, and so the result will be the reciprocal of the normalization co- efficient.
We know fromthe form of the normalized Gaussian givenby (2.43), that this coefficient is independent of the mean and depends only on the determinant of the covariance matrix.
Thus, by completing the square with respect to xb, we can integrateoutxb andtheonlytermremainingfromthecontributionsontheleft-hand sideof(2.84)thatdependsonxa isthelasttermontheright-handsideof(2.84)in which m is given by (2.85).
Combining this term with the remaining terms from 2.3.
The Gaussian Distribution 89 (2.70)thatdependonxa, weobtain 1 2 [Λbb µ b −Λba(xa −µ a )] T Λ − bb 1[Λbb µ b −Λba(xa −µ a )] 1 − x T a Λaaxa+x T a (Λaa µ a +Λab µ b )+const 2 1 = − 2 x T a (Λaa −ΛabΛ − bb 1Λba)xa +x T a (Λaa −ΛabΛ − bb 1Λba) −1µ a +const (2.87) where ‘const’ denotes quantities independent of xa.
Again, by comparison with (2.71), weseethatthecovarianceofthemarginaldistributionofp(xa)isgivenby Σa =(Λaa −ΛabΛ − bb 1Λba) −1.
(2.88) Similarly, themeanisgivenby Σa(Λaa −ΛabΛ − bb 1Λba)µ a =µ a (2.89) where we have used (2.88).
The covariance in (2.88) is expressed in terms of the partitioned precision matrix given by (2.69).
We can rewrite this in terms of the corresponding partitioning of the covariance matrix given by (2.67), as we did for theconditionaldistribution.
Thesepartitionedmatricesarerelatedby −1 Λaa Λab Σaa Σab = (2.90) Λba Λbb Σba Σbb Makinguseof(2.76), wethenhave Λaa −ΛabΛ − bb 1Λba −1 =Σaa.
(2.91) Thus we obtain the intuitively satisfying result that the marginal distribution p(xa) hasmeanandcovariancegivenby E[xa] = µ a (2.92) cov[xa] = Σaa.
(2.93) Weseethatforamarginaldistribution, themeanandcovariancearemostsimplyex- pressed in terms of the partitioned covariance matrix, in contrast to the conditional distribution for which the partitioned precision matrix gives rise to simpler expres- sions.
Ourresultsforthemarginalandconditionaldistributionsofapartitioned Gaus- sianaresummarizedbelow.
Partitioned Gaussians Givenajoint Gaussiandistribution N(x|µ,Σ)withΛ≡Σ −1 and x= xa , µ= µ a (2.94) xb µ b 90 2.
PROBABILITYDISTRIBUTIONS 1 10 xb xb =0.7 p(xa |xb=0.7) 0.5 5 p(xa, xb) p(xa) 0 0 0 0.5 xa 1 0 0.5 xa 1 Figure2.9 Theplotontheleftshowsthecontoursofa Gaussiandistributionp(x a , x b )overtwovariables, and theplotontherightshowsthemarginaldistributionp(x a )(bluecurve)andtheconditionaldistributionp(x a |x b ) forx b =0.7(redcurve).
Σaa Σab Λaa Λab Σ= , Λ= .
(2.95) Σba Σbb Λba Λbb Conditionaldistribution: p(xa |xb) = N(x|µ a|b ,Λ − aa 1) (2.96) µ a|b = µ a −Λ − aa 1Λab(xb −µ b ).
(2.97) Marginaldistribution: p(xa)=N(xa |µ a ,Σaa).
(2.98) We illustrate the idea of conditional and marginal distributions associated with amultivariate Gaussianusinganexampleinvolvingtwovariablesin Figure2.9.
2.3.3 Bayes’ theorem for Gaussian variables In Sections 2.3.1 and 2.3.2, we considered a Gaussian p(x) in which we parti- tionedthevectorxintotwosubvectorsx=(xa, xb)andthenfoundexpressionsfor theconditionaldistributionp(xa |xb)andthemarginaldistributionp(xa).
Wenoted that the mean of the conditional distribution p(xa |xb) was a linear function of xb.
Hereweshallsupposethatwearegivena Gaussianmarginaldistributionp(x)anda Gaussianconditionaldistributionp(y|x)inwhichp(y|x)hasameanthatisalinear function of x, and a covariance which is independent of x.
This is an example of 2.3.
The Gaussian Distribution 91 a linear Gaussian model (Roweis and Ghahramani, 1999), which we shall study in greater generality in Section 8.1.4.
We wish to find the marginal distribution p(y) andtheconditionaldistributionp(x|y).
Thisisaproblemthatwillarisefrequently insubsequentchapters, anditwillproveconvenienttoderivethegeneralresultshere.
Weshalltakethemarginalandconditionaldistributionstobe p(x) = N x|µ,Λ −1 (2.99) p(y|x) = N y|Ax+b, L −1 (2.100) whereµ, A, andbareparametersgoverningthemeans, andΛand Lareprecision matrices.
Ifxhasdimensionality M andyhasdimensionality D, thenthematrix A hassize D×M.
Firstwefindanexpressionforthejointdistributionoverxandy.
Todothis, we define x z= (2.101) y andthenconsiderthelogofthejointdistribution lnp(z) = lnp(x)+lnp(y|x) 1 = − (x−µ)TΛ(x−µ) 2 1 − (y−Ax−b)TL(y−Ax−b)+const (2.102) 2 where‘const’denotestermsindependentofxandy.
Asbefore, weseethatthisisa quadraticfunctionofthecomponentsofz, andhencep(z)is Gaussiandistribution.
Tofindtheprecisionofthis Gaussian, weconsiderthesecondordertermsin(2.102), whichcanbewrittenas 1 1 1 1 − x T(Λ+ATLA)x− y TLy+ y TLAx+ x TATLy 2 2 2 2 T 1 x Λ+ATLA −ATL x 1 = − =− z TRz (2.103) 2 y −LA L y 2 and so the Gaussian distribution over z has precision (inverse covariance) matrix givenby Λ+ATLA −ATL R= .
(2.104) −LA L Thecovariancematrixisfoundbytakingtheinverseoftheprecision, whichcanbe Exercise 2.29 doneusingthematrixinversionformula(2.76)togive Λ −1 Λ −1AT cov[z]=R −1 = AΛ −1 L−1+AΛ −1AT .
(2.105) 92 2.
PROBABILITYDISTRIBUTIONS Similarly, wecanfindthemeanofthe Gaussiandistributionoverzbyidentify- ingthelineartermsin(2.102), whicharegivenby T x Λµ−ATLb x TΛµ−x TATLb+y TLb= .
(2.106) y Lb Usingourearlierresult(2.71)obtainedbycompletingthesquareoverthequadratic formofamultivariate Gaussian, wefindthatthemeanofzisgivenby Λµ−ATLb E[z]=R −1 .
(2.107) Lb Exercise 2.30 Makinguseof(2.105), wethenobtain µ E[z]= .
(2.108) Aµ+b Nextwefindanexpressionforthemarginaldistributionp(y)inwhichwehave marginalizedoverx.
Recallthatthemarginaldistributionoverasubsetofthecom- ponents of a Gaussian random vector takes a particularly simple form when ex- Section2.3 pressed in terms of the partitioned covariance matrix.
Specifically, its mean and covariance are given by (2.92) and (2.93), respectively.
Making use of (2.105) and (2.108) we see that the mean and covariance of the marginal distribution p(y) are givenby E[y] = Aµ+b (2.109) cov[y] = L −1+AΛ −1AT.
(2.110) Aspecialcaseofthisresultiswhen A=I, inwhichcaseitreducestotheconvolu- tionoftwo Gaussians, forwhichweseethatthemeanoftheconvolutionisthesum ofthemeanofthetwo Gaussians, andthecovarianceoftheconvolutionisthesum oftheircovariances.
Finally, weseekanexpressionfortheconditionalp(x|y).
Recallthattheresults fortheconditionaldistributionaremosteasilyexpressedintermsofthepartitioned (2.108) we see that the conditional distribution p(x|y) has mean and covariance givenby E[x|y] = (Λ+ATLA) −1 ATL(y−b)+Λµ (2.111) cov[x|y] = (Λ+ATLA) −1.
(2.112) Theevaluationofthisconditionalcanbeseenasanexampleof Bayes’theorem.
We can interpret the distribution p(x) as a prior distribution over x.
If the variable yisobserved, thentheconditionaldistributionp(x|y)representsthecorresponding posterior distribution over x.
Having found the marginal and conditional distribu- tions, weeffectivelyexpressedthejointdistributionp(z) = p(x)p(y|x)intheform p(x|y)p(y).
Theseresultsaresummarizedbelow.
2.3.
The Gaussian Distribution 93 Marginaland Conditional Gaussians Givenamarginal Gaussiandistributionforxandaconditional Gaussiandistri- butionforygivenxintheform p(x) = N(x|µ,Λ −1) (2.113) p(y|x) = N(y|Ax+b, L −1) (2.114) the marginal distribution of y and the conditional distribution of x given y are givenby p(y) = N(y|Aµ+b, L −1+AΛ −1AT) (2.115) p(x|y) = N(x|Σ{ATL(y−b)+Λµ},Σ) (2.116) where Σ=(Λ+ATLA) −1.
(2.117) 2.3.4 Maximum likelihood for the Gaussian Given a data set X = (x 1 ,..., x N)T in which the observations {xn } are as- sumedtobedrawnindependentlyfromamultivariate Gaussiandistribution, wecan estimate the parameters of the distribution by maximum likelihood.
The log likeli- hoodfunctionisgivenby N ND N 1 lnp(X|µ,Σ)=− ln(2π)− ln|Σ|− (xn −µ)TΣ −1(xn −µ).
(2.118) 2 2 2 n=1 Bysimplerearrangement, weseethatthelikelihoodfunctiondependsonthedataset onlythroughthetwoquantities N N xn, xnx T n .
(2.119) n=1 n=1 These are known as the sufficient statistics for the Gaussian distribution.
Using Appendix C (C.19), thederivativeoftheloglikelihoodwithrespecttoµisgivenby N ∂ ∂µ lnp(X|µ,Σ)= Σ −1(xn −µ) (2.120) n=1 andsettingthisderivativetozero, weobtainthesolutionforthemaximumlikelihood estimateofthemeangivenby N 1 µ ML = N xn (2.121) n=1 94 2.
PROBABILITYDISTRIBUTIONS which is the mean of the observed set of data points.
The maximization of (2.118) with respect to Σ is rather more involved.
The simplest approach is to ignore the Exercise 2.34 symmetry constraint and show that the resulting solution is symmetric as required.
Alternativederivationsofthisresult, whichimposethesymmetryandpositivedefi- nitenessconstraintsexplicitly, canbefoundin Magnusand Neudecker(1999).
The resultisasexpectedandtakestheform N 1 Σ ML = N (xn −µ ML )(xn −µ ML )T (2.122) n=1 which involves µ because this is the result of a joint maximization with respect ML toµandΣ.
Notethatthesolution(2.121)forµ ML doesnotdependonΣ ML, andso wecanfirstevaluateµ ML andthenusethistoevaluateΣ ML.
If we evaluate the expectations of the maximum likelihood solutions under the Exercise 2.35 truedistribution, weobtainthefollowingresults E[µ ] = µ (2.123) ML N −1 E[Σ ML ] = Σ.
(2.124) N Weseethattheexpectationofthemaximumlikelihoodestimateforthemeanisequal tothetruemean.
However, themaximumlikelihoodestimateforthecovariancehas anexpectationthatislessthanthetruevalue, andhenceitisbiased.
Wecancorrect thisbiasbydefiningadifferentestimatorΣgivenby N Σ = N 1 −1 (xn −µ ML )(xn −µ ML )T.
(2.125) n=1 Clearlyfrom(2.122)and(2.124), theexpectationofΣisequaltoΣ.
2.3.5 Sequential estimation Ourdiscussionofthemaximumlikelihoodsolutionfortheparametersofa Gaus- siandistributionprovidesaconvenientopportunitytogiveamoregeneraldiscussion of the topic of sequential estimation for maximum likelihood.
Sequential methods allowdatapointstobeprocessedoneatatimeandthendiscardedandareimportant for on-line applications, and also where large data sets are involved so that batch processingofalldatapointsatonceisinfeasible.
Consider the result (2.121) for the maximum likelihood estimator of the mean µ , which we will denote by µ(N) when it is based on N observations.
If we ML ML 2.3.
The Gaussian Distribution 95 z Figure2.10 Aschematicillustrationoftwocorrelatedran- dom variables z and θ, together with the regression function f(θ) given by the con- ditional expectation E[z|θ].
The Robbins- f(θ) Monroalgorithmprovidesageneralsequen- tial procedure for finding the root θ of such functions.
θ θ dissectoutthecontributionfromthefinaldatapointx N, weobtain N 1 µ( M N L ) = N xn n=1 N −1 1 1 = x N + xn N N n=1 1 N −1 = N x N + N µ( M N L −1) 1 = µ( M N L −1) + N (x N −µ M (N L −1) ).
(2.126) Thisresulthasaniceinterpretation, asfollows.
After observing N −1datapoints wehaveestimatedµbyµ( M N L −1) .
Wenowobservedatapointx N, andweobtainour revised estimate µ(N) by moving the old estimate a small amount, proportional to ML 1/N, inthedirectionofthe‘errorsignal’(x N −µ M (N L −1) ).
Notethat, as N increases, sothecontributionfromsuccessivedatapointsgetssmaller.
Theresult(2.126)willclearlygivethesameanswer asthebatchresult(2.121) becausethetwoformulaeareequivalent.
However, wewillnotalwaysbeabletode- riveasequentialalgorithmbythisroute, andsoweseekamoregeneralformulation ofsequentiallearning, whichleadsustothe Robbins-Monroalgorithm.
Considera pair of random variables θ and z governed by a joint distribution p(z,θ).
The con- ditional expectation of z given θ defines a deterministic function f(θ) that is given by f(θ)≡E[z|θ]= zp(z|θ)dz (2.127) and is illustrated schematically in Figure 2.10.
Functions defined in this way are calledregressionfunctions.
Our goal is to find the root θ at which f(θ ) = 0.
If we had a large data set ofobservationsofz andθ, thenwecouldmodeltheregressionfunctiondirectlyand then obtain an estimate of its root.
Suppose, however, that we observe values of z one at a time and we wish to find a corresponding sequential estimation scheme for θ .
The following general procedure for solving such problems was given by 96 2.
PROBABILITYDISTRIBUTIONS Robbins and Monro (1951).
We shall assume that the conditional variance of z is finitesothat E (z−f)2 |θ <∞ (2.128) and we shall also, without loss of generality, consider the case wheref(θ) > 0 for θ > θ andf(θ) < 0forθ < θ , asisthecasein Figure2.10.
The Robbins-Monro procedurethendefinesasequenceofsuccessiveestimatesoftherootθ givenby θ(N) =θ(N−1)+a N−1 z(θ(N−1)) (2.129) wherez(θ(N))isanobservedvalueofzwhenθtakesthevalueθ(N).
Thecoefficients {a N }representasequenceofpositivenumbersthatsatisfytheconditions lim a N = 0 (2.130) N→∞ ∞ a N = ∞ (2.131) N=1 ∞ a2 < ∞.
(2.132) N N=1 Itcanthenbeshown(Robbinsand Monro,1951; Fukunaga,1990)thatthesequence ofestimatesgivenby(2.129)doesindeedconvergetotherootwithprobabilityone.
Notethatthefirstcondition(2.130)ensuresthatthesuccessivecorrectionsdecrease inmagnitudesothattheprocesscanconvergetoalimitingvalue.
Thesecondcon- dition(2.131)isrequiredtoensurethatthealgorithmdoesnotconvergeshortofthe root, andthethirdcondition(2.132)isneededtoensurethattheaccumulatednoise hasfinitevarianceandhencedoesnotspoilconvergence.
Nowletusconsiderhowageneralmaximumlikelihoodproblemcanbesolved sequentiallyusingthe Robbins-Monroalgorithm.
Bydefinition, themaximumlike- lihood solution θ ML is a stationary point of the log likelihood function and hence satisfies N ∂ 1 ∂θ N lnp(xn |θ) =0.
(2.133) n=1 θ ML Exchangingthederivativeandthesummation, andtakingthelimit N →∞wehave N 1 ∂ ∂ lim lnp(xn |θ)=E x lnp(x|θ) (2.134) N→∞N ∂θ ∂θ n=1 and so we see that finding the maximum likelihood solution corresponds to find- ing the root of a regression function.
We can therefore apply the Robbins-Monro procedure, whichnowtakestheform ∂ θ(N) =θ(N−1)+a N−1 ∂θ(N−1) lnp(x N |θ(N−1)).
(2.135) 2.3.
The Gaussian Distribution 97 Figure2.11 In the case of a Gaussian distribution, with θ z corresponding to the mean µ, the regression functionillustratedin Figure2.10takestheform p(z|µ) of a straight line, as shown in red.
In this case, therandomvariablezcorrespondstothe d g e iv r e iv n a b ti y ve (x o − f t µ h M e L lo )/ g σ l 2 ik , e a l n ih d o i o ts d e f x u p n e c c ti t o a n tio a n nd tha is t µML definestheregressionfunctionisastraightline givenby(µ−µ ML )/σ2.
Therootoftheregres- sionfunctioncorrespondstothemaximumlike- µ lihoodestimatorµ ML.
As a specific example, we consider once again the sequential estimation of the mean of a Gaussian distribution, in which case the parameter θ(N) is the estimate (N) µ ofthemeanofthe Gaussian, andtherandomvariablez isgivenby ML ∂ 1 z = ∂µ lnp(x|µ ML ,σ2)= σ2 (x−µ ML ).
(2.136) ML Thus the distribution of z is Gaussian with mean µ − µ ML, as illustrated in Fig- provided we choose the coefficients a N to have the form a N = σ2/N.
Note that although we have focussed on the case of a single variable, the same technique, together with the same restrictions (2.130)–(2.132) on the coefficients a N, apply equallytothemultivariatecase(Blum,1965).
2.3.6 Bayesian inference for the Gaussian Themaximumlikelihoodframeworkgavepointestimatesfortheparametersµ and Σ.
Now we develop a Bayesian treatment by introducing prior distributions over these parameters.
Let us begin with a simple example in which we consider a single Gaussianrandomvariablex.
Weshallsupposethatthevarianceσ2 isknown, and we consider the task of inferring the mean µ given a set of N observations X = {x 1 ,..., x N }.
Thelikelihoodfunction, thatistheprobabilityoftheobserved datagivenµ, viewedasafunctionofµ, isgivenby N N 1 1 p(X|µ)= p(xn |µ)= (2πσ2)N/2 exp − 2σ2 (xn −µ)2 .
(2.137) n=1 n=1 Againweemphasize thatthelikelihood functionp(X|µ)isnotaprobabilitydistri- butionoverµandisnotnormalized.
Weseethatthelikelihoodfunctiontakestheformoftheexponentialofaquad- ratic form in µ.
Thus if we choose a prior p(µ) given by a Gaussian, it will be a 98 2.
PROBABILITYDISTRIBUTIONS conjugatedistributionforthislikelihoodfunctionbecausethecorrespondingposte- riorwillbeaproductoftwoexponentialsofquadraticfunctionsofµandhencewill alsobe Gaussian.
Wethereforetakeourpriordistributiontobe p(µ)=N µ|µ 0 ,σ 0 2 (2.138) andtheposteriordistributionisgivenby p(µ|X)∝p(X|µ)p(µ).
(2.139) Exercise 2.38 Simplemanipulationinvolvingcompletingthesquareintheexponentshowsthatthe posteriordistributionisgivenby p(µ|X)=N µ|µN,σ N 2 (2.140) where σ2 Nσ2 µN = Nσ2+σ2 µ 0 + Nσ2+ 0 σ2 µ ML (2.141) 0 0 1 1 N = + (2.142) σ2 σ2 σ2 N 0 inwhichµ ML isthemaximumlikelihoodsolutionforµgivenbythesamplemean N 1 µ ML = xn.
(2.143) N n=1 It is worth spending a moment studying the form of the posterior mean and variance.
First of all, we note that the mean of the posterior distribution given by (2.141) is a compromise between the prior mean µ 0 and the maximum likelihood solution µ ML.
If the number of observed data points N = 0, then (2.141) reduces to the prior mean as expected.
For N → ∞, the posterior mean is given by the maximumlikelihoodsolution.
Similarly, considertheresult(2.142)forthevariance of the posterior distribution.
We see that this is most naturally expressed in terms of the inverse variance, which is called the precision.
Furthermore, the precisions are additive, so that the precision of the posterior is given by the precision of the prior plus one contribution of the data precision from each of the observed data points.
As we increase the number of observed data points, the precision steadily increases, correspondingtoaposteriordistributionwithsteadilydecreasingvariance.
Withnoobserveddatapoints, wehavethepriorvariance, whereasifthenumberof data points N → ∞, the variance σ2 goes to zero and the posterior distribution N becomes infinitely peaked around the maximum likelihood solution.
We therefore seethatthemaximumlikelihoodresultofapointestimateforµgivenby(2.143)is recovered precisely from the Bayesian formalism in the limit of an infinite number ofobservations.
Notealsothatforfinite N, ifwetakethelimitσ2 →∞inwhichthe 0 priorhasinfinitevariancethentheposteriormean(2.141)reducestothemaximum likelihoodresult, whilefrom(2.142)theposteriorvarianceisgivenbyσ2 =σ2/N.
N 2.3.
The Gaussian Distribution 99 Figure2.12 Illustration of Bayesian inference for 5 the mean µ of a Gaussian distri- bution, in which the variance is as- sumed to be known.
The curves show the prior distribution over µ N =10 (the curve labelled N = 0), which inthiscaseisitself Gaussian, along N =2 with the posterior distribution given by(2.140)forincreasingnumbers N N =1 of data points.
The data points are generatedfroma Gaussianofmean N =0 0.8andvariance0.1, andtheprioris chosentohavemean0.
Inboththe 0 priorandthelikelihoodfunction, the −1 0 1 varianceissettothetruevalue.
We illustrate our analysis of Bayesian inference for the mean of a Gaussian distribution in Figure 2.12.
The generalization of this result to the case of a D- dimensional Gaussianrandomvariablexwithknowncovarianceandunknownmean Exercise 2.40 isstraightforward.
Wehavealreadyseenhowthemaximumlikelihoodexpressionforthemeanof Section2.3.5 a Gaussian can be re-cast as a sequential update formula in which the mean after observing N datapointswasexpressedintermsofthemeanafterobserving N −1 datapointstogetherwiththecontributionfromdatapointx N.
Infact, the Bayesian paradigmleadsverynaturallytoasequentialviewoftheinferenceproblem.
Tosee thisinthecontextoftheinferenceofthemeanofa Gaussian, wewritetheposterior distributionwiththecontributionfromthefinaldatapointx N separatedoutsothat N −1 p(µ|D)∝ p(µ) p(xn |µ) p(x N |µ).
(2.144) n=1 The term in square brackets is (up to a normalization coefficient) just the posterior distribution after observing N −1 data points.
We see that this can be viewed as a prior distribution, which is combined using Bayes’ theorem with the likelihood function associated with data point x N to arrive at the posterior distribution after observing N datapoints.
Thissequentialviewof Bayesianinferenceisverygeneral andappliestoanyprobleminwhichtheobserveddataareassumedtobeindependent andidenticallydistributed.
Sofar, wehaveassumedthatthevarianceofthe Gaussiandistributionoverthe data is known and our goal is to infer the mean.
Now let us suppose that the mean isknownandwewishtoinferthevariance.
Again, ourcalculationswillbegreatly simplifiedifwechooseaconjugateformforthepriordistribution.
Itturnsouttobe mostconvenienttoworkwiththeprecisionλ≡1/σ2.
Thelikelihoodfunctionforλ takestheform N N λ p(X|λ)= N(xn |µ,λ −1)∝λ N/2exp − (xn −µ)2 .
(2.145) 2 n=1 n=1 100 2.
PROBABILITYDISTRIBUTIONS 2 2 2 a=0.1 a=1 a=4 b=0.1 b=1 b=6 1 1 1 0 0 0 0 λ 1 2 0 λ 1 2 0 λ 1 2 Figure2.13 Plotofthegammadistribution Gam(λ|a, b)definedby(2.146)forvariousvaluesoftheparameters aandb.
The corresponding conjugate prior should therefore be proportional to the product of a power of λ and the exponential of a linear function of λ.
This corresponds to thegammadistributionwhichisdefinedby 1 Gam(λ|a, b)= b a λ a−1exp(−bλ).
(2.146) Γ(a) Here Γ(a) is the gamma function that is defined by (1.141) and that ensures that Exercise 2.41 (2.146)iscorrectlynormalized.
Thegammadistributionhasafiniteintegralifa>0, andthedistributionitselfisfiniteifa 1.
Itisplotted, forvariousvaluesofaand Exercise 2.42 b, in Figure2.13.
Themeanandvarianceofthegammadistributionaregivenby a E[λ] = (2.147) b a var[λ] = .
(2.148) b2 Consider a prior distribution Gam(λ|a 0 , b 0 ).
If we multiply by the likelihood function(2.145), thenweobtainaposteriordistribution N λ p(λ|X)∝λ a 0 −1λ N/2exp −b 0 λ− (xn −µ)2 (2.149) 2 n=1 whichwerecognizeasagammadistributionoftheform Gam(λ|a N, b N)where N a N = a 0 + (2.150) 2 N 1 N b N = b 0 + 2 (xn −µ)2 =b 0 + 2 σ M 2 L (2.151) n=1 whereσ2 isthemaximumlikelihoodestimatorofthevariance.
Notethatin(2.149) ML there is no need to keep track of the normalization constants in the prior and the likelihood function because, if required, the correct coefficient can be found at the endusingthenormalizedform(2.146)forthegammadistribution.
2.3.
The Gaussian Distribution 101 From (2.150), we see that the effect of observing N data points is to increase the value of the coefficient a by N/2.
Thus we can interpret the parameter a 0 in the prior in terms of 2a 0 ‘effective’ prior observations.
Similarly, from (2.151) we see that the N data points contribute Nσ2 /2 to the parameter b, where σ2 is ML ML the variance, and so we can interpret the parameter b 0 in the prior as arising from the 2a 0 ‘effective’ prior observations having variance 2b 0 /(2a 0 ) = b 0 /a 0.
Recall Section2.2 thatwemadeananalogousinterpretationforthe Dirichletprior.
Thesedistributions are examples of the exponential family, and we shall see that the interpretation of a conjugate prior in terms of effective fictitious data points is a general one for the exponentialfamilyofdistributions.
Insteadofworkingwiththeprecision, wecanconsiderthevarianceitself.
The conjugate prior in this case is called the inverse gamma distribution, although we shall not discuss this further because we will find it more convenient to work with theprecision.
Now suppose that both the mean and the precision are unknown.
To find a conjugateprior, weconsiderthedependenceofthelikelihoodfunctiononµandλ N 1/2 λ λ p(X|µ,λ)= exp − (xn −µ)2 2π 2 n=1 λµ2 N N λ N ∝ λ1/2exp − exp λµ xn − x2 n .
(2.152) 2 2 n=1 n=1 We now wish to identify a prior distribution p(µ,λ) that has the same functional dependenceonµandλasthelikelihoodfunctionandthatshouldthereforetakethe form λµ2 β p(µ,λ)∝ λ1/2exp − exp{cλµ−dλ} 2 βλ c2 = exp − (µ−c/β)2 λ β/2exp − d− λ (2.153) 2 2β wherec, d, andβ areconstants.
Sincewecanalwayswritep(µ,λ) = p(µ|λ)p(λ), we can find p(µ|λ) and p(λ) by inspection.
In particular, we see that p(µ|λ) is a Gaussianwhoseprecisionisalinearfunctionofλandthatp(λ)isagammadistri- bution, sothatthenormalizedpriortakestheform p(µ,λ)=N(µ|µ 0 ,(βλ) −1)Gam(λ|a, b) (2.154) where we have defined new constants given by µ 0 = c/β, a = 1 + β/2, b = d−c2/2β.
Thedistribution(2.154)iscalledthenormal-gammaor Gaussian-gamma distribution and is plotted in Figure 2.14.
Note that this is not simply the product of an independent Gaussian prior over µ and a gamma prior over λ, because the precision of µ is a linear function of λ.
Even if we chose a prior in which µ and λ were independent, the posterior distribution would exhibit a coupling between the precisionofµandthevalueofλ.
102 2.
PROBABILITYDISTRIBUTIONS Figure2.14 Contourplotofthenormal-gamma 2 distribution (2.154) for parameter values µ 0 = 0, β = 2, a = 5 and b=6.
λ 1 0 −2 0 2 µ In the case of the multivariate Gaussian distribution N x|µ,Λ −1 for a D- dimensional variable x, the conjugate prior distribution for the mean µ, assuming theprecisionisknown, isagaina Gaussian.
Forknownmeanandunknownprecision Exercise 2.45 matrixΛ, theconjugateprioristhe Wishartdistributiongivenby 1 W(Λ|W,ν)=B|Λ|(ν−D−1)/2exp − Tr(W −1Λ) (2.155) 2 whereνiscalledthenumberofdegreesoffreedomofthedistribution, Wisa D×D scalematrix, and Tr(·)denotesthetrace.
Thenormalizationconstant B isgivenby D −1 ν+1−i B(W,ν)=|W|−ν/2 2 νD/2π D(D−1)/4 Γ .
(2.156) 2 i=1 Again, itisalsopossibletodefineaconjugateprioroverthecovariancematrixitself, rather than over the precision matrix, which leads to the inverse Wishart distribu- tion, although we shall not discuss this further.
If both the mean and the precision are unknown, then, following a similar line of reasoning to the univariate case, the conjugatepriorisgivenby p(µ,Λ|µ ,β, W,ν)=N(µ|µ ,(βΛ) −1)W(Λ|W,ν) (2.157) 0 0 whichisknownasthenormal-Wishartor Gaussian-Wishartdistribution.
2.3.7 Student’s t-distribution We have seen that the conjugate prior for the precision of a Gaussian is given Section2.3.6 by a gamma distribution.
If we have a univariate Gaussian N(x|µ,τ−1) together witha Gammaprior Gam(τ|a, b)andweintegrateouttheprecision, weobtainthe Exercise 2.46 marginaldistributionofxintheform 2.3.
The Gaussian Distribution 103 Figure2.15 Plotof Student’st-distribution(2.159) 0.5 forµ=0andλ=1forvariousvalues of ν.
The limit ν → ∞ corresponds ν →∞ to a Gaussian distribution with mean 0.4 ν =1.0 µandprecisionλ.
ν =0.1 0.3 0.2 0.1 0 −5 0 5 ∞ p(x|µ, a, b) = N(x|µ,τ −1)Gam(τ|a, b)dτ (2.158) 0 ∞ bae(−bτ)τa−1 τ 1/2 τ = exp − (x−µ)2 dτ Γ(a) 2π 2 0 ba 1 1/2 (x−µ)2 −a−1/2 = b+ Γ(a+1/2) Γ(a) 2π 2 wherewehavemadethechangeofvariablez = τ[b+(x−µ)2/2].
Byconvention we define new parameters given by ν = 2a and λ = a/b, in terms of which the distributionp(x|µ, a, b)takestheform Γ(ν/2+1/2) λ 1/2 λ(x−µ)2 −ν/2−1/2 St(x|µ,λ,ν)= 1+ (2.159) Γ(ν/2) πν ν whichisknownas Student’st-distribution.
Theparameterλissometimescalledthe precision of the t-distribution, even though it is not in general equal to the inverse of the variance.
The parameter ν is called the degrees of freedom, and its effect is illustratedin Figure2.15.
Fortheparticularcaseofν =1, thet-distributionreduces tothe Cauchydistribution, whileinthelimitν →∞thet-distribution St(x|µ,λ,ν) Exercise 2.47 becomesa Gaussian N(x|µ,λ−1)withmeanµandprecisionλ.
From (2.158), we see that Student’s t-distribution is obtained by adding up an infinitenumberof Gaussiandistributionshavingthesamemeanbutdifferentpreci- sions.
Thiscanbeinterpretedasaninfinitemixtureof Gaussians(Gaussianmixtures will be discussed in detail in Section 2.3.9.
The result is a distribution that in gen- eralhaslonger‘tails’thana Gaussian, aswasseenin Figure2.15.
Thisgivesthet- distributionanimportantpropertycalledrobustness, whichmeansthatitismuchless sensitive than the Gaussian to the presence of a few data points which are outliers.
Therobustnessofthet-distributionisillustratedin Figure2.16, whichcomparesthe maximumlikelihoodsolutionsfora Gaussianandat-distribution.
Notethatthemax- imum likelihood solution for the t-distribution can be found using the expectation- Exercise 12.24 maximization (EM) algorithm.
Here we see that the effect of a small number of 104 2.
PROBABILITYDISTRIBUTIONS 0.5 0.5 0.4 0.4 0.3 0.3 0.2 0.2 0.1 0.1 0 0 −5 0 5 10 −5 0 5 10 (a) (b) Figure 2.16 Illustration of the robustness of Student’s t-distribution compared to a Gaussian.
(a) Histogram distribution of 30 data points drawn from a Gaussian distribution, together with the maximum likelihood fit ob- tainedfromat-distribution(redcurve)anda Gaussian(greencurve, largelyhiddenbytheredcurve).
Because the t-distribution contains the Gaussian as a special case it gives almost the same solution as the Gaussian.
(b)Thesamedatasetbutwiththreeadditionaloutlyingdatapointsshowinghowthe Gaussian(greencurve)is stronglydistortedbytheoutliers, whereasthet-distribution(redcurve)isrelativelyunaffected.
outliersismuchlesssignificantforthet-distributionthanforthe Gaussian.
Outliers canariseinpracticalapplicationseitherbecausetheprocessthatgeneratesthedata correspondstoadistributionhavingaheavytailorsimplythroughmislabelleddata.
Robustness is also an important property for regression problems.
Unsurprisingly, theleastsquaresapproachtoregressiondoesnotexhibitrobustness, becauseitcor- responds to maximum likelihood under a (conditional) Gaussian distribution.
By basingaregressionmodelonaheavy-taileddistributionsuchasat-distribution, we obtainamorerobustmodel.
Ifwegobackto(2.158)andsubstitutethealternativeparametersν = 2a, λ = a/b, andη =τb/a, weseethatthet-distributioncanbewrittenintheform ∞ St(x|µ,λ,ν)= N x|µ,(ηλ) −1 Gam(η|ν/2,ν/2)dη.
(2.160) 0 Wecanthengeneralizethistoamultivariate Gaussian N(x|µ,Λ)toobtainthecor- respondingmultivariate Student’st-distributionintheform ∞ St(x|µ,Λ,ν)= N(x|µ,(ηΛ) −1)Gam(η|ν/2,ν/2)dη.
(2.161) 0 Usingthesametechniqueasfortheunivariatecase, wecanevaluatethisintegralto Exercise 2.48 give 2.3.
The Gaussian Distribution 105 Γ(D/2+ν/2) |Λ|1/2 ∆2 −D/2−ν/2 St(x|µ,Λ,ν)= 1+ (2.162) Γ(ν/2) (πν)D/2 ν where D is the dimensionality of x, and ∆2 is the squared Mahalanobis distance definedby ∆2 =(x−µ)TΛ(x−µ).
(2.163) This is the multivariate form of Student’s t-distribution and satisfies the following Exercise 2.49 properties E[x] = µ, if ν >1 (2.164) ν cov[x] = Λ −1, if ν >2 (2.165) (ν−2) mode[x] = µ (2.166) withcorrespondingresultsfortheunivariatecase.
2.3.8 Periodic variables Although Gaussiandistributionsareofgreatpracticalsignificance, bothintheir own right and as building blocks for more complex probabilistic models, there are situations in which they are inappropriate as density models for continuous vari- ables.
Oneimportantcase, whicharisesinpracticalapplications, isthatofperiodic variables.
An example of a periodic variable would be the wind direction at a particular geographicallocation.
Wemight, forinstance, measurevaluesofwinddirectionona numberofdaysandwishtosummarizethisusingaparametricdistribution.
Another example is calendar time, where we may be interested in modelling quantities that are believed to be periodic over 24 hours or over an annual cycle.
Such quantities canconvenientlyberepresentedusinganangular(polar)coordinate0 θ <2π.
We might be tempted to treat periodic variables by choosing some direction as the origin and then applying a conventional distribution such as the Gaussian.
Suchanapproach, however, wouldgiveresultsthatwerestronglydependentonthe arbitrary choice of origin.
Suppose, for instance, that we have two observations at θ 1 = 1◦ and θ 2 = 359◦, and we model them using a standard univariate Gaussian distribution.
If we choose the origin at 0◦, then the sample mean of this data set willbe180◦ withstandarddeviation179◦, whereasifwechoosetheoriginat180◦, then the mean will be 0◦ and the standard deviation will be 1◦.
We clearly need to developaspecialapproachforthetreatmentofperiodicvariables.
Let us consider the problem of evaluating the mean of a set of observations D = {θ 1 ,...,θN } of a periodic variable.
From now on, we shall assume that θ is measuredinradians.
Wehavealreadyseenthatthesimpleaverage(θ 1 +···+θN)/N willbestronglycoordinatedependent.
Tofindaninvariantmeasureofthemean, we notethattheobservationscanbeviewedaspointsontheunitcircleandcantherefore be described instead by two-dimensional unit vectorsx 1 ,..., x N where xn = 1 106 2.
PROBABILITYDISTRIBUTIONS Figure2.17 Illustration of the representation of val- x2 ues θ n of a periodic variable as two- x4 x3 dimensionalvectorsx n livingontheunit circle.
Also shown is the average x of thosevectors.
x¯ r¯ x2 θ¯ x1 x1 insteadtogive N 1 x= xn (2.167) N n=1 andthenfindthecorrespondingangleθ ofthisaverage.
Clearly, thisdefinitionwill ensurethatthelocationofthemeanisindependentoftheoriginoftheangularcoor- dinate.
Notethatxwilltypicallylieinsidetheunitcircle.
The Cartesiancoordinates oftheobservationsaregivenbyxn = (cosθn, sinθn), andwecanwritethe Carte- siancoordinatesofthesamplemeanintheformx = (rcosθ, rsinθ).
Substituting into(2.167)andequatingthex 1 andx 2 componentsthengives N N 1 1 rcosθ = cosθn, rsinθ = sinθn.
(2.168) N N n=1 n=1 Taking the ratio, and using the identity tanθ = sinθ/cosθ, we can solve for θ to give θ =tan −1 n sinθn .
(2.169) n cosθn Shortly, we shall see how this result arises naturally as the maximum likelihood estimatorforanappropriatelydefineddistributionoveraperiodicvariable.
Wenowconsideraperiodicgeneralizationofthe Gaussiancalledthevon Mises distribution.
Here we shall limit our attention to univariate distributions, although periodic distributions can also be found over hyperspheres of arbitrary dimension.
Foranextensivediscussionofperiodicdistributions, see Mardiaand Jupp(2000).
By convention, we will consider distributions p(θ) that have period 2π.
Any probability density p(θ) defined over θ must not only be nonnegative and integrate 2.3.
The Gaussian Distribution 107 Figure2.18 Thevon Misesdistributioncanbederivedbyconsidering x2 a two-dimensional Gaussian of the form (2.173), whose density contours are shown in blue and conditioning on p(x) theunitcircleshowninred.
x1 r =1 toone, butitmustalsobeperiodic.
Thusp(θ)mustsatisfythethreeconditions p(θ) 0 (2.170) 2π p(θ)dθ = 1 (2.171) 0 p(θ+2π) = p(θ).
(2.172) From(2.172), itfollowsthatp(θ+M2π)=p(θ)foranyinteger M.
Wecaneasilyobtaina Gaussian-likedistributionthatsatisfiesthesethreeprop- ertiesasfollows.
Considera Gaussiandistributionovertwovariablesx = (x 1 , x 2 ) having mean µ = (µ 1 ,µ 2 ) and a covariance matrix Σ = σ2I where I is the 2×2 identitymatrix, sothat 1 (x −µ )2+(x −µ )2 p(x 1 , x 2 )= 2πσ2 exp − 1 1 2σ2 2 2 .
(2.173) Thecontoursofconstantp(x)arecircles, asillustratedin Figure2.18.
Nowsuppose weconsiderthevalueofthisdistributionalongacircleoffixedradius.
Thenbycon- structionthisdistributionwillbeperiodic, althoughitwillnotbenormalized.
Wecan determine the form of this distribution by transforming from Cartesian coordinates (x 1 , x 2 )topolarcoordinates(r,θ)sothat x 1 =rcosθ, x 2 =rsinθ.
(2.174) Wealsomapthemeanµintopolarcoordinatesbywriting µ 1 =r 0 cosθ 0 , µ 2 =r 0 sinθ 0 .
(2.175) Nextwesubstitutethesetransformationsintothetwo-dimensional Gaussiandistribu- tion(2.173), andthenconditionontheunitcircler =1, notingthatweareinterested onlyinthedependenceonθ.
Focussingontheexponentinthe Gaussiandistribution wehave 1 − (rcosθ−r cosθ )2+(rsinθ−r sinθ )2 2σ2 0 0 0 0 1 = − 1+r2−2r cosθcosθ −2r sinθsinθ 2σ2 0 0 0 0 0 r = σ 0 2 cos(θ−θ 0 )+const (2.176) 108 2.
PROBABILITYDISTRIBUTIONS π/4 m=5, θ0 =π/4 3π/4 m=1, θ0 =3π/4 0 2π m=5, θ0 =π/4 m=1, θ0 =3π/4 Figure 2.19 The von Mises distribution plotted for two different parameter values, shown as a Cartesian plot ontheleftandasthecorrespondingpolarplotontheright.
where‘const’denotestermsindependentofθ, andwehavemadeuseofthefollowing Exercise 2.51 trigonometricalidentities cos2A+sin2A = 1 (2.177) cos Acos B+sin Asin B = cos(A−B).
(2.178) Ifwenowdefinem = r 0 /σ2, weobtainourfinalexpressionforthedistribution of p(θ)alongtheunitcircler =1intheform 1 p(θ|θ 0 , m)= exp{mcos(θ−θ 0 )} (2.179) 2πI (m) 0 whichiscalledthevon Misesdistribution, orthecircularnormal.
Heretheparam- eter θ 0 corresponds to the mean of the distribution, while m, which is known as theconcentrationparameter, isanalogoustotheinversevariance(precision)forthe Gaussian.
The normalization coefficient in (2.179) is expressed in terms of I 0 (m), whichisthezeroth-order Besselfunctionofthefirstkind(Abramowitzand Stegun, 1965)andisdefinedby 2π 1 I 0 (m)= exp{mcosθ} dθ.
(2.180) 2π 0 Exercise 2.52 Forlargem, thedistributionbecomesapproximately Gaussian.
Thevon Misesdis- tributionisplottedin Figure2.19, andthefunction I 0 (m)isplottedin Figure2.20.
Nowconsiderthemaximumlikelihoodestimatorsfortheparametersθ 0 andm forthevon Misesdistribution.
Theloglikelihoodfunctionisgivenby N lnp(D|θ 0 , m)=−Nln(2π)−Nln I 0 (m)+m cos(θn −θ 0 ).
(2.181) n=1 2.3.
The Gaussian Distribution 109 3000 1 2000 I0(m) A(m) 0.5 1000 0 0 0 5 10 0 5 10 m m Figure2.20 Plotofthe Besselfunction I 0 (m)definedby(2.180), togetherwiththefunction A(m)definedby (2.186).
Settingthederivativewithrespecttoθ 0 equaltozerogives N sin(θn −θ 0 )=0.
(2.182) n=1 Tosolveforθ 0, wemakeuseofthetrigonometricidentity sin(A−B)=cos Bsin A−cos Asin B (2.183) Exercise 2.53 fromwhichweobtain θML =tan −1 n sinθn (2.184) 0 n cosθn whichwerecognizeastheresult(2.169)obtainedearlierforthemeanoftheobser- vationsviewedinatwo-dimensional Cartesianspace.
Similarly, maximizing (2.181) with respect to m, and making use of I (m) = 0 I 1 (m)(Abramowitzand Stegun,1965), wehave N 1 A(m)= N cos(θn −θ 0 ML) (2.185) n=1 where we have substituted for the maximum likelihood solution for θML (recalling 0 thatweareperformingajointoptimizationoverθandm), andwehavedefined I (m) 1 A(m)= .
(2.186) I (m) 0 Thefunction A(m)isplottedin Figure2.20.
Makinguseofthetrigonometriciden- tity(2.178), wecanwrite(2.185)intheform N N 1 1 A(m ML )= N cosθn cosθ 0 ML− N sinθn sinθ 0 ML.
(2.187) n=1 n=1 110 2.
PROBABILITYDISTRIBUTIONS Figure2.21 Plots of the ‘old faith- 100 100 ful’ data in which the blue curves show contours of constant proba- bility density.
On the left is a single Gaussian distribution which 80 80 has been fitted to the data us- ing maximum likelihood.
Note that this distribution fails to capture the 60 60 two clumps in the data and indeed places much of its probability mass in the central region between the clumpswherethedataarerelatively 40 40 sparse.
Ontherightthedistribution 1 2 3 4 5 6 1 2 3 4 5 6 is given by a linear combination of two Gaussianswhichhasbeenfitted to the data by maximum likelihood using techniques discussed Chap- ter 9, and which gives a better rep- resentationofthedata.
The right-hand side of (2.187) is easily evaluated, and the function A(m) can be invertednumerically.
For completeness, we mention briefly some alternative techniques for the con- struction of periodic distributions.
The simplest approach is to use a histogram of observationsinwhichtheangularcoordinateisdividedintofixedbins.
Thishasthe virtueofsimplicityandflexibilitybutalsosuffersfromsignificantlimitations, aswe shallseewhenwediscusshistogrammethodsinmoredetailin Section2.5.
Another approachstarts, likethevon Misesdistribution, froma Gaussiandistributionovera Euclidean space but now marginalizes onto the unit circle rather than conditioning (Mardiaand Jupp,2000).
However, thisleadstomorecomplexformsofdistribution and will not be discussed further.
Finally, any valid distribution over the real axis (such as a Gaussian) can be turned into a periodic distribution by mapping succes- sive intervals of width 2π onto the periodic variable (0,2π), which corresponds to ‘wrapping’therealaxisaroundunitcircle.
Again, theresultingdistributionismore complextohandlethanthevon Misesdistribution.
One limitation of the von Mises distribution is that it is unimodal.
By forming mixtures of von Mises distributions, we obtain a flexible framework for modelling periodicvariablesthatcanhandlemultimodality.
Foranexampleofamachinelearn- ingapplicationthatmakesuseofvon Misesdistributions, see Lawrenceetal.
(2002), and for extensions to modelling conditional densities for regression problems, see Bishopand Nabney(1996).
2.3.9 Mixtures of Gaussians Whilethe Gaussiandistributionhassomeimportantanalyticalproperties, itsuf- fersfromsignificantlimitationswhenitcomestomodellingrealdatasets.
Consider the example shown in Figure 2.21.
This is known as the ‘Old Faithful’ data set, and comprises 272 measurements of the eruption of the Old Faithful geyser at Yel- Appendix A lowstone National Park in the USA.
Each measurement comprises the duration of 2.3.
The Gaussian Distribution 111 Figure2.22 Example of a Gaussian mixture distribution p(x) in one dimension showing three Gaussians (each scaled by a coefficient) in blue and theirsuminred.
x the eruption in minutes (horizontal axis) and the time in minutes to the next erup- tion (vertical axis).
We see that the data set forms two dominant clumps, and that a simple Gaussian distribution is unable to capture this structure, whereas a linear superpositionoftwo Gaussiansgivesabettercharacterizationofthedataset.
Such superpositions, formed by taking linear combinations of more basic dis- tributions such as Gaussians, can be formulated as probabilistic models known as mixture distributions (Mc Lachlan and Basford, 1988; Mc Lachlan and Peel, 2000).
In Figure 2.22 we see that a linear combination of Gaussians can give rise to very complexdensities.
Byusingasufficientnumberof Gaussians, andbyadjustingtheir means and covariances as well as the coefficients in the linear combination, almost anycontinuousdensitycanbeapproximatedtoarbitraryaccuracy.
Wethereforeconsiderasuperpositionof K Gaussiandensitiesoftheform K p(x)= πk N(x|µ k ,Σk) (2.188) k=1 which is called a mixture of Gaussians.
Each Gaussian density N(x|µ k ,Σk) is called a component of the mixture and has its own mean µ k and covariance Σk.
Contourandsurfaceplotsfora Gaussianmixturehaving3componentsareshownin Figure2.23.
In this section we shall consider Gaussian components to illustrate the frame- workofmixturemodels.
Moregenerally, mixturemodelscancompriselinearcom- binations of other distributions.
For instance, in Section 9.3.3 we shall consider mixtures of Bernoulli distributions as an example of a mixture model for discrete Section9.3.3 variables.
Theparametersπk in(2.188)arecalledmixingcoefficients.
Ifweintegrateboth sidesof(2.188)withrespecttox, andnotethatbothp(x)andtheindividual Gaussian componentsarenormalized, weobtain K πk =1.
(2.189) k=1 Also, the requirement that p(x) 0, together with N(x|µ k ,Σk) 0, implies πk 0forallk.
Combiningthiswiththecondition(2.189)weobtain 0 πk 1.
(2.190) 112 2.
PROBABILITYDISTRIBUTIONS 1 1 (a) (b) 0.5 0.5 0.2 0.3 0.5 0 0 0 0.5 1 0 0.5 1 Figure 2.23 Illustration of a mixture of 3 Gaussians in a two-dimensional space.
(a) Contours of constant densityforeachofthemixturecomponents, inwhichthe3componentsaredenotedred, blueandgreen, and thevaluesofthemixingcoefficientsareshownbeloweachcomponent.
(b)Contoursofthemarginalprobability densityp(x)ofthemixturedistribution.
(c)Asurfaceplotofthedistributionp(x).
Wethereforeseethatthemixingcoefficientssatisfytherequirementstobeprobabil- ities.
Fromthesumandproductrules, themarginaldensityisgivenby K p(x)= p(k)p(x|k) (2.191) k=1 which is equivalent to (2.188) in which we can view πk = p(k) as the prior prob- ability of picking the kth component, and the density N(x|µ k ,Σk) = p(x|k) as the probability of x conditioned on k.
As we shall see in later chapters, an impor- tant role is played by the posterior probabilities p(k|x), which are also known as responsibilities.
From Bayes’theoremthesearegivenby γk(x) ≡ p(k|x) p(k)p(x|k) = p(l)p(x|l) l πk N(x|µ k ,Σk) = .
(2.192) l πl N(x|µ l ,Σl) Weshalldiscusstheprobabilisticinterpretationofthemixturedistributioningreater detailin Chapter9.
Theformofthe Gaussianmixturedistributionisgovernedbytheparametersπ, µandΣ, wherewehaveusedthenotationπ ≡ {π 1 ,...,πK },µ ≡ {µ 1 ,...,µ K } and Σ ≡ {Σ 1 ,...ΣK }.
One way to set the values of these parameters is to use maximumlikelihood.
From(2.188)thelogofthelikelihoodfunctionisgivenby N K lnp(X|π,µ,Σ)= ln πk N(xn |µ k ,Σk) (2.193) n=1 k=1 2.4.
The Exponential Family 113 where X = {x 1 ,..., x N }.
We immediately see that the situation is now much more complex than with a single Gaussian, due to the presence of the summation over k inside the logarithm.
As a result, the maximum likelihood solution for the parametersnolongerhasaclosed-formanalyticalsolution.
Oneapproachtomaxi- mizingthelikelihoodfunctionistouseiterativenumericaloptimizationtechniques (Fletcher, 1987; Nocedal and Wright, 1999; Bishop and Nabney, 2008).
Alterna- tivelywecanemployapowerfulframeworkcalledexpectationmaximization, which willbediscussedatlengthin Chapter9.
2.4.
The Exponential Family The probability distributions that we have studied so far in this chapter (with the exception of the Gaussian mixture) are specific examples of a broad class of distri- butions called the exponential family (Duda and Hart, 1973; Bernardo and Smith, 1994).
Membersoftheexponentialfamilyhavemanyimportantpropertiesincom- mon, anditisilluminatingtodiscussthesepropertiesinsomegenerality.
Theexponentialfamilyofdistributionsoverx, givenparametersη, isdefinedto bethesetofdistributionsoftheform p(x|η)=h(x)g(η)exp ηTu(x) (2.194) where x may be scalar or vector, and may be discrete or continuous.
Here η are called the natural parameters of the distribution, and u(x) is some function of x.
Thefunctiong(η)canbeinterpretedasthecoefficientthatensuresthatthedistribu- tionisnormalizedandthereforesatisfies g(η) h(x)exp ηTu(x) dx=1 (2.195) wheretheintegrationisreplacedbysummationifxisadiscretevariable.
We begin by taking some examples of the distributions introduced earlier in the chapter and showing that they are indeed members of the exponential family.
Considerfirstthe Bernoullidistribution p(x|µ)=Bern(x|µ)=µ x (1−µ)1−x .
(2.196) Expressingtheright-handsideastheexponentialofthelogarithm, wehave p(x|µ) = exp{xlnµ+(1−x)ln(1−µ)} µ = (1−µ)exp ln x .
(2.197) 1−µ Comparisonwith(2.194)allowsustoidentify µ η =ln (2.198) 1−µ 114 2.
PROBABILITYDISTRIBUTIONS whichwecansolveforµtogiveµ=σ(η), where 1 σ(η)= (2.199) 1+exp(−η) iscalledthelogisticsigmoid function.
Thuswecanwritethe Bernoullidistribution usingthestandardrepresentation(2.194)intheform p(x|η)=σ(−η)exp(ηx) (2.200) wherewehaveused1−σ(η)=σ(−η), whichiseasilyprovedfrom(2.199).
Com- parisonwith(2.194)showsthat u(x) = x (2.201) h(x) = 1 (2.202) g(η) = σ(−η).
(2.203) Nextconsiderthemultinomialdistributionthat, forasingleobservationx, takes theform M M p(x|µ)= µ x k k =exp xklnµk (2.204) k=1 k=1 where x = (x 1 ,..., x N)T.
Again, we can write this in the standard representation (2.194)sothat p(x|η)=exp(ηTx) (2.205) whereηk =lnµk, andwehavedefinedη =(η 1 ,...,ηM)T.
Again, comparingwith (2.194)wehave u(x) = x (2.206) h(x) = 1 (2.207) g(η) = 1.
(2.208) Notethattheparametersηk arenotindependentbecausetheparametersµk aresub- jecttotheconstraint M µk =1 (2.209) k=1 sothat, givenany M −1oftheparametersµk, thevalueoftheremainingparameter is fixed.
In some circumstances, it will be convenient to remove this constraint by expressingthedistributionintermsofonly M−1parameters.
Thiscanbeachieved by using the relationship (2.209) to eliminate µM by expressing it in terms of the remaining{µk }wherek =1,..., M −1, therebyleaving M −1parameters.
Note thattheseremainingparametersarestillsubjecttotheconstraints M −1 0 µk 1, µk 1.
(2.210) k=1 2.4.
The Exponential Family 115 Makinguseoftheconstraint(2.209), themultinomialdistributioninthisrepresenta- tionthenbecomes M exp xklnµk k=1 M −1 M −1 M −1 = exp xklnµk + 1− xk ln 1− µk k=1 k=1 k=1 M −1 M −1 = exp k=1 xkln 1− µ M j k = − 1 1 µj +ln 1− k=1 µk .
(2.211) Wenowidentify µ k ln 1− j µj =ηk (2.212) whichwecansolveforµk byfirstsummingbothsidesoverk andthenrearranging andback-substitutingtogive e xp(ηk) µk = .
(2.213) 1+ j exp(ηj) Thisiscalledthesoftmaxfunction, orthenormalizedexponential.
Inthisrepresen- tation, themultinomialdistributionthereforetakestheform M −1 −1 p(x|η)= 1+ exp(ηk) exp(ηTx).
(2.214) k=1 This is the standard form of the exponential family, with parameter vector η = (η 1 ,...,ηM−1 )T inwhich u(x) = x (2.215) h(x) = 1 (2.216) M −1 −1 g(η) = 1+ exp(ηk) .
(2.217) k=1 Finally, let us consider the Gaussian distribution.
For the univariate Gaussian, wehave 1 1 p(x|µ,σ2) = exp − (x−µ)2 (2.218) (2πσ2)1/2 2σ2 1 1 µ 1 = exp − x2+ x− µ2 (2.219) (2πσ2)1/2 2σ2 σ2 2σ2 116 2.
PROBABILITYDISTRIBUTIONS which, after some simple rearrangement, can be cast in the standard exponential Exercise 2.57 familyform(2.194)with µ/σ2 η = (2.220) −1/2σ2 x u(x) = (2.221) x2 h(x) = (2π) −1/2 (2.222) η2 g(η) = (−2η 2 )1/2exp 1 .
(2.223) 4η 2 2.4.1 Maximum likelihood and sufficient statistics Letusnowconsidertheproblemofestimatingtheparametervectorηinthegen- eralexponentialfamilydistribution(2.194)usingthetechniqueofmaximumlikeli- hood.
Takingthegradientofbothsidesof(2.195)withrespecttoη, wehave ∇g(η) h(x)exp ηTu(x) dx + g(η) h(x)exp ηTu(x) u(x)dx=0.
(2.224) Rearranging, andmakinguseagainof(2.195)thengives 1 − ∇g(η)=g(η) h(x)exp ηTu(x) u(x)dx=E[u(x)] (2.225) g(η) wherewehaveused(2.194).
Wethereforeobtaintheresult −∇lng(η)=E[u(x)].
(2.226) Notethatthecovarianceofu(x)canbeexpressedintermsofthesecondderivatives Exercise 2.58 ofg(η), andsimilarlyforhigherordermoments.
Thus, providedwecannormalizea distributionfromtheexponentialfamily, wecanalwaysfinditsmomentsbysimple differentiation.
Nowconsiderasetofindependentidenticallydistributeddatadenotedby X = {x 1 ,..., xn }, forwhichthelikelihoodfunctionisgivenby N N p(X|η)= h(xn) g(η) N exp ηT u(xn) .
(2.227) n=1 n=1 Setting the gradient of lnp(X|η) with respect to η to zero, we get the following conditiontobesatisfiedbythemaximumlikelihoodestimatorη ML N 1 −∇lng(η ML )= N u(xn) (2.228) n=1 2.4.
The Exponential Family 117 which can in principle be solved to obtain η ML .
We see that th e solution for the maximumlikelihoodestimatordependsonthedataonlythrough n u(xn), which isthereforecalledthesufficientstatisticofthedistribution(2.194).
Wedonotneed to store the entire data set itself but only the value of the sufficient statistic.
For the Bernoulli distribution, for example, the function u(x) is given just by x and so we need only keep the sum of the data points {xn }, whereas for the Gaussian u(x)=(x, x2)T, andsoweshouldkeepboththesumof{xn }andthesumof{x2 n }.
If we consider the limit N → ∞, then the right-hand side of (2.228) becomes E[u(x)], andsobycomparingwith(2.226)weseethatinthislimitη willequal ML thetruevalueη.
In fact, this sufficiency property holds also for Bayesian inference, although we shall defer discussion of this until Chapter 8 when we have equipped ourselves with the tools of graphical models and can thereby gain a deeper insight into these importantconcepts.
2.4.2 Conjugate priors Wehavealreadyencounteredtheconceptofaconjugatepriorseveraltimes, for example in the context of the Bernoulli distribution (for which the conjugate prior is the beta distribution) or the Gaussian (where the conjugate prior for the mean is a Gaussian, andtheconjugatepriorfortheprecisionisthe Wishartdistribution).
In general, foragivenprobabilitydistributionp(x|η), wecanseekapriorp(η)thatis conjugate to the likelihood function, so that the posterior distribution has the same functionalformastheprior.
Foranymemberoftheexponentialfamily(2.194), there existsaconjugatepriorthatcanbewrittenintheform p(η|χ,ν)=f(χ,ν)g(η) ν exp νηTχ (2.229) where f(χ,ν) is a normalization coefficient, and g(η) is the same function as ap- pearsin(2.194).
Toseethatthisisindeedconjugate, letusmultiplytheprior(2.229) by the likelihood function (2.227) to obtain the posterior distribution, up to a nor- malizationcoefficient, intheform N p(η|X,χ,ν)∝g(η) ν+N exp ηT u(xn)+νχ .
(2.230) n=1 Thisagaintakesthesamefunctionalformastheprior(2.229), confirmingconjugacy.
Furthermore, weseethattheparameterν canbeinterpretedasaeffectivenumberof pseudo-observationsintheprior, eachofwhichhasavalueforthesufficientstatistic u(x)givenbyχ.
2.4.3 Noninformative priors In some applications of probabilistic inference, we may have prior knowledge that can be conveniently expressed through the prior distribution.
For example, if the prior assigns zero probability to some value of variable, then the posterior dis- tribution will necessarily also assign zero probability to that value, irrespective of 118 2.
PROBABILITYDISTRIBUTIONS any subsequent observations of data.
In many cases, however, we may have little idea of what form the distribution should take.
We may then seek a form of prior distribution, calledanoninformative prior, whichisintendedtohaveaslittleinflu- ence on the posterior distribution as possible (Jeffries, 1946; Box and Tao, 1973; Bernardoand Smith,1994).
Thisissometimesreferredtoas‘lettingthedataspeak forthemselves’.
Ifwehaveadistributionp(x|λ)governedbyaparameterλ, wemightbetempted to propose a prior distribution p(λ) = const as a suitable prior.
If λ is a discrete variable with K states, this simply amounts to setting the prior probability of each stateto1/K.
Inthecaseofcontinuousparameters, however, therearetwopotential difficulties with this approach.
The first is that, if the domain of λ is unbounded, this prior distribution cannot be correctly normalized because the integral over λ diverges.
Such priors are called improper.
In practice, improper priors can often be used provided the corresponding posterior distribution is proper, i.
e., that it can be correctly normalized.
For instance, if we put a uniform prior distribution over the mean of a Gaussian, then the posterior distribution for the mean, once we have observedatleastonedatapoint, willbeproper.
A second difficulty arises from the transformation behaviour of a probability density under a nonlinear change of variables, given by (1.27).
If a function h(λ) is constant, and we change variables to λ = η2, then h(η) = h(η2) will also be constant.
However, if we choose the density pλ(λ) to be constant, then the density ofηwillbegiven, from(1.27), by pη(η)=pλ(λ) dλ =pλ(η2)2η ∝η (2.231) dη andsothedensityoverηwillnotbeconstant.
Thisissuedoesnotarisewhenweuse maximumlikelihood, becausethelikelihoodfunctionp(x|λ)isasimplefunctionof λandsowearefreetouseanyconvenientparameterization.
If, however, weareto chooseaprior distributionthatisconstant, wemusttakecaretouseanappropriate representationfortheparameters.
Hereweconsidertwosimpleexamplesofnoninformativepriors(Berger,1985).
Firstofall, ifadensitytakestheform p(x|µ)=f(x−µ) (2.232) then the parameter µ is known as a location parameter.
This family of densities exhibitstranslationinvariancebecauseifweshiftxbyaconstanttogive x=x+c, then p( x|µ )=f( x−µ ) (2.233) where we have defined µ = µ + c.
Thus the density takes the same form in the new variable as in the original one, and so the density is independent of the choice of origin.
We would like to choose a prior distribution that reflects this translation invarianceproperty, andsowechooseapriorthatassignsequalprobabilitymassto 2.4.
The Exponential Family 119 aninterval A µ B astotheshiftedinterval A−c µ B−c.
Thisimplies B B−c B p(µ)dµ= p(µ)dµ= p(µ−c)dµ (2.234) A A−c A andbecausethismustholdforallchoicesof Aand B, wehave p(µ−c)=p(µ) (2.235) which implies that p(µ) is constant.
An example of a location parameter would be the mean µ of a Gaussian distribution.
As we have seen, the conjugate prior distri- butionforµinthiscaseisa Gaussianp(µ|µ 0 ,σ 0 2) = N(µ|µ 0 ,σ 0 2), andweobtaina noninformativepriorbytakingthelimitσ2 →∞.
Indeed, from(2.141)and(2.142) 0 weseethatthisgivesaposteriordistributionoverµinwhichthecontributionsfrom thepriorvanish.
Asasecondexample, consideradensityoftheform 1 x p(x|σ)= f (2.236) σ σ whereσ > 0.
Notethatthiswillbeanormalizeddensityprovidedf(x)iscorrectly Exercise 2.59 normalized.
Theparameterσisknownasascaleparameter, andthedensityexhibits scaleinvariancebecauseifwescalexbyaconstanttogive x=cx, then 1 x p( x|σ )= f (2.237) σ σ where we have defined σ = cσ.
This transformation corresponds to a change of scale, for example from meters to kilometers if x is a length, and we would like to choose a prior distribution that reflects this scale invariance.
If we consider an interval A σ B, and a scaled interval A/c σ B/c, then the prior should assignequalprobabilitymasstothesetwointervals.
Thuswehave B B/c B 1 1 p(σ)dσ = p(σ)dσ = p σ dσ (2.238) c c A A/c A andbecausethismustholdforchoicesof Aand B, wehave 1 1 p(σ)=p σ (2.239) c c andhencep(σ)∝1/σ.
Notethatagainthisisanimproperpriorbecausetheintegral of the distribution over 0 σ ∞ is divergent.
It is sometimes also convenient tothinkofthepriordistributionforascaleparameterintermsofthedensityofthe log of the parameter.
Using the transformation rule (1.27) for densities we see that p(lnσ) = const.
Thus, forthispriorthereisthesameprobabilitymassintherange 1 σ 10asintherange10 σ 100andin100 σ 1000.
120 2.
PROBABILITYDISTRIBUTIONS Anexampleofascaleparameterwouldbethestandarddeviationσofa Gaussian distribution, afterwehavetakenaccountofthelocationparameterµ, because N(x|µ,σ2)∝σ −1exp −( x/σ)2 (2.240) where x=x−µ.
Asdiscussedearlier, itisoftenmoreconvenienttoworkinterms of the precision λ = 1/σ2 rather than σ itself.
Using the transformation rule for densities, weseethatadistributionp(σ)∝1/σ correspondstoadistributionoverλ oftheformp(λ)∝1/λ.
Wehaveseenthattheconjugatepriorforλwasthegamma Section2.3 distribution Gam(λ|a 0 , b 0 ) given by (2.146).
The noninformative prior is obtained asthespecialcasea 0 =b 0 =0.
Again, ifweexaminetheresults(2.150)and(2.151) fortheposteriordistributionofλ, weseethatfora 0 =b 0 =0, theposteriordepends onlyontermsarisingfromthedataandnotfromtheprior.
2.5.
Nonparametric Methods Throughout this chapter, we have focussed on the use of probability distributions having specific functional forms governed by a small number of parameters whose valuesaretobedeterminedfromadataset.
Thisiscalledtheparametricapproach to density modelling.
An important limitation of this approach is that the chosen densitymightbeapoormodelofthedistributionthatgeneratesthedata, whichcan resultinpoorpredictiveperformance.
Forinstance, iftheprocessthatgeneratesthe data is multimodal, then this aspect of the distribution can never be captured by a Gaussian, whichisnecessarilyunimodal.
Inthisfinalsection, weconsidersomenonparametricapproachestodensityes- timationthatmakefewassumptionsabouttheformofthedistribution.
Hereweshall focusmainlyonsimplefrequentistmethods.
Thereadershouldbeaware, however, thatnonparametric Bayesianmethodsareattractingincreasinginterest(Walkeretal., 1999; Neal,2000; Mu¨llerand Quintana,2004; Tehetal.,2006).
Letusstartwithadiscussionofhistogrammethodsfordensityestimation, which wehavealreadyencounteredinthecontextofmarginalandconditionaldistributions in Figure1.11andinthecontextofthecentrallimittheoremin Figure2.6.
Herewe explorethepropertiesofhistogramdensitymodelsinmoredetail, focussingonthe case of a single continuous variablex.
Standard histograms simply partitionxinto distinctbinsofwidth∆i andthencountthenumberni ofobservationsofxfalling inbini.
Inordertoturnthiscountintoanormalizedprobabilitydensity, wesimply divide by the total number N of observations and by the width ∆i of the bins to obtainprobabilityvaluesforeachbingivenby ni pi = (2.241) N∆i for which it is easily seen that p(x)dx = 1.
This gives a model for the density p(x)thatisconstantoverthewidthofeachbin, andoftenthebinsarechosentohave thesamewidth∆i =∆.
2.5.
Nonparametric Methods 121 Figure2.24 An illustration of the histogram approach 5 to density estimation, in which a data set ∆=0.04 of 50 data points is generated from the distribution shown by the green curve.
0 Histogram density estimates, based on 0 0.5 1 (2.241), with a common bin width ∆ are 5 shownforvariousvaluesof∆.
∆=0.08 0 0 0.5 1 5 ∆=0.25 0 0 0.5 1 In Figure 2.24, we show an example of histogram density estimation.
Here the data is drawn from the distribution, corresponding to the green curve, which is formed from a mixture of two Gaussians.
Also shown are three examples of his- togram density estimates corresponding to three different choices for the bin width ∆.
Weseethatwhen∆isverysmall(topfigure), theresultingdensitymodelisvery spiky, with a lot of structure that is not present in the underlying distribution that generatedthedataset.
Conversely, if∆istoolarge(bottomfigure)thentheresultis amodelthatistoosmoothandthatconsequentlyfailstocapturethebimodalprop- erty of the green curve.
The best results are obtained for some intermediate value of ∆ (middle figure).
In principle, a histogram density model is also dependent on thechoiceofedgelocationforthebins, thoughthisistypicallymuchlesssignificant thanthevalueof∆.
Notethatthehistogrammethodhastheproperty(unlikethemethodstobedis- cussed shortly) that, once the histogram has been computed, the data set itself can bediscarded, whichcanbeadvantageousifthedatasetislarge.
Also, thehistogram approachiseasilyappliedifthedatapointsarearrivingsequentially.
Inpractice, thehistogramtechniquecanbeusefulforobtainingaquickvisual- ization of data in one or two dimensions but is unsuited to most density estimation applications.
One obvious problem is that the estimated density has discontinuities thatareduetothebinedgesratherthananypropertyoftheunderlyingdistribution that generated the data.
Another major limitation of the histogram approach is its scaling with dimensionality.
If we divide each variable in a D-dimensional space into M bins, then the total number of bins will be MD.
This exponential scaling Section1.4 with Disanexampleofthecurseofdimensionality.
Inaspaceofhighdimensional- ity, thequantityofdataneededtoprovidemeaningfulestimatesoflocalprobability densitywouldbeprohibitive.
The histogram approach to density estimation does, however, teach us two im- portant lessons.
First, to estimate the probability density at a particular location, weshouldconsiderthedatapointsthatliewithinsomelocalneighbourhoodofthat point.
Note that the concept of locality requires that we assume some form of dis- tancemeasure, andherewehavebeenassuming Euclideandistance.
Forhistograms, 122 2.
PROBABILITYDISTRIBUTIONS thisneighbourhoodpropertywasdefinedbythebins, andthereisanatural‘smooth- ing’ parameter describing the spatial extent of the local region, in this case the bin width.
Second, thevalueofthesmoothingparametershouldbeneithertoolargenor toosmallinordertoobtaingoodresults.
Thisisreminiscentofthechoiceofmodel complexity in polynomial curve fitting discussed in Chapter 1 where the degree M of the polynomial, or alternatively the value α of the regularization parameter, was optimal for some intermediate value, neither too large nor too small.
Armed with theseinsights, weturnnowtoadiscussionoftwowidelyusednonparametrictech- niquesfordensityestimation, kernelestimatorsandnearestneighbours, whichhave betterscalingwithdimensionalitythanthesimplehistogrammodel.
2.5.1 Kernel density estimators Letussupposethatobservationsarebeingdrawnfromsomeunknownprobabil- itydensityp(x)insome D-dimensionalspace, whichweshalltaketobe Euclidean, and we wish to estimate the value of p(x).
From our earlier discussion of locality, letusconsidersomesmallregion Rcontainingx.
Theprobabilitymassassociated withthisregionisgivenby P = p(x)dx.
(2.242) R Now suppose that we have collected a data set comprising N observations drawn fromp(x).
Becauseeachdatapointhasaprobability P offallingwithin R, thetotal number K of points that lie inside R will be distributed according to the binomial Section2.1 distribution N! Bin(K|N, P)= P K (1−P)1−K .
(2.243) K!(N −K)! Using (2.11), we see that the mean fraction of points falling inside the region is E[K/N] = P, andsimilarlyusing(2.12)weseethatthevariancearoundthismean isvar[K/N] = P(1−P)/N.
Forlarge N, thisdistributionwillbesharplypeaked aroundthemeanandso K NP.
(2.244) If, however, wealsoassumethattheregion Rissufficientlysmallthattheprobability densityp(x)isroughlyconstantovertheregion, thenwehave P p(x)V (2.245) where V isthevolumeof R.
Combining(2.244)and(2.245), weobtainourdensity estimateintheform K p(x)= .
(2.246) NV Notethatthevalidityof(2.246)dependsontwocontradictoryassumptions, namely thattheregion Rbesufficientlysmallthatthedensityisapproximatelyconstantover theregionandyetsufficientlylarge(inrelationtothevalueofthatdensity)thatthe number Kofpointsfallinginsidetheregionissufficientforthebinomialdistribution tobesharplypeaked.
2.5.
Nonparametric Methods 123 Wecanexploittheresult(2.246)intwodifferentways.
Eitherwecanfix K and determinethevalueof V fromthedata, whichgivesrisetothe K-nearest-neighbour technique discussed shortly, or we can fix V and determine K from the data, giv- ing rise to the kernel approach.
It can be shown that both the K-nearest-neighbour density estimator and the kernel density estimator converge to the true probability densityinthelimit N →∞provided V shrinkssuitablywith N, and K growswith N (Dudaand Hart,1973).
We begin by discussing the kernel method in detail, and to start with we take the region R to be a small hypercube centred on the point x at which we wish to determinetheprobability density.
Inorder tocountthenumber K ofpoints falling withinthisregion, itisconvenienttodefinethefollowingfunction 1, |ui | 1/2, i=1,..., D, k(u)= (2.247) 0, otherwise whichrepresentsaunitcubecentredontheorigin.
Thefunctionk(u)isanexample ofakernelfunction, andinthiscontextisalsocalleda Parzenwindow.
From(2.247), thequantityk((x−xn)/h)willbeoneifthedatapointxnliesinsideacubeofside hcentredonx, andzerootherwise.
Thetotalnumberofdatapointslyinginsidethis cubewillthereforebe N x−xn K = k .
(2.248) h n=1 Substitutingthisexpressioninto(2.246)thengivesthefollowingresultfortheesti- mateddensityatx N 1 1 x−xn p(x)= k (2.249) N h D h n=1 where we have used V = h D for the volume of a hypercube of side h in D di- mensions.
Using the symmetry of the function k(u), we can now re-interpret this equation, notasasinglecubecentredonxbutasthesumover N cubescentredon the N datapointsxn.
Asitstands, thekerneldensityestimator(2.249)willsufferfromoneofthesame problemsthatthehistogrammethodsufferedfrom, namelythepresenceofartificial discontinuities, inthiscaseattheboundariesofthecubes.
Wecanobtainasmoother densitymodelifwechooseasmootherkernelfunction, andacommonchoiceisthe Gaussian, whichgivesrisetothefollowingkerneldensitymodel N p(x)= 1 1 exp − x−xn 2 (2.250) N (2πh2)1/2 2h2 n=1 where h represents the standard deviation of the Gaussian components.
Thus our densitymodelisobtainedbyplacinga Gaussianovereachdatapointandthenadding upthecontributionsoverthewholedataset, andthendividingby N sothattheden- sityiscorrectlynormalized.
In Figure2.25, weapplythemodel(2.250)tothedata 124 2.
PROBABILITYDISTRIBUTIONS Figure2.25 Illustration of the kernel density model 5 (2.250)appliedtothesamedatasetused h=0.005 todemonstratethehistogramapproachin Figure 2.24.
We see that h acts as a 0 smoothing parameter and that if it is set 0 0.5 1 too small (top panel), the result is a very 5 noisy density model, whereas if it is set h=0.07 toolarge(bottompanel), thenthebimodal nature of the underlying distribution from 0 whichthedataisgenerated(shownbythe 0 0.5 1 greencurve)iswashedout.
Thebestden- 5 h=0.2 sitymodelisobtainedforsomeintermedi- atevalueofh(middlepanel).
0 0 0.5 1 set used earlier to demonstrate the histogram technique.
We see that, as expected, the parameter h plays the role of a smoothing parameter, and there is a trade-off between sensitivity to noise at small h and over-smoothing at large h.
Again, the optimizationofhisaprobleminmodelcomplexity, analogoustothechoiceofbin widthinhistogramdensityestimation, orthedegreeofthepolynomialusedincurve fitting.
We can choose any other kernel function k(u) in (2.249) subject to the condi- tions k(u) 0, (2.251) k(u)du = 1 (2.252) which ensure that the resulting probability distribution is nonnegative everywhere andintegratestoone.
Theclassofdensitymodelgivenby(2.249)iscalledakernel density estimator, or Parzen estimator.
It has a great merit that there is no compu- tation involved in the ‘training’ phase because this simply requires storage of the trainingset.
However, thisisalsooneofitsgreatweaknessesbecausethecomputa- tionalcostofevaluatingthedensitygrowslinearlywiththesizeofthedataset.
2.5.2 Nearest-neighbour methods Oneofthedifficultieswiththekernelapproachtodensityestimationisthatthe parameter h governing the kernel width is fixed for all kernels.
In regions of high data density, a large value of h may lead to over-smoothing and a washing out of structurethatmightotherwisebeextractedfromthedata.
However, reducinghmay lead to noisy estimates elsewhere in data space where the density is smaller.
Thus the optimal choice for h may be dependent on location within the data space.
This issueisaddressedbynearest-neighbourmethodsfordensityestimation.
We therefore return to our general result (2.246) for local density estimation, andinsteadoffixing V anddeterminingthevalueof K fromthedata, weconsider a fixed value of K and use the data to find an appropriate value for V.
To do this, weconsiderasmallspherecentredonthepointxatwhichwewishtoestimatethe 2.5.
Nonparametric Methods 125 Figure2.26 Illustration of K-nearest-neighbour den- 5 sity estimation using the same data set K=1 as in Figures 2.25 and 2.24.
We see thattheparameter K governsthedegree 0 of smoothing, so that a small value of 0 0.5 1 K leads to a very noisy density model 5 (top panel), whereas a large value (bot- K=5 tompanel)smoothesoutthebimodalna- tureofthetruedistribution(shownbythe 0 greencurve)fromwhichthedatasetwas 0 0.5 1 generated.
5 K=30 0 0 0.5 1 densityp(x), andweallowtheradiusofthespheretogrowuntilitcontainsprecisely Kdatapoints.
Theestimateofthedensityp(x)isthengivenby(2.246)with V setto thevolumeoftheresultingsphere.
Thistechniqueisknownas Knearestneighbours and is illustrated in Figure 2.26, for various choices of the parameter K, using the same data set as used in Figure 2.24 and Figure 2.25.
We see that the value of K nowgovernsthedegreeofsmoothingandthatagainthereisanoptimumchoicefor K thatisneithertoolargenortoosmall.
Notethatthemodelproducedby K nearest Exercise 2.61 neighboursisnotatruedensitymodelbecausetheintegraloverallspacediverges.
We close this chapter by showing how the K-nearest-neighbour technique for density estimation can be extended to the problem of classification.
To do this, we applythe K-nearest-neighbourdensityestimationtechniquetoeachclassseparately andthenmakeuseof Bayes’theorem.
Letussupposethatwe haveadatasetcom- prising Nk points in class C k with N points in total, so that k Nk = N.
If we wishtoclassifyanewpointx, wedrawaspherecentredonxcontainingprecisely K pointsirrespectiveoftheirclass.
Supposethisspherehasvolume V andcontains Kkpointsfromclass C k.
Then(2.246)providesanestimateofthedensityassociated witheachclass p(x|C k)= Kk .
(2.253) Nk V Similarly, theunconditionaldensityisgivenby K p(x)= (2.254) NV whiletheclasspriorsaregivenby p(C k)= Nk .
(2.255) N We can now combine (2.253), (2.254), and (2.255) using Bayes’ theorem to obtain theposteriorprobabilityofclassmembership p(C k |x)= p(x|C k)p(C k) = Kk .
(2.256) p(x) K 126 2.
PROBABILITYDISTRIBUTIONS Figure2.27 (a) In the K-nearest- x2 x2 neighbour classifier, a new point, shownbytheblackdiamond, isclas- sifiedaccordingtothemajorityclass membership of the K closest train- ing data points, in this case K = 3.
(b) In the nearest-neighbour (K = 1) approach to classification, the resulting decision boundary is composed of hyperplanes that form perpendicular bisectors of pairs of pointsfromdifferentclasses.
x1 x1 (a) (b) Ifwewishtominimizetheprobabilityofmisclassification, thisisdonebyassigning thetestpointxtotheclasshavingthelargestposteriorprobability, correspondingto thelargestvalueof Kk/K.
Thustoclassifyanewpoint, weidentifythe K nearest pointsfromthetrainingdatasetandthenassignthenewpointtotheclasshavingthe largest number of representatives amongst this set.
Ties can be broken at random.
The particular case of K = 1 is called the nearest-neighbour rule, because a test pointissimplyassignedtothesameclassasthenearestpointfromthetrainingset.
Theseconceptsareillustratedin Figure2.27.
In Figure 2.28, we show the results of applying the K-nearest-neighbour algo- rithm to the oil flow data, introduced in Chapter 1, for various values of K.
As expected, weseethat K controlsthedegreeofsmoothing, sothatsmall K produces manysmallregionsofeachclass, whereaslarge K leadstofewerlargerregions.
K=1 K=3 K=31 2 2 2 x7 x7 x7 1 1 1 0 0 0 0 1 x6 2 0 1 x6 2 0 1 x6 2 Figure 2.28 Plot of 200 data points from the oil data set showing values of x 6 plotted against x 7, where the red, green, andbluepointscorrespondtothe‘laminar’,‘annular’, and‘homogeneous’classes, respectively.
Also shownaretheclassificationsoftheinputspacegivenbythe K-nearest-neighbouralgorithmforvariousvalues of K.
Exercises 127 Aninterestingpropertyofthenearest-neighbour(K =1)classifieristhat, inthe limit N →∞, theerrorrateisnevermorethantwicetheminimumachievableerror rateofanoptimalclassifier, i.
e., onethatusesthetrueclassdistributions(Coverand Hart,1967).
Asdiscussedsofar, boththe K-nearest-neighbourmethod, andthekernelden- sity estimator, require the entire training data set to be stored, leading to expensive computationifthedatasetislarge.
Thiseffectcanbeoffset, attheexpenseofsome additionalone-offcomputation, byconstructingtree-basedsearchstructurestoallow (approximate) near neighbours to be found efficiently without doing an exhaustive search of the data set.
Nevertheless, these nonparametric methods are still severely limited.
On the other hand, we have seen that simple parametric models are very restrictedintermsoftheformsofdistributionthattheycanrepresent.
Wetherefore need to find density models that are very flexible and yet for which the complexity ofthemodelscanbecontrolledindependentlyofthesizeofthetrainingset, andwe shallseeinsubsequentchaptershowtoachievethis.
Exercises 2.1 ( ) www Verifythatthe Bernoullidistribution(2.2)satisfiesthefollowingprop- erties 1 p(x|µ) = 1 (2.257) x=0 E[x] = µ (2.258) var[x] = µ(1−µ).
(2.259) Show that the entropy H[x] of a Bernoulli distributed random binary variable x is givenby H[x]=−µlnµ−(1−µ)ln(1−µ).
(2.260) 2.2 ( ) The form of the Bernoulli distribution given by (2.2) is not symmetric be- tween the two values of x.
In some situations, it will be more convenient to use an equivalentformulationforwhichx∈{−1,1}, inwhichcasethedistributioncanbe written 1−µ (1−x)/2 1+µ (1+x)/2 p(x|µ)= (2.261) 2 2 whereµ∈[−1,1].
Showthatthedistribution(2.261)isnormalized, andevaluateits mean, variance, andentropy.
2.3 ( ) www In this exercise, we prove that the binomial distribution (2.9) is nor- malized.
Firstusethedefinition(2.10)ofthenumberofcombinationsofmidentical objectschosenfromatotalof N toshowthat N N N +1 + = .
(2.262) m m−1 m 128 2.
PROBABILITYDISTRIBUTIONS Usethisresulttoprovebyinductionthefollowingresult N N N m (1+x) = x (2.263) m m=0 whichisknownasthebinomialtheorem, andwhichisvalidforallrealvaluesofx.
Finally, showthatthebinomialdistributionisnormalized, sothat N N µ m (1−µ) N−m =1 (2.264) m m=0 whichcanbedonebyfirstpullingoutafactor(1−µ)N outofthesummationand thenmakinguseofthebinomialtheorem.
2.4 ( ) Showthatthemeanofthebinomialdistributionisgivenby(2.11).
Todothis, differentiatebothsidesofthenormalizationcondition(2.264)withrespecttoµand thenrearrangetoobtainanexpressionforthemeanofn.
Similarly, bydifferentiating (2.264) twice with respect to µ and making use of the result (2.11) for the mean of thebinomialdistributionprovetheresult(2.12)forthevarianceofthebinomial.
2.5 ( ) www Inthisexercise, weprovethatthebetadistribution, givenby(2.13), is correctlynormalized, sothat(2.14)holds.
Thisisequivalenttoshowingthat 1 Γ(a)Γ(b) µ a−1(1−µ) b−1dµ= .
(2.265) Γ(a+b) 0 Fromthedefinition(1.141)ofthegammafunction, wehave ∞ ∞ Γ(a)Γ(b)= exp(−x)x a−1dx exp(−y)y b−1dy.
(2.266) 0 0 Usethisexpressiontoprove(2.265)asfollows.
Firstbringtheintegraloveryinside the integrand of the integral over x, next make the change of variable t = y + x where x is fixed, then interchange the order of the x and t integrations, and finally makethechangeofvariablex=tµwheretisfixed.
2.6 ( ) Makeuseoftheresult(2.265)toshowthatthemean, variance, andmodeofthe betadistribution(2.13)aregivenrespectivelyby a E[µ] = (2.267) a+b ab var[µ] = (2.268) (a+b)2(a+b+1) a−1 mode[µ] = .
(2.269) a+b−2 Exercises 129 2.7 ( ) Considerabinomialrandomvariablexgivenby(2.9), withpriordistribution forµgivenbythebetadistribution(2.13), andsupposewehaveobservedmoccur- rencesofx=1andloccurrencesofx=0.
Showthattheposteriormeanvalueofx liesbetweenthepriormeanandthemaximumlikelihoodestimateforµ.
Todothis, showthattheposteriormeancanbewrittenasλtimesthepriormeanplus(1−λ) timesthemaximumlikelihoodestimate, where0 λ 1.
Thisillustratesthecon- ceptoftheposteriordistributionbeingacompromisebetweenthepriordistribution andthemaximumlikelihoodsolution.
2.8 ( ) Considertwovariablesxandywithjointdistributionp(x, y).
Provethefollow- ingtworesults E[x] = E y[E x[x|y]] (2.270) var[x] = E y[varx[x|y]]+vary[E x[x|y]].
(2.271) Here E x[x|y]denotestheexpectationofxundertheconditionaldistributionp(x|y), withasimilarnotationfortheconditionalvariance.
2.9 ( ) www .
In this exercise, we prove the normalization of the Dirichlet dis- tribution (2.38) using induction.
We have already shown in Exercise 2.5 that the betadistribution, whichisaspecialcaseofthe Dirichletfor M = 2, isnormalized.
We now assume that the Dirichlet distribution is normalized for M − 1 variables and prove that it is normalized for M variables.
To do this, cons ider the Dirichlet M distribution over M variables, and take account of the constraint k=1 µk = 1 by eliminatingµM, sothatthe Dirichletiswritten M −1 M −1 α M −1 p M(µ 1 ,...,µM−1 )=CM µ α k k −1 1− µj (2.272) k=1 j=1 andourgoalistofindanexpressionfor CM.
Todothis, integrateoverµM−1, taking care over the limits of integration, and then make a change of variable so that this integralhaslimits0and1.
Byassumingthecorrectresultfor CM−1andmakinguse of(2.265), derivetheexpressionfor CM.
2.10 ( ) Using the property Γ(x + 1) = xΓ(x) of the gamma function, derive the followingresultsforthemean, variance, andcovarianceofthe Dirichletdistribution givenby(2.38) E[µj] = αj (2.273) α 0 αj(α 0 −αj) var[µj] = α2(α +1) (2.274) 0 0 cov[µjµl] = − α2( α α jα + l 1) , j =l (2.275) 0 0 whereα 0 isdefinedby(2.39).
130 2.
PROBABILITYDISTRIBUTIONS 2.11 ( ) www By expressing the expectation of lnµj under the Dirichlet distribution (2.38)asaderivativewithrespecttoαj, showthat E[lnµj]=ψ(αj)−ψ(α 0 ) (2.276) whereα 0 isgivenby(2.39)and d ψ(a)≡ lnΓ(a) (2.277) da isthedigammafunction.
2.12 ( ) Theuniformdistributionforacontinuousvariablexisdefinedby 1 U(x|a, b)= , a x b.
(2.278) b−a Verify that this distribution is normalized, and find expressions for its mean and variance.
2.13 ( ) Evaluate the Kullback-Leibler divergence (1.113) between two Gaussians p(x)=N(x|µ,Σ)andq(x)=N(x|m, L).
2.14 ( ) www Thisexercisedemonstratesthatthemultivariatedistributionwithmax- imum entropy, for a given covariance, is a Gaussian.
The entropy of a distribution p(x)isgivenby H[x]=− p(x)lnp(x)dx.
(2.279) Wewishtomaximize H[x]overalldistributionsp(x)subjecttotheconstraintsthat p(x)benormalizedandthatithaveaspecificmeanandcovariance, sothat p(x)dx=1 (2.280) p(x)xdx=µ (2.281) p(x)(x−µ)(x−µ)Tdx=Σ.
(2.282) Byperformingavariationalmaximizationof(2.279)andusing Lagrangemultipliers to enforce the constraints (2.280), (2.281), and (2.282), show that the maximum likelihooddistributionisgivenbythe Gaussian(2.43).
2.15 ( ) Showthattheentropyofthemultivariate Gaussian N(x|µ,Σ)isgivenby 1 D H[x]= ln|Σ|+ (1+ln(2π)) (2.283) 2 2 where Disthedimensionalityofx.
Exercises 131 2.16 ( ) www Consider two random variables x 1 and x 2 having Gaussian distri- butions with means µ 1 ,µ 2 and precisions τ 1, τ 2 respectively.
Derive an expression for the differential entropy of the variable x = x 1 +x 2.
To do this, first find the distributionofxbyusingtherelation ∞ p(x)= p(x|x 2 )p(x 2 )dx 2 (2.284) −∞ and completing the square in the exponent.
Then observe that this represents the convolutionoftwo Gaussiandistributions, whichitselfwillbe Gaussian, andfinally makeuseoftheresult(1.110)fortheentropyoftheunivariate Gaussian.
2.17 ( ) www Consider the multivariate Gaussian distribution given by (2.43).
By writing the precision matrix (inverse covariance matrix) Σ −1 as the sum of a sym- metric and an anti-symmetric matrix, show that the anti-symmetric term does not appearintheexponentofthe Gaussian, andhencethattheprecisionmatrixmaybe takentobesymmetricwithoutlossofgenerality.
Becausetheinverseofasymmetric matrix is also symmetric (see Exercise 2.22), it follows that the covariance matrix mayalsobechosentobesymmetricwithoutlossofgenerality.
2.18 ( ) Consider a real, symmetric matrix Σ whose eigenvalue equation is given by (2.45).
By taking the complex conjugate of this equation and subtracting the originalequation, andthenformingtheinnerproductwitheigenvectorui, showthat theeigenvaluesλi arereal.
Similarly, usethesymmetrypropertyofΣtoshowthat twoeigenvectorsui anduj willbeorthogonalprovidedλj =λi.
Finally, showthat without loss of generality, theset of eigenvectors canbechosen to beorthonormal, sothattheysatisfy(2.46), evenifsomeoftheeigenvaluesarezero.
2.19 ( ) Showthatareal, symmetricmatrixΣhavingtheeigenvectorequation(2.45) canbeexpressedasanexpansionintheeigenvectors, withcoefficientsgivenbythe eigenvalues, of the form (2.48).
Similarly, show that the inverse matrix Σ −1 has a representationoftheform(2.49).
2.20 ( ) www A positive definite matrix Σ can be defined as one for which the quadraticform a TΣa (2.285) is positive for any real value of the vector a.
Show that a necessary and sufficient conditionforΣtobepositivedefiniteisthatalloftheeigenvaluesλi ofΣ, defined by(2.45), arepositive.
2.21 ( ) Showthatareal, symmetricmatrixofsize D×Dhas D(D+1)/2independent parameters.
2.22 ( ) www Showthattheinverseofasymmetricmatrixisitselfsymmetric.
2.23 ( ) Bydiagonalizingthecoordinatesystemusingtheeigenvectorexpansion(2.45), showthatthevolumecontainedwithinthehyperellipsoidcorrespondingtoaconstant 132 2.
PROBABILITYDISTRIBUTIONS Mahalanobisdistance∆isgivenby VD |Σ|1/2∆ D (2.286) where VD is the volume of the unit sphere in D dimensions, and the Mahalanobis distanceisdefinedby(2.44).
2.24 ( ) www Provetheidentity(2.76)bymultiplyingbothsidesbythematrix A B (2.287) C D andmakinguseofthedefinition(2.77).
butionsforamultivariate Gaussian.
Moregenerally, wecanconsiderapartitioning ofthecomponentsofxintothreegroupsxa, xb, andxc, withacorrespondingpar- titioningofthemeanvectorµandofthecovariancematrixΣintheform µ a Σaa Σab Σac µ= µ b , Σ= Σba Σbb Σbc .
(2.288) µ c Σca Σcb Σcc By making use of the results of Section 2.3, find an expression for the conditional distributionp(xa |xb)inwhichxc hasbeenmarginalizedout.
2.26 ( ) A very useful result from linear algebra is the Woodbury matrix inversion formulagivenby (A+BCD) −1 =A −1−A −1B(C −1+DA −1B) −1DA −1.
(2.289) Bymultiplyingbothsidesby(A+BCD)provethecorrectnessofthisresult.
2.27 ( ) Let x and z be two independent random vectors, so that p(x, z) = p(x)p(z).
Showthatthemeanoftheirsumy =x+zisgivenbythesumofthemeansofeach ofthevariableseparately.
Similarly, showthatthecovariancematrixofyisgivenby the sum of the covariance matrices of x and z.
Confirm that this result agrees with thatof Exercise 1.10.
2.28 ( ) www Considerajointdistributionoverthevariable x z= (2.290) y whosemeanandcovariancearegivenby(2.108)and(2.105)respectively.
Bymak- ing use of the results (2.92) and (2.93) show that the marginal distribution p(x) is given(2.99).
Similarly, bymakinguseoftheresults(2.81)and(2.82)showthatthe conditionaldistributionp(y|x)isgivenby(2.100).
Exercises 133 2.29 ( ) Usingthepartitionedmatrixinversionformula(2.76), showthattheinverseof theprecisionmatrix(2.104)isgivenbythecovariancematrix(2.105).
2.30 ( ) Bystartingfrom(2.107)andmakinguseoftheresult(2.105), verifytheresult (2.108).
2.31 ( ) Consider two multidimensional random vectors x and z having Gaussian distributionsp(x) = N(x|µ x ,Σ x )andp(z) = N(z|µ z ,Σ z )respectively, together withtheirsumy =x+z.
Usetheresults(2.109)and(2.110)tofindanexpressionfor themarginaldistributionp(y)byconsideringthelinear-Gaussianmodelcomprising theproductofthemarginaldistributionp(x)andtheconditionaldistributionp(y|x).
2.32 ( ) www This exercise and the next provide practice at manipulating the quadratic forms that arise in linear-Gaussian models, as well as giving an indepen- dent check of results derived in the main text.
Consider a joint distributionp(x, y) defined by the marginal and conditional distributions given by (2.99) and (2.100).
Byexaminingthequadraticformintheexponentofthejointdistribution, andusing the technique of ‘completing the square’ discussed in Section 2.3, find expressions forthemeanandcovarianceofthemarginaldistributionp(y)inwhichthevariable x has been integrated out.
To do this, make use of the Woodbury matrix inversion formula (2.289).
Verify that these results agree with (2.109) and (2.110) obtained usingtheresultsof Chapter2.
2.33 ( ) Consider the same joint distribution as in Exercise 2.32, but now use the techniqueofcompletingthesquaretofindexpressionsforthemeanandcovariance oftheconditionaldistributionp(x|y).
Again, verifythattheseagreewiththecorre- spondingexpressions(2.111)and(2.112).
2.34 ( ) www To find the maximum likelihood solution for the covariance matrix ofamultivariate Gaussian, weneedtomaximizetheloglikelihoodfunction(2.118) withrespecttoΣ, notingthatthecovariancematrixmustbesymmetricandpositive definite.
Hereweproceedbyignoringtheseconstraintsanddoingastraightforward maximization.
Usingtheresults(C.21),(C.26), and(C.28)from Appendix C, show that the covariance matrix Σ that maximizes the log likelihood function (2.118) is given by the sample covariance (2.122).
We note that the final result is necessarily symmetricandpositivedefinite(providedthesamplecovarianceisnonsingular).
showthat E[xnxm]=µµT+InmΣ (2.291) where xn denotes a data point sampled from a Gaussian distribution with mean µ andcovarianceΣ, and Inmdenotesthe(n, m)elementoftheidentitymatrix.
Hence provetheresult(2.124).
2.36 ( ) www Using an analogous procedure to that used to obtain (2.126), derive anexpressionfor thesequentialestimation ofthevarianceof aunivariate Gaussian 134 2.
PROBABILITYDISTRIBUTIONS distribution, bystartingwiththemaximumlikelihoodexpression N 1 σ M 2 L = N (xn −µ)2.
(2.292) n=1 Verifythatsubstitutingtheexpressionfora Gaussiandistributionintothe Robbins- Monro sequential estimation formula (2.135) gives a result of the same form, and henceobtainanexpressionforthecorrespondingcoefficientsa N.
2.37 ( ) Using an analogous procedure to that used to obtain (2.126), derive an ex- pression for the sequential estimation of the covariance of a multivariate Gaussian distribution, bystartingwiththemaximumlikelihoodexpression(2.122).
Verifythat substituting the expression for a Gaussian distribution into the Robbins-Monro se- quentialestimationformula(2.135)givesaresultofthesameform, andhenceobtain anexpressionforthecorrespondingcoefficientsa N.
2.38 ( ) Usethetechniqueofcompletingthesquareforthequadraticformintheexpo- nenttoderivetheresults(2.141)and(2.142).
2.39 ( ) Starting from the results (2.141) and (2.142) for the posterior distribution of the mean of a Gaussian random variable, dissect out the contributions from the first N − 1 data points and hence obtain expressions for the sequential update of µN and σ N 2 .
Now derive the same results starting from the posterior distribution p(µ|x 1 ,..., x N−1 ) = N(µ|µN−1 ,σ N 2 −1 ) and multiplying by the likelihood func- tion p(x N |µ) = N(x N |µ,σ2) and then completing the square and normalizing to obtaintheposteriordistributionafter N observations.
2.40 ( ) www Considera D-dimensional Gaussianrandomvariablexwithdistribu- tion N(x|µ,Σ)inwhichthecovarianceΣisknownandforwhichwewishtoinfer themeanµfromasetofobservations X={x 1 ,..., x N }.
Givenapriordistribution p(µ)=N(µ|µ 0 ,Σ 0 ), findthecorrespondingposteriordistributionp(µ|X).
2.41 ( ) Usethedefinitionofthegammafunction(1.141)toshowthatthegammadis- tribution(2.146)isnormalized.
2.42 ( ) Evaluatethemean, variance, andmodeofthegammadistribution(2.146).
2.43 ( ) Thefollowingdistribution q |x|q p(x|σ2, q)= exp − (2.293) 2(2σ2)1/qΓ(1/q) 2σ2 isageneralizationoftheunivariate Gaussiandistribution.
Showthatthisdistribution isnormalizedsothat ∞ p(x|σ2, q)dx=1 (2.294) −∞ and that it reduces to the Gaussian when q = 2.
Consider a regression model in which the target variable is given by t = y(x, w) + and is a random noise Exercises 135 variable drawn from the distribution (2.293).
Show that the log likelihood function over w and σ2, for an observed data set of input vectors X = {x 1 ,..., x N } and correspondingtargetvariablest=(t 1 ,..., t N)T, isgivenby N 1 N lnp(t|X, w,σ2)=− 2σ2 |y(xn, w)−tn |q − q ln(2σ2)+const (2.295) n=1 where‘const’denotestermsindependentofbothwandσ2.
Notethat, asafunction ofw, thisisthe Lq errorfunctionconsideredin Section1.5.5.
2.44 ( ) Consider a univariate Gaussian distribution N(x|µ,τ−1) having conjugate observations.
Show that the posterior distribution is also a Gaussian-gamma distri- butionofthesamefunctionalformastheprior, andwritedownexpressionsforthe parametersofthisposteriordistribution.
2.45 ( ) Verify that the Wishart distribution defined by (2.155) is indeed a conjugate priorfortheprecisionmatrixofamultivariate Gaussian.
2.46 ( ) www Verifythatevaluatingtheintegralin(2.158)leadstotheresult(2.159).
2.47 ( ) www Show that in the limit ν → ∞, the t-distribution (2.159) becomes a Gaussian.
Hint: ignorethenormalizationcoefficient, andsimplylookatthedepen- denceonx.
2.48 ( ) By following analogous steps to those used to derive the univariate Student’s t-distribution (2.159), verify the result (2.162) for the multivariate form of the Stu- dent’s t-distribution, by marginalizing over the variable η in (2.161).
Using the definition (2.161), show by exchanging integration variables that the multivariate t-distributioniscorrectlynormalized.
2.49 ( ) Byusingthedefinition(2.161)ofthemultivariate Student’st-distributionasa convolutionofa Gaussianwithagammadistribution, verifytheproperties(2.164), (2.165), and(2.166)forthemultivariatet-distributiondefinedby(2.162).
2.50 ( ) Showthatinthelimitν → ∞, themultivariate Student’st-distribution(2.162) reducestoa GaussianwithmeanµandprecisionΛ.
2.51 ( ) www The various trigonometric identities used in the discussion of periodic variablesinthischaptercanbeproveneasilyfromtherelation exp(i A)=cos A+isin A (2.296) inwhichiisthesquarerootofminusone.
Byconsideringtheidentity exp(i A)exp(−i A)=1 (2.297) provetheresult(2.177).
Similarly, usingtheidentity cos(A−B)= exp{i(A−B)} (2.298) 136 2.
PROBABILITYDISTRIBUTIONS where denotes the real part, prove (2.178).
Finally, by using sin(A − B) = exp{i(A−B)}, where denotestheimaginarypart, provetheresult(2.183).
2.52 ( ) For large m, the von Mises distribution (2.179) becomes sharply peaked around the mode θ 0.
By defining ξ = m1/2(θ − θ 0 ) and making the Taylor ex- pansionofthecosinefunctiongivenby α2 cosα=1− +O(α4) (2.299) 2 showthatasm→∞, thevon Misesdistributiontendstoa Gaussian.
2.53 ( ) Usingthetrigonometricidentity(2.183), showthatsolutionof(2.182)forθ 0 is givenby(2.184).
2.54 ( ) Bycomputingfirstandsecondderivativesofthevon Misesdistribution(2.179), andusing I 0 (m) > 0form > 0, showthatthemaximumofthedistributionoccurs whenθ =θ 0 andthattheminimumoccurswhenθ =θ 0 +π(mod2π).
2.55 ( ) Bymakinguseoftheresult(2.168), togetherwith(2.184)andthetrigonometric identity(2.178), showthatthemaximumlikelihoodsolutionm MLfortheconcentra- tionofthevon Misesdistributionsatisfies A(m ML )=rwhereristheradiusofthe mean of the observations viewed as unit vectors in the two-dimensional Euclidean plane, asillustratedin Figure2.17.
2.56 ( ) www Express the beta distribution (2.13), the gamma distribution (2.146), andthevon Misesdistribution(2.179)asmembersoftheexponentialfamily(2.194) andtherebyidentifytheirnaturalparameters.
2.57 ( ) Verify that the multivariate Gaussian distribution can be cast in exponential familyform(2.194)andderiveexpressionsforη, u(x), h(x)andg(η)analogousto (2.220)–(2.223).
2.58 ( ) Theresult(2.226)showedthatthenegativegradientoflng(η)fortheexponen- tialfamilyisgivenbytheexpectationofu(x).
Bytakingthesecondderivativesof (2.195), showthat −∇∇lng(η)=E[u(x)u(x)T]−E[u(x)]E[u(x)T]=cov[u(x)].
(2.300) 2.59 ( ) By changing variables using y = x/σ, show that the density (2.236) will be correctlynormalized, providedf(x)iscorrectlynormalized.
2.60 ( ) www Consider a histogram-like density model in which the space x is di- videdintofixedregionsforwhichthedensityp(x)takestheconstantvaluehi over theith region, andthatthevolumeofregioniisdenoted∆i.
Supposewehaveaset of N observations of x such that ni of these observations fall in region i.
Using a Lagrangemultipliertoenforcethenormalizationconstraintonthedensity, derivean expressionforthemaximumlikelihoodestimatorforthe{hi }.
2.61 ( ) Showthatthe K-nearest-neighbourdensitymodeldefinesanimproperdistribu- tionwhoseintegraloverallspaceisdivergent.
3 Linear Models for Regression The focus so far in this book has been on unsupervised learning, including topics suchasdensityestimationanddataclustering.
Weturnnowtoadiscussionofsuper- visedlearning, startingwithregression.
Thegoalofregressionistopredictthevalue ofoneormorecontinuoustargetvariablestgiventhevalueofa D-dimensionalvec- tor x of input variables.
We have already encountered an example of a regression problemwhenweconsideredpolynomialcurvefittingin Chapter1.
Thepolynomial is a specific example of a broad class of functions called linear regression models, whichsharethepropertyofbeinglinearfunctionsoftheadjustableparameters, and which will form the focus of this chapter.
The simplest form of linear regression models are also linear functions of the input variables.
However, we can obtain a muchmoreusefulclassoffunctionsbytakinglinearcombinationsofafixedsetof nonlinear functions of the input variables, known as basis functions.
Such models arelinearfunctionsoftheparameters, whichgivesthemsimpleanalyticalproperties, andyetcanbenonlinearwithrespecttotheinputvariables.
137 138 3.
LINEARMODELSFORREGRESSION Givenatrainingdatasetcomprising N observations{xn }, wheren=1,..., N, together with corresponding target values {tn }, the goal is to predict the value of t for a new value of x.
In the simplest approach, this can be done by directly con- structinganappropriatefunctiony(x)whosevaluesfornewinputsxconstitutethe predictions for the corresponding values of t.
More generally, from a probabilistic perspective, weaimtomodelthepredictivedistributionp(t|x)becausethisexpresses our uncertainty about the value of t for each value of x.
From this conditional dis- tributionwecanmakepredictionsoft, foranynewvalueofx, insuchawayasto minimizetheexpectedvalueofasuitablychosenlossfunction.
Asdiscussedin Sec- tion1.5.5, acommonchoiceoflossfunctionforreal-valuedvariablesisthesquared loss, forwhichtheoptimalsolutionisgivenbytheconditionalexpectationoft.
Although linear models have significant limitations as practical techniques for patternrecognition, particularlyforproblemsinvolvinginputspacesofhighdimen- sionality, theyhaveniceanalyticalpropertiesandformthefoundationformoreso- phisticatedmodelstobediscussedinlaterchapters.
3.1.
Linear Basis Function Models Thesimplestlinearmodelforregressionisonethatinvolvesalinearcombinationof theinputvariables y(x, w)=w 0 +w 1 x 1 +...+w Dx D (3.1) propertyofthismodelisthatitisalinearfunctionoftheparametersw 0 ,..., w D.
Itis also, however, alinearfunctionoftheinputvariablesxi, andthisimposessignificant limitations on the model.
We therefore extend the class of models by considering linearcombinationsoffixednonlinearfunctionsoftheinputvariables, oftheform M −1 y(x, w)=w 0 + wjφj(x) (3.2) j=1 whereφj(x)areknownasbasisfunctions.
Bydenoting themaximum valueof the indexj by M −1, thetotalnumberofparametersinthismodelwillbe M.
Theparameterw 0allowsforanyfixedoffsetinthedataandissometimescalled a bias parameter (not to be confused with ‘bias’ in a statistical sense).
It is often convenienttodefineanadditionaldummy‘basisfunction’φ 0 (x)=1sothat M −1 y(x, w)= wjφj(x)=w Tφ(x) (3.3) j=0 plications of pattern recognition, we will apply some form of fixed pre-processing, 3.1.
Linear Basis Function Models 139 or feature extraction, to the original data variables.
If the original variables com- prisethevectorx, thenthefeaturescanbeexpressedintermsofthebasisfunctions {φj(x)}.
Byusingnonlinearbasisfunctions, weallowthefunctiony(x, w)tobeanon- linear function of the input vector x.
Functions of the form (3.2) are called linear models, however, because this function is linear in w.
It is this linearity in the pa- rameters that will greatly simplify the analysis of this class of models.
However, it alsoleadstosomesignificantlimitations, aswediscussin Section3.6.
The example of polynomial regression considered in Chapter 1 is a particular exampleofthismodelinwhichthereisasingleinputvariablex, andthebasisfunc- tionstaketheformofpowersofxsothatφj(x)=xj.
Onelimitationofpolynomial basisfunctionsisthattheyareglobalfunctionsoftheinputvariable, sothatchanges inoneregionofinputspaceaffectallotherregions.
Thiscanberesolvedbydividing theinputspaceupintoregionsandfitadifferentpolynomialineachregion, leading tosplinefunctions(Hastieetal.,2001).
Therearemanyotherpossiblechoicesforthebasisfunctions, forexample φj(x)=exp − (x− 2s µ 2 j)2 (3.4) wheretheµj governthelocationsofthebasisfunctionsininputspace, andthepa- rameter s governs their spatial scale.
These are usually referred to as ‘Gaussian’ basisfunctions, althoughitshouldbenotedthattheyarenotrequiredtohaveaprob- abilisticinterpretation, andinparticularthenormalizationcoefficientisunimportant becausethesebasisfunctionswillbemultipliedbyadaptiveparameterswj.
Anotherpossibilityisthesigmoidalbasisfunctionoftheform x−µj φj(x)=σ (3.5) s whereσ(a)isthelogisticsigmoidfunctiondefinedby 1 σ(a)= .
(3.6) 1+exp(−a) Equivalently, we can use the ‘tanh’ function because this is related to the logistic sigmoid by tanh(a) = 2σ(a)−1, and so a general linear combination of logistic sigmoidfunctionsisequivalenttoagenerallinearcombinationof‘tanh’functions.
Thesevariouschoicesofbasisfunctionareillustratedin Figure3.1.
Yetanotherpossiblechoiceofbasisfunctionisthe Fourierbasis, whichleadsto an expansion in sinusoidal functions.
Each basis function represents a specific fre- quencyandhasinfinitespatialextent.
Bycontrast, basisfunctionsthatarelocalized to finite regions of input space necessarily comprise a spectrum of different spatial frequencies.
Inmanysignalprocessingapplications, itisofinteresttoconsiderba- sis functions that are localized in both space and frequency, leading to a class of functions known as wavelets.
These are also defined to be mutually orthogonal, to simplify their application.
Wavelets are most applicable when the input values live 140 3.
LINEARMODELSFORREGRESSION 1 1 1 0.5 0.75 0.75 0 0.5 0.5 −0.5 0.25 0.25 −1 0 0 −1 0 1 −1 0 1 −1 0 1 Figure 3.1 Examples of basis functions, showing polynomials on the left, Gaussians of the form (3.4) in the centre, andsigmoidaloftheform(3.5)ontheright.
onaregularlattice, suchasthesuccessivetimepointsinatemporalsequence, orthe pixels in an image.
Useful texts on wavelets include Ogden (1997), Mallat (1999), and Vidakovic(1999).
Mostofthediscussioninthischapter, however, isindependentoftheparticular choice of basis function set, and so for most of our discussion we shall not specify the particular form of the basis functions, except for the purposes of numerical il- lustration.
Indeed, muchofourdiscussionwillbeequallyapplicabletothesituation in which the vector φ(x) of basis functions is simply the identity φ(x) = x.
Fur- thermore, inordertokeepthenotationsimple, weshallfocusonthecaseofasingle target variable t.
However, in Section 3.1.5, we consider briefly the modifications neededtodealwithmultipletargetvariables.
3.1.1 Maximum likelihood and least squares In Chapter1, wefittedpolynomialfunctionstodatasetsbyminimizingasum- of-squareserrorfunction.
Wealsoshowedthatthiserrorfunctioncouldbemotivated as the maximum likelihood solution under an assumed Gaussian noise model.
Let usreturntothisdiscussionandconsidertheleastsquaresapproach, anditsrelation tomaximumlikelihood, inmoredetail.
Asbefore, weassumethatthetargetvariabletisgivenbyadeterministicfunc- tiony(x, w)withadditive Gaussiannoisesothat t=y(x, w)+ (3.7) where is a zero mean Gaussian random variable with precision (inverse variance) β.
Thuswecanwrite p(t|x, w,β)=N(t|y(x, w),β −1).
(3.8) Recallthat, ifweassumeasquaredlossfunction, thentheoptimalprediction, fora Section1.5.5 newvalueofx, willbegivenbytheconditionalmeanofthetargetvariable.
Inthe case of a Gaussian conditional distribution of the form (3.8), the conditional mean 3.1.
Linear Basis Function Models 141 willbesimply E[t|x]= tp(t|x)dt=y(x, w).
(3.9) Notethatthe Gaussiannoiseassumptionimpliesthattheconditionaldistributionof t given x is unimodal, which may be inappropriate for some applications.
An ex- tensiontomixturesofconditional Gaussiandistributions, whichpermitmultimodal conditionaldistributions, willbediscussedin Section14.5.1.
Nowconsideradatasetofinputs X={x 1 ,..., x N }withcorrespondingtarget values t 1 ,..., t N.
We group the target variables {tn } into a column vector that we denotebytwherethetypefaceischosentodistinguishitfromasingleobservation of a multivariate target, which would be denoted t.
Making the assumption that thesedatapointsaredrawnindependentlyfromthedistribution(3.8), weobtainthe followingexpressionforthelikelihoodfunction, whichisafunctionoftheadjustable parameterswandβ, intheform N p(t|X, w,β)= N(tn |w Tφ(xn),β −1) (3.10) n=1 wherewehaveused(3.3).
Notethatinsupervisedlearningproblemssuchasregres- sion (and classification), we are not seeking to model the distribution of the input variables.
Thus x will always appear in the set of conditioning variables, and so fromnowonwewilldroptheexplicitxfromexpressionssuchasp(t|x, w,β)inor- dertokeepthenotationuncluttered.
Takingthelogarithmofthelikelihoodfunction, andmakinguseofthestandardform(1.46)fortheunivariate Gaussian, wehave N lnp(t|w,β) = ln N(tn |w Tφ(xn),β −1) n=1 N N = lnβ− ln(2π)−βED(w) (3.11) 2 2 wherethesum-of-squareserrorfunctionisdefinedby N 1 ED(w)= {tn −w Tφ(xn)}2.
(3.12) 2 n=1 Havingwrittendownthelikelihoodfunction, wecanusemaximumlikelihoodto determinewandβ.
Considerfirstthemaximizationwithrespecttow.
Asobserved alreadyin Section1.2.5, weseethatmaximizationofthelikelihoodfunctionundera conditional Gaussiannoisedistributionforalinearmodelisequivalenttominimizing asum-of-squareserrorfunctiongivenby ED(w).
Thegradientoftheloglikelihood function(3.11)takestheform N ∇lnp(t|w,β)= tn −w Tφ(xn) φ(xn)T.
(3.13) n=1 142 3.
LINEARMODELSFORREGRESSION Settingthisgradienttozerogives N N 0= tn φ(xn)T−w T φ(xn)φ(xn)T .
(3.14) n=1 n=1 Solvingforwweobtain w ML = ΦTΦ −1 ΦT t (3.15) whichareknownasthenormalequationsfortheleastsquaresproblem.
HereΦisan N×M matrix, calledthedesignmatrix, whoseelementsaregivenbyΦnj =φj(xn), sothat ⎛ ⎞ φ 0 (x 1 ) φ 1 (x 1 ) ··· φM−1 (x 1 ) ⎜ ⎜ φ 0 (x 2 ) φ 1 (x 2 ) ··· φM−1 (x 2 ) ⎟ ⎟ Φ=⎝ .
.
.
.
.
.
.
⎠.
(3.16) φ 0 (x N) φ 1 (x N) ··· φM−1 (x N) Thequantity Φ † ≡ ΦTΦ −1 ΦT (3.17) is known as the Moore-Penrose pseudo-inverse of the matrix Φ (Rao and Mitra, 1971; Golub and Van Loan, 1996).
It can be regarded as a generalization of the notionofmatrixinversetononsquarematrices.
Indeed, ifΦissquareandinvertible, thenusingtheproperty(AB)−1 =B−1A−1 weseethatΦ † ≡Φ −1 .
Atthispoint, wecangainsomeinsightintotheroleofthebiasparameterw 0.
If wemakethebiasparameterexplicit, thentheerrorfunction(3.12)becomes N M −1 1 ED(w)= {tn −w 0 − wjφj(xn)}2.
(3.18) 2 n=1 j=1 Settingthederivativewithrespecttow 0equaltozero, andsolvingforw 0, weobtain M −1 w 0 =t− wjφj (3.19) j=1 wherewehavedefined N N 1 1 t= tn, φj = φj(xn).
(3.20) N N n=1 n=1 Thus the bias w 0 compensates for the difference between the averages (over the training set) of the target values and the weighted sum of the averages of the basis functionvalues.
Wecanalsomaximizetheloglikelihoodfunction(3.11)withrespecttothenoise precisionparameterβ, giving N 1 1 β = N {tn −w M T L φ(xn)}2 (3.21) ML n=1 3.1.
Linear Basis Function Models 143 Figure3.2 Geometrical interpretation of the least-squares S solution, inan N-dimensionalspacewhoseaxes t are the values of t 1 ,..., t N.
The least-squares regressionfunctionisobtainedbyfindingtheor- ϕ thogonalprojectionofthedatavectortontothe ϕ y 2 subspacespannedbythebasisfunctionsφ j (x) 1 inwhicheachbasisfunctionisviewedasavec- torϕ j oflength N withelementsφ j (x n ).
andsoweseethattheinverseofthenoiseprecisionisgivenbytheresidualvariance ofthetargetvaluesaroundtheregressionfunction.
3.1.2 Geometry of least squares At this point, it is instructive to consider the geometrical interpretation of the least-squaressolution.
Todothisweconsider an N-dimensionalspacewhoseaxes are given by the tn, so that t = (t 1 ,..., t N)T is a vector in this space.
Each basis functionφj(xn), evaluatedatthe N datapoints, canalsoberepresentedasavectorin thesamespace, denotedbyϕ , asillustratedin Figure3.2.
Notethatϕ corresponds j j to the jth column of Φ, whereas φ(xn) corresponds to the nth row of Φ.
If the number M ofbasisfunctionsissmallerthanthenumber N ofdatapoints, thenthe M vectors φj(xn) will span a linear subspace S of dimensionality M.
We define y to be an N-dimensional vector whose nth element is given by y(xn, w), where n=1,..., N.
Becauseyisanarbitrarylinearcombinationofthevectorsϕ , itcan j live anywhere in the M-dimensional subspace.
The sum-of-squares error (3.12) is then equal (up to a factor of 1/2) to the squared Euclidean distance between y and t.
Thus the least-squares solution for w corresponds to that choice of y that lies in subspace S and that is closest to t.
Intuitively, from Figure 3.2, we anticipate that thissolutioncorrespondstotheorthogonalprojectionoftontothesubspace S.
This isindeedthecase, ascaneasilybeverifiedbynotingthatthesolutionforyisgiven Exercise 3.2 byΦw ML, andthenconfirmingthatthistakestheformofanorthogonalprojection.
Inpractice, adirectsolutionofthenormalequationscanleadtonumericaldiffi- cultieswhenΦTΦisclosetosingular.
Inparticular, whentwoormoreofthebasis vectorsϕ areco-linear, ornearlyso, theresultingparametervaluescanhavelarge j magnitudes.
Suchneardegeneracieswillnotbeuncommonwhendealingwithreal datasets.
Theresultingnumerical difficulties canbeaddressed usingthetechnique of singular value decomposition, or SVD (Press et al., 1992; Bishop and Nabney, 2008).
Notethattheadditionofaregularizationtermensuresthatthematrixisnon- singular, eveninthepresenceofdegeneracies.
3.1.3 Sequential learning Batch techniques, such as the maximum likelihood solution (3.15), which in- volveprocessingtheentiretrainingsetinonego, canbecomputationallycostlyfor largedatasets.
Aswehavediscussedin Chapter1, ifthedatasetissufficientlylarge, itmaybeworthwhiletousesequentialalgorithms, alsoknownason-linealgorithms, 144 3.
LINEARMODELSFORREGRESSION inwhichthedatapointsareconsideredoneatatime, andthemodelparametersup- dated after each such presentation.
Sequential learning is also appropriate for real- timeapplicationsinwhichthedataobservationsarearrivinginacontinuousstream, andpredictionsmustbemadebeforeallofthedatapointsareseen.
We can obtain a sequential learning algorithm by applying the technique of stochasticgradientdescent, alsoknownassequentialgrad ientdescent, asfollows.
If theerrorfunctioncomprisesasumoverdatapoints E = n En, thenafterpresen- tation of pattern n, the stochastic gradient descent algorithm updates the parameter vectorwusing w(τ+1) =w(τ)−η∇En (3.22) where τ denotes the iteration number, and η is a learning rate parameter.
We shall discussthechoiceofvalueforηshortly.
Thevalueofwisinitializedtosomestarting vectorw(0).
Forthecaseofthesum-of-squareserrorfunction(3.12), thisgives w(τ+1) =w(τ)+η(tn −w(τ)Tφ n )φ n (3.23) where φ n = φ(xn).
This is known as least-mean-squares or the LMS algorithm.
Thevalueof η needstobechosenwithcaretoensure thatthealgorithm converges (Bishopand Nabney,2008).
3.1.4 Regularized least squares In Section 1.1, we introduced the idea of adding a regularization term to an error function in order to control over-fitting, so that the total error function to be minimizedtakestheform ED(w)+λEW(w) (3.24) where λ is the regularization coefficient that controls the relative importance of the data-dependent error ED(w) and the regularization term EW(w).
One of the sim- plest forms of regularizer is given by the sum-of-squares of the weight vector ele- ments 1 EW(w)= w Tw.
(3.25) 2 Ifwealsoconsiderthesum-of-squareserrorfunctiongivenby N 1 E(w)= {tn −w Tφ(xn)}2 (3.26) 2 n=1 thenthetotalerrorfunctionbecomes N 1 λ {tn −w Tφ(xn)}2+ w Tw.
(3.27) 2 2 n=1 This particular choice of regularizer is known in the machine learning literature as weightdecaybecauseinsequentiallearningalgorithms, itencouragesweightvalues todecaytowardszero, unlesssupportedbythedata.
Instatistics, itprovidesanex- ampleofaparametershrinkagemethodbecauseitshrinksparametervaluestowards 3.1.
Linear Basis Function Models 145 q =0.5 q =1 q =2 q =4 Figure3.3 Contoursoftheregularizationtermin(3.29)forvariousvaluesoftheparameterq.
zero.
It has the advantage that the error function remains a quadratic function of w, andsoitsexactminimizercanbefoundinclosedform.
Specifically, settingthe gradientof(3.27)withrespecttowtozero, andsolvingforwasbefore, weobtain w = λI+ΦTΦ −1 ΦT t.
(3.28) Thisrepresentsasimpleextensionoftheleast-squaressolution(3.15).
A more general regularizer is sometimes used, for which the regularized error takestheform N M 1 λ {tn −w Tφ(xn)}2+ |wj |q (3.29) 2 2 n=1 j=1 whereq = 2correspondstothequadraticregularizer(3.27).
Figure3.3showscon- toursoftheregularizationfunctionfordifferentvaluesofq.
The case of q = 1 is know as the lasso in the statistics literature (Tibshirani, 1996).
It has the property that if λ is sufficiently large, some of the coefficients wj are driven to zero, leading to a sparse model in which the corresponding basis functionsplaynorole.
Toseethis, wefirstnotethatminimizing(3.29)isequivalent Exercise 3.5 tominimizingtheunregularizedsum-of-squareserror(3.12)subjecttotheconstraint M |wj |q η (3.30) j=1 foranappropriatevalueoftheparameterη, wherethetwoapproachescanberelated Appendix E using Lagrangemultipliers.
Theoriginofthesparsitycanbeseenfrom Figure3.4, whichshowsthattheminimumoftheerrorfunction, subjecttotheconstraint(3.30).
Asλisincreased, soanincreasingnumberofparametersaredriventozero.
Regularizationallowscomplexmodelstobetrainedondatasetsoflimitedsize without severe over-fitting, essentially by limiting the effective model complexity.
However, theproblemofdetermining theoptimalmodelcomplexityisthenshifted fromoneoffindingtheappropriatenumberofbasisfunctionstooneofdetermining a suitable value of the regularization coefficient λ.
We shall return to the issue of modelcomplexitylaterinthischapter.
146 3.
LINEARMODELSFORREGRESSION Figure3.4 Plot of the contours w2 w2 of the unregularized error function (blue) along with the constraint re- gion(3.30)forthequadraticregular- izer q = 2 on the left and the lasso regularizer q = 1 on the right, in whichtheoptimumvalueforthepa- rametervectorw isdenotedbyw .
Thelassogivesasparsesolutionin whichw 1 =0.
w w w1 w1 For the remainder of this chapter we shall focus on the quadratic regularizer (3.27)bothforitspracticalimportanceanditsanalyticaltractability.
3.1.5 Multiple outputs Sofar, wehaveconsideredthecaseofasingletargetvariablet.
Insomeapplica- tions, wemaywishtopredict K > 1targetvariables, whichwedenotecollectively bythetargetvectort.
Thiscouldbedonebyintroducingadifferentsetofbasisfunc- tionsforeachcomponentoft, leadingtomultiple, independentregressionproblems.
However, amoreinteresting, andmorecommon, approachistousethesamesetof basisfunctionstomodelallofthecomponentsofthetargetvectorsothat y(x, w)=WTφ(x) (3.31) whereyisa K-dimensionalcolumnvector, Wisan M ×K matrixofparameters, andφ(x)isan M-dimensionalcolumnvectorwithelementsφj(x), withφ 0 (x)=1 asbefore.
Supposewetaketheconditionaldistributionofthetargetvectortobean isotropic Gaussianoftheform p(t|x, W,β)=N(t|WTφ(x),β −1I).
(3.32) If we have a set of observations t 1 ,..., t N, we can combine these into a matrix T ofsize N ×K suchthatthenth rowisgivenbyt T.
Similarly, wecancombinethe n inputvectorsx 1 ,..., x N intoamatrix X.
Theloglikelihoodfunctionisthengiven by N lnp(T|X, W,β) = ln N(tn |WTφ(xn),β −1I) n=1 N ’ ’ = NK ln β − β ’ tn −WTφ(xn) ’2 .
(3.33) 2 2π 2 n=1 3.2.
The Bias-Variance Decomposition 147 Asbefore, wecanmaximizethisfunctionwithrespectto W, giving W ML = ΦTΦ −1 ΦTT.
(3.34) Ifweexaminethisresultforeachtargetvariabletk, wehave wk = ΦTΦ −1 ΦT tk =Φ † tk (3.35) wheretk isan N-dimensionalcolumnvectorwithcomponentstnk forn=1,...
N.
Thus the solution to the regression problem decouples between the different target † variables, and we need only compute a single pseudo-inverse matrix Φ , which is sharedbyallofthevectorswk.
The extension to general Gaussian noise distributions having arbitrary covari- Exercise 3.6 ance matrices is straightforward.
Again, this leads to a decoupling into K inde- pendentregressionproblems.
Thisresultisunsurprisingbecausetheparameters W define only the mean of the Gaussian noise distribution, and we know from Sec- tion2.3.4thatthemaximumlikelihoodsolutionforthemeanofamultivariate Gaus- sian is independent of the covariance.
From now on, we shall therefore consider a singletargetvariabletforsimplicity.
3.2.
The Bias-Variance Decomposition So far in our discussion of linear models for regression, we have assumed that the form and number of basis functions are both fixed.
As we have seen in Chapter 1, the use of maximum likelihood, or equivalently least squares, can lead to severe over-fittingif complexmodelsaretrainedusingdatasetsof limitedsize.
However, limiting the number of basis functions in order to avoid over-fitting has the side effect of limiting the flexibility of the model to capture interesting and important trends in the data.
Although the introduction of regularization terms can control over-fitting for models with many parameters, this raises the question of how to determineasuitablevaluefortheregularizationcoefficientλ.
Seekingthesolution thatminimizestheregularizederrorfunctionwithrespecttoboththeweightvector w and the regularization coefficient λ is clearly not the right approach since this leadstotheunregularizedsolutionwithλ=0.
Aswehaveseeninearlierchapters, thephenomenonofover-fittingisreallyan unfortunatepropertyofmaximumlikelihoodanddoesnotarisewhenwemarginalize overparametersina Bayesiansetting.
Inthischapter, weshallconsiderthe Bayesian viewofmodelcomplexityinsomedepth.
Beforedoingso, however, itisinstructive toconsiderafrequentistviewpointofthemodelcomplexityissue, knownasthebias- variancetrade-off.
Althoughweshallintroducethisconceptinthecontextoflinear basisfunctionmodels, whereitiseasytoillustratetheideasusingsimpleexamples, thediscussionhasmoregeneralapplicability.
In Section 1.5.5, when we discussed decision theory for regression problems, weconsideredvariouslossfunctionseachofwhichleadstoacorrespondingoptimal predictiononcewearegiventheconditionaldistributionp(t|x).
Apopularchoiceis 148 3.
LINEARMODELSFORREGRESSION thesquaredlossfunction, forwhichtheoptimalpredictionisgivenbytheconditional expectation, whichwedenotebyh(x)andwhichisgivenby h(x)=E[t|x]= tp(t|x)dt.
(3.36) At this point, it is worth distinguishing between the squared loss function arising from decision theory and the sum-of-squares error function that arose in the maxi- mum likelihood estimation of model parameters.
We might use more sophisticated techniques than least squares, for example regularization or a fully Bayesian ap- proach, todeterminetheconditionaldistributionp(t|x).
Thesecanallbecombined withthesquaredlossfunctionforthepurposeofmakingpredictions.
Weshowedin Section1.5.5thattheexpectedsquaredlosscanbewritteninthe form E[L]= {y(x)−h(x)}2 p(x)dx+ {h(x)−t}2p(x, t)dxdt.
(3.37) Recallthatthesecondterm, whichisindependentofy(x), arisesfromtheintrinsic noiseonthedataandrepresentstheminimumachievablevalueoftheexpectedloss.
The first term depends on our choice for the function y(x), and we will seek a so- lution for y(x) which makes this term a minimum.
Because it is nonnegative, the smallestthatwecanhopetomakethistermiszero.
Ifwehadanunlimitedsupplyof data(andunlimitedcomputationalresources), wecouldinprinciplefindtheregres- sion function h(x) to any desired degree of accuracy, and this would represent the optimalchoicefory(x).
However, inpracticewehaveadataset D containingonly a finite number N of data points, and consequently we do not know the regression functionh(x)exactly.
If we model the h(x) using a parametric function y(x, w) governed by a pa- rametervectorw, thenfroma Bayesianperspectivetheuncertaintyinourmodelis expressedthroughaposteriordistributionoverw.
Afrequentisttreatment, however, involves making a point estimate of w based on the data set D, and tries instead to interpret the uncertainty of this estimate through the following thought experi- ment.
Suppose we had a large number of data sets each of size N and each drawn independently from the distribution p(t, x).
For any given data set D, we can run ourlearningalgorithmandobtainapredictionfunctiony(x; D).
Differentdatasets fromtheensemblewillgivedifferentfunctionsandconsequentlydifferentvaluesof thesquaredloss.
Theperformanceofaparticularlearningalgorithmisthenassessed bytakingtheaverageoverthisensembleofdatasets.
Considertheintegrandofthefirsttermin(3.37), whichforaparticulardataset Dtakestheform {y(x; D)−h(x)}2.
(3.38) Becausethisquantitywillbedependentontheparticulardataset D, wetakeitsaver- ageovertheensembleofdatasets.
Ifweaddandsubtractthequantity E D[y(x; D)] 3.2.
The Bias-Variance Decomposition 149 insidethebraces, andthenexpand, weobtain {y(x; D)−E D[y(x; D)]+E D[y(x; D)]−h(x)}2 = {y(x; D)−E D[y(x; D)]}2+{E D[y(x; D)]−h(x)}2 +2{y(x; D)−E D[y(x; D)]}{E D[y(x; D)]−h(x)}.
(3.39) We now take the expectation of this expression with respect to D and note that the finaltermwillvanish, giving E D {y(x; D)−h(x)}2 = ( {E D[y(x; D )* )]−h(x)} + 2+ ( E D {y(x; D)− )* E D[y(x; D)]}2 + .
(3.40) (bias)2 variance We see that the expected squared difference between y(x; D) and the regression function h(x) can be expressed as the sum of two terms.
The first term, called the squaredbias, representstheextenttowhichtheaveragepredictionoveralldatasets differs from the desired regression function.
The second term, called the variance, measurestheextenttowhichthesolutionsforindividualdatasetsvaryaroundtheir average, andhencethismeasurestheextenttowhichthefunctiony(x; D)issensitive totheparticularchoiceofdataset.
Weshallprovidesomeintuitiontosupportthese definitionsshortlywhenweconsiderasimpleexample.
Sofar, wehaveconsideredasingleinputvaluex.
Ifwesubstitutethisexpansion backinto(3.37), weobtainthefollowingdecompositionoftheexpectedsquaredloss expectedloss=(bias)2+variance+noise (3.41) where (bias)2 = {E D[y(x; D)]−h(x)}2p(x)dx (3.42) variance = E D {y(x; D)−E D[y(x; D)]}2 p(x)dx (3.43) noise = {h(x)−t}2p(x, t)dxdt (3.44) andthebiasandvariancetermsnowrefertointegratedquantities.
Ourgoalistominimizetheexpectedloss, whichwehavedecomposedintothe sumofa(squared)bias, avariance, andaconstantnoiseterm.
Asweshallsee, there is a trade-off between bias and variance, with very flexible models having low bias and high variance, and relatively rigid models having high bias and low variance.
The model with the optimal predictive capability is the one that leads to the best balancebetweenbiasandvariance.
Thisisillustratedbyconsideringthesinusoidal Appendix A data set from Chapter 1.
Here we generate 100 data sets, each containing N = 25 data points, independently from the sinusoidal curve h(x) = sin(2πx).
The data sets are indexed by l = 1,..., L, where L = 100, and for each data set D(l) we 150 3.
LINEARMODELSFORREGRESSION 1 1 lnλ=2.6 t t 0 0 −1 −1 0 1 0 1 x x 1 1 lnλ=−0.31 t t 0 0 −1 −1 0 1 0 1 x x 1 1 lnλ=−2.4 t t 0 0 −1 −1 0 1 0 1 x x Figure3.5 Illustrationofthedependenceofbiasandvarianceonmodelcomplexity, governedbyaregulariza- tionparameterλ, usingthesinusoidaldatasetfrom Chapter1.
Thereare L=100datasets, eachhaving N =25 data points, and there are 24 Gaussian basis functions in the model so that the total number of parameters is M = 25includingthebiasparameter.
Theleftcolumnshowstheresultoffittingthemodeltothedatasetsfor variousvaluesoflnλ(forclarity, only20ofthe100fitsareshown).
Therightcolumnshowsthecorresponding averageofthe100fits(red)alongwiththesinusoidalfunctionfromwhichthedatasetsweregenerated(green).
3.2.
The Bias-Variance Decomposition 151 Figure3.6 Plot of squared bias and variance, 0.15 togetherwiththeirsum, correspond- ing to the results shown in Fig- (bias)2 ure 3.5.
Also shown is the average 0.12 variance testseterrorforatestdatasetsize (bias)2 + variance of1000points.
Theminimumvalue 0.09 test error of(bias)2+variance occursaround lnλ = −0.31, which is close to the 0.06 value that gives the minimum error onthetestdata.
0.03 0 −3 −2 −1 0 1 2 lnλ fit a model with 24 Gaussian basis functions by minimizing the regularized error function (3.27) to give a prediction function y(l)(x) as shown in Figure 3.5.
The toprowcorrespondstoalargevalueoftheregularizationcoefficientλthatgiveslow variance(becausetheredcurvesintheleftplotlooksimilar)buthighbias(because thetwocurvesintherightplotareverydifferent).
Converselyonthebottomrow, for whichλissmall, thereislargevariance(shownbythehighvariabilitybetweenthe redcurvesintheleftplot)butlowbias(shownbythegoodfitbetweentheaverage modelfitandtheoriginalsinusoidalfunction).
Notethattheresultofaveragingmany solutions for the complex model with M = 25 is a very good fit to the regression function, which suggests that averaging may be a beneficial procedure.
Indeed, a weighted averaging of multiple solutions lies at the heart of a Bayesian approach, althoughtheaveragingiswithrespecttotheposteriordistributionofparameters, not withrespecttomultipledatasets.
Wecanalsoexaminethebias-variancetrade-offquantitativelyforthisexample.
Theaveragepredictionisestimatedfrom L 1 y(x)= y(l)(x) (3.45) L l=1 andtheintegratedsquaredbiasandintegratedvariancearethengivenby N 1 (bias)2 = {y(xn)−h(xn)}2 (3.46) N n=1 N L variance = 1 1 y(l)(xn)−y(xn) 2 (3.47) N L n=1 l=1 where the integral over x weighted by the distribution p(x) is approximated by a finite sum over data points drawn from that distribution.
These quantities, along with their sum, are plotted as a function of lnλ in Figure 3.6.
We see that small values of λ allow the model to become finely tuned to the noise on each individual 152 3.
LINEARMODELSFORREGRESSION data set leading to large variance.
Conversely, a large value of λ pulls the weight parameterstowardszeroleadingtolargebias.
Although the bias-variance decomposition may provide some interesting in- sights into the model complexity issue from a frequentist perspective, it is of lim- ited practical value, because the bias-variance decomposition is based on averages with respect to ensembles of data sets, whereas in practice we have only the single observeddataset.
Ifwehadalargenumberofindependenttrainingsetsofagiven size, we would be better off combining them into a single large training set, which ofcoursewouldreducethelevelofover-fittingforagivenmodelcomplexity.
Given these limitations, we turn in the next section to a Bayesian treatment of linear basis function models, which not only provides powerful insights into the issuesofover-fittingbutwhichalsoleadstopracticaltechniquesforaddressingthe questionmodelcomplexity.
3.3.
Bayesian Linear Regression In our discussion of maximum likelihood for setting the parameters of a linear re- gressionmodel, wehaveseenthattheeffectivemodelcomplexity, governedbythe number of basis functions, needs to be controlled according to the size of the data set.
Addingaregularizationtermtotheloglikelihoodfunctionmeanstheeffective model complexity can then be controlled by the value of the regularization coeffi- cient, althoughthechoiceofthenumberandformofthebasisfunctionsisofcourse stillimportantindeterminingtheoverallbehaviourofthemodel.
Thisleavestheissueofdecidingtheappropriatemodelcomplexityforthepar- ticularproblem, whichcannotbedecidedsimplybymaximizingthelikelihoodfunc- tion, because this always leads to excessively complex models and over-fitting.
In- dependent hold-out data can be used to determine model complexity, as discussed in Section1.3, butthiscanbebothcomputationallyexpensiveandwastefulofvalu- abledata.
Wethereforeturntoa Bayesiantreatmentoflinearregression, whichwill avoid the over-fitting problem of maximum likelihood, and which will also lead to automatic methods of determining model complexity using the training data alone.
Again, for simplicity we will focus on the case of a single target variable t.
Ex- tension to multiple target variables is straightforward and follows the discussion of Section3.1.5.
3.3.1 Parameter distribution We begin our discussion of the Bayesian treatment of linear regression by in- troducingapriorprobabilitydistributionoverthemodelparametersw.
Forthemo- ment, weshalltreatthenoiseprecisionparameterβ asaknownconstant.
Firstnote thatthelikelihoodfunctionp(t|w)definedby(3.10)istheexponentialofaquadratic function of w.
The corresponding conjugate prior is therefore given by a Gaussian distributionoftheform p(w)=N(w|m 0 , S 0 ) (3.48) havingmeanm 0 andcovariance S 0.
3.3.
Bayesian Linear Regression 153 Nextwecomputetheposteriordistribution, whichisproportionaltotheproduct of the likelihood function and the prior.
Due to the choice of a conjugate Gaus- sian prior distribution, the posterior will also be Gaussian.
We can evaluate this distributionbytheusualprocedureofcompletingthesquareintheexponential, and thenfindingthenormalizationcoefficientusingthestandardresultforanormalized Exercise 3.7 Gaussian.
However, we have already done the necessary work in deriving the gen- eralresult(2.116), whichallowsustowritedowntheposteriordistributiondirectly intheform p(w|t)=N(w|m N, SN) (3.49) where m N = SN S − 0 1m 0 +βΦT t (3.50) S −1 = S −1+βΦTΦ.
(3.51) N 0 Notethatbecausetheposteriordistributionis Gaussian, itsmodecoincideswithits mean.
Thusthemaximumposteriorweightvectorissimplygivenbyw MAP =m N.
If we consider an infinitely broad prior S 0 = α−1I with α → 0, the mean m N of the posterior distribution reduces to the maximum likelihood value w ML given by (3.15).
Similarly, if N = 0, then the posterior distribution reverts to the prior.
Furthermore, ifdatapointsarrivesequentially, thentheposteriordistributionatany stage acts as the prior distribution for the subsequent data point, such that the new Exercise 3.8 posteriordistributionisagaingivenby(3.49).
For the remainder of this chapter, we shall consider a particular form of Gaus- sian prior in order to simplify the treatment.
Specifically, we consider a zero-mean isotropic Gaussiangovernedbyasingleprecisionparameterαsothat p(w|α)=N(w|0,α −1I) (3.52) andthecorrespondingposteriordistributionoverwisthengivenby(3.49)with m N = βSNΦT t (3.53) S −1 = αI+βΦTΦ.
(3.54) N Thelogoftheposteriordistributionisgivenbythesumoftheloglikelihoodand thelogofthepriorand, asafunctionofw, takestheform N β α lnp(w|t)=− {tn −w Tφ(xn)}2− w Tw+const.
(3.55) 2 2 n=1 Maximization of this posterior distribution with respect to w is therefore equiva- lenttotheminimizationofthesum-of-squareserrorfunctionwiththeadditionofa quadraticregularizationterm, correspondingto(3.27)withλ=α/β.
Wecanillustrate Bayesianlearninginalinear basisfunctionmodel, aswellas the sequential update of a posterior distribution, using a simple example involving straight-linefitting.
Considerasingleinputvariablex, asingletargetvariabletand 154 3.
LINEARMODELSFORREGRESSION a linear model of the form y(x, w) = w 0 +w 1 x.
Because this has just two adap- tiveparameters, wecanplotthepriorandposteriordistributionsdirectlyinparameter space.
Wegeneratesyntheticdatafromthefunctionf(x, a)=a 0 +a 1 xwithparam- etervaluesa 0 =−0.3anda 1 =0.5byfirstchoosingvaluesofxn fromtheuniform distribution U(x|−1,1), thenevaluatingf(xn, a), andfinallyadding Gaussiannoise with standard deviation of0.2 to obtain the target valuestn.
Our goal is to recover the values of a 0 and a 1 from such data, and we will explore the dependence on the sizeofthedataset.
Weassumeherethatthenoisevarianceisknownandhencewe set the precision parameter to its true value β = (1/0.2)2 = 25.
Similarly, we fix the parameter α to 2.0.
We shall shortly discuss strategies for determining α and β from the training data.
Figure 3.7 shows the results of Bayesian learning in this modelasthesizeofthedatasetisincreasedanddemonstratesthesequentialnature of Bayesianlearninginwhichthecurrentposteriordistributionformsthepriorwhen anewdatapointisobserved.
Itisworthtakingtimetostudythisfigureindetailas it illustrates several important aspects of Bayesian inference.
The first row of this figurecorrespondstothesituationbeforeanydatapointsareobservedandshowsa plot of the prior distribution in w space together with six samples of the function y(x, w) in which the values of w are drawn from the prior.
In the second row, we see the situation after observing a single data point.
The location (x, t) of the data pointisshownbyabluecircleintheright-handcolumn.
Intheleft-handcolumnisa plotofthelikelihoodfunctionp(t|x, w)forthisdatapointasafunctionofw.
Note thatthelikelihoodfunctionprovidesasoftconstraintthatthelinemustpasscloseto thedatapoint, wherecloseisdeterminedbythenoiseprecisionβ.
Forcomparison, the true parameter values a 0 = −0.3 and a 1 = 0.5 used to generate the data set are shown by a white cross in the plots in the left column of Figure 3.7.
When we multiply this likelihood function by the prior from the top row, and normalize, we obtain the posterior distribution shown in the middle plot on the second row.
Sam- plesoftheregressionfunctiony(x, w)obtainedbydrawingsamplesofwfromthis posteriordistributionareshownintheright-handplot.
Notethatthesesamplelines all pass close to the data point.
The third row of this figure shows the effect of ob- servingaseconddatapoint, againshownbyabluecircleintheplotintheright-hand column.
The corresponding likelihood function for this second data point alone is shown in the left plot.
When we multiply this likelihood function by the posterior distribution from the second row, we obtain the posterior distribution shown in the middleplotofthethirdrow.
Notethatthisisexactlythesameposteriordistribution as would be obtained by combining the original prior with the likelihood function forthetwodatapoints.
Thisposteriorhasnowbeeninfluencedbytwodatapoints, and because two points are sufficient to define a line this already gives a relatively compactposteriordistribution.
Samplesfromthisposteriordistributiongiveriseto thefunctionsshowninredinthethirdcolumn, andweseethatthesefunctionspass closetobothofthedatapoints.
Thefourthrowshowstheeffectofobservingatotal of20datapoints.
Theleft-handplotshowsthelikelihoodfunctionforthe20th data point alone, and the middle plot shows the resulting posterior distribution that has nowabsorbedinformationfromall20observations.
Notehowtheposteriorismuch sharper than in the third row.
In the limit of an infinite number of data points, the 3.3.
Bayesian Linear Regression 155 Figure3.7 Illustrationofsequential Bayesianlearningforasimplelinearmodeloftheformy(x, w)= w 0 +w 1 x.
Adetaileddescriptionofthisfigureisgiveninthetext.
156 3.
LINEARMODELSFORREGRESSION posterior distribution would become a delta function centred on the true parameter values, shownbythewhitecross.
Other forms of prior over the parameters can be considered.
For instance, we cangeneralizethe Gaussianpriortogive M M q α 1/q 1 α p(w|α)= exp − |wj |q (3.56) 2 2 Γ(1/q) 2 j=1 inwhichq = 2correspondstothe Gaussiandistribution, andonlyinthiscaseisthe priorconjugatetothelikelihoodfunction(3.10).
Findingthemaximumoftheposte- riordistributionoverwcorrespondstominimizationoftheregularizederrorfunction (3.29).
Inthecaseofthe Gaussianprior, themodeoftheposteriordistributionwas equaltothemean, althoughthiswillnolongerholdifq =2.
3.3.2 Predictive distribution In practice, we are not usually interested in the value of w itself but rather in making predictions of t for new values of x.
This requires that we evaluate the predictivedistributiondefinedby p(t|t,α,β)= p(t|w,β)p(w|t,α,β)dw (3.57) inwhichtisthevectoroftargetvaluesfromthetrainingset, andwehaveomittedthe correspondinginputvectorsfromtheright-handsideoftheconditioningstatements tosimplifythenotation.
Theconditionaldistributionp(t|x, w,β)ofthetargetvari- able is given by (3.8), and the posterior weight distribution is given by (3.49).
We seethat(3.57)involvestheconvolutionoftwo Gaussiandistributions, andsomaking use of the result (2.115) from Section 8.1.4, we see that the predictive distribution Exercise 3.10 takestheform p(t|x, t,α,β)=N(t|m Tφ(x),σ2 (x)) (3.58) N N wherethevarianceσ2 (x)ofthepredictivedistributionisgivenby N 1 σ N 2 (x)= +φ(x)TSN φ(x).
(3.59) β The first term in (3.59) represents the noise on the data whereas the second term reflectstheuncertaintyassociatedwiththeparametersw.
Becausethenoiseprocess and the distribution of w are independent Gaussians, their variances are additive.
Notethat, asadditionaldatapointsareobserved, theposteriordistributionbecomes narrower.
As a consequence it can be shown (Qazaz et al., 1997) that σ2 (x) N+1 Exercise 3.11 σ2 (x).
Inthelimit N →∞, thesecondtermin(3.59)goestozero, andthevariance N of the predictive distribution arises solely from the additive noise governed by the parameterβ.
As an illustration of the predictive distribution for Bayesian linear regression models, letusreturntothesyntheticsinusoidaldatasetof Section1.1.
In Figure3.8, 3.3.
Bayesian Linear Regression 157 1 1 t t 0 0 −1 −1 0 1 0 1 x x 1 1 t t 0 0 −1 −1 0 1 0 1 x x Figure3.8 Examplesofthepredictivedistribution(3.58)foramodelconsistingof9Gaussianbasisfunctions oftheform(3.4)usingthesyntheticsinusoidaldatasetof Section1.1.
Seethetextforadetaileddiscussion.
wefitamodel comprising a linear combination of Gaussian basisfunctions todata setsofvarioussizesandthenlookatthecorrespondingposteriordistributions.
Here the green curves correspond to the function sin(2πx) from which the data points were generated (with the addition of Gaussian noise).
Data sets of size N = 1, N = 2, N = 4, and N = 25 are shown in the four plots by the blue circles.
For each plot, the red curve shows the mean of the corresponding Gaussian predictive distribution, and the red shaded region spans one standard deviation either side of the mean.
Note that the predictive uncertainty depends on x and is smallest in the neighbourhood of the data points.
Also note that the level of uncertainty decreases asmoredatapointsareobserved.
Theplotsin Figure3.8onlyshowthepoint-wisepredictivevarianceasafunc- tion of x.
In order to gain insight into the covariance between the predictions at different values of x, we can draw samples from the posterior distribution over w, andthenplotthecorrespondingfunctionsy(x, w), asshownin Figure3.9.
158 3.
LINEARMODELSFORREGRESSION 1 1 t t 0 0 −1 −1 0 1 0 1 x x 1 1 t t 0 0 −1 −1 0 1 0 1 x x Figure3.9 Plotsofthefunctiony(x, w)usingsamplesfromtheposteriordistributionsoverwcorrespondingto theplotsin Figure3.8.
If we used localized basis functions such as Gaussians, then in regions away fromthebasisfunctioncentres, thecontributionfromthesecondterminthepredic- tive variance (3.59) will go to zero, leaving only the noise contributionβ−1.
Thus, themodelbecomesveryconfident initspredictions whenextrapolating outsidethe regionoccupiedbythebasisfunctions, whichisgenerallyanundesirablebehaviour.
This problem can be avoided by adopting an alternative Bayesian approach to re- Section6.4 gressionknownasa Gaussianprocess.
Note that, if both w and β are treated as unknown, then we can introduce a conjugate prior distribution p(w,β) that, from the discussion in Section 2.3.6, will Exercise 3.12 begivenbya Gaussian-gammadistribution(Denisonetal.,2002).
Inthiscase, the Exercise 3.13 predictivedistributionisa Student’st-distribution.
3.3.
Bayesian Linear Regression 159 Figure3.10 The equivalent ker- nel k(x, x ) for the Gaussian basis functions in Figure 3.1, shown as a plot of x versus x , together with threeslicesthroughthismatrixcor- respondingtothreedifferentvalues ofx.
Thedatasetusedtogenerate thiskernelcomprised200valuesof x equally spaced over the interval (−1,1).
3.3.3 Equivalent kernel Theposteriormeansolution(3.53)forthelinearbasisfunctionmodelhasanin- terestinginterpretationthatwillsetthestageforkernelmethods, including Gaussian Chapter6 processes.
Ifwesubstitute(3.53)intotheexpression(3.3), weseethatthepredictive meancanbewrittenintheform N y(x, m N)=m T N φ(x)=βφ(x)TSNΦT t= βφ(x)TSN φ(xn)tn (3.60) n=1 where SN isdefinedby(3.51).
Thusthemeanofthepredictivedistributionatapoint xisgivenbyalinearcombinationofthetrainingsettargetvariablestn, sothatwe canwrite N y(x, m N)= k(x, xn)tn (3.61) n=1 wherethefunction k(x, x )=βφ(x)TSN φ(x ) (3.62) isknownasthesmoothermatrixortheequivalentkernel.
Regressionfunctions, such as this, which make predictions by taking linear combinations of the training set targetvaluesareknownaslinearsmoothers.
Notethattheequivalentkerneldepends on the input values xn from the data set because these appear in the definition of SN.
The equivalent kernel is illustrated for the case of Gaussian basis functions in Figure3.10inwhichthekernelfunctionsk(x, x )havebeenplottedasafunctionof x forthreedifferentvaluesofx.
Weseethattheyarelocalizedaroundx, andsothe meanofthepredictivedistributionatx, givenbyy(x, m N), isobtainedbyforming aweightedcombinationofthetargetvaluesinwhichdatapointsclosetoxaregiven higher weight than points further removed from x.
Intuitively, it seems reasonable thatweshouldweightlocalevidencemorestronglythandistantevidence.
Notethat this localization property holds not only for the localized Gaussian basis functions butalsoforthenonlocalpolynomialandsigmoidalbasisfunctions, asillustratedin Figure3.11.
160 3.
LINEARMODELSFORREGRESSION Figure3.11 Examples of equiva- lent kernels k(x, x ) for x = 0 plotted as a function of x , corre- 0.04 0.04 sponding(left)tothepolynomialba- sis functions and (right) to the sig- 0.02 0.02 moidalbasisfunctionsshownin Fig- ure 3.1.
Note that these are local- 0 0 izedfunctionsofx eventhoughthe corresponding basis functions are nonlocal.
−1 0 1 −1 0 1 Furtherinsightintotheroleoftheequivalentkernelcanbeobtainedbyconsid- eringthecovariancebetweeny(x)andy(x ), whichisgivenby cov[y(x), y(x )] = cov[φ(x)Tw, w Tφ(x )] = φ(x)TSN φ(x )=β −1k(x, x ) (3.63) where we have made use of (3.49) and (3.62).
From the form of the equivalent kernel, we see that the predictive mean at nearby points will be highly correlated, whereasformoredistantpairsofpointsthecorrelationwillbesmaller.
Thepredictivedistributionshownin Figure3.8allowsustovisualizethepoint- wiseuncertaintyinthepredictions, governedby(3.59).
However, bydrawingsam- ples from the posterior distribution over w, and plotting the corresponding model functions y(x, w) as in Figure 3.9, we are visualizing the joint uncertainty in the posteriordistributionbetweentheyvaluesattwo(ormore)xvalues, asgovernedby theequivalentkernel.
The formulation of linear regression in terms of a kernel function suggests an alternative approach to regression as follows.
Instead of introducing a set of basis functions, which implicitly determines an equivalent kernel, we can instead define alocalizedkerneldirectlyandusethistomakepredictionsfornewinputvectorsx, given the observed training set.
This leads to a practical framework for regression (and classification) called Gaussian processes, which will be discussed in detail in Section6.4.
Wehaveseenthattheeffectivekerneldefinestheweightsbywhichthetraining settargetvaluesarecombinedinordertomakeapredictionatanewvalueofx, and itcanbeshownthattheseweightssumtoone, inotherwords N k(x, xn)=1 (3.64) n=1 Exercise 3.14 for all values of x.
This intuitively pleasing result can easily be proven informally bynotingthatthesummationisequivalenttoconsideringthepredictivemean y(x) for a set of target data in which tn = 1 for all n.
Provided the basis functions are linearly independent, that there are more data points than basis functions, and that oneofthebasisfunctionsisconstant(correspondingtothebiasparameter), thenitis clearthatwecanfitthetrainingdataexactlyandhencethatthepredictivemeanwill 3.4.
Bayesian Model Comparison 161 besimply y(x)=1, fromwhichweobtain(3.64).
Notethatthekernelfunctioncan be negative as well as positive, so although it satisfies a summation constraint, the corresponding predictions are not necessarily convex combinations of the training settargetvariables.
Finally, wenotethattheequivalentkernel(3.62)satisfiesanimportantproperty Chapter6 sharedbykernelfunctionsingeneral, namelythatitcanbeexpressedintheforman innerproductwithrespecttoavectorψ(x)ofnonlinearfunctions, sothat k(x, z)=ψ(x)Tψ(z) (3.65) whereψ(x)=β1/2S 1/2φ(x).
N 3.4.
Bayesian Model Comparison In Chapter1, wehighlightedtheproblemofover-fittingaswellastheuseofcross- validation as a technique for setting the values of regularization parameters or for choosing between alternative models.
Here we consider the problem of model se- lection from a Bayesian perspective.
In this section, our discussion will be very general, and then in Section 3.5 we shall see how these ideas can be applied to the determinationofregularizationparametersinlinearregression.
As we shall see, the over-fitting associated with maximum likelihood can be avoided by marginalizing (summing or integrating) over the model parameters in- stead of making point estimates of their values.
Models can then be compared di- rectly on the training data, without the need for a validation set.
This allows all available data to be used for training and avoids the multiple training runs for each modelassociatedwithcross-validation.
Italsoallowsmultiplecomplexityparame- ters to be determined simultaneously as part of the training process.
For example, in Chapter 7 we shall introduce the relevance vector machine, which is a Bayesian modelhavingonecomplexityparameterforeverytrainingdatapoint.
The Bayesianviewofmodelcomparisonsimplyinvolvestheuseofprobabilities to represent uncertainty in the choice of model, along with a consistent application ofthesumandproductrulesofprobability.
Supposewewishtocompareasetof L models{M i }wherei = 1,..., L.
Hereamodelreferstoaprobabilitydistribution over the observed data D.
In the case of the polynomial curve-fitting problem, the distributionisdefinedoverthesetoftargetvaluest, whilethesetofinputvalues X is assumed to be known.
Other types of model define a joint distributions over X Section1.5.4 andt.
Weshallsupposethatthedataisgeneratedfromoneofthesemodelsbutwe are uncertain which one.
Our uncertainty is expressed through a prior probability distribution p(M i).
Given a training set D, we then wish to evaluate the posterior distribution p(M i |D)∝p(M i)p(D|M i).
(3.66) The prior allows us to express a preference for different models.
Let us simply assume that all models are given equal prior probability.
The interesting term is themodelevidencep(D|M i)whichexpressesthepreferenceshownbythedatafor 162 3.
LINEARMODELSFORREGRESSION differentmodels, andweshallexaminethisterminmoredetailshortly.
Themodel evidenceissometimesalsocalledthemarginallikelihood becauseitcanbeviewed asalikelihoodfunctionoverthespaceofmodels, inwhichtheparametershavebeen marginalizedout.
Theratioofmodelevidencesp(D|M i)/p(D|M j)fortwomodels isknownasa Bayesfactor(Kassand Raftery,1995).
Onceweknowtheposteriordistributionovermodels, thepredictivedistribution isgiven, fromthesumandproductrules, by L p(t|x, D)= p(t|x, M i, D)p(M i |D).
(3.67) i=1 Thisisanexampleofamixturedistributioninwhichtheoverallpredictivedistribu- tionisobtainedbyaveragingthepredictivedistributionsp(t|x, M i, D)ofindividual models, weighted by the posterior probabilities p(M i |D) of those models.
For in- stance, if we have two models that are a-posteriori equally likely and one predicts a narrow distribution around t = a while the other predicts a narrow distribution aroundt = b, theoverallpredictivedistribution willbeabimodaldistribution with modesatt=aandt=b, notasinglemodelatt=(a+b)/2.
A simple approximation to model averaging is to use the single most probable modelalonetomakepredictions.
Thisisknownasmodelselection.
For a model governed by a set of parameters w, the model evidence is given, fromthesumandproductrulesofprobability, by p(D|M i)= p(D|w, M i)p(w|M i)dw.
(3.68) Chapter11 From a sampling perspective, the marginal likelihood can be viewed as the proba- bility of generating the data set D from a model whose parameters are sampled at randomfromtheprior.
Itisalsointerestingtonotethattheevidenceispreciselythe normalizingtermthatappearsinthedenominatorin Bayes’theoremwhenevaluating theposteriordistributionoverparametersbecause p(w|D, M i)= p(D|w p , ( M D| i M )p( i w ) |M i) .
(3.69) Wecanobtainsomeinsightintothemodelevidencebymakingasimpleapprox- imationtotheintegraloverparameters.
Considerfirstthecaseofamodelhavinga single parameter w.
The posterior distribution over parameters is proportional to p(D|w)p(w), whereweomitthedependenceonthemodel M i tokeepthenotation uncluttered.
Ifweassumethattheposteriordistributionissharplypeakedaroundthe mostprobablevaluew MAP, withwidth∆w posterior, thenwecanapproximatethein- tegralbythevalueoftheintegrandatitsmaximumtimesthewidthofthepeak.
Ifwe further assume that the prior is flat with width ∆w prior so that p(w) = 1/∆w prior, thenwehave ∆w p(D)= p(D|w)p(w)dw p(D|w MAP ) posterior (3.70) ∆w prior 3.4.
Bayesian Model Comparison 163 Figure3.12 Wecanobtainaroughapproximationto ∆wposterior the model evidence if we assume that the posterior distribution over parame- ters is sharply peaked around its mode w MAP.
w MAP w ∆wprior andsotakinglogsweobtain ∆w lnp(D) lnp(D|w MAP )+ln posterior .
(3.71) ∆w prior This approximation is illustrated in Figure 3.12.
The first term represents the fit to thedatagivenbythemostprobableparametervalues, andforaflatpriorthiswould correspondtotheloglikelihood.
Thesecondtermpenalizesthemodelaccordingto itscomplexity.
Because∆w posterior <∆w priorthistermisnegative, anditincreases inmagnitudeastheratio∆w posterior /∆w prior getssmaller.
Thus, ifparametersare finelytunedtothedataintheposteriordistribution, thenthepenaltytermislarge.
Foramodelhavingasetof M parameters, wecanmakeasimilarapproximation for each parameter in turn.
Assuming that all parameters have the same ratio of ∆w posterior /∆w prior, weobtain ∆w lnp(D) lnp(D|w MAP )+Mln posterior .
(3.72) ∆w prior Thus, inthisverysimpleapproximation, thesizeofthecomplexitypenaltyincreases linearly with the number M of adaptive parameters in the model.
As we increase the complexity of the model, the first term will typically decrease, because a more complex model is better able to fit the data, whereas the second term will increase due to the dependence on M.
The optimal model complexity, as determined by the maximum evidence, will be given by a trade-off between these two competing terms.
Weshalllaterdevelopamorerefinedversionofthisapproximation, basedon Section4.4.1 a Gaussianapproximationtotheposteriordistribution.
We can gain further insight into Bayesian model comparison and understand howthemarginallikelihoodcanfavourmodelsofintermediatecomplexitybycon- sidering Figure 3.13.
Here the horizontal axis is a one-dimensional representation of the space of possible data sets, so that each point on this axis corresponds to a specificdataset.
Wenowconsiderthreemodels M 1, M 2 and M 3 ofsuccessively increasingcomplexity.
Imaginerunningthesemodelsgenerativelytoproduceexam- pledatasets, andthenlookingatthedistributionofdatasetsthatresult.
Anygiven 164 3.
LINEARMODELSFORREGRESSION Figure3.13 Schematic illustration of the distribution of data sets for p(D) M three models of different com- 1 plexity, in which M 1 is the simplest and M 3 is the most M complex.
Note that the dis- 2 tributions are normalized.
In this example, for the partic- M ular observed data set D 0, 3 the model M 2 with intermedi- ate complexity has the largest evidence.
D D 0 modelcangenerateavarietyofdifferentdatasetssincetheparametersaregoverned by a prior probability distribution, and for any choice of the parameters there may berandomnoiseonthetargetvariables.
Togenerateaparticulardatasetfromaspe- cificmodel, wefirstchoosethevaluesoftheparametersfromtheirpriordistribution p(w), andthenfortheseparametervalueswesamplethedatafromp(D|w).
Asim- ple model (for example, based on afirst order polynomial) has little variability and so will generate data sets that are fairly similar to each other.
Its distribution p(D) isthereforeconfinedtoarelativelysmallregionofthehorizontalaxis.
Bycontrast, acomplexmodel(suchasaninthorderpolynomial)cangenerateagreatvarietyof different data sets, and so its distribution p(D) is spread over a large region of the space of data sets.
Because the distributions p(D|M i) are normalized, we see that the particular data set D 0 can have the highest value of the evidence for the model of intermediate complexity.
Essentially, the simpler model cannot fit the data well, whereasthemorecomplexmodelspreadsitspredictiveprobabilityovertoobroada rangeofdatasetsandsoassignsrelativelysmallprobabilitytoanyoneofthem.
Implicit in the Bayesian model comparison framework is the assumption that thetruedistributionfromwhichthedataaregeneratediscontainedwithinthesetof models under consideration.
Provided this is so, we can show that Bayesian model comparison will on average favour the correct model.
To see this, consider two models M 1 and M 2 inwhichthetruthcorresponds to M 1.
Foragivenfinitedata set, itispossibleforthe Bayesfactortobelargerfortheincorrectmodel.
However, if weaveragethe Bayesfactoroverthedistributionofdatasets, weobtaintheexpected Bayesfactorintheform p(D|M ) p(D|M 1 )ln p(D|M 1 ) d D (3.73) 2 where the average has been taken with respect to the true distribution of the data.
Section1.6.1 Thisquantityisanexampleofthe Kullback-Leiblerdivergenceandsatisfiestheprop- ertyofalwaysbeingpositiveunlessthetwodistributionsareequalinwhichcaseit iszero.
Thusonaveragethe Bayesfactorwillalwaysfavourthecorrectmodel.
We have seen that the Bayesian framework avoids the problem of over-fitting andallowsmodelstobecomparedonthebasisofthetrainingdataalone.
However, 3.5.
The Evidence Approximation 165 a Bayesian approach, like any approach to pattern recognition, needs to make as- sumptionsabouttheformofthemodel, andiftheseareinvalidthentheresultscan be misleading.
In particular, we see from Figure 3.12 that the model evidence can besensitivetomanyaspectsoftheprior, suchasthebehaviourinthetails.
Indeed, the evidence is not defined if the prior is improper, as can be seen by noting that an improper prior has an arbitrary scaling factor (in other words, the normalization coefficientisnotdefinedbecausethedistributioncannotbenormalized).
Ifwecon- sideraproperpriorandthentakeasuitablelimitinordertoobtainanimproperprior (for example, a Gaussian prior in which we take the limit of infinite variance) then the evidence will go to zero, as can be seen from (3.70) and Figure 3.12.
It may, however, be possible to consider the evidence ratio between two models first and thentakealimittoobtainameaningfulanswer.
Inapracticalapplication, therefore, itwillbewisetokeepasideanindependent testsetofdataonwhichtoevaluatetheoverallperformanceofthefinalsystem.
3.5.
The Evidence Approximation In a fully Bayesian treatment of the linear basis function model, we would intro- ducepriordistributionsoverthehyperparametersαandβ andmakepredictionsby marginalizing with respect to these hyperparameters as well as with respect to the parameters w.
However, although we can integrate analytically over either w or over the hyperparameters, the complete marginalization over all of these variables is analytically intractable.
Here we discuss an approximation in which we set the hyperparameters to specific values determined by maximizing the marginal likeli- hood function obtained by first integrating over the parameters w.
This framework is known in the statistics literature as empirical Bayes (Bernardo and Smith, 1994; Gelman et al., 2004), or type 2 maximum likelihood (Berger, 1985), or generalized maximum likelihood (Wahba, 1975), and in the machine learning literature is also calledtheevidenceapproximation(Gull,1989; Mac Kay,1992a).
Ifweintroducehyperpriorsoverαandβ, thepredictivedistributionisobtained bymarginalizingoverw,αandβ sothat p(t|t)= p(t|w,β)p(w|t,α,β)p(α,β|t)dwdαdβ (3.74) wherep(t|w,β)isgivenby(3.8)andp(w|t,α,β)isgivenby(3.49)withm N and SN definedby(3.53)and(3.54)respectively.
Herewehaveomittedthedependence ontheinputvariablextokeepthenotationuncluttered.
Iftheposteriordistribution p(α,β|t)issharplypeakedaroundvaluesα andβ , thenthepredictivedistributionis obtainedsimplybymarginalizingoverwinwhichαandβ arefixedtothevaluesα andβ, sothat p(t|t) p(t|t,α ,β )= p(t|w,β )p(w|t,α ,β )dw.
(3.75) 166 3.
LINEARMODELSFORREGRESSION From Bayes’theorem, theposteriordistributionforαandβ isgivenby p(α,β|t)∝p(t|α,β)p(α,β).
(3.76) If the prior is relatively flat, then in the evidence framework the values of α and β are obtained by maximizing the marginal likelihood functionp(t|α,β).
We shall proceedbyevaluatingthemarginallikelihoodforthelinearbasisfunctionmodeland then finding its maxima.
This will allow us to determine values for these hyperpa- rameters from the training data alone, without recourse to cross-validation.
Recall thattheratioα/β isanalogoustoaregularizationparameter.
Asanasideitisworthnotingthat, ifwedefineconjugate(Gamma)priordistri- butionsoverαandβ, thenthemarginalizationoverthesehyperparametersin(3.74) can be performed analytically to give a Student’s t-distribution over w (see Sec- tion2.3.7).
Althoughtheresultingintegraloverwisnolongeranalyticallytractable, it might be thought that approximating this integral, for example using the Laplace approximation discussed(Section4.4)whichisbasedonalocal Gaussianapproxi- mation centred on the mode of the posterior distribution, might provide a practical alternative to the evidence framework (Buntine and Weigend, 1991).
However, the integrandasafunctionofwtypicallyhasastronglyskewedmodesothatthe Laplace approximationfailstocapturethebulkoftheprobabilitymass, leadingtopoorerre- sultsthanthoseobtainedbymaximizingtheevidence(Mac Kay,1999).
Returningtotheevidenceframework, wenotethattherearetwoapproachesthat wecantaketothemaximizationofthelogevidence.
Wecanevaluatetheevidence functionanalyticallyandthensetitsderivativeequaltozerotoobtainre-estimation equations for α and β, which we shall do in Section 3.5.2.
Alternatively we use a technique called the expectation maximization (EM) algorithm, which will be dis- cussedin Section9.3.4whereweshallalsoshowthatthesetwoapproachesconverge tothesamesolution.
3.5.1 Evaluation of the evidence function The marginal likelihood function p(t|α,β) is obtained by integrating over the weightparametersw, sothat p(t|α,β)= p(t|w,β)p(w|α)dw.
(3.77) One way to evaluate this integral is to make use once again of the result (2.115) Exercise 3.16 for the conditional distribution in a linear-Gaussian model.
Here we shall evaluate theintegralinsteadbycompletingthesquareintheexponentandmakinguseofthe standardformforthenormalizationcoefficientofa Gaussian.
Exercise 3.17 From(3.11),(3.12), and(3.52), wecanwritetheevidencefunctionintheform β N/2 α M/2 p(t|α,β)= exp{−E(w)} dw (3.78) 2π 2π 3.5.
The Evidence Approximation 167 where M isthedimensionalityofw, andwehavedefined E(w) = βED(w)+αEW(w) β α = t−Φw 2 + w Tw.
(3.79) 2 2 We recognize (3.79) as being equal, up to a constant of proportionality, to the reg- Exercise 3.18 ularizedsum-of-squareserrorfunction(3.27).
Wenowcompletethesquareoverw giving 1 E(w)=E(m N)+ (w−m N)TA(w−m N) (3.80) 2 wherewehaveintroduced A=αI+βΦTΦ (3.81) togetherwith β α E(m N)= t−Φm N 2 + m T N m N.
(3.82) 2 2 Notethat Acorrespondstothematrixofsecondderivativesoftheerrorfunction A=∇∇E(w) (3.83) andisknownasthe Hessianmatrix.
Herewehavealsodefinedm N givenby m N =βA −1ΦT t.
(3.84) Using (3.54), we see that A = S −1 , and hence (3.84) is equivalent to the previous N definition(3.53), andthereforerepresentsthemeanoftheposteriordistribution.
The integral over w can now be evaluated simply by appealing to the standard Exercise 3.19 resultforthenormalizationcoefficientofamultivariate Gaussian, giving exp{−E(w)} dw 1 = exp{−E(m N)} exp − (w−m N)TA(w−m N) dw 2 = exp{−E(m N)}(2π) M/2|A|−1/2.
(3.85) Using(3.78)wecanthenwritethelogofthemarginallikelihoodintheform M N 1 N lnp(t|α,β)= lnα+ lnβ−E(m N)− ln|A|− ln(2π) (3.86) 2 2 2 2 whichistherequiredexpressionfortheevidencefunction.
Returningtothepolynomialregressionproblem, wecanplotthemodelevidence againsttheorderofthepolynomial, asshownin Figure3.14.
Herewehaveassumed a prior of the form (1.65) with the parameter α fixed at α = 5×10−3.
The form ofthisplotisveryinstructive.
Referringbackto Figure1.4, weseethatthe M = 0 polynomialhasverypoorfittothedataandconsequentlygivesarelativelylowvalue 168 3.
LINEARMODELSFORREGRESSION Figure3.14 Plot of the model evidence versus theorder M, forthepolynomialre- −18 gression model, showing that the evidence favours the model with M =3.
−20 −22 −24 −26 0 2 4 6 8 M fortheevidence.
Goingtothe M = 1polynomialgreatlyimprovesthedatafit, and hence the evidence is significantly higher.
However, in going to M = 2, the data fit is improved only very marginally, due to the fact that the underlying sinusoidal functionfromwhichthedataisgeneratedisanoddfunctionandsohasnoeventerms in a polynomial expansion.
Indeed, Figure 1.5 shows that the residual data error is reduced only slightly in going from M = 1 to M = 2.
Because this richer model suffersagreatercomplexitypenalty, theevidenceactuallyfallsingoingfrom M =1 to M = 2.
When we go to M = 3 we obtain a significant further improvement in data fit, as seen in Figure 1.4, and so the evidence is increased again, giving the highest overall evidence for any of the polynomials.
Further increases in the value of M produce only small improvements in the fit to the data but suffer increasing complexity penalty, leading overall to a decrease in the evidence values.
Looking againat Figure1.5, weseethatthegeneralizationerrorisroughlyconstantbetween M = 3 and M = 8, and it would be difficult to choose between these models on the basis of this plot alone.
The evidence values, however, show a clear preference for M = 3, sincethisisthesimplestmodelwhichgivesagoodexplanationforthe observeddata.
3.5.2 Maximizing the evidence function Let us first consider the maximization of p(t|α,β) with respect to α.
This can bedonebyfirstdefiningthefollowingeigenvectorequation βΦTΦ ui =λiui.
(3.87) From(3.81), itthenfollowsthat Ahaseigenvaluesα+λi.
Nowconsiderthederiva- tiveoftheterminvolvingln|A|in(3.86)withrespecttoα.
Wehave d d d 1 ln|A|= ln (λi+α)= ln(λi+α)= .
(3.88) dα dα dα λi+α i i i Thusthestationarypointsof(3.86)withrespecttoαsatisfy M 1 1 1 0= − m T N m N − .
(3.89) 2α 2 2 λi+α i 3.5.
The Evidence Approximation 169 Multiplyingthroughby2αandrearranging, weobtain 1 αm T N m N =M −α =γ.
(3.90) λi+α i Sincethereare M termsinthesumoveri, thequantityγ canbewritten λi γ = .
(3.91) α+λi i The interpretation of the quantity γ will be discussed shortly.
From (3.90) we see Exercise 3.20 thatthevalueofαthatmaximizesthemarginallikelihoodsatisfies γ α= .
(3.92) m T N m N Notethatthisisanimplicitsolutionforαnotonlybecauseγ dependsonα, butalso because the mode m N of the posterior distribution itself depends on the choice of α.
Wethereforeadoptaniterativeprocedureinwhichwemakeaninitialchoicefor α and use this to find m N, which is given by (3.53), and also to evaluate γ, which isgivenby(3.91).
Thesevaluesarethenusedtore-estimateαusing(3.92), andthe processrepeateduntilconvergence.
NotethatbecausethematrixΦTΦisfixed, we cancomputeitseigenvaluesonceatthestartandthensimplymultiplythesebyβ to obtaintheλi.
Itshouldbeemphasizedthatthevalueofαhasbeendeterminedpurelybylook- ingatthetrainingdata.
Incontrasttomaximumlikelihoodmethods, noindependent datasetisrequiredinordertooptimizethemodelcomplexity.
Wecansimilarlymaximizethelogmarginallikelihood(3.86)withrespecttoβ.
To do this, we note that the eigenvalues λi defined by (3.87) are proportional to β, andhencedλi/dβ =λi/β giving d ln|A|= d ln(λi+α)= 1 λi = γ .
(3.93) dβ dβ β λi+α β i i Thestationarypointofthemarginallikelihoodthereforesatisfies N 0= N − 1 tn −m T N φ(xn) 2 − γ (3.94) 2β 2 2β n=1 Exercise 3.22 andrearrangingweobtain N β 1 = N 1 −γ tn −m T N φ(xn) 2 .
(3.95) n=1 Again, this is an implicit solution for β and can be solved by choosing an initial valueforβ andthenusingthistocalculatem N andγ andthenre-estimateβ using (3.95), repeating until convergence.
If both α and β are to be determined from the data, thentheirvaluescanbere-estimatedtogetheraftereachupdateofγ.
170 3.
LINEARMODELSFORREGRESSION Figure3.15 Contours of the likelihood function (red) w2 and the prior (green) in which the axes in parameter spacehavebeenrotatedtoalignwith theeigenvectors u i of the Hessian.
For α = 0, the mode of the poste- u2 rior is given by the maximum likelihood solution w ML, t w h h e e d re ir a e s ct f i o o r n n w o 1 nz th e e ro e α ige th n e va m lu o e de λ 1 is , d a e t fi w ne M d AP by = (3 m .8 N 7) .
, I i n s w ML u1 smallcomparedwithαandsothequantityλ 1 /(λ 1 +α) w MAP is close to zero, and the corresponding MAP value of w 1 isalsoclosetozero.
Bycontrast, inthedirectionw 2 the eigenvalue λ 2 is large comparedwith α and so the quantityλ 2 /(λ 2 +α)isclosetounity, andthe MAPvalue ofw 2isclosetoitsmaximumlikelihoodvalue.
w1 3.5.3 Effective number of parameters Theresult(3.92)hasanelegantinterpretation(Mac Kay,1992a), whichprovides insightintothe Bayesiansolutionforα.
Toseethis, considerthecontoursofthelike- lihood function and the prior as illustrated in Figure 3.15.
Here we have implicitly transformed to a rotated set of axes in parameter space aligned with the eigenvec- tors ui defined in (3.87).
Contours of the likelihood function are then axis-aligned ellipses.
The eigenvalues λi measure the curvature of the likelihood function, and so in Figure 3.15 the eigenvalue λ 1 is small compared with λ 2 (because a smaller curvaturecorrespondstoagreaterelongationofthecontoursofthelikelihoodfunc- tion).
BecauseβΦTΦisapositivedefinitematrix, itwillhavepositiveeigenvalues, andsotheratioλi/(λi+α)willliebetween0and1.
Consequently, thequantityγ definedby(3.91)willlieintherange0 γ M.
Fordirectionsinwhichλi α, thecorrespondingparameterwi willbeclosetoitsmaximumlikelihoodvalue, and theratioλi/(λi+α)willbecloseto1.
Suchparametersarecalledwelldetermined because their values are tightly constrained by the data.
Conversely, for directions inwhichλi α, thecorrespondingparameterswi willbeclosetozero, aswillthe ratiosλi/(λi+α).
Thesearedirectionsinwhichthelikelihoodfunctionisrelatively insensitivetotheparametervalueandsotheparameterhasbeensettoasmallvalue bytheprior.
Thequantityγ definedby(3.91)thereforemeasurestheeffectivetotal numberofwelldeterminedparameters.
We can obtain some insight into the result (3.95) for re-estimating β by com- paring it with the corresponding maximum likelihood result given by (3.21).
Both of these formulae express the variance (the inverse precision) as an average of the squared differences between the targets and the model predictions.
However, they differinthatthenumberofdatapoints N inthedenominatorofthemaximumlike- lihoodresultisreplacedby N −γ inthe Bayesianresult.
Werecallfrom(1.56)that themaximumlikelihoodestimateofthevariancefora Gaussiandistributionovera 3.5.
The Evidence Approximation 171 singlevariablexisgivenby N 1 σ M 2 L = N (xn −µ ML )2 (3.96) n=1 and that this estimate is biased because the maximum likelihood solution µ ML for the mean has fitted some of the noise on the data.
In effect, this has used up one degree of freedom in the model.
The corresponding unbiased estimate is given by (1.59)andtakestheform N 1 σ M 2 AP = N −1 (xn −µ ML )2.
(3.97) n=1 Weshallseein Section10.1.3thatthisresultcanbeobtainedfroma Bayesiantreat- mentinwhichwemarginalizeovertheunknownmean.
Thefactorof N −1inthe denominatorofthe Bayesianresulttakesaccountofthefactthatonedegreeoffree- domhasbeenusedinfittingthemeanandremovesthebiasofmaximumlikelihood.
Now consider the corresponding results for the linear regression model.
The mean of the target distribution is now given by the function w Tφ(x), which contains M parameters.
However, notalloftheseparametersaretunedtothedata.
Theeffective numberofparametersthataredeterminedbythedataisγ, withtheremaining M−γ parameters set to small values by the prior.
This is reflected in the Bayesian result for the variance that has a factor N −γ in the denominator, thereby correcting for thebiasofthemaximumlikelihoodresult.
Wecanillustratetheevidenceframeworkforsettinghyperparametersusingthe sinusoidalsyntheticdatasetfrom Section1.1, togetherwiththe Gaussianbasisfunc- tion model comprising 9 basis functions, so that the total number of parameters in the model is given by M = 10 including the bias.
Here, for simplicity of illustra- tion, wehavesetβ toitstruevalueof11.1andthenusedtheevidenceframeworkto determineα, asshownin Figure3.16.
Wecanalsoseehowtheparameterαcontrolsthemagnitudeoftheparameters {wi }, byplottingtheindividualparametersversustheeffectivenumberγ ofparam- eters, asshownin Figure3.17.
Ifweconsiderthelimit N M inwhichthenumberofdatapointsislargein relationtothenumberofparameters, thenfrom(3.87)alloftheparameterswillbe welldeterminedbythedatabecauseΦTΦinvolvesanimplicitsumoverdatapoints, andsotheeigenvaluesλi increasewiththesizeofthedataset.
Inthiscase,γ =M, andthere-estimationequationsforαandβ become M α = (3.98) 2EW(m N) N β = (3.99) 2ED(m N) where EW and ED are defined by (3.25) and (3.26), respectively.
These results canbeusedasaneasy-to-computeapproximationtothefullevidencere-estimation 172 3.
LINEARMODELSFORREGRESSION −5 0 5 −5 0 5 lnα lnα Figure 3.16 The left plot shows γ (red curve) and 2αE W (m N ) (blue curve) versus lnα for the sinusoidal synthetic data set.
It is the intersection of these two curves that defines the optimum value for α given by the evidenceprocedure.
Therightplotshowsthecorrespondinggraphoflogevidencelnp(t|α,β)versuslnα(red curve)showingthatthepeakcoincideswiththecrossingpointofthecurvesintheleftplot.
Alsoshownisthe testseterror(bluecurve)showingthattheevidencemaximumoccursclosetothepointofbestgeneralization.
formulae, because they do not require evaluation of the eigenvalue spectrum of the Hessian.
Figure3.17 Plot of the 10 parameters w i fromthe Gaussianbasisfunction 0 model versus the effective num- 2 8 berofparametersγ, inwhichthe wi hyperparameterαisvariedinthe 4 range 0 α ∞ causing γ to 1 5 varyintherange0 γ M.
2 0 6 3 −1 1 7 −2 9 0 2 4 6 8 10 γ 3.6.
Limitations of Fixed Basis Functions Throughoutthischapter, wehavefocussedonmodelscomprisingalinearcombina- tionoffixed, nonlinearbasisfunctions.
Wehaveseenthattheassumptionoflinearity intheparametersledtoarangeofusefulpropertiesincludingclosed-formsolutions totheleast-squaresproblem, aswellasatractable Bayesiantreatment.
Furthermore, forasuitablechoiceofbasisfunctions, wecanmodelarbitrarynonlinearitiesinthe Exercises 173 mappingfrominputvariablestotargets.
Inthenextchapter, weshallstudyananal- ogousclassofmodelsforclassification.
It might appear, therefore, that such linear models constitute a general purpose framework for solving problems in pattern recognition.
Unfortunately, there are some significant shortcomings with linear models, which will cause us to turn in laterchapterstomorecomplexmodelssuchassupportvectormachinesandneural networks.
Thedifficultystemsfromtheassumptionthatthebasisfunctionsφj(x)arefixed beforethetrainingdatasetisobservedandisamanifestationofthecurseofdimen- sionalitydiscussedin Section1.4.
Asaconsequence, thenumberofbasisfunctions needs to grow rapidly, often exponentially, with the dimensionality D of the input space.
Fortunately, therearetwopropertiesofrealdatasetsthatwecanexploittohelp alleviatethisproblem.
Firstofall, thedatavectors{xn }typicallylieclosetoanon- linearmanifoldwhoseintrinsicdimensionalityissmallerthanthatoftheinputspace asaresultofstrongcorrelationsbetweentheinputvariables.
Wewillseeanexample ofthiswhenweconsiderimagesofhandwrittendigitsin Chapter12.
Ifweareusing localizedbasisfunctions, wecanarrangethattheyarescatteredininputspaceonly in regions containing data.
This approach is used in radial basis function networks and also in support vector and relevance vector machines.
Neural network models, which use adaptive basis functions having sigmoidal nonlinearities, can adapt the parameters so that the regions of input space over which the basis functions vary corresponds to the data manifold.
The second property is that target variables may havesignificantdependenceononlyasmallnumberofpossibledirectionswithinthe datamanifold.
Neuralnetworkscanexploitthispropertybychoosingthedirections ininputspacetowhichthebasisfunctionsrespond.
Exercises 3.1 ( ) www Show that the ‘tanh’ function and the logistic sigmoid function (3.6) arerelatedby tanh(a)=2σ(2a)−1.
(3.100) Hence show that a general linear combination of logistic sigmoid functions of the form M x−µj y(x, w)=w 0 + wjσ (3.101) s j=1 isequivalenttoalinearcombinationof‘tanh’functionsoftheform M x−µj y(x, u)=u 0 + ujtanh (3.102) s j=1 and find expressions to relate the new parameters {u 1 ,..., u M } to the original pa- rameters{w 1 ,..., w M }.
174 3.
LINEARMODELSFORREGRESSION 3.2 ( ) Showthatthematrix Φ(ΦTΦ) −1ΦT (3.103) takesanyvectorvandprojectsitontothespacespannedbythecolumnsofΦ.
Use thisresulttoshowthattheleast-squaressolution(3.15)correspondstoanorthogonal projectionofthevectortontothemanifold S asshownin Figure3.2.
3.3 ( ) Consider a data set inwhich eachdata point tn isassociated with a weighting factorrn >0, sothatthesum-of-squareserrorfunctionbecomes N ED(w)= 1 rn tn −w Tφ(xn) 2 .
(3.104) 2 n=1 Findanexpressionforthesolutionw thatminimizesthiserrorfunction.
Givetwo alternativeinterpretationsoftheweightedsum-of-squareserrorfunctionintermsof (i)datadependentnoisevarianceand(ii)replicateddatapoints.
3.4 ( ) www Consideralinearmodeloftheform D y(x, w)=w 0 + wixi (3.105) i=1 togetherwithasum-of-squareserrorfunctionoftheform N 1 ED(w)= {y(xn, w)−tn }2 .
(3.106) 2 n=1 Now suppose that Gaussian noise i with zero mean and variance σ2 is added in- dependently to each of the input variables xi.
By making use of E[ i] = 0 and E[ i j] = δijσ2, show that minimizing ED averaged over the noise distribution is equivalenttominimizingthesum-of-squareserrorfornoise-freeinputvariableswith the addition of a weight-decay regularization term, in which the bias parameter w 0 isomittedfromtheregularizer.
3.5 ( ) www Usingthetechniqueof Lagrangemultipliers, discussedin Appendix E, showthatminimizationoftheregularizederrorfunction(3.29)isequivalenttomini- mizingtheunregularizedsum-of-squareserror(3.12)subjecttotheconstraint(3.30).
Discusstherelationshipbetweentheparametersηandλ.
3.6 ( ) www Consider a linear basis function regression model for a multivariate targetvariablethavinga Gaussiandistributionoftheform p(t|W,Σ)=N(t|y(x, W),Σ) (3.107) where y(x, W)=WTφ(x) (3.108) Exercises 175 together with a training data set comprising input basis vectors φ(xn) and corre- spondingtargetvectorstn, withn = 1,..., N.
Showthatthemaximumlikelihood solution W ML for the parameter matrix W has the property that each column is given by an expression of the form (3.15), which was the solution for an isotropic noise distribution.
Note that this is independent of the covariance matrix Σ.
Show thatthemaximumlikelihoodsolutionforΣisgivenby N Σ= N 1 tn −W M T L φ(xn) tn −W M T L φ(xn) T .
(3.109) n=1 3.7 ( ) Byusingthetechniqueofcompletingthesquare, verifytheresult(3.49)forthe posteriordistributionoftheparameterswinthelinearbasisfunctionmodelinwhich m N and SN aredefinedby(3.50)and(3.51)respectively.
3.8 ( ) www Considerthelinearbasisfunctionmodelin Section3.1, andsuppose thatwehavealreadyobserved N datapoints, sothattheposteriordistributionover w isgivenby(3.49).
Thisposteriorcanberegardedasthepriorforthenextobser- vation.
By considering an additional data point (x N+1 , t N+1 ), and by completing the square in the exponential, show that the resulting posterior distribution is again givenby(3.49)butwith SN replacedby SN+1 andm N replacedbym N+1.
3.9 ( ) Repeat the previous exercise but instead of completing the square by hand, makeuseofthegeneralresultforlinear-Gaussianmodelsgivenby(2.116).
3.10 ( ) www Bymakinguseoftheresult(2.115)toevaluatetheintegralin(3.57), verify that the predictive distribution for the Bayesian linear regression model is givenby(3.58)inwhichtheinput-dependentvarianceisgivenby(3.59).
3.11 ( ) Wehaveseenthat, asthesizeofadatasetincreases, theuncertaintyassociated with the posterior distribution over model parameters decreases.
Make use of the matrixidentity(Appendix C) M+vv T −1 =M −1− (M−1v) v TM−1 (3.110) 1+v TM−1v to show that the uncertainty σ2 (x) associated with the linear regression function N givenby(3.59)satisfies σ2 (x) σ2 (x).
(3.111) N+1 N 3.12 ( ) We saw in Section 2.3.6 that the conjugate prior for a Gaussian distribution with unknown mean and unknown precision (inverse variance) is a normal-gamma distribution.
This property also holds for the case of the conditional Gaussian dis- tribution p(t|x, w,β) of the linear regression model.
If we consider the likelihood function(3.10), thentheconjugatepriorforwandβ isgivenby p(w,β)=N(w|m 0 ,β −1S 0 )Gam(β|a 0 , b 0 ).
(3.112) 176 3.
LINEARMODELSFORREGRESSION Show that the corresponding posterior distribution takes the same functional form, sothat p(w,β|t)=N(w|m N,β −1SN)Gam(β|a N, b N) (3.113) andfindexpressionsfortheposteriorparametersm N, SN, a N, andb N.
3.13 ( ) Showthatthepredictivedistributionp(t|x, t)forthemodeldiscussedin Ex- ercise 3.12isgivenbya Student’st-distributionoftheform p(t|x, t)=St(t|µ,λ,ν) (3.114) andobtainexpressionsforµ,λandν.
3.14 ( ) In this exercise, we explore in more detail the properties of the equivalent kernel defined by (3.62), where SN is defined by (3.54).
Suppose that the basis functions φj(x) are linearly independent and that the number N of data points is greater than the number M of basis functions.
Furthermore, let one of the basis functions be constant, say φ 0 (x) = 1.
By taking suitable linear combinations of these basis functions, we can construct a new basis set ψj(x) spanning the same spacebutthatareorthonormal, sothat N ψj(xn)ψk(xn)=Ijk (3.115) n=1 where Ijk isdefinedtobe1ifj =kand0otherwise, andwetakeψ 0 (x)=1.
Show that for α = 0, the equivalent kernel can be written as k(x, x ) = ψ(x)Tψ(x ) where ψ = (ψ 1 ,...,ψM)T.
Use this result to show that the kernel satisfies the summationconstraint N k(x, xn)=1.
(3.116) n=1 3.15 ( ) www Consider a linear basis function model for regression in which the pa- rameters α and β are set using the evidence framework.
Show that the function E(m N)definedby(3.82)satisfiestherelation2E(m N)=N.
3.16 ( ) Derive the result (3.86) for the log evidence function p(t|α,β) of the linear regressionmodelbymakinguseof(2.115)toevaluatetheintegral(3.77)directly.
3.17 ( ) Show that the evidence function for the Bayesian linear regression model can bewrittenintheform(3.78)inwhich E(w)isdefinedby(3.79).
3.18 ( ) www Bycompletingthesquareoverw, showthattheerrorfunction(3.79) in Bayesianlinearregressioncanbewrittenintheform(3.80).
3.19 ( ) Showthattheintegrationoverwinthe Bayesianlinearregressionmodelgives theresult(3.85).
Henceshowthatthelogmarginallikelihoodisgivenby(3.86).
Exercises 177 3.20 ( ) www Startingfrom(3.86)verifyallofthestepsneededtoshowthatmaxi- mizationofthelogmarginallikelihoodfunction(3.86)withrespecttoαleadstothe re-estimationequation(3.92).
3.21 ( ) Analternativewaytoderivetheresult(3.92)fortheoptimalvalueofαinthe evidenceframeworkistomakeuseoftheidentity d d ln|A|=Tr A −1 A .
(3.117) dα dα Prove this identity by considering the eigenvalue expansion of a real, symmetric matrix A, and making use of the standard results for the determinant and trace of Aexpressedintermsofitseigenvalues(Appendix C).
Thenmakeuseof(3.117)to derive(3.92)startingfrom(3.86).
3.22 ( ) Starting from (3.86) verify all of the steps needed to show that maximiza- tion of the log marginal likelihood function (3.86) with respect to β leads to the re-estimationequation(3.95).
3.23 ( ) www Show that the marginal probability of the data, in other words the modelevidence, forthemodeldescribedin Exercise 3.12isgivenby p(t)= 1 b a 0 0 Γ(a N)|SN |1/2 (3.118) (2π)N/2b a N N Γ(a 0 ) |S 0 |1/2 byfirstmarginalizingwithrespecttowandthenwithrespecttoβ.
3.24 ( ) Repeatthepreviousexercisebutnowuse Bayes’theoremintheform p(t|w,β)p(w,β) p(t)= (3.119) p(w,β|t) andthensubstituteforthepriorandposterior distributionsandthelikelihoodfunc- tioninordertoderivetheresult(3.118).
4 Linear Models for Classification Inthepreviouschapter, weexploredaclassofregressionmodelshavingparticularly simpleanalyticalandcomputationalproperties.
Wenowdiscussananalogousclass ofmodelsforsolvingclassificationproblems.
Thegoalinclassificationistotakean inputvectorxandtoassignittooneof K discreteclasses C k wherek = 1,..., K.
Inthemostcommonscenario, theclassesaretakentobedisjoint, sothateachinputis assignedtooneandonlyoneclass.
Theinputspaceistherebydividedintodecision regions whose boundaries are called decision boundaries or decision surfaces.
In thischapter, weconsiderlinearmodelsforclassification, bywhichwemeanthatthe decision surfaces are linear functions of the input vector x and hence are defined by (D −1)-dimensional hyperplanes within the D-dimensional input space.
Data setswhoseclassescanbeseparatedexactlybylineardecisionsurfacesaresaidtobe linearlyseparable.
Forregressionproblems, thetargetvariabletwassimplythevectorofrealnum- berswhosevalueswewishtopredict.
Inthecaseofclassification, therearevarious 179 180 4.
LINEARMODELSFORCLASSIFICATION ways of using target values to represent class labels.
For probabilistic models, the most convenient, in the case of two-class problems, is the binary representation in whichthereisasingletargetvariablet ∈ {0,1}suchthatt = 1representsclass C 1 andt = 0representsclass C 2.
Wecaninterpretthevalueoftastheprobabilitythat theclassis C 1, withthevaluesofprobabilitytakingonlytheextremevaluesof0and 1.
For K > 2classes, itisconvenienttousea1-of-K codingschemeinwhichtis a vector of length K such that if the class is C j, then all elements tk of t are zero exceptelementtj, whichtakesthevalue1.
Forinstance, ifwehave K = 5classes, thenapatternfromclass2wouldbegiventhetargetvector t=(0,1,0,0,0)T.
(4.1) Again, we can interpret the value of tk as the probability that the class is C k.
For nonprobabilistic models, alternative choices of target variable representation will sometimesproveconvenient.
In Chapter 1, we identified three distinct approaches to the classification prob- lem.
Thesimplestinvolvesconstructingadiscriminantfunctionthatdirectlyassigns each vector x to a specific class.
A more powerful approach, however, models the conditional probability distribution p(C k |x) in an inference stage, and then subse- quently uses this distribution to make optimal decisions.
By separating inference and decision, we gain numerous benefits, as discussed in Section 1.5.4.
There are two different approaches to determining the conditional probabilitiesp(C k |x).
One techniqueistomodelthemdirectly, forexamplebyrepresentingthemasparametric models and then optimizing the parameters using a training set.
Alternatively, we can adopt a generative approach in which we model the class-conditional densities givenbyp(x|C k), togetherwiththepriorprobabilitiesp(C k)fortheclasses, andthen wecomputetherequiredposteriorprobabilitiesusing Bayes’theorem p(C k |x)= p(x|C k)p(C k) .
(4.2) p(x) Weshalldiscussexamplesofallthreeapproachesinthischapter.
In the linear regression models considered in Chapter 3, the model prediction y(x, w) was given by a linear function of the parameters w.
In the simplest case, the model is also linear in the input variables and therefore takes the form y(x) = w Tx+w 0, sothatyisarealnumber.
Forclassificationproblems, however, wewish to predict discrete class labels, or more generally posterior probabilities that lie in therange(0,1).
Toachievethis, weconsiderageneralizationofthismodelinwhich wetransformthelinearfunctionofwusinganonlinearfunctionf(·)sothat y(x)=f w Tx+w 0 .
(4.3) Inthemachinelearningliteraturef(·)isknownasanactivationfunction, whereas its inverse is called a link function in the statistics literature.
The decision surfaces correspondtoy(x)=constant, sothatw Tx+w 0 =constantandhencethedeci- sionsurfacesarelinearfunctionsofx, evenifthefunctionf(·)isnonlinear.
Forthis reason, the class of models described by (4.3) are called generalized linear models 4.1.
Discriminant Functions 181 (Mc Cullagh and Nelder, 1989).
Note, however, that in contrast to the models used forregression, theyarenolongerlinearintheparametersduetothepresenceofthe nonlinear function f(·).
This will lead to more complex analytical and computa- tional properties than for linear regression models.
Nevertheless, these models are still relatively simple compared to the more general nonlinear models that will be studiedinsubsequentchapters.
The algorithms discussed in this chapter will be equally applicable if we first makeafixednonlineartransformationoftheinputvariablesusingavectorofbasis functionsφ(x)aswedidforregressionmodelsin Chapter3.
Webeginbyconsider- ingclassificationdirectlyintheoriginalinputspacex, whilein Section4.3weshall find it convenient to switch to a notation involving basis functions for consistency withlaterchapters.
4.1.
Discriminant Functions A discriminant is a function that takes an input vector x and assigns it to one of K classes, denoted C k.
Inthischapter, weshallrestrictattentiontolineardiscriminants, namely those for which the decision surfaces are hyperplanes.
To simplify the dis- cussion, weconsiderfirstthecaseoftwoclassesandtheninvestigatetheextension to K >2classes.
4.1.1 Two classes Thesimplestrepresentationofalineardiscriminantfunctionisobtainedbytak- ingalinearfunctionoftheinputvectorsothat y(x)=w Tx+w 0 (4.4) wherewiscalledaweightvector, andw 0 isabias(nottobeconfusedwithbiasin the statistical sense).
The negative of the bias is sometimes called a threshold.
An inputvectorxisassignedtoclass C 1ify(x) 0andtoclass C 2otherwise.
Thecor- responding decision boundary is therefore defined by the relationy(x) = 0, which corresponds to a (D −1)-dimensional hyperplane within the D-dimensional input space.
Consider two points x A and x B both of which lie on the decision surface.
Becausey(x A )=y(x B )=0, wehavew T(x A −x B )=0andhencethevectorwis orthogonaltoeveryvectorlyingwithinthedecisionsurface, andsowdeterminesthe orientationofthedecisionsurface.
Similarly, ifxisapointonthedecisionsurface, theny(x) = 0, andsothenormaldistancefromtheorigintothedecisionsurfaceis givenby w Tx w =− 0 .
(4.5) w w Wethereforeseethatthebiasparameterw 0 determinesthelocationofthedecision surface.
Thesepropertiesareillustratedforthecaseof D =2in Figure4.1.
Furthermore, wenotethatthevalueofy(x)givesasignedmeasureoftheper- pendiculardistancer ofthepointxfromthedecisionsurface.
Toseethis, consider 182 4.
LINEARMODELSFORCLASSIFICATION Figure4.1 Illustration of the geometry of a y >0 x2 lineardiscriminantfunctionintwodimensions.
Thedecisionsurface, showninred, isperpen- y =0 dicular to w, and its displacement from the y <0 R 1 origin is controlled by the bias parameter w 0.
R 2 Also, thesignedorthogonaldistanceofagen- eralpointxfromthedecisionsurfaceisgiven byy(x)/ w .
x w y(x) w x⊥ x1 −w0 w anarbitrarypointxandletx⊥beitsorthogonalprojectionontothedecisionsurface, sothat w x=x⊥+r w .
(4.6) Multiplyingbothsidesofthisresultbyw Tandaddingw 0, andmakinguseofy(x)= w Tx+w 0 andy(x⊥)=w Tx⊥+w 0 =0, wehave y(x) r = .
(4.7) w Thisresultisillustratedin Figure4.1.
As with the linear regression models in Chapter 3, it is sometimes convenient touseamorecompactnotationinwhichweintroduceanadditionaldummy‘input’ valuex 0 =1andthendefinew =(w 0 , w)andx =(x 0 , x)sothat y(x)=w Tx .
(4.8) In this case, the decision surfaces are D-dimensional hyperplanes passing through theoriginofthe D+1-dimensionalexpandedinputspace.
4.1.2 Multiple classes Nowconsidertheextensionoflineardiscriminantsto K >2classes.
Wemight betemptedbetobuilda K-classdiscriminantbycombininganumberoftwo-class discriminant functions.
However, this leads to some serious difficulties (Duda and Hart,1973)aswenowshow.
Considertheuseof K−1classifierseachofwhichsolvesatwo-classproblemof separatingpointsinaparticularclass C k frompointsnotinthatclass.
Thisisknown as a one-versus-the-rest classifier.
The left-hand example in Figure 4.2 shows an 4.1.
Discriminant Functions 183 C 3 C 1 ? R 1 R R 1 3 C R 1 ? 2 C C 3 1 R R 2 3 C C 2 2 not C 1 C 2 not C 2 Figure4.2 Attemptingtoconstructa K classdiscriminantfromasetoftwoclassdiscriminantsleadstoam- biguousregions, showningreen.
Ontheleftisanexampleinvolvingtheuseoftwodiscriminantsdesignedto distinguishpointsinclass C k frompointsnotinclass C k.
Ontherightisanexampleinvolvingthreediscriminant functionseachofwhichisusedtoseparateapairofclasses C k and C j.
exampleinvolvingthreeclasseswherethisapproachleadstoregionsofinputspace thatareambiguouslyclassified.
An alternative is to introduce K(K −1)/2 binary discriminant functions, one foreverypossiblepairofclasses.
Thisisknownasaone-versus-oneclassifier.
Each pointisthenclassifiedaccordingtoamajorityvoteamongstthediscriminantfunc- tions.
However, this too runs into the problem of ambiguous regions, as illustrated intheright-handdiagramof Figure4.2.
We can avoid these difficulties by considering a single K-class discriminant comprising K linearfunctionsoftheform yk(x)=w k Tx+wk0 (4.9) andthenassigningapointxtoclass C k ifyk(x)>yj(x)forallj =k.
Thedecision boundary between class C k and class C j is therefore given by yk(x) = yj(x) and hencecorrespondstoa(D−1)-dimensionalhyperplanedefinedby (wk −wj)Tx+(wk0 −wj0 )=0.
(4.10) Thishasthesameformasthedecisionboundaryforthetwo-classcasediscussedin Section4.1.1, andsoanalogousgeometricalpropertiesapply.
The decision regions of such a discriminant are always singly connected and convex.
Toseethis, considertwopointsx Aandx Bbothofwhichlieinsidedecision region R k, asillustratedin Figure4.3.
Anypointx thatliesonthelineconnecting x A andx B canbeexpressedintheform x =λx A +(1−λ)x B (4.11) 184 4.
LINEARMODELSFORCLASSIFICATION Figure4.3 Illustration of the decision regions for a mul- ticlass linear discriminant, with the decision boundaries shown in red.
If two points x A R j andx B bothlieinsidethesamedecisionre- gion R k, thenanypointxbthatliesontheline R i connecting these two points must also lie in R k, and hence the decision region must be singlyconnectedandconvex.
R k x B x A xˆ where0 λ 1.
Fromthelinearityofthediscriminantfunctions, itfollowsthat yk(x )=λyk(x A )+(1−λ)yk(x B ).
(4.12) Because both x A and x B lie inside R k, it follows that yk(x A ) > yj(x A ), and yk(x B ) > yj(x B ), for all j = k, and hence yk(x ) > yj(x ), and so x also lies inside R k.
Thus R k issinglyconnectedandconvex.
Note that for two classes, we can either employ the formalism discussed here, based on two discriminant functions y 1 (x) and y 2 (x), or else use the simpler but equivalent formulation described in Section 4.1.1 based on a single discriminant functiony(x).
Wenowexplorethreeapproachestolearningtheparametersoflineardiscrimi- nantfunctions, basedonleastsquares, Fisher’slineardiscriminant, andthepercep- tronalgorithm.
4.1.3 Least squares for classification In Chapter 3, we considered models that were linear functions of the parame- ters, and we saw that the minimization of a sum-of-squares error function led to a simpleclosed-formsolutionfortheparametervalues.
Itisthereforetemptingtosee if we can apply the same formalism to classification problems.
Consider a general classification problem with K classes, with a 1-of-K binary coding scheme for the target vector t.
One justification for using least squares in such a context is that it approximatestheconditionalexpectation E[t|x]ofthetargetvaluesgiventheinput vector.
For the binary coding scheme, this conditional expectation is given by the vector of posterior class probabilities.
Unfortunately, however, these probabilities aretypicallyapproximatedratherpoorly, indeedtheapproximationscanhavevalues outside the range (0,1), due to the limited flexibility of a linear model as we shall seeshortly.
Eachclass C k isdescribedbyitsownlinearmodelsothat yk(x)=w k Tx+wk0 (4.13) where k = 1,..., K.
We can conveniently group these together using vector nota- tionsothat y(x)=W , Tx (4.14) 4.1.
Discriminant Functions 185 where W , is a matrix whose kth column comprises the D +1-dimensional vector w k =(wk0 , w k T)Tandx isthecorrespondingaugmentedinputvector(1, x T)Twith adummyinputx 0 =1.
Thisrepresentationwasdiscussedindetailin Section3.1.
A newinputxisthenassignedtotheclassforwhichtheoutputyk =w k Tx islargest.
, We now determine the parameter matrix W by minimizing a sum-of-squares error function, as we did for regression in Chapter 3.
Consider a training data set {xn, tn }wheren=1,..., N, anddefineamatrix Twhosenthrowisthevectort T n , together with a matrix X whose nth row is x T.
The sum-of-squares error function n canthenbewrittenas ED(W , )= 1 Tr (X W , −T)T(X W , −T) .
(4.15) 2 , Settingthederivativewithrespectto Wtozero, andrearranging, wethenobtainthe , solutionfor Wintheform W , =(X TX ) −1X TT=X † T (4.16) where X † isthe pseudo-inverseof the matrix X , asdiscussed in Section 3.1.1.
We thenobtainthediscriminantfunctionintheform T y(x)=W , Tx =TT X † x .
(4.17) Aninterestingpropertyofleast-squaressolutionswithmultipletargetvariables isthatifeverytargetvectorinthetrainingsetsatisfiessomelinearconstraint a Ttn+b=0 (4.18) forsomeconstantsaandb, thenthemodelpredictionforanyvalueofxwillsatisfy Exercise 4.2 thesameconstraintsothat a Ty(x)+b=0.
(4.19) Thus if we use a 1-of-K coding scheme for K classes, then the predictions made bythemodelwillhavethepropertythattheelementsofy(x)willsumto1forany value of x.
However, this summation constraint alone is not sufficient to allow the model outputs to be interpreted as probabilities because they are not constrained to liewithintheinterval(0,1).
Theleast-squaresapproachgivesanexactclosed-formsolutionforthediscrimi- nantfunctionparameters.
However, evenasadiscriminantfunction(whereweuseit tomakedecisionsdirectlyanddispensewithanyprobabilisticinterpretation)itsuf- Section2.3.7 fers from some severe problems.
We have already seen that least-squares solutions lackrobustnesstooutliers, andthisappliesequallytotheclassificationapplication, asillustratedin Figure4.4.
Hereweseethattheadditionaldatapointsintheright- hand figure produce a significant change in the location of the decision boundary, eventhoughthesepointwouldbecorrectlyclassifiedbytheoriginaldecisionbound- aryintheleft-handfigure.
Thesum-of-squareserrorfunctionpenalizespredictions that are ‘too correct’ in that they lie a long way on the correct side of the decision 186 4.
LINEARMODELSFORCLASSIFICATION 4 4 2 2 0 0 −2 −2 −4 −4 −6 −6 −8 −8 −4 −2 0 2 4 6 8 −4 −2 0 2 4 6 8 Figure4.4 Theleftplotshowsdatafromtwoclasses, denotedbyredcrossesandbluecircles, togetherwith thedecisionboundaryfoundbyleastsquares(magentacurve)andalsobythelogisticregressionmodel(green curve), whichisdiscussedlaterin Section4.3.2.
Theright-handplotshowsthecorrespondingresultsobtained whenextradatapointsareaddedatthebottomleftofthediagram, showingthatleastsquaresishighlysensitive tooutliers, unlikelogisticregression.
boundary.
In Section7.1.2, weshallconsiderseveralalternativeerrorfunctionsfor classificationandweshallseethattheydonotsufferfromthisdifficulty.
However, problems with least squares can be more severe than simply lack of robustness, asillustrated in Figure 4.5.
Thisshows asynthetic data setdrawnfrom threeclassesinatwo-dimensionalinputspace(x 1 , x 2 ), havingthepropertythatlin- ear decision boundaries can give excellent separation between the classes.
Indeed, the technique of logistic regression, described later in this chapter, gives a satisfac- torysolutionasseenintheright-handplot.
However, theleast-squaressolutiongives poorresults, withonlyasmallregionoftheinputspaceassignedtothegreenclass.
The failure of least squares should not surprise us when we recall that it cor- responds to maximum likelihood under the assumption of a Gaussian conditional distribution, whereasbinarytargetvectorsclearlyhaveadistributionthatisfarfrom Gaussian.
Byadoptingmoreappropriateprobabilisticmodels, weshallobtainclas- sificationtechniqueswithmuchbetterpropertiesthanleastsquares.
Forthemoment, however, wecontinuetoexplorealternativenonprobabilisticmethodsforsettingthe parametersinthelinearclassificationmodels.
4.1.4 Fisher’s linear discriminant One way to view a linear classification model is in terms of dimensionality reduction.
Consider first the case of two classes, and suppose we take the D- 4.1.
Discriminant Functions 187 6 6 4 4 2 2 0 0 −2 −2 −4 −4 −6 −6 −6 −4 −2 0 2 4 6 −6 −4 −2 0 2 4 6 Figure4.5 Exampleofasyntheticdatasetcomprisingthreeclasses, withtrainingdatapointsdenotedinred (×), green (+), and blue (◦).
Lines denote the decision boundaries, and the background colours denote the respectiveclassesofthedecisionregions.
Ontheleftistheresultofusingaleast-squaresdiscriminant.
Wesee thattheregionofinputspaceassignedtothegreenclassistoosmallandsomostofthepointsfromthisclass aremisclassified.
Ontherightistheresultofusinglogisticregressionsasdescribedin Section4.3.2showing correctclassificationofthetrainingdata.
dimensionalinputvectorxandprojectitdowntoonedimensionusing y =w Tx.
(4.20) Ifweplaceathresholdony andclassifyy −w 0 asclass C 1, andotherwiseclass C 2, then we obtain our standard linear classifier discussed in the previous section.
In general, the projection onto one dimension leads to a considerable loss of infor- mation, andclassesthatarewellseparatedintheoriginal D-dimensionalspacemay become strongly overlapping in one dimension.
However, by adjusting the com- ponentsoftheweightvectorw, wecanselectaprojectionthatmaximizestheclass separation.
Tobeginwith, consideratwo-classprobleminwhichthereare N 1points ofclass C 1 and N 2 pointsofclass C 2, sothatthemeanvectorsofthetwoclassesare givenby 1 1 m 1 = xn, m 2 = xn.
(4.21) N N 1 2 n∈C n∈C 1 2 Thesimplestmeasureoftheseparationoftheclasses, whenprojectedontow, isthe separationoftheprojectedclassmeans.
Thissuggeststhatwemightchoosewsoas tomaximize m 2 −m 1 =w T(m 2 −m 1 ) (4.22) where mk =w Tmk (4.23) 188 4.
LINEARMODELSFORCLASSIFICATION 4 4 2 2 0 0 −2 −2 −2 2 6 −2 2 6 Figure4.6 Theleftplotshowssamplesfromtwoclasses(depictedinredandblue)alongwiththehistograms resultingfromprojectionontothelinejoiningtheclassmeans.
Notethatthereisconsiderableclassoverlapin theprojectedspace.
Therightplotshowsthecorrespondingprojectionbasedonthe Fisherlineardiscriminant, showingthegreatlyimprovedclassseparation.
is the mean of the projected data from class C k.
However, this expression can be made arbitrarily large simply by increasing the magnitude o f w.
To solve this problem, we could constrain w to have unit length, so that w2 = 1.
Using i i Appendix E a Lagrange multiplier to perform the constrained maximization, we then find that Exercise 4.4 w ∝(m 2 −m 1 ).
Thereisstillaproblemwiththisapproach, however, asillustrated in Figure 4.6.
This shows two classes that are well separated in the original two- dimensional space (x 1 , x 2 ) but that have considerable overlap when projected onto the line joining their means.
This difficulty arises from the strongly nondiagonal covariances of the class distributions.
The idea proposed by Fisher is to maximize afunctionthatwillgivealargeseparationbetweentheprojectedclassmeanswhile alsogivingasmallvariancewithineachclass, therebyminimizingtheclassoverlap.
The projection formula (4.20) transforms the set of labelled data points in x intoalabelledsetintheone-dimensionalspacey.
Thewithin-classvarianceofthe transformeddatafromclass C k isthereforegivenby s2 k = (yn −mk)2 (4.24) n∈C k where yn = w Txn.
We can define the total within-class variance for the whole data set to be simply s2 +s2.
The Fisher criterion is defined to be the ratio of the 1 2 between-classvariancetothewithin-classvarianceandisgivenby (m −m )2 2 1 J(w)= .
(4.25) s2+s2 1 2 We can make the dependence on w explicit by using (4.20), (4.23), and (4.24) to Exercise 4.5 rewritethe Fishercriterionintheform 4.1.
Discriminant Functions 189 w TS w B J(w)= (4.26) w TS w W where S B isthebetween-classcovariancematrixandisgivenby S B =(m 2 −m 1 )(m 2 −m 1 )T (4.27) and S W isthetotalwithin-classcovariancematrix, givenby S W = (xn −m 1 )(xn −m 1 )T+ (xn −m 2 )(xn −m 2 )T.
(4.28) n∈C n∈C 1 2 Differentiating(4.26)withrespecttow, wefindthat J(w)ismaximizedwhen (w TS B w)S W w =(w TS W w)S B w.
(4.29) From(4.27), weseethat S B wisalwaysinthedirectionof(m 2 −m 1 ).
Furthermore, wedonotcareaboutthemagnitudeofw, onlyitsdirection, andsowecandropthe scalar factors (w TS B w) and (w TS W w).
Multiplying both sides of (4.29) by S − W 1 wethenobtain w ∝S − W 1(m 2 −m 1 ).
(4.30) Notethatifthewithin-classcovarianceisisotropic, sothat S W isproportionaltothe unit matrix, we find that w is proportional to the difference of the class means, as discussedabove.
The result (4.30) is known as Fisher’s linear discriminant, although strictly it is not a discriminant but rather a specific choice of direction for projection of the datadowntoonedimension.
However, theprojecteddatacansubsequentlybeused to construct a discriminant, by choosing a threshold y 0 so that we classify a new point as belonging to C 1 if y(x) y 0 and classify it as belonging to C 2 otherwise.
For example, we can model the class-conditional densities p(y|C k) using Gaussian distributions and then use the techniques of Section 1.2.4 to find the parameters of the Gaussian distributions by maximum likelihood.
Having found Gaussian ap- proximations to the projected classes, the formalism of Section 1.5.1 then gives an expressionfortheoptimalthreshold.
Somejustificationforthe Gaussianassumption comesfromthecentrallimittheorembynotingthaty = w Txisthesumofasetof randomvariables.
4.1.5 Relation to least squares The least-squares approach to the determination of a linear discriminant was based on the goal of making the model predictions as close as possible to a set of target values.
By contrast, the Fisher criterion was derived by requiring maximum classseparationintheoutputspace.
Itisinterestingtoseetherelationshipbetween these two approaches.
In particular, we shall show that, for the two-class problem, the Fishercriterioncanbeobtainedasaspecialcaseofleastsquares.
Sofarwehaveconsidered1-of-K codingforthetargetvalues.
If, however, we adopt a slightly different target coding scheme, then the least-squares solution for 190 4.
LINEARMODELSFORCLASSIFICATION the weights becomes equivalent to the Fisher solution (Duda and Hart, 1973).
In particular, weshalltakethetargetsforclass C 1tobe N/N 1, where N 1isthenumber of patterns in class C 1, and N is the total number of patterns.
This target value approximates the reciprocal of the prior probability for class C 1.
For class C 2, we shalltakethetargetstobe−N/N 2, where N 2 isthenumberofpatternsinclass C 2.
Thesum-of-squareserrorfunctioncanbewritten N E = 1 w Txn+w 0 −tn 2 .
(4.31) 2 n=1 Settingthederivativesof E withrespecttow 0andwtozero, weobtainrespectively N w Txn+w 0 −tn = 0 (4.32) n=1 N w Txn+w 0 −tn xn = 0.
(4.33) n=1 From (4.32), and making use of our choice of target coding scheme for the tn, we obtainanexpressionforthebiasintheform w 0 =−w Tm (4.34) wherewehaveused N N N tn =N 1 −N 2 =0 (4.35) N N 1 2 n=1 andwheremisthemeanofthetotaldatasetandisgivenby N 1 1 m= xn = (N 1 m 1 +N 2 m 2 ).
(4.36) N N n=1 After some straightforward algebra, and again making use of the choice of tn, the Exercise 4.6 secondequation(4.33)becomes N N S W + 1 2 S B w =N(m 1 −m 2 ) (4.37) N where S W isdefinedby(4.28), S B isdefinedby(4.27), andwehavesubstitutedfor the bias using (4.34).
Using (4.27), we note that S B w is always in the direction of (m 2 −m 1 ).
Thuswecanwrite w ∝S − W 1(m 2 −m 1 ) (4.38) where we have ignored irrelevant scale factors.
Thus the weight vector coincides withthatfoundfromthe Fishercriterion.
Inaddition, wehavealsofoundanexpres- sionforthebiasvaluew 0givenby(4.34).
Thistellsusthatanewvectorxshouldbe classifiedasbelongingtoclass C 1ify(x)=w T(x−m)>0andclass C 2otherwise.
4.1.
Discriminant Functions 191 4.1.6 Fisher’s discriminant for multiple classes Wenowconsiderthegeneralizationofthe Fisherdiscriminantto K >2classes, andweshallassumethatthedimensionality Doftheinputspaceisgreaterthanthe number K ofclasses.
Next, weintroduce D >1linear‘features’yk =w k Tx, where k = 1,..., D .
These feature values can conveniently be grouped together to form avectory.
Similarly, theweightvectors{wk }canbeconsideredtobethecolumns ofamatrix W, sothat y =WTx.
(4.39) Notethatagainwearenotincludinganybiasparametersinthedefinitionofy.
The generalizationofthewithin-classcovariancematrixtothecaseof K classesfollows from(4.28)togive K S W = Sk (4.40) k=1 where Sk = (xn −mk)(xn −mk)T (4.41) n∈C k 1 mk = xn (4.42) Nk n∈C k and Nk isthenumberofpatternsinclass C k.
Inordertofindageneralizationofthe between-classcovariancematrix, wefollow Dudaand Hart(1973)andconsiderfirst thetotalcovariancematrix N S T = (xn −m)(xn −m)T (4.43) n=1 wheremisthemeanofthetotaldataset N K 1 1 m= xn = Nkmk (4.44) N N n=1 k=1 and N = k Nk isthetotalnumberofdatapoints.
Thetotalcovariancematrixcan be decomposed into the sum of the within-class covariance matrix, given by (4.40) and (4.41), plus an additional matrix S B, which we identify as a measure of the between-classcovariance S T =S W +S B (4.45) where K S B = Nk(mk −m)(mk −m)T.
(4.46) k=1 192 4.
LINEARMODELSFORCLASSIFICATION These covariance matrices have been defined in the original x-space.
We can now definesimilarmatricesintheprojected D -dimensionaly-space K s W = (yn −µ k )(yn −µ k )T (4.47) k=1n∈C k and K s B = Nk(µ k −µ)(µ k −µ)T (4.48) k=1 where K 1 1 µ k = yn, µ= Nk µ k .
(4.49) Nk N n∈C k k=1 Againwewishtoconstructascalarthatislargewhenthebetween-classcovariance islargeandwhenthewithin-classcovarianceissmall.
Therearenowmanypossible choicesofcriterion(Fukunaga,1990).
Oneexampleisgivenby J(W)=Tr s − W 1s B .
(4.50) This criterion can then be rewritten as an explicit function of the projection matrix Wintheform J(w)=Tr (WS W WT) −1(WS B WT) .
(4.51) Maximizationofsuchcriteriaisstraightforward, thoughsomewhatinvolved, andis discussedatlengthin Fukunaga(1990).
Theweightvaluesaredeterminedbythose eigenvectorsof S − W 1S B thatcorrespondtothe D largesteigenvalues.
Thereisoneimportantresultthatiscommontoallsuchcriteria, whichisworth emphasizing.
We first note from (4.46) that S B is composed of the sum of K ma- trices, each of which is an outer product of two vectors and therefore of rank 1.
In addition, only(K−1)ofthesematricesareindependentasaresultoftheconstraint (4.44).
Thus, S Bhasrankatmostequalto(K−1)andsothereareatmost(K−1) nonzeroeigenvalues.
Thisshowsthattheprojectionontothe(K −1)-dimensional subspace spanned by the eigenvectors of S B does not alter the value of J(w), and sowearethereforeunabletofindmorethan(K−1)linear‘features’bythismeans (Fukunaga,1990).
4.1.7 The perceptron algorithm Anotherexampleofalineardiscriminantmodelistheperceptronof Rosenblatt (1962), which occupies an important place in the history of pattern recognition al- gorithms.
It corresponds to a two-class model in which the input vector x is first transformed using a fixed nonlinear transformation to give a feature vector φ(x), andthisisthenusedtoconstructageneralizedlinearmodeloftheform y(x)=f w Tφ(x) (4.52) 4.1.
Discriminant Functions 193 wherethenonlinearactivationfunctionf(·)isgivenbyastepfunctionoftheform +1, a 0 f(a)= (4.53) −1, a<0.
The vector φ(x) will typically include a bias component φ 0 (x) = 1.
In earlier discussionsoftwo-classclassificationproblems, wehavefocussedonatargetcoding scheme in which t ∈ {0,1}, which is appropriate in the context of probabilistic models.
For the perceptron, however, it is more convenient to use target values t=+1forclass C 1 andt=−1forclass C 2, whichmatchesthechoiceofactivation function.
The algorithm used to determine the parameters w of the perceptron can most easilybemotivatedbyerrorfunctionminimization.
Anaturalchoiceoferrorfunc- tionwouldbethetotalnumberofmisclassifiedpatterns.
However, thisdoesnotlead to a simple learning algorithm because the error is a piecewise constant function ofw, withdiscontinuitieswhereverachangeinw causesthedecisionboundaryto moveacrossoneofthedatapoints.
Methodsbasedonchangingw usingthegradi- entoftheerrorfunctioncannotthenbeapplied, becausethegradientiszeroalmost everywhere.
Wethereforeconsideranalternativeerrorfunctionknownastheperceptroncri- terion.
To derive this, we note that we are seeking a weight vector w such that patterns xn in class C 1 will have w Tφ(xn) > 0, whereas patterns xn in class C 2 have w Tφ(xn) < 0.
Using the t ∈ {−1,+1} target coding scheme it follows that we would like all patterns to satisfy w Tφ(xn)tn > 0.
The perceptron criterion associateszeroerrorwithanypatternthatiscorrectlyclassified, whereasforamis- classifiedpatternxn ittriestominimizethequantity−w Tφ(xn)tn.
Theperceptron criterionisthereforegivenby E P (w)=− w Tφ n tn (4.54) n∈M Frank Rosenblatt Seymour Papert.
This book was widely misinter- 1928–1969 preted at the time as showing that neural networks were fatally flawed and could only learn solutions for Rosenblatt’s perceptron played an linearly separable problems.
In fact, it only proved important role in the history of ma- such limitations in the case of single-layer networks chine learning.
Initially, Rosenblatt such as the perceptron and merely conjectured (in- simulatedtheperceptrononan IBM correctly) that they applied to more general network 704 computer at Cornell in 1957, models.
Unfortunately, however, thisbookcontributed but by the early 1960s he had built tothesubstantialdeclineinresearchfundingforneu- special-purposehardwarethatprovidedadirect, par- ral computing, a situation that was not reversed un- allel implementation of perceptron learning.
Many of til the mid-1980s.
Today, there are many hundreds, his ideas were encapsulated in “Principles of Neuro- if not thousands, of applications of neural networks dynamics: Perceptronsandthe Theoryof Brain Mech- in widespread use, with examples in areas such as anisms” published in 1962.
Rosenblatt’s work was handwriting recognition and information retrieval be- criticized by Marvin Minksy, whose objections were ingusedroutinelybymillionsofpeople.
publishedinthebook“Perceptrons”, co-authoredwith 194 4.
LINEARMODELSFORCLASSIFICATION where Mdenotesthesetofallmisclassifiedpatterns.
Thecontributiontotheerror associatedwithaparticularmisclassifiedpatternisalinearfunctionofwinregions ofwspacewherethepatternismisclassifiedandzeroinregionswhereitiscorrectly classified.
Thetotalerrorfunctionisthereforepiecewiselinear.
Section3.1.3 We now apply the stochastic gradient descent algorithm to this error function.
Thechangeintheweightvectorwisthengivenby w(τ+1) =w(τ)−η∇E P (w)=w(τ)+ηφ n tn (4.55) where η is the learning rate parameter and τ is an integer that indexes the steps of thealgorithm.
Becausetheperceptronfunctiony(x, w)isunchangedifwemultiply w by a constant, we can set the learning rate parameter η equal to 1 without of generality.
Notethat, astheweightvectorevolvesduringtraining, thesetofpatterns thataremisclassifiedwillchange.
The perceptron learning algorithm has a simple interpretation, as follows.
We cycle through the training patterns in turn, and for each pattern xn we evaluate the perceptron function (4.52).
If the pattern is correctly classified, then the weight vector remains unchanged, whereas if it is incorrectly classified, then for class C 1 we add the vector φ(xn) onto the current estimate of weight vector w while for class C 2 wesubtractthevectorφ(xn)fromw.
Theperceptronlearningalgorithmis illustratedin Figure4.7.
Ifweconsidertheeffectofasingleupdateintheperceptronlearningalgorithm, weseethatthecontributiontotheerrorfromamisclassifiedpatternwillbereduced becausefrom(4.55)wehave −w(τ+1)Tφ n tn =−w(τ)Tφ n tn −(φ n tn)Tφ n tn <−w(τ)Tφ n tn (4.56) where we have set η = 1, and made use of φ n tn 2 > 0.
Of course, this does not imply that the contribution to the error function from the other misclassified patternswillhavebeenreduced.
Furthermore, thechangeinweightvectormayhave caused some previously correctly classified patterns to become misclassified.
Thus the perceptron learning rule is not guaranteed to reduce the total error function at eachstage.
However, the perceptron convergence theorem states that if there exists an ex- act solution (in other words, if the training data set is linearly separable), then the perceptronlearningalgorithmisguaranteedtofindanexactsolutioninafinitenum- berofsteps.
Proofsofthistheoremcanbefoundforexamplein Rosenblatt(1962), Block (1962), Nilsson (1965), Minsky and Papert (1969), Hertz et al.
(1991), and Bishop (1995a).
Note, however, that the number of steps required to achieve con- vergence could still be substantial, and in practice, until convergence is achieved, we will not be able to distinguish between a nonseparable problem and one that is simplyslowtoconverge.
Evenwhenthedatasetislinearlyseparable, theremaybemanysolutions, and whichoneisfoundwilldependontheinitializationoftheparametersandontheor- derofpresentationofthedatapoints.
Furthermore, fordatasetsthatarenotlinearly separable, theperceptronlearningalgorithmwillneverconverge.
4.1.
Discriminant Functions 195 1 1 0.5 0.5 0 0 −0.5 −0.5 −1 −1 −1 −0.5 0 0.5 1 −1 −0.5 0 0.5 1 1 1 0.5 0.5 0 0 −0.5 −0.5 −1 −1 −1 −0.5 0 0.5 1 −1 −0.5 0 0.5 1 Figure4.7 Illustrationoftheconvergenceoftheperceptronlearningalgorithm, showingdatapointsfromtwo classes(redandblue)inatwo-dimensionalfeaturespace(φ 1 ,φ 2 ).
Thetopleftplotshowstheinitialparameter vector w shown as a black arrow together with the corresponding decision boundary (black line), in which the arrowpointstowardsthedecisionregionwhichclassifiedasbelongingtotheredclass.
Thedatapointcircled ingreenismisclassifiedandsoitsfeaturevectorisaddedtothecurrentweightvector, givingthenewdecision boundaryshowninthetoprightplot.
Thebottomleftplotshowsthenextmisclassifiedpointtobeconsidered, indicated by the green circle, and its feature vector is again added to the weight vector giving the decision boundaryshowninthebottomrightplotforwhichalldatapointsarecorrectlyclassified.
196 4.
LINEARMODELSFORCLASSIFICATION Figure 4.8 Illustration of the Mark 1 perceptron hardware.
The photograph on the left shows how the inputs were obtained using a simple camera system in which an input scene, in this case a printed character, was illuminated by powerful lights, and an image focussed onto a 20×20 array of cadmium sulphide photocells, giving a primitive 400 pixel image.
The perceptron also had a patch board, shown in the middle photograph, which allowed different configurations of input features to be tried.
Often these were wired up at random to demonstrate the ability of the perceptron to learn without the need for precise wiring, in contrast to a modern digital computer.
The photograph on the right shows one of the racks of adaptive weights.
Each weight was implemented using a rotary variable resistor, also called a potentiometer, driven by an electric motor thereby allowingthevalueoftheweighttobeadjustedautomaticallybythelearningalgorithm.
Asidefromdifficultieswiththelearningalgorithm, theperceptrondoesnotpro- videprobabilisticoutputs, nordoesitgeneralizereadilyto K >2classes.
Themost important limitation, however, arises from the fact that (in common with all of the models discussed in this chapter and the previous one) it is based on linear com- binations of fixed basis functions.
More detailed discussions of the limitations of perceptronscanbefoundin Minskyand Papert(1969)and Bishop(1995a).
Analoguehardwareimplementationsoftheperceptronwerebuiltby Rosenblatt, based on motor-driven variable resistors to implement the adaptive parameters wj.
Theseareillustratedin Figure4.8.
Theinputswereobtainedfromasimplecamera system based on an array of photo-sensors, while the basis functions φ could be choseninavarietyofways, forexamplebasedonsimplefixedfunctionsofrandomly chosensubsetsofpixelsfromtheinputimage.
Typicalapplicationsinvolvedlearning todiscriminatesimpleshapesorcharacters.
At the same time that the perceptron was being developed, a closely related system called the adaline, which is short for ‘adaptive linear element’, was being exploredby Widrowandco-workers.
Thefunctionalformofthemodelwasthesame asfortheperceptron, butadifferentapproachtotrainingwasadopted(Widrowand Hoff,1960; Widrowand Lehr,1990).
4.2.
Probabilistic Generative Models We turn next to a probabilistic view of classification and show how models with linear decision boundaries arise from simple assumptions about the distribution of the data.
In Section 1.5.4, we discussed the distinction between the discriminative and the generative approaches to classification.
Here we shall adopt a generative 4.2.
Probabilistic Generative Models 197 Figure4.9 Plotofthelogisticsigmoidfunction 1 σ(a) defined by (4.59), shown in red, together with the scaled pro- bit function Φ(λa), for λ2 = π/8, shownindashedblue, whereΦ(a) is defined by (4.114).
The scal- ingfactorπ/8ischosensothatthe 0.5 derivatives of the two curves are equalfora=0.
0 −5 0 5 approach in which we model the class-conditional densitiesp(x|C k), as well as the class priors p(C k), and then use these to compute posterior probabilities p(C k |x) through Bayes’theorem.
Consider first of all the case of two classes.
The posterior probability for class C 1 canbewrittenas p(x|C )p(C ) p(C |x) = 1 1 1 p(x|C )p(C )+p(x|C )p(C ) 1 1 2 2 1 = =σ(a) (4.57) 1+exp(−a) wherewehavedefined p(x|C )p(C ) 1 1 a=ln (4.58) p(x|C )p(C ) 2 2 andσ(a)isthelogisticsigmoidfunctiondefinedby 1 σ(a)= (4.59) 1+exp(−a) which is plotted in Figure 4.9.
The term ‘sigmoid’ means S-shaped.
This type of functionissometimesalsocalleda‘squashingfunction’becauseitmapsthewhole real axis into a finite interval.
The logistic sigmoid has been encountered already in earlier chapters and plays an important role in many classification algorithms.
It satisfiesthefollowingsymmetryproperty σ(−a)=1−σ(a) (4.60) asiseasilyverified.
Theinverseofthelogisticsigmoidisgivenby σ a=ln (4.61) 1−σ andisknownasthelogit function.
Itrepresentsthelogoftheratioofprobabilities ln[p(C 1 |x)/p(C 2 |x)]forthetwoclasses, alsoknownasthelogodds.
198 4.
LINEARMODELSFORCLASSIFICATION Note that in (4.57) we have simply rewritten the posterior probabilities in an equivalentform, andsotheappearanceofthelogisticsigmoidmayseemrathervac- uous.
However, it will have significance provided a(x) takes a simple functional form.
Weshallshortlyconsidersituationsinwhicha(x)isalinearfunctionofx, in whichcasetheposteriorprobabilityisgovernedbyageneralizedlinearmodel.
Forthecaseof K >2classes, wehave p(C k |x) = p j (x p( |C x k |C )p j) ( p C ( k C ) j) exp(ak) = (4.62) j exp(aj) which is known as the normalized exponential and can be regarded as a multiclass generalizationofthelogisticsigmoid.
Herethequantitiesak aredefinedby ak =lnp(x|C k)p(C k).
(4.63) The normalized exponential is also known as the softmax function, as it represents a smoothed version of the ‘max’ function because, if ak aj for all j = k, then p(C k |x) 1, andp(C j |x) 0.
We now investigate the consequences of choosing specific forms for the class- conditional densities, looking first at continuous input variables x and then dis- cussingbrieflythecaseofdiscreteinputs.
4.2.1 Continuous inputs Letusassumethattheclass-conditionaldensitiesare Gaussianandthenexplore theresultingformfortheposteriorprobabilities.
Tostartwith, weshallassumethat all classes share the same covariance matrix.
Thus the density for class C k is given by 1 1 1 p(x|C k)= (2π)D/2|Σ|1/2 exp − 2 (x−µ k )TΣ −1(x−µ k ) .
(4.64) Considerfirstthecaseoftwoclasses.
From(4.57)and(4.58), wehave p(C 1 |x)=σ(w Tx+w 0 ) (4.65) wherewehavedefined w = Σ −1(µ −µ ) (4.66) 1 2 1 1 p(C ) w 0 = − 2 µT 1 Σ −1µ 1 + 2 µT 2 Σ −1µ 2 +ln p(C 1 ) .
(4.67) 2 We see that the quadratic terms in x from the exponents of the Gaussian densities have cancelled (due to the assumption of common covariance matrices) leading to a linear function of x in the argument of the logistic sigmoid.
This result is illus- tratedforthecaseofatwo-dimensionalinputspacexin Figure4.10.
Theresulting 4.2.
Probabilistic Generative Models 199 Figure 4.10 The left-hand plot shows the class-conditional densities for two classes, denoted red and blue.
On the right is the corresponding posterior probability p(C 1 |x), which is given by a logistic sigmoid of a linear functionofx.
Thesurfaceintheright-handplotiscolouredusingaproportionofredinkgivenbyp(C 1 |x)anda proportionofblueinkgivenbyp(C 2 |x)=1−p(C 1 |x).
decision boundaries correspond to surfaces along which the posterior probabilities p(C k |x) are constant and so will be given by linear functions of x, and therefore thedecisionboundariesarelinearininputspace.
Thepriorprobabilitiesp(C k)enter only through the bias parameter w 0 so that changes in the priors have the effect of making parallel shifts of the decision boundary and more generally of the parallel contoursofconstantposteriorprobability.
Forthegeneralcaseof K classeswehave, from(4.62)and(4.63), ak(x)=w k Tx+wk0 (4.68) wherewehavedefined wk = Σ −1µ k (4.69) 1 wk0 = − µT k Σ −1µ k +lnp(C k).
(4.70) 2 Weseethattheak(x)areagainlinearfunctionsofxasaconsequenceofthecancel- lation of the quadratic terms due to the shared covariances.
The resulting decision boundaries, corresponding to the minimum misclassification rate, will occur when twoof theposterior probabilities (thetwolargest) areequal, andsowillbedefined bylinearfunctionsofx, andsoagainwehaveageneralizedlinearmodel.
Ifwerelaxtheassumptionofasharedcovariancematrixandalloweachclass- conditional density p(x|C k) to have its own covariance matrix Σk, then the earlier cancellationswillnolongeroccur, andwewillobtainquadraticfunctionsofx, giv- ing rise to a quadratic discriminant.
The linear and quadratic decision boundaries areillustratedin Figure4.11.
200 4.
LINEARMODELSFORCLASSIFICATION 2.5 2 1.5 1 0.5 0 −0.5 −1 −1.5 −2 −2.5 −2 −1 0 1 2 Figure4.11 Theleft-handplotshowstheclass-conditionaldensitiesforthreeclasseseachhavinga Gaussian distribution, colouredred, green, andblue, inwhichtheredandgreenclasseshavethesamecovariancematrix.
Theright-handplotshowsthecorrespondingposteriorprobabilities, inwhichthe RGBcolourvectorrepresents theposteriorprobabilitiesfortherespectivethreeclasses.
Thedecisionboundariesarealsoshown.
Noticethat the boundary between the red and green classes, which have the same covariance matrix, is linear, whereas thosebetweentheotherpairsofclassesarequadratic.
4.2.2 Maximum likelihood solution Once we have specified a parametric functional form for the class-conditional densitiesp(x|C k), wecanthendeterminethevaluesoftheparameters, togetherwith thepriorclassprobabilitiesp(C k), usingmaximumlikelihood.
Thisrequiresadata setcomprisingobservationsofxalongwiththeircorrespondingclasslabels.
Considerfirstthecaseoftwoclasses, eachhavinga Gaussianclass-conditional density with a shared covariance matrix, and suppose we have a data set {xn, tn } denotethepriorclassprobabilityp(C 1 )=π, sothatp(C 2 )=1−π.
Foradatapoint xn fromclass C 1, wehavetn =1andhence p(xn, C 1 )=p(C 1 )p(xn |C 1 )=πN(xn |µ 1 ,Σ).
Similarlyforclass C 2, wehavetn =0andhence p(xn, C 2 )=p(C 2 )p(xn |C 2 )=(1−π)N(xn |µ 2 ,Σ).
Thusthelikelihoodfunctionisgivenby N p(t|π,µ 1 ,µ 2 ,Σ)= [πN(xn |µ 1 ,Σ)] t n[(1−π)N(xn |µ 2 ,Σ)] 1−t n (4.71) n=1 where t = (t 1 ,..., t N)T.
As usual, it is convenient to maximize the log of the likelihoodfunction.
Considerfirstthemaximizationwithrespecttoπ.
Thetermsin 4.2.
Probabilistic Generative Models 201 theloglikelihoodfunctionthatdependonπare N {tnlnπ+(1−tn)ln(1−π)}.
(4.72) n=1 Settingthederivativewithrespecttoπequaltozeroandrearranging, weobtain N 1 N N 1 1 π = tn = = (4.73) N N N +N 1 2 n=1 where N 1denotesthetotalnumberofdatapointsinclass C 1, and N 2denotesthetotal number of data points in class C 2.
Thus the maximum likelihood estimate for π is simplythefractionofpointsinclass C 1asexpected.
Thisresultiseasilygeneralized to the multiclass case where again the maximum likelihood estimate of the prior probabilityassociatedwithclass C k isgivenbythefractionofthetrainingsetpoints Exercise 4.9 assignedtothatclass.
Now consider the maximization with respect to µ .
Again we can pick out of 1 theloglikelihoodfunctionthosetermsthatdependonµ giving 1 N N 1 tnln N(xn |µ 1 ,Σ)=− 2 tn(xn −µ 1 )TΣ −1(xn −µ 1 )+const.
(4.74) n=1 n=1 Settingthederivativewithrespecttoµ tozeroandrearranging, weobtain 1 N 1 µ 1 = N tnxn (4.75) 1 n=1 which is simply the mean of all the input vectors xn assigned to class C 1.
By a similarargument, thecorrespondingresultforµ isgivenby 2 N 1 µ 2 = N (1−tn)xn (4.76) 2 n=1 whichagainisthemeanofalltheinputvectorsxn assignedtoclass C 2.
Finally, consider the maximum likelihood solution for the shared covariance matrixΣ.
PickingoutthetermsintheloglikelihoodfunctionthatdependonΣ, we have N N 1 1 − 2 tnln|Σ|− 2 tn(xn −µ 1 )TΣ −1(xn −µ 1 ) n=1 n=1 N N 1 1 − 2 (1−tn)ln|Σ|− 2 (1−tn)(xn −µ 2 )TΣ −1(xn −µ 2 ) n=1 n=1 N N =− ln|Σ|− Tr Σ −1S (4.77) 2 2 202 4.
LINEARMODELSFORCLASSIFICATION wherewehavedefined N N 1 2 S = S 1 + S 2 (4.78) N N 1 S 1 = N (xn −µ 1 )(xn −µ 1 )T (4.79) 1 n∈C 1 1 S 2 = N (xn −µ 2 )(xn −µ 2 )T.
(4.80) 2 n∈C 2 Usingthestandardresultforthemaximumlikelihoodsolutionfora Gaussiandistri- bution, we see that Σ = S, which represents a weighted average of the covariance matricesassociatedwitheachofthetwoclassesseparately.
Thisresultiseasilyextendedtothe K classproblemtoobtainthecorresponding maximum likelihood solutions for the parameters in which each class-conditional Exercise 4.10 densityis Gaussianwithasharedcovariancematrix.
Notethattheapproachoffitting Gaussiandistributionstotheclassesisnotrobusttooutliers, becausethemaximum Section2.3.7 likelihoodestimationofa Gaussianisnotrobust.
4.2.3 Discrete features Let us now consider the case of discrete feature values xi.
For simplicity, we begin by looking at binary feature values xi ∈ {0,1} and discuss the extension to moregeneraldiscretefeaturesshortly.
Ifthereare Dinputs, thenageneraldistribu- tion would correspond to a table of 2D numbers for each class, containing 2D −1 independentvariables(duetothesummationconstraint).
Becausethisgrowsexpo- nentially with the number of features, we might seek a more restricted representa- Section8.2.2 tion.
Herewewillmakethenaive Bayesassumptioninwhichthefeaturevaluesare treatedasindependent, conditionedontheclass C k.
Thuswehaveclass-conditional distributionsoftheform D p(x|C k)= µ x ki i(1−µki)1−x i (4.81) i=1 whichcontain Dindependentparametersforeachclass.
Substitutinginto(4.63)then gives D ak(x)= {xilnµki+(1−xi)ln(1−µki)}+lnp(C k) (4.82) i=1 whichagainarelinearfunctionsoftheinputvaluesxi.
Forthecaseof K =2classes, wecanalternativelyconsiderthelogisticsigmoidformulationgivenby(4.57).
Anal- ogous results are obtained for discrete variables each of which can take M > 2 Exercise 4.11 states.
4.2.4 Exponential family Aswehaveseen, forboth Gaussiandistributedanddiscreteinputs, theposterior classprobabilitiesaregivenbygeneralizedlinearmodelswithlogisticsigmoid(K = 4.3.
Probabilistic Discriminative Models 203 2classes)orsoftmax(K 2classes)activationfunctions.
Theseareparticularcases of a more general result obtained by assuming that the class-conditional densities p(x|C k)aremembersoftheexponentialfamilyofdistributions.
Using the form (2.194) for members of the exponential family, we see that the distributionofxcanbewrittenintheform p(x|λ k)=h(x)g(λ k)exp λT k u(x) .
(4.83) Wenowrestrictattentiontothesubclassofsuchdistributionsforwhichu(x) = x.
Then we make use of (2.236) to introduce a scaling parameter s, so that we obtain therestrictedsetofexponentialfamilyclass-conditionaldensitiesoftheform 1 1 1 p(x|λ k, s)= h x g(λ k)exp λT k x .
(4.84) s s s Notethatweareallowingeachclasstohaveitsownparametervectorλ k butweare assumingthattheclassessharethesamescaleparameters.
Forthetwo-classproblem, wesubstitutethisexpressionfortheclass-conditional densitiesinto(4.58)andweseethattheposteriorclassprobabilityisagaingivenby alogisticsigmoidactingonalinearfunctiona(x)whichisgivenby a(x)=(λ 1 −λ 2 )Tx+lng(λ 1 )−lng(λ 2 )+lnp(C 1 )−lnp(C 2 ).
(4.85) Similarly, for the K-class problem, we substitute the class-conditional density ex- pressioninto(4.63)togive ak(x)=λT k x+lng(λ k)+lnp(C k) (4.86) andsoagainisalinearfunctionofx.
4.3.
Probabilistic Discriminative Models Forthetwo-classclassificationproblem, wehaveseenthattheposteriorprobability ofclass C 1 canbewrittenasalogisticsigmoidactingonalinearfunctionofx, fora widechoiceofclass-conditionaldistributionsp(x|C k).
Similarly, forthemulticlass case, theposteriorprobabilityofclass C k isgivenbyasoftmaxtransformationofa linearfunctionofx.
Forspecificchoicesoftheclass-conditionaldensitiesp(x|C k), we have used maximum likelihood to determine the parameters of the densities as wellastheclasspriorsp(C k)andthenused Bayes’theoremtofindtheposteriorclass probabilities.
However, analternativeapproachistousethefunctionalformofthegeneralized linear model explicitly and to determine its parameters directly by using maximum likelihood.
We shall see that there is an efficient algorithm finding such solutions knownasiterativereweightedleastsquares, or IRLS.
The indirect approach to finding the parameters of a generalized linear model, by fitting class-conditional densities and class priors separately and then applying 204 4.
LINEARMODELSFORCLASSIFICATION 1 1 φ2 x2 0 0.5 −1 0 −1 0 x1 1 0 0.5 φ1 1 Figure 4.12 Illustration of the role of nonlinear basis functions in linear classification models.
The left plot shows the original input space (x 1 , x 2 ) together with data points from two classes labelled red and blue.
Two ‘Gaussian’basisfunctionsφ 1 (x)andφ 2 (x)aredefinedinthisspacewithcentresshownbythegreencrosses and with contours shown by the green circles.
The right-hand plot shows the corresponding feature space (φ 1 ,φ 2 ) together with the linear decision boundary obtained given by a logistic regression model of the form discussed in Section 4.3.2.
This corresponds to a nonlinear decision boundary in the original input space, shownbytheblackcurveintheleft-handplot.
Bayes’ theorem, represents an example of generative modelling, because we could take such a model and generate synthetic data by drawing values of x from the marginal distribution p(x).
In the direct approach, we are maximizing a likelihood function defined through the conditional distribution p(C k |x), which represents a form of discriminative training.
One advantage of the discriminative approach is thattherewilltypicallybefeweradaptiveparameterstobedetermined, asweshall seeshortly.
Itmayalsoleadtoimprovedpredictiveperformance, particularlywhen theclass-conditionaldensityassumptionsgiveapoorapproximationtothetruedis- tributions.
4.3.1 Fixed basis functions So far in this chapter, we have considered classification models that work di- rectly with the original input vector x.
However, all of the algorithms are equally applicable if we first make a fixed nonlinear transformation of the inputs using a vector of basis functions φ(x).
The resulting decision boundaries will be linear in the feature space φ, and these correspond to nonlinear decision boundaries in the original x space, as illustrated in Figure 4.12.
Classes that are linearly separable in the feature space φ(x) need not be linearly separable in the original observation space x.
Note that as in our discussion of linear models for regression, one of the 4.3.
Probabilistic Discriminative Models 205 basisfunctionsistypicallysettoaconstant, sayφ 0 (x) = 1, sothatthecorrespond- ingparameterw 0 playstheroleofabias.
Fortheremainderofthischapter, weshall includeafixedbasisfunctiontransformationφ(x), asthiswillhighlightsomeuseful similaritiestotheregressionmodelsdiscussedin Chapter3.
For many problems of practical interest, there is significant overlap between the class-conditional densities p(x|C k).
This corresponds to posterior probabilities p(C k |x), which, foratleastsomevaluesofx, arenot0or1.
Insuchcases, theopti- malsolutionisobtainedbymodellingtheposteriorprobabilitiesaccuratelyandthen applying standard decision theory, as discussed in Chapter 1.
Note that nonlinear transformations φ(x) cannot remove such class overlap.
Indeed, they can increase thelevelofoverlap, orcreateoverlapwherenoneexistedintheoriginalobservation space.
However, suitablechoicesofnonlinearitycanmaketheprocessofmodelling theposteriorprobabilitieseasier.
Section3.6 Such fixed basis function models have important limitations, and these will be resolvedinlaterchaptersbyallowingthebasisfunctionsthemselvestoadapttothe data.
Notwithstandingtheselimitations, modelswithfixednonlinearbasisfunctions play an important role in applications, and a discussion of such models will intro- duce manyof thekey concepts needed for anunderstanding of their more complex counterparts.
4.3.2 Logistic regression Webeginourtreatmentofgeneralizedlinearmodelsbyconsideringtheproblem oftwo-classclassification.
Inourdiscussionofgenerativeapproachesin Section4.2, we saw that under rather general assumptions, the posterior probability of class C 1 canbewrittenasalogisticsigmoidactingonalinearfunctionofthefeaturevector φsothat p(C 1 |φ)=y(φ)=σ w Tφ (4.87) withp(C 2 |φ) = 1−p(C 1 |φ).
Hereσ(·)isthelogisticsigmoid functiondefinedby (4.59).
In the terminology of statistics, this model is known as logistic regression, although it should be emphasized that this is a model for classification rather than regression.
Foran M-dimensionalfeaturespaceφ, thismodelhas M adjustableparameters.
By contrast, if we had fitted Gaussian class conditional densities using maximum likelihood, we would have used 2M parameters for the means and M(M + 1)/2 parameters for the (shared) covariance matrix.
Together with the class prior p(C 1 ), thisgivesatotalof M(M+5)/2+1parameters, whichgrowsquadraticallywith M, in contrast to the linear dependence on M of the number of parameters in logistic regression.
For large values of M, there is a clear advantage in working with the logisticregressionmodeldirectly.
We now use maximum likelihood to determine the parameters of the logistic regressionmodel.
Todothis, weshallmakeuseofthederivativeofthelogisticsig- moidfunction, whichcanconvenientlybeexpressedintermsofthesigmoidfunction Exercise 4.12 itself dσ =σ(1−σ).
(4.88) da 206 4.
LINEARMODELSFORCLASSIFICATION For a data set {φ n , tn }, where tn ∈ {0,1} and φ n = φ(xn), with n = 1,..., N, thelikelihoodfunctioncanbewritten N p(t|w)= y n t n{1−yn }1−t n (4.89) n=1 where t = (t 1 ,..., t N)T and yn = p(C 1 |φ n ).
As usual, we can define an error function by taking the negative logarithm of the likelihood, which gives the cross- entropyerrorfunctionintheform N E(w)=−lnp(t|w)=− {tnlnyn+(1−tn)ln(1−yn)} (4.90) n=1 whereyn = σ(an)andan = w Tφ n .
Takingthegradientoftheerrorfunctionwith Exercise 4.13 respecttow, weobtain N ∇E(w)= (yn −tn)φ n (4.91) n=1 where we have made use of (4.88).
We see that the factor involving the derivative of the logistic sigmoid has cancelled, leading to a simplified form for the gradient of the log likelihood.
In particular, the contribution to the gradient from data point n is given by the ‘error’ yn −tn between the target value and the prediction of the model, times the basis function vector φ .
Furthermore, comparison with (3.13) n showsthatthistakespreciselythesameformasthegradientofthesum-of-squares Section3.1.1 errorfunctionforthelinearregressionmodel.
Ifdesired, wecouldmakeuseoftheresult(4.91)togiveasequentialalgorithm inwhichpatternsarepresentedoneatatime, inwhicheachoftheweightvectorsis updatedusing(3.22)inwhich∇En isthenth termin(4.91).
It is worth noting that maximum likelihood can exhibit severe over-fitting for datasetsthatarelinearlyseparable.
Thisarisesbecausethemaximumlikelihoodso- lutionoccurswhenthehyperplanecorrespondingtoσ = 0.5, equivalenttow Tφ = 0, separatesthetwoclassesandthemagnitudeofwgoestoinfinity.
Inthiscase, the logisticsigmoidfunctionbecomesinfinitelysteepinfeaturespace, correspondingto a Heavisidestepfunction, sothateverytrainingpointfromeachclassk isassigned Exercise 4.14 a posterior probability p(C k |x) = 1.
Furthermore, there is typically a continuum ofsuchsolutionsbecauseanyseparatinghyperplanewillgiverisetothesamepos- terior probabilities at the training data points, as will be seen later in Figure 10.13.
Maximumlikelihoodprovidesnowaytofavouronesuchsolutionoveranother, and which solution isfoundin practicewill dependon thechoice of optimization algo- rithm and on the parameter initialization.
Note that the problem will arise even if the number of data points is large compared with the number of parameters in the model, so long as the training data set is linearly separable.
The singularity can be avoidedbyinclusionofapriorandfindinga MAPsolutionforw, orequivalentlyby addingaregularizationtermtotheerrorfunction.
4.3.
Probabilistic Discriminative Models 207 4.3.3 Iterative reweighted least squares In the case of the linear regression models discussed in Chapter 3, the maxi- mum likelihood solution, on the assumption of a Gaussian noise model, leads to a closed-form solution.
This was a consequence of the quadratic dependence of the log likelihood function on the parameter vector w.
For logistic regression, there is no longer a closed-form solution, due to the nonlinearity of the logistic sigmoid function.
However, the departure from a quadratic form is not substantial.
To be precise, theerrorfunctionisconcave, asweshallseeshortly, andhencehasaunique minimum.
Furthermore, theerrorfunctioncanbeminimizedbyanefficientiterative techniquebasedonthe Newton-Raphsoniterativeoptimizationscheme, whichusesa localquadraticapproximationtotheloglikelihoodfunction.
The Newton-Raphson update, forminimizingafunction E(w), takestheform(Fletcher,1987; Bishopand Nabney,2008) w(new) =w(old)−H −1∇E(w).
(4.92) where H is the Hessian matrix whose elements comprise the second derivatives of E(w)withrespecttothecomponentsofw.
Let us first of all apply the Newton-Raphson method to the linear regression model(3.3)withthesum-of-squareserrorfunction(3.12).
Thegradientand Hessian ofthiserrorfunctionaregivenby N ∇E(w) = (w Tφ n −tn)φ n =ΦTΦw−ΦT t (4.93) n=1 N H=∇∇E(w) = φ φT =ΦTΦ (4.94) n n n=1 Section3.1.1 whereΦisthe N ×M designmatrix, whosenth rowisgivenbyφT .
The Newton- n Raphsonupdatethentakestheform w(new) = w(old)−(ΦTΦ) −1 ΦTΦw(old)−ΦT t = (ΦTΦ) −1ΦT t (4.95) whichwerecognizeasthestandardleast-squaressolution.
Notethattheerrorfunc- tioninthiscaseisquadraticandhencethe Newton-Raphsonformulagivestheexact solutioninonestep.
Nowletusapplythe Newton-Raphsonupdatetothecross-entropyerrorfunction (4.90) for the logistic regression model.
From (4.91) we see that the gradient and Hessianofthiserrorfunctionaregivenby N ∇E(w) = (yn −tn)φ n =ΦT(y−t) (4.96) n=1 N H = ∇∇E(w)= yn(1−yn)φ n φT n =ΦTRΦ (4.97) n=1 208 4.
LINEARMODELSFORCLASSIFICATION where we have made use of (4.88).
Also, we have introduced the N ×N diagonal matrix Rwithelements Rnn =yn(1−yn).
(4.98) Weseethatthe Hessianisnolongerconstantbutdependsonwthroughtheweight- ingmatrix R, correspondingtothefactthattheerrorfunctionisnolongerquadratic.
Usingtheproperty0<yn <1, whichfollowsfromtheformofthelogisticsigmoid function, weseethatu THu>0foranarbitraryvectoru, andsothe Hessianmatrix H is positive definite.
It follows that the error function is a concave function of w Exercise 4.15 andhencehasauniqueminimum.
The Newton-Raphsonupdateformulaforthelogisticregressionmodelthenbe- comes w(new) = w(old)−(ΦTRΦ) −1ΦT(y−t) = (ΦTRΦ) −1 ΦTRΦw(old)−ΦT(y−t) = (ΦTRΦ) −1ΦTRz (4.99) wherezisan N-dimensionalvectorwithelements z=Φw(old)−R −1(y−t).
(4.100) Weseethattheupdateformula(4.99)takestheformofasetofnormalequationsfora weightedleast-squaresproblem.
Becausetheweighingmatrix Risnotconstantbut dependsontheparametervectorw, wemustapplythenormalequationsiteratively, each time using the new weight vector w to compute a revised weighing matrix R.
Forthisreason, thealgorithmisknownasiterativereweightedleastsquares, or IRLS (Rubin, 1983).
Asintheweightedleast-squares problem, theelementsof the diagonalweightingmatrix Rcanbeinterpretedasvariancesbecausethemeanand varianceoftinthelogisticregressionmodelaregivenby E[t] = σ(x)=y (4.101) var[t] = E[t2]−E[t]2 =σ(x)−σ(x)2 =y(1−y) (4.102) wherewehaveusedthepropertyt2 =tfort∈{0,1}.
Infact, wecaninterpret IRLS as the solution to a linearized problem in the space of the variable a = w Tφ.
The quantityzn, whichcorrespondstothenth elementofz, canthenbegivenasimple interpretation as an effective target value in this space obtained by making a local linear approximation to the logistic sigmoid function around the current operating pointw(old) an(w) an(w(old))+ dan (tn −yn) dy n w(old) = φT n w(old)− y ( n y ( n 1 − − t y n n ) ) =zn.
(4.103) 4.3.
Probabilistic Discriminative Models 209 4.3.4 Multiclass logistic regression Section4.2 In our discussion of generative models for multiclass classification, we have seen that for a large class of distributions, the posterior probabilities are given by a softmaxtransformationoflinearfunctionsofthefeaturevariables, sothat p(C k |φ)=yk(φ)= exp(ak) (4.104) j exp(aj) wherethe‘activations’ak aregivenby ak =w k Tφ.
(4.105) There we used maximum likelihood to determine separately the class-conditional densitiesandtheclasspriorsandthenfoundthecorrespondingposteriorprobabilities using Bayes’theorem, therebyimplicitlydeterminingtheparameters{wk }.
Herewe consider the use of maximum likelihood to determine the parameters {wk } of this modeldirectly.
Todothis, wewillrequirethederivativesofyk withrespecttoallof Exercise 4.17 theactivationsaj.
Thesearegivenby ∂yk =yk(Ikj −yj) (4.106) ∂aj where Ikj aretheelementsoftheidentitymatrix.
Next we write down the likelihood function.
This is most easily done using the 1-of-K coding scheme in which the target vector tn for a feature vector φ n belongingtoclass C k isabinaryvectorwithallelementszeroexceptforelementk, whichequalsone.
Thelikelihoodfunctionisthengivenby N K N K p(T|w 1 ,..., w K)= p(C k |φ n ) t nk = y n t n k k (4.107) n=1k=1 n=1k=1 whereynk = yk(φ n ), and Tisan N ×K matrixoftargetvariableswithelements tnk.
Takingthenegativelogarithmthengives N K n=1k=1 which is known as the cross-entropy error function for the multiclass classification problem.
Wenowtakethegradientoftheerrorfunctionwithrespecttooneoftheparam- etervectorswj.
Makinguseoftheresult(4.106)forthederivativesofthesoftmax Exercise 4.18 function, weobtain N ∇ wj E(w 1 ,..., w K)= (ynj −tnj)φ n (4.109) n=1 210 4.
LINEARMODELSFORCLASSIFICATION wherewehavemadeuseof k tnk = 1.
Onceagain, weseethesameformarising for the gradient as was found for the sum-of-squares error function with the linear modelandthecross-entropyerrorforthelogisticregressionmodel, namelytheprod- uct of the error (ynj −tnj) times the basis function φ n .
Again, we could use this toformulateasequentialalgorithminwhichpatternsarepresentedoneatatime, in whicheachoftheweightvectorsisupdatedusing(3.22).
Wehaveseenthatthederivativeoftheloglikelihoodfunctionforalinearregres- sionmodelwithrespecttotheparameter vectorw for adatapointntooktheform of the ‘error’ yn −tn times the feature vector φ n .
Similarly, for the combination of logistic sigmoid activation function and cross-entropy error function (4.90), and for the softmax activation function with the multiclass cross-entropy error function (4.108), weagainobtainthissamesimpleform.
Thisisanexampleofamoregeneral result, asweshallseein Section4.3.6.
To find a batch algorithm, we again appeal to the Newton-Raphson update to obtain the corresponding IRLS algorithm for the multiclass problem.
This requires evaluation of the Hessian matrix that comprises blocks of size M × M in which blockj, kisgivenby N n=1 Aswiththetwo-classproblem, the Hessianmatrixforthemulticlasslogisticregres- Exercise 4.20 sionmodelispositivedefiniteandsotheerrorfunctionagainhasauniqueminimum.
Practicaldetailsof IRLSforthemulticlasscasecanbefoundin Bishopand Nabney (2008).
4.3.5 Probit regression Wehaveseenthat, forabroadrangeofclass-conditionaldistributions, described by the exponential family, the resulting posterior class probabilities are given by a logistic (or softmax) transformation acting on a linear function of the feature vari- ables.
However, notallchoicesofclass-conditionaldensitygiverisetosuchasimple formfortheposteriorprobabilities(forinstance, iftheclass-conditionaldensitiesare modelledusing Gaussianmixtures).
Thissuggeststhatitmightbeworthexploring other types of discriminative probabilistic model.
For the purposes of this chapter, however, we shall return to the two-class case, and again remain within the frame- workofgeneralizedlinearmodelssothat p(t=1|a)=f(a) (4.111) wherea=w Tφ, andf(·)istheactivationfunction.
Onewaytomotivateanalternativechoiceforthelinkfunctionistoconsider a noisythresholdmodel, asfollows.
Foreachinputφ n , weevaluatean =w Tφ n and thenwesetthetargetvalueaccordingto tn =1 ifan θ (4.112) tn =0 otherwise.
4.3.
Probabilistic Discriminative Models 211 Figure4.13 Schematic example of a probability density p(θ) 1 shown by the blue curve, given in this example by a mixture oftwo Gaussians, alongwithitscumulativedistributionfunction f(a), shown by the red curve.
Note that the value of the blue 0.8 curve at any point, such as that indicated by the vertical green line, correspondstotheslopeoftheredcurveatthesamepoint.
0.6 Conversely, thevalueoftheredcurveatthispointcorresponds totheareaunderthebluecurveindicatedbytheshadedgreen region.
Inthestochasticthresholdmodel, theclasslabeltakes 0.4 thevaluet=1ifthevalueofa=w Tφexceedsathreshold, oth- erwiseittakesthevaluet=0.
Thisisequivalenttoanactivation 0.2 functiongivenbythecumulativedistributionfunctionf(a).
0 0 1 2 3 4 If the value of θ is drawn from a probability density p(θ), then the corresponding activationfunctionwillbegivenbythecumulativedistributionfunction a f(a)= p(θ)dθ (4.113) −∞ asillustratedin Figure4.13.
As a specific example, suppose that the density p(θ) is given by a zero mean, unitvariance Gaussian.
Thecorrespondingcumulativedistributionfunctionisgiven by a Φ(a)= N(θ|0,1)dθ (4.114) −∞ which is known as the probit function.
It has a sigmoidal shape and is compared with the logistic sigmoid function in Figure 4.9.
Note that the use of a more gen- eral Gaussian distribution does not change the model because this is equivalent to a re-scaling of the linear coefficients w.
Many numerical packages provide for the evaluationofacloselyrelatedfunctiondefinedby a 2 erf(a)= √ exp(−θ2/2)dθ (4.115) π 0 and known as the erf function or error function (not to be confused with the error Exercise 4.21 functionofamachinelearningmodel).
Itisrelatedtotheprobitfunctionby 1 1 Φ(a)= 1+ √ erf(a) .
(4.116) 2 2 Thegeneralizedlinearmodelbasedonaprobitactivationfunctionisknownasprobit regression.
Wecandeterminetheparametersofthismodelusingmaximumlikelihood, bya straightforwardextensionoftheideasdiscussedearlier.
Inpractice, theresultsfound using probit regression tend to be similar to those of logistic regression.
We shall, 212 4.
LINEARMODELSFORCLASSIFICATION however, findanotherusefortheprobitmodelwhenwediscuss Bayesiantreatments oflogisticregressionin Section4.5.
One issue that can occur in practical applications is that of outliers, which can arise for instance through errors in measuring the input vector x or through misla- bellingofthetargetvaluet.
Becausesuchpointscanliealongwaytothewrongside oftheidealdecisionboundary, theycanseriouslydistorttheclassifier.
Notethatthe logistic and probit regression models behave differently in this respect because the tailsofthelogisticsigmoiddecayasymptoticallylikeexp(−x)forx→∞, whereas fortheprobitactivationfunctiontheydecaylikeexp(−x2), andsotheprobitmodel canbesignificantlymoresensitivetooutliers.
However, both the logistic and the probit models assume the data is correctly labelled.
Theeffectofmislabellingiseasilyincorporatedintoaprobabilisticmodel by introducing a probability that the target value t has been flipped to the wrong value(Opperand Winther,2000a), leadingtoatargetvaluedistributionfordatapoint xoftheform p(t|x) = (1− )σ(x)+ (1−σ(x)) = +(1−2 )σ(x) (4.117) where σ(x) is the activation function with input vector x.
Here may be set in advance, or it may be treated as a hyperparameter whose value is inferred from the data.
4.3.6 Canonical link functions For the linear regression model with a Gaussian noise distribution, the error function, correspondingtothenegativeloglikelihood, isgivenby(3.12).
Ifwetake thederivativewithrespecttotheparametervectorwofthecontributiontotheerror function from a data point n, this takes the form of the ‘error’ yn − tn times the featurevectorφ n , whereyn =w Tφ n .
Similarly, forthecombinationofthelogistic sigmoid activation function and the cross-entropy error function (4.90), and for the softmaxactivationfunctionwiththemulticlasscross-entropyerrorfunction(4.108), we again obtain this same simple form.
We now show that this is a general result of assuming a conditional distribution for the target variable from the exponential family, along with a corresponding choice for the activation function known as the canonicallinkfunction.
Weagainmakeuseoftherestrictedform(4.84)ofexponentialfamilydistribu- tions.
Notethathereweareapplyingtheassumptionofexponentialfamilydistribu- tion to the target variable t, in contrast to Section 4.2.4 where we applied it to the inputvectorx.
Wethereforeconsiderconditionaldistributionsofthetargetvariable oftheform 1 t ηt p(t|η, s)= h g(η)exp .
(4.118) s s s Usingthesamelineofargumentasledtothederivationoftheresult(2.226), wesee thattheconditionalmeanoft, whichwedenotebyy, isgivenby d y ≡E[t|η]=−s lng(η).
(4.119) dη 4.4.
The Laplace Approximation 213 Thusyandηmustrelated, andwedenotethisrelationthroughη =ψ(y).
Following Nelderand Wedderburn(1972), wedefineageneralizedlinearmodel tobeoneforwhichyisanonlinearfunctionofalinearcombinationoftheinput(or feature)variablessothat y =f(w Tφ) (4.120) wheref(·)isknownastheactivationfunctioninthemachinelearningliterature, and f−1(·)isknownasthelinkfunctioninstatistics.
Nowconsidertheloglikelihoodfunctionforthismodel, which, asafunctionof η, isgivenby N N lnp(t|η, s)= lnp(tn |η, s)= lng(ηn)+ ηntn +const (4.121) s n=1 n=1 whereweareassumingthatallobservationsshareacommonscaleparameter(which corresponds to the noise variance for a Gaussian distribution for instance) and so s is independent of n.
The derivative of the log likelihood with respect to the model parameterswisthengivenby N ∇ w lnp(t|η, s) = d lng(ηn)+ tn dηn dyn∇an dηn s dyndan n=1 N 1 = {tn −yn }ψ (yn)f (an)φ n (4.122) s n=1 wherean = w Tφ n , andwehaveusedyn = f(an)togetherwiththeresult(4.119) for E[t|η].
We now see that there is a considerable simplification if we choose a particularformforthelinkfunctionf−1(y)givenby f −1(y)=ψ(y) (4.123) which gives f(ψ(y)) = y and hence f (ψ)ψ (y) = 1.
Also, because a = f−1(y), we have a = ψ and hence f (a)ψ (y) = 1.
In this case, the gradient of the error functionreducesto N 1 ∇ln E(w)= {yn −tn }φ n .
(4.124) s n=1 Forthe Gaussians=β−1, whereasforthelogisticmodels=1.
4.4.
The Laplace Approximation In Section 4.5 we shall discuss the Bayesian treatment of logistic regression.
As we shall see, this is more complex than the Bayesian treatment of linear regression models, discussedin Sections3.3and3.5.
Inparticular, wecannotintegrateexactly 214 4.
LINEARMODELSFORCLASSIFICATION over the parameter vector w since the posterior distribution is no longer Gaussian.
It is therefore necessary to introduce some form of approximation.
Later in the Chapter10 book we shall consider a range of techniques based on analytical approximations Chapter11 andnumericalsampling.
Hereweintroduceasimple, butwidelyused, frameworkcalledthe Laplaceap- proximation, that aims to find a Gaussian approximation to a probability density definedoverasetofcontinuousvariables.
Considerfirstthecaseofasinglecontin- uousvariablez, andsupposethedistributionp(z)isdefinedby 1 p(z)= f(z) (4.125) Z where Z = f(z)dz is the normalization coefficient.
We shall suppose that the valueof Z isunknown.
Inthe Laplacemethodthegoalistofinda Gaussianapprox- imationq(z)whichiscentredonamodeofthedistributionp(z).
Thefirststepisto findamodeofp(z), inotherwordsapointz 0 suchthatp (z 0 )=0, orequivalently df(z) =0.
(4.126) dz z=z 0 AGaussiandistributionhasthepropertythatitslogarithmisaquadraticfunction ofthevariables.
Wethereforeconsidera Taylorexpansionoflnf(z)centredonthe modez 0 sothat 1 lnf(z) lnf(z 0 )− A(z−z 0 )2 (4.127) 2 where d2 A=− lnf(z) .
(4.128) dz2 z=z 0 Note that the first-order term in the Taylor expansion does not appear since z 0 is a localmaximumofthedistribution.
Takingtheexponentialweobtain A f(z) f(z 0 )exp − (z−z 0 )2 .
(4.129) 2 We can then obtain a normalized distribution q(z) by making use of the standard resultforthenormalizationofa Gaussian, sothat 1/2 A A q(z)= exp − (z−z 0 )2 .
(4.130) 2π 2 The Laplace approximation is illustrated in Figure 4.14.
Note that the Gaussian approximation will only be well defined if its precision A > 0, in other words the stationary point z 0 must be a local maximum, so that the second derivative off(z) atthepointz 0 isnegative.
4.4.
The Laplace Approximation 215 0.8 40 0.6 30 0.4 20 0.2 10 0 0 −2 −1 0 1 2 3 4 −2 −1 0 1 2 3 4 Figure4.14 Illustrationofthe Laplaceapproximationappliedtothedistributionp(z) ∝ exp(−z2/2)σ(20z+4) whereσ(z)isthelogisticsigmoidfunctiondefinedbyσ(z) = (1+e−z)−1.
Theleftplotshowsthenormalized distributionp(z)inyellow, togetherwiththe Laplaceapproximationcentredonthemodez 0 ofp(z)inred.
The rightplotshowsthenegativelogarithmsofthecorrespondingcurves.
Wecanextendthe Laplacemethodtoapproximateadistributionp(z)=f(z)/Z definedoveran M-dimensionalspacez.
Atastationarypointz 0thegradient∇f(z) willvanish.
Expandingaroundthisstationarypointwehave 1 lnf(z) lnf(z 0 )− (z−z 0 )TA(z−z 0 ) (4.131) 2 wherethe M ×M Hessianmatrix Aisdefinedby A=−∇∇lnf(z)| (4.132) z=z 0 and∇isthegradientoperator.
Takingtheexponentialofbothsidesweobtain 1 f(z) f(z 0 )exp − (z−z 0 )TA(z−z 0 ) .
(4.133) 2 Thedistributionq(z)isproportionaltof(z)andtheappropriatenormalizationcoef- ficientcanbefoundbyinspection, usingthestandardresult(2.43)foranormalized multivariate Gaussian, giving |A|1/2 1 q(z)= (2π)M/2 exp − 2 (z−z 0 )TA(z−z 0 ) =N(z|z 0 , A −1) (4.134) where |A| denotes the determinant of A.
This Gaussian distribution will be well definedprovideditsprecisionmatrix, givenby A, ispositivedefinite, whichimplies that the stationary point z 0 must be a local maximum, not a minimum or a saddle point.
In order to apply the Laplace approximation we first need to find the mode z 0, and then evaluate the Hessian matrix at that mode.
In practice a mode will typi- cally be found by running some form of numerical optimization algorithm (Bishop 216 4.
LINEARMODELSFORCLASSIFICATION and Nabney, 2008).
Many of the distributions encountered in practice will be mul- timodal and so there will be different Laplace approximations according to which modeisbeingconsidered.
Notethatthenormalizationconstant Z ofthetruedistri- butiondoesnotneedtobeknowninordertoapplythe Laplacemethod.
Asaresult of the central limit theorem, the posterior distribution for a model is expected to becomeincreasinglybetterapproximatedbya Gaussianasthenumberofobserved data points is increased, and so we would expect the Laplace approximation to be mostusefulinsituationswherethenumberofdatapointsisrelativelylarge.
Onemajorweaknessofthe Laplaceapproximationisthat, sinceitisbasedona Gaussian distribution, it is only directly applicable to real variables.
In other cases it may be possible to apply the Laplace approximation to a transformation of the variable.
Forinstanceif0 τ < ∞thenwecanconsidera Laplaceapproximation of lnτ.
The most serious limitation of the Laplace framework, however, is that it is based purely on the aspects of the true distribution at a specific value of the variable, and so can fail to capture important global properties.
In Chapter 10 we shallconsideralternativeapproacheswhichadoptamoreglobalperspective.
4.4.1 Model comparison and BIC As well as approximating the distribution p(z) we can also obtain an approxi- mationtothenormalizationconstant Z.
Usingtheapproximation(4.133)wehave Z = f(z)dz 1 f(z ) exp − (z−z )TA(z−z ) dz 0 0 0 2 (2π)M/2 = f(z 0 ) |A|1/2 (4.135) where we have noted that the integrand is Gaussian and made use of the standard result(2.43)foranormalized Gaussiandistribution.
Wecanusetheresult(4.135)to obtain an approximation to the model evidence which, as discussed in Section 3.4, playsacentralrolein Bayesianmodelcomparison.
Consider a data set D and a set of models {M i } having parameters {θ i }.
For each model we define a likelihood function p(D|θ i, M i).
If we introduce a prior p(θ i |M i) over the parameters, then we are interested in computing the model evi- dencep(D|M i)forthevariousmodels.
Fromnowonweomittheconditioningon M i to keep the notation uncluttered.
From Bayes’ theorem the model evidence is givenby p(D)= p(D|θ)p(θ)dθ.
(4.136) Identifyingf(θ) = p(D|θ)p(θ)and Z = p(D), andapplyingtheresult(4.135), we Exercise 4.22 obtain M 1 lnp(D) lnp(D|θ MAP )+lnp(θ MAP )+ ln(2π)− ln|A| (4.137) ( 2)* 2 + Occamfactor 4.5.
Bayesian Logistic Regression 217 whereθ MAP isthevalueofθ atthemodeoftheposteriordistribution, and Aisthe Hessianmatrixofsecondderivativesofthenegativelogposterior A=−∇∇lnp(D|θ MAP )p(θ MAP )=−∇∇lnp(θ MAP |D).
(4.138) The first term on the right hand side of (4.137) represents the log likelihood evalu- ated using the optimized parameters, while the remaining three terms comprise the ‘Occamfactor’whichpenalizesmodelcomplexity.
Ifweassumethatthe Gaussianpriordistributionoverparametersisbroad, and Exercise 4.23 thatthe Hessianhasfullrank, thenwecanapproximate(4.137)veryroughlyusing 1 lnp(D) lnp(D|θ MAP )− Mln N (4.139) 2 where N is the number of data points, M is the number of parameters in θ and we have omitted additive constants.
This is known as the Bayesian Information Criterion (BIC) or the Schwarz criterion (Schwarz, 1978).
Note that, compared to AICgivenby(1.73), thispenalizesmodelcomplexitymoreheavily.
Complexity measures such as AIC and BIC have the virtue of being easy to evaluate, butcanalsogivemisleadingresults.
Inparticular, theassumptionthatthe Hessianmatrixhasfullrankisoftennotvalidsincemanyoftheparametersarenot Section3.5.3 ‘well-determined’.
Wecanusetheresult(4.137)toobtainamoreaccurateestimate of the model evidence starting from the Laplace approximation, as we illustrate in thecontextofneuralnetworksin Section5.7.
4.5.
Bayesian Logistic Regression We now turn to a Bayesian treatment of logistic regression.
Exact Bayesian infer- ence for logistic regression is intractable.
In particular, evaluation of the posterior distributionwouldrequirenormalizationoftheproductofapriordistributionanda likelihoodfunctionthatitselfcomprisesaproductoflogisticsigmoidfunctions, one foreverydatapoint.
Evaluationofthepredictivedistributionissimilarlyintractable.
Here we consider the application of the Laplace approximation to the problem of Bayesianlogisticregression(Spiegelhalterand Lauritzen,1990; Mac Kay,1992b).
4.5.1 Laplace approximation Recall from Section 4.4 that the Laplace approximation is obtained by finding the mode of the posterior distribution and then fitting a Gaussian centred at that mode.
Thisrequiresevaluationofthesecondderivativesofthelogposterior, which isequivalenttofindingthe Hessianmatrix.
Because we seek a Gaussian representation for the posterior distribution, it is naturaltobeginwitha Gaussianprior, whichwewriteinthegeneralform p(w)=N(w|m 0 , S 0 ) (4.140) 218 4.
LINEARMODELSFORCLASSIFICATION where m 0 and S 0 are fixed hyperparameters.
The posterior distribution over w is givenby p(w|t)∝p(w)p(t|w) (4.141) wheret=(t 1 ,..., t N)T.
Takingthelogofbothsides, andsubstitutingfortheprior distributionusing(4.140), andforthelikelihoodfunctionusing(4.89), weobtain 1 lnp(w|t) = − 2 (w−m 0 )TS − 0 1(w−m 0 ) N + {tnlnyn+(1−tn)ln(1−yn)}+const (4.142) n=1 where yn = σ(w Tφ n ).
To obtain a Gaussian approximation to the posterior dis- tribution, we first maximize the posterior distribution to give the MAP (maximum posterior)solutionw MAP, whichdefinesthemeanofthe Gaussian.
Thecovariance is then given by the inverse of the matrix of second derivatives of the negative log likelihood, whichtakestheform N SN =−∇∇lnp(w|t)=S − 0 1+ yn(1−yn)φ n φT n .
(4.143) n=1 The Gaussianapproximationtotheposteriordistributionthereforetakestheform q(w)=N(w|w MAP , SN).
(4.144) Having obtained a Gaussian approximation to the posterior distribution, there remains the task of marginalizing with respect to this distribution in order to make predictions.
4.5.2 Predictive distribution The predictive distribution for class C 1, given a new feature vector φ(x), is obtainedbymarginalizingwithrespecttotheposteriordistributionp(w|t), whichis itselfapproximatedbya Gaussiandistributionq(w)sothat p(C 1 |φ, t)= p(C 1 |φ, w)p(w|t)dw σ(w Tφ)q(w)dw (4.145) withthecorrespondingprobabilityforclass C 2givenbyp(C 2 |φ, t)=1−p(C 1 |φ, t).
To evaluate the predictive distribution, we first note that the function σ(w Tφ) de- pendsonwonlythroughitsprojectionontoφ.
Denotinga=w Tφ, wehave σ(w Tφ)= δ(a−w Tφ)σ(a)da (4.146) whereδ(·)isthe Diracdeltafunction.
Fromthisweobtain σ(w Tφ)q(w)dw = σ(a)p(a)da (4.147) 4.5.
Bayesian Logistic Regression 219 where p(a)= δ(a−w Tφ)q(w)dw.
(4.148) We can evaluate p(a) by noting that the delta function imposes a linear constraint onw andsoformsamarginaldistributionfromthejointdistributionq(w)byinte- gratingoutalldirectionsorthogonaltoφ.
Becauseq(w)is Gaussian, weknowfrom Section 2.3.2 that the marginal distribution will also be Gaussian.
We can evaluate the mean and covariance of this distribution by taking moments, and interchanging theorderofintegrationoveraandw, sothat µa =E[a]= p(a)ada= q(w)w Tφdw =w M T AP φ (4.149) wherewehaveusedtheresult(4.144)forthevariationalposteriordistributionq(w).
Similarly σ2 = var[a]= p(a) a2−E[a]2 da a = q(w) (w Tφ)2−(m T N φ)2 dw =φT SN φ.
(4.150) Note that the distribution of a takes the same form as the predictive distribution (3.58)forthelinearregressionmodel, withthenoisevariancesettozero.
Thusour variationalapproximationtothepredictivedistributionbecomes p(C 1 |t)= σ(a)p(a)da= σ(a)N(a|µa,σ a 2)da.
(4.151) Thisresultcanalsobederiveddirectlybymakinguseoftheresultsforthemarginal Exercise 4.24 ofa Gaussiandistributiongivenin Section2.3.2.
Theintegraloverarepresentstheconvolutionofa Gaussianwithalogisticsig- moid, andcannotbeevaluatedanalytically.
Wecan, however, obtainagoodapprox- imation (Spiegelhalter and Lauritzen, 1990; Mac Kay, 1992b; Barber and Bishop, 1998a) by making use of the close similarity between the logistic sigmoid function σ(a)definedby(4.59)andtheprobitfunctionΦ(a)definedby(4.114).
Inorderto obtain the best approximation to the logistic function we need to re-scale the hori- zontalaxis, sothatweapproximateσ(a)byΦ(λa).
Wecanfindasuitablevalueof λbyrequiringthatthetwofunctionshavethesameslopeattheorigin, whichgives Exercise 4.25 λ2 = π/8.
The similarity of the logistic sigmoid and the probit function, for this choiceofλ, isillustratedin Figure4.9.
Theadvantageofusingaprobitfunctionisthatitsconvolutionwitha Gaussian can be expressed analytically in terms of another probit function.
Specifically we Exercise 4.26 canshowthat µ Φ(λa)N(a|µ,σ2)da=Φ .
(4.152) (λ−2+σ2)1/2 220 4.
LINEARMODELSFORCLASSIFICATION We now apply the approximation σ(a) Φ(λa) to the probit functions appearing onbothsidesofthisequation, leadingtothefollowingapproximationfortheconvo- lutionofalogisticsigmoidwitha Gaussian σ(a)N(a|µ,σ2)da σ κ(σ2)µ (4.153) wherewehavedefined κ(σ2)=(1+πσ2/8) −1/2.
(4.154) Applyingthisresultto(4.151)weobtaintheapproximatepredictivedistribution intheform p(C 1 |φ, t)=σ κ(σ a 2)µa (4.155) where µa and σ a 2 are defined by (4.149) and (4.150), respectively, and κ(σ a 2) is de- finedby(4.154).
Note that the decision boundary corresponding to p(C 1 |φ, t) = 0.5 is given by µa = 0, which is the same as the decision boundary obtained by using the MAP value for w.
Thus if the decision criterion is based on minimizing misclassifica- tion rate, with equal prior probabilities, then the marginalization over w has no ef- fect.
However, for more complex decision criteria it will play an important role.
Marginalization of the logistic sigmoid model under a Gaussian approximation to theposteriordistributionwillbeillustratedinthecontextofvariationalinferencein Figure10.13.
Exercises 4.1 ( ) Givenasetofdatapoints{xn }, wecandefinetheconvexhulltobethesetof allpointsxgivenby x= αnxn (4.156) n whereαn 0and n αn =1.
Considerasecondsetofpoints{yn }togetherwith theircorrespondingconvexhull.
Bydefinition, thetwosetsofpointswillbelinearly separableifthereexistsavectorw andascalarw 0suchthatw Txn+w 0 >0forall xn, andw Tyn+w 0 <0forallyn.
Showthatiftheirconvexhullsintersect, thetwo sets of points cannot be linearly separable, and conversely that if they are linearly separable, theirconvexhullsdonotintersect.
4.2 ( ) www Considertheminimizationofasum-of-squareserrorfunction(4.15), andsupposethatallofthetargetvectorsinthetrainingsetsatisfyalinearconstraint a Ttn+b=0 (4.157) where tn corresponds to the nth row of the matrix T in (4.15).
Show that as a consequenceofthisconstraint, theelementsofthemodelpredictiony(x)givenby theleast-squaressolution(4.17)alsosatisfythisconstraint, sothat a Ty(x)+b=0.
(4.158) Exercises 221 Todoso, assumethatoneofthebasisfunctionsφ 0 (x)=1sothatthecorresponding parameterw 0 playstheroleofabias.
4.3 ( ) Extend the result of Exercise 4.2 to show that if multiple linear constraints aresatisfiedsimultaneouslybythetargetvectors, thenthesameconstraintswillalso besatisfiedbytheleast-squarespredictionofalinearmodel.
4.4 ( ) www Showthatmaximizationoftheclassseparationcriteriongivenby(4.23) withrespecttow, usinga Lagrangemultipliertoenforcetheconstraintw Tw = 1, leadstotheresultthatw ∝(m 2 −m 1 ).
canbewrittenintheform(4.26).
4.6 ( ) Usingthedefinitionsofthebetween-classandwithin-classcovariancematrices given by (4.27) and (4.28), respectively, together with (4.34) and (4.36) and the choice of target values described in Section 4.1.5, show that the expression (4.33) thatminimizesthesum-of-squareserrorfunctioncanbewrittenintheform(4.37).
4.7 ( ) www Show that the logistic sigmoid function (4.59) satisfies the property σ(−a)=1−σ(a)andthatitsinverseisgivenbyσ−1(y)=ln{y/(1−y)}.
4.8 ( ) Using(4.57)and(4.58), derivetheresult(4.65)fortheposteriorclassprobability in the two-class generative model with Gaussian densities, and verify the results (4.66)and(4.67)fortheparameterswandw 0.
4.9 ( ) www Consider a generative classification model for K classes defined by priorclassprobabilitiesp(C k)=πk andgeneralclass-conditionaldensitiesp(φ|C k) whereφistheinputfeaturevector.
Supposewearegivenatrainingdataset{φ n , tn } wheren=1,..., N, andtn isabinarytargetvectoroflength K thatusesthe1-of- K codingscheme, sothatithascomponentstnj =Ijk ifpatternnisfromclass C k.
Assuming that the data points are drawn independently from this model, show that themaximum-likelihoodsolutionforthepriorprobabilitiesisgivenby Nk πk = (4.159) N where Nk isthenumberofdatapointsassignedtoclass C k.
4.10 ( ) Consider the classification model of Exercise 4.9 and now suppose that the class-conditionaldensitiesaregivenby Gaussiandistributionswithasharedcovari- ancematrix, sothat p(φ|C k)=N(φ|µ k ,Σ).
(4.160) Showthatthemaximumlikelihoodsolutionforthemeanofthe Gaussiandistribution forclass C k isgivenby N 1 µ k = tnk φ n (4.161) Nk n=1 222 4.
LINEARMODELSFORCLASSIFICATION which represents the mean of those feature vectors assigned to class C k.
Similarly, showthatthemaximumlikelihoodsolutionforthesharedcovariancematrixisgiven by K Nk Σ= Sk (4.162) N k=1 where N 1 Sk = tnk(φ n −µ k )(φ n −µ k )T.
(4.163) Nk n=1 ThusΣisgivenbyaweightedaverageofthecovariancesofthedataassociatedwith eachclass, inwhichtheweightingcoefficientsaregivenbythepriorprobabilitiesof theclasses.
4.11 ( ) Consideraclassificationproblemwith K classesforwhichthefeaturevector φhas M componentseachofwhichcantake Ldiscretestates.
Letthevaluesofthe componentsberepresentedbya1-of-Lbinarycodingscheme.
Furthersupposethat, conditioned on the class C k, the M components of φ are independent, so that the class-conditional density factorizes with respect to the feature vector components.
Show that the quantities ak given by (4.63), which appear in the argument to the softmaxfunctiondescribingtheposteriorclassprobabilities, arelinearfunctionsof thecomponentsofφ.
Notethatthisrepresentsanexampleofthenaive Bayesmodel whichisdiscussedin Section8.2.2.
4.12 ( ) www Verifytherelation(4.88)forthederivativeofthelogisticsigmoidfunc- tiondefinedby(4.59).
4.13 ( ) www Bymakinguseoftheresult(4.88)forthederivativeofthelogisticsig- moid, showthatthederivativeoftheerrorfunction(4.90)forthelogisticregression modelisgivenby(4.91).
4.14 ( ) Show that for a linearly separable data set, the maximum likelihood solution for the logistic regression model is obtained by finding a vector w whose decision boundaryw Tφ(x) = 0separatestheclassesandthentakingthemagnitudeofwto infinity.
4.15 ( ) Show that the Hessian matrix H for the logistic regression model, given by (4.97), is positive definite.
Here R is a diagonal matrix with elements yn(1−yn), andynistheoutputofthelogisticregressionmodelforinputvectorxn.
Henceshow thattheerrorfunctionisaconcavefunctionofwandthatithasauniqueminimum.
4.16 ( ) Considerabinaryclassificationprobleminwhicheachobservationxnisknown tobelongtooneoftwoclasses, correspondingtot = 0andt = 1, andsupposethat the procedure for collecting training data is imperfect, so that training points are sometimesmislabelled.
Foreverydatapointxn, insteadofhavingavaluetforthe class label, we have instead a value πn representing the probability that tn = 1.
Given a probabilistic model p(t = 1|φ), write down the log likelihood function appropriatetosuchadataset.
Exercises 223 4.17 ( ) www Show that the derivatives of the softmax activation function (4.104), wheretheak aredefinedby(4.105), aregivenby(4.106).
4.18 ( ) Using the result (4.91) for the derivatives of the softmax activation function, showthatthegradientsofthecross-entropyerror(4.108)aregivenby(4.109).
4.19 ( ) www Write down expressions for the gradient of the log likelihood, as well asthecorresponding Hessianmatrix, fortheprobitregressionmodeldefinedin Sec- tion4.3.5.
Thesearethequantitiesthatwouldberequiredtotrainsuchamodelusing IRLS.
4.20 ( ) Show that the Hessian matrix for the multiclass logistic regression problem, defined by (4.110), is positive semidefinite.
Note that the full Hessian matrix for this problem is of size MK ×MK, where M is the number of parameters and K is the number of classes.
To prove the positive semidefinite property, consider the productu THuwhereuisanarbitraryvectoroflength MK, andthenapply Jensen’s inequality.
4.21 ( ) Showthattheprobitfunction(4.114)andtheerffunction(4.115)arerelatedby (4.116).
4.22 ( ) Using the result (4.135), derive the expression (4.137) for the log model evi- denceunderthe Laplaceapproximation.
4.23 ( ) www In this exercise, we derive the BIC result (4.139) starting from the Laplace approximation to the model evidence given by (4.137).
Show that if the prior over parameters is Gaussian of the form p(θ) = N(θ|m, V 0 ), the log model evidenceunderthe Laplaceapproximationtakestheform 1 1 lnp(D) lnp(D|θ )− (θ −m)TV −1(θ −m)− ln|H|+const MAP 2 MAP 0 MAP 2 where Histhematrixofsecondderivativesoftheloglikelihoodlnp(D|θ)evaluated at θ MAP.
Now assume that the prior is broad so that V 0 −1 is small and the second termontheright-handsideabovecanbeneglected.
Furthermore, considerthecase ofindependent, identicallydistributeddatasothat Histhesumoftermsoneforeach datapoint.
Show that thelogmodel evidence canthen bewritten approximately in theformofthe BICexpression(4.139).
4.24 ( ) Usetheresultsfrom Section2.3.2toderivetheresult(4.151)forthemarginal- izationofthelogisticregressionmodelwithrespecttoa Gaussianposteriordistribu- tionovertheparametersw.
4.25 ( ) Supposewewishtoapproximatethelogisticsigmoidσ(a)definedby(4.59) by a scaled probit function Φ(λa), where Φ(a) is defined by (4.114).
Show that if λ is chosen so that the derivatives of the two functions are equal at a = 0, then λ2 =π/8.
224 4.
LINEARMODELSFORCLASSIFICATION 4.26 ( ) Inthisexercise, weprovetherelation(4.152)fortheconvolutionofaprobit functionwitha Gaussiandistribution.
Todothis, showthatthederivativeoftheleft- handsidewithrespecttoµisequaltothederivativeoftheright-handside, andthen integratebothsideswithrespecttoµandthenshowthattheconstantofintegration vanishes.
Note that before differentiating the left-hand side, it is convenient first to introduce a change of variable given by a = µ+σz so that the integral over a is replaced by an integral over z.
When we differentiate the left-hand side of the relation(4.152), wewillthenobtaina Gaussianintegraloverzthatcanbeevaluated analytically.
5 Neural Networks In Chapters3and4weconsideredmodelsforregressionandclassificationthatcom- prised linear combinations of fixed basis functions.
We saw that such models have useful analytical and computational properties but that their practical applicability waslimitedbythecurseofdimensionality.
Inordertoapplysuchmodelstolarge- scaleproblems, itisnecessarytoadaptthebasisfunctionstothedata.
Support vector machines (SVMs), discussed in Chapter 7, address this by first definingbasisfunctionsthatarecentredonthetrainingdatapointsandthenselecting a subset of these during training.
One advantage of SVMs is that, although the traininginvolvesnonlinearoptimization, theobjectivefunctionisconvex, andsothe solution of the optimization problem is relatively straightforward.
The number of basisfunctionsintheresultingmodelsisgenerallymuchsmallerthanthenumberof trainingpoints, althoughitisoftenstillrelativelylargeandtypicallyincreaseswith thesizeofthetrainingset.
Therelevancevectormachine, discussedin Section7.2, alsochoosesasubsetfromafixedsetofbasisfunctionsandtypicallyresultsinmuch 225 226 5.
NEURALNETWORKS sparsermodels.
Unlikethe SVMitalsoproducesprobabilisticoutputs, althoughthis isattheexpenseofanonconvexoptimizationduringtraining.
An alternative approach is to fix the number of basis functions in advance but allowthemtobeadaptive, inotherwordstouseparametricformsforthebasisfunc- tionsinwhichtheparametervaluesareadaptedduringtraining.
Themostsuccessful model of this type in the context of pattern recognition is the feed-forward neural network, alsoknownasthemultilayerperceptron, discussedinthischapter.
Infact, ‘multilayer perceptron’ is really a misnomer, because the model comprises multi- plelayersoflogisticregressionmodels(withcontinuousnonlinearities) ratherthan multipleperceptrons(withdiscontinuousnonlinearities).
Formanyapplications, the resultingmodelcanbesignificantlymorecompact, andhencefastertoevaluate, than asupportvectormachinehavingthesamegeneralizationperformance.
Thepriceto bepaidforthiscompactness, aswiththerelevancevectormachine, isthatthelike- lihood function, which forms the basis for network training, is no longer a convex function of the model parameters.
In practice, however, it is often worth investing substantial computational resources during the training phase in order to obtain a compactmodelthatisfastatprocessingnewdata.
The term ‘neural network’ has its origins in attempts to find mathematical rep- resentations of information processing in biological systems (Mc Culloch and Pitts, 1943; Widrow and Hoff, 1960; Rosenblatt, 1962; Rumelhart et al., 1986).
Indeed, it has been used very broadly to cover a wide range of different models, many of which have been the subject of exaggerated claims regarding their biological plau- sibility.
From the perspective of practical applications of pattern recognition, how- ever, biologicalrealismwouldimposeentirelyunnecessaryconstraints.
Ourfocusin thischapteristhereforeonneuralnetworksasefficientmodelsforstatisticalpattern recognition.
Inparticular, weshallrestrictourattentiontothespecificclassofneu- ralnetworksthathaveproventobeofgreatestpracticalvalue, namelythemultilayer perceptron.
We begin by considering the functional form of the network model, including the specific parameterization of the basis functions, and we then discuss the prob- lem of determining the network parameters within a maximum likelihood frame- work, whichinvolvesthesolutionofanonlinearoptimizationproblem.
Thisrequires the evaluation of derivatives of the log likelihood function with respect to the net- work parameters, and we shall see how these can be obtained efficiently using the technique of error backpropagation.
We shall also show how the backpropagation framework can be extended to allow other derivatives to be evaluated, such as the Jacobian and Hessian matrices.
Next we discuss various approaches to regulariza- tionofneuralnetworktrainingandtherelationshipsbetweenthem.
Wealsoconsider some extensions to the neural network model, and in particular we describe a gen- eralframeworkformodellingconditionalprobabilitydistributionsknownasmixture density networks.
Finally, we discuss the use of Bayesian treatments of neural net- works.
Additional background on neural network models can be found in Bishop (1995a).
5.1.
Feed-forward Network Functions 227 5.1.
Feed-forward Network Functions Thelinearmodelsforregressionandclassificationdiscussedin Chapters3and4, re- spectively, arebasedonlinearcombinationsoffixednonlinearbasisfunctionsφj(x) andtaketheform M y(x, w)=f wjφj(x) (5.1) j=1 where f(·) is a nonlinear activation function in the case of classification and is the identity in the case of regression.
Our goal is to extend this model by making the basis functions φj(x) depend on parameters and then to allow these parameters to beadjusted, alongwiththecoefficients{wj }, duringtraining.
Thereare, ofcourse, many ways to construct parametric nonlinear basis functions.
Neural networks use basisfunctionsthatfollowthesameformas(5.1), sothateachbasisfunctionisitself anonlinearfunctionofalinearcombinationoftheinputs, wherethecoefficientsin thelinearcombinationareadaptiveparameters.
This leads to the basic neural network model, which can be described a series offunctionaltransformations.
Firstweconstruct M linearcombinationsoftheinput variablesx 1 ,..., x D intheform D (1) (1) aj = w ji xi+w j0 (5.2) i=1 wherej =1,..., M, andthesuperscript(1)indicatesthatthecorrespondingparam- (1) etersareinthefirst‘layer’ofthenetwork.
Weshallrefertotheparametersw as ji (1) weightsandtheparametersw asbiases, followingthenomenclatureof Chapter3.
j0 Thequantitiesaj areknownasactivations.
Eachofthemisthentransformedusing adifferentiable, nonlinearactivationfunctionh(·)togive zj =h(aj).
(5.3) Thesequantitiescorrespondtotheoutputsofthebasisfunctionsin(5.1)that, inthe contextofneuralnetworks, arecalledhiddenunits.
Thenonlinearfunctionsh(·)are generallychosentobesigmoidalfunctionssuchasthelogisticsigmoidorthe‘tanh’ Exercise 5.1 function.
Following (5.1), these values are again linearly combined to give output unitactivations M (2) (2) ak = w kj zj +w k0 (5.4) j=1 wherek =1,..., K, and K isthetotalnumberofoutputs.
Thistransformationcor- (2) respondstothesecondlayerofthenetwork, andagainthew arebiasparameters.
k0 Finally, the output unit activations are transformed using an appropriate activation function to give a set of network outputs yk.
The choice of activation function is determinedbythenatureofthedataandtheassumeddistributionoftargetvariables 228 5.
NEURALNETWORKS Figure5.1 Network diagram for the two- hidden units layer neural network corre- z M sponding to (5.7).
The input, (1) w hidden, and output variables MD w (2) KM are represented by nodes, and x D theweightparametersarerep- y K resented by links between the nodes, in which the bias pa- inputs outputs rameters are denoted by links coming from additional input and hidden variables x 0 and y1 z 0.
Arrows denote the direc- x1 tionofinformationflowthrough the network during forward propagation.
z1 w (2) x0 10 z0 andfollowsthesameconsiderationsasforlinearmodelsdiscussedin Chapters3and 4.
Thus for standard regression problems, the activation function is the identity so thatyk =ak.
Similarly, formultiplebinaryclassificationproblems, eachoutputunit activationistransformedusingalogisticsigmoidfunctionsothat yk =σ(ak) (5.5) where 1 σ(a)= .
(5.6) 1+exp(−a) Finally, for multiclass problems, a softmax activation function of the form (4.62) is used.
The choice of output unit activation function is discussed in detail in Sec- tion5.2.
We can combine these various stages to give the overall network function that, forsigmoidaloutputunitactivationfunctions, takestheform M D (2) (1) (1) (2) yk(x, w)=σ w kj h w ji xi+w j0 +w k0 (5.7) j=1 i=1 where the set of all weight and bias parameters have been grouped together into a vectorw.
Thustheneuralnetworkmodelissimplyanonlinearfunctionfromaset ofinputvariables{xi }toasetofoutputvariables{yk }controlledbyavectorwof adjustableparameters.
This function can be represented in the form of a network diagram as shown in Figure 5.1.
The process of evaluating (5.7) can then be interpreted as a forward propagationofinformationthroughthenetwork.
Itshouldbeemphasizedthatthese diagrams do not represent probabilistic graphical models of the kind to be consid- eredin Chapter8becausetheinternalnodesrepresentdeterministicvariablesrather thanstochasticones.
Forthisreason, wehaveadoptedaslightlydifferentgraphical 5.1.
Feed-forward Network Functions 229 notation for the two kinds of model.
We shall see later how to give a probabilistic interpretationtoaneuralnetwork.
As discussed in Section 3.1, the bias parameters in (5.2) can be absorbed into thesetofweightparametersbydefininganadditionalinputvariablex 0 whosevalue isclampedatx 0 =1, sothat(5.2)takestheform D (1) aj = w ji xi.
(5.8) i=0 We can similarly absorb the second-layer biases into the second-layer weights, so thattheoverallnetworkfunctionbecomes M D (2) (1) yk(x, w)=σ w kj h w ji xi .
(5.9) j=0 i=0 Ascanbeseenfrom Figure5.1, theneuralnetworkmodelcomprisestwostages of processing, each of which resembles the perceptron model of Section 4.1.7, and for this reason the neural network is also known as the multilayer perceptron, or MLP.
Akeydifferencecomparedtotheperceptron, however, isthattheneuralnet- workusescontinuoussigmoidalnonlinearitiesinthehiddenunits, whereastheper- ceptronusesstep-functionnonlinearities.
Thismeansthattheneuralnetworkfunc- tion is differentiable with respect to the network parameters, and this property will playacentralroleinnetworktraining.
If the activation functions of all the hidden units in a network are taken to be linear, thenforanysuchnetworkwecanalwaysfindanequivalentnetworkwithout hidden units.
This follows from the fact that the composition of successive linear transformations is itself a linear transformation.
However, if the number of hidden unitsissmallerthaneitherthenumberofinputoroutputunits, thenthetransforma- tions that the network can generate are not the most general possible linear trans- formations from inputs to outputs because information is lost in the dimensionality reduction at the hidden units.
In Section 12.4.2, we show that networks of linear units give rise to principal component analysis.
In general, however, there is little interestinmultilayernetworksoflinearunits.
The network architecture shown in Figure 5.1 is the most commonly used one inpractice.
However, itiseasilygeneralized, forinstancebyconsideringadditional layers of processing each consisting of a weighted linear combination of the form (5.4)followedbyanelement-wisetransformationusinganonlinearactivationfunc- tion.
Note that there is some confusion in the literature regarding the terminology forcountingthenumberoflayersinsuchnetworks.
Thusthenetworkin Figure5.1 maybedescribedasa3-layernetwork(whichcountsthenumberoflayersofunits, andtreatstheinputsasunits)orsometimesasasingle-hidden-layernetwork(which countsthenumberoflayersofhiddenunits).
Werecommendaterminologyinwhich Figure5.1iscalledatwo-layernetwork, becauseitisthenumberoflayersofadap- tiveweightsthatisimportantfordeterminingthenetworkproperties.
Anothergeneralizationofthenetworkarchitectureistoincludeskip-layer con- nections, eachofwhichisassociatedwithacorrespondingadaptiveparameter.
For 230 5.
NEURALNETWORKS Figure5.2 Example of a neural network having a generalfeed-forwardtopology.
Notethat z2 each hidden and output unit has an associated bias parameter (omitted for x2 y2 clarity).
inputs z1 outputs x1 y1 z3 instance, in a two-layer network these would go directly from inputs to outputs.
In principle, a network with sigmoidal hidden units can always mimic skip layer con- nections (for bounded input values) by using a sufficiently small first-layer weight that, over its operating range, the hidden unit is effectively linear, and then com- pensating with a large weight value from the hidden unit to the output.
In practice, however, itmaybeadvantageoustoincludeskip-layerconnectionsexplicitly.
Furthermore, thenetworkcanbesparse, withnotallpossibleconnectionswithin alayerbeingpresent.
Weshallseeanexampleofasparsenetworkarchitecturewhen weconsiderconvolutionalneuralnetworksin Section5.5.6.
Because there is a direct correspondence between a network diagram and its mathematical function, we can develop more general network mappings by con- sidering more complex network diagrams.
However, these must be restricted to a feed-forwardarchitecture, inotherwordstoonehavingnocloseddirectedcycles, to ensure that the outputs are deterministic functions of the inputs.
This is illustrated withasimpleexamplein Figure5.2.
Each(hiddenoroutput)unitinsuchanetwork computesafunctiongivenby zk =h wkjzj (5.10) j wherethesumrunsoverallunitsthatsendconnectionstounitk (andabiasparam- eterisincludedinthesummation).
Foragivensetofvaluesappliedtotheinputsof thenetwork, successiveapplicationof(5.10)allowstheactivationsofallunitsinthe networktobeevaluatedincludingthoseoftheoutputunits.
Theapproximationpropertiesoffeed-forwardnetworkshavebeenwidelystud- ied(Funahashi,1989; Cybenko,1989; Horniketal.,1989; Stinchecombeand White, 1989; Cotter, 1990; Ito, 1991; Hornik, 1991; Kreinovich, 1991; Ripley, 1996) and found to be very general.
Neural networks are therefore said to be universal ap- proximators.
For example, a two-layer network with linear outputs can uniformly approximateanycontinuousfunctiononacompactinputdomaintoarbitraryaccu- racyprovidedthenetworkhasasufficientlylargenumberofhiddenunits.
Thisresult holdsforawiderangeofhiddenunitactivationfunctions, butexcludingpolynomi- als.
Althoughsuchtheoremsarereassuring, thekeyproblemishowtofindsuitable parametervaluesgivenasetoftrainingdata, andinlatersectionsofthischapterwe 5.1.
Feed-forward Network Functions 231 Figure5.3 Illustration of the ca- pability of a multilayer perceptron to approximate four different func- tions comprising (a) f(x) = x2, (b) f(x) = sin(x), (c), f(x) = |x|, and (d) f(x) = H(x) where H(x) is the Heaviside step function.
In each case, N = 50 data points, shownasbluedots, havebeensam- pled uniformly in x over the interval (−1,1) and the corresponding val- ues of f(x) evaluated.
These data points are then used to train a two- (a) (b) layer network having 3 hidden units with ‘tanh’ activation functions and linear output units.
The resulting network functions are shown by the red curves, and the outputs of the threehiddenunitsareshownbythe threedashedcurves.
(c) (d) willshowthatthereexisteffectivesolutionstothisproblembasedonbothmaximum likelihoodand Bayesianapproaches.
The capability of a two-layer network to model a broad range of functions is illustrated in Figure 5.3.
This figure also shows how individual hidden units work collaborativelytoapproximatethefinalfunction.
Theroleofhiddenunitsinasimple classification problem is illustrated in Figure 5.4 using the synthetic classification datasetdescribedin Appendix A.
5.1.1 Weight-space symmetries Onepropertyoffeed-forwardnetworks, whichwillplayarolewhenweconsider Bayesian model comparison, is that multiple distinct choices for the weight vector wcanallgiverisetothesamemappingfunctionfrominputstooutputs(Chenetal., 1993).
Consideratwo-layernetworkoftheformshownin Figure5.1with M hidden units having ‘tanh’ activation functions and full connectivity in both layers.
If we change the sign of all of the weights and the bias feeding into a particular hidden unit, then, foragiveninputpattern, thesignoftheactivationofthehiddenunitwill bereversed, because‘tanh’isanoddfunction, sothattanh(−a)=−tanh(a).
This transformationcanbeexactlycompensatedbychangingthesignofalloftheweights leadingoutofthathiddenunit.
Thus, bychangingthesignsofaparticulargroupof weights(andabias), theinput–outputmappingfunctionrepresentedbythenetwork is unchanged, and so we have found two different weight vectors that give rise to the same mapping function.
For M hidden units, there will be M such ‘sign-flip’ 232 5.
NEURALNETWORKS Figure5.4 Exampleofthesolutionofasimpletwo- 3 class classification problem involving synthetic data using a neural network 2 havingtwoinputs, twohiddenunitswith ‘tanh’activationfunctions, andasingle output having a logistic sigmoid activa- 1 tion function.
The dashed blue lines show the z = 0.5 contours for each of 0 thehiddenunits, andtheredlineshows they=0.5decisionsurfaceforthenet- −1 work.
For comparison, the green line denotes the optimal decision boundary −2 computedfromthedistributionsusedto generatethedata.
−2 −1 0 1 2 symmetries, and thus any given weight vector will be one of a set 2M equivalent weightvectors.
Similarly, imaginethatweinterchangethevaluesofalloftheweights(andthe bias) leading both into and out of a particular hidden unit with the corresponding valuesoftheweights(andbias)associatedwithadifferenthiddenunit.
Again, this clearly leaves the network input–output mapping function unchanged, but it corre- spondstoadifferentchoiceofweightvector.
For M hiddenunits, anygivenweight vectorwillbelongtoasetof M! equivalentweightvectorsassociatedwiththisinter- change symmetry, corresponding to the M! different orderings of the hidden units.
Thenetworkwillthereforehaveanoverallweight-spacesymmetryfactorof M!2M.
Fornetworkswithmorethantwolayersofweights, thetotallevelofsymmetrywill begivenbytheproductofsuchfactors, oneforeachlayerofhiddenunits.
Itturns out that these factors account for all of the symmetries inweight space (exceptforpossibleaccidentalsymmetriesduetospecificchoicesfortheweightval- ues).
Furthermore, theexistenceofthesesymmetriesisnotaparticularpropertyof the‘tanh’functionbutappliestoawiderangeofactivationfunctions(Ku˙rkova´ and Kainen,1994).
Inmanycases, thesesymmetriesinweightspaceareoflittlepracti- calconsequence, althoughin Section5.7weshallencounterasituationinwhichwe needtotakethemintoaccount.
5.2.
Network Training So far, we have viewed neural networks as a general class of parametric nonlinear functions from a vector x of input variables to a vector y of output variables.
A simpleapproachtotheproblemofdeterminingthenetworkparametersistomakean analogywiththediscussionofpolynomialcurvefittingin Section1.1, andtherefore to minimize a sum-of-squares error function.
Given a training set comprising a set of input vectors {xn }, where n = 1,..., N, together with a corresponding set of 5.2.
Network Training 233 targetvectors{tn }, weminimizetheerrorfunction N 1 E(w)= y(xn, w)−tn 2.
(5.11) 2 n=1 However, wecanprovideamuchmoregeneralviewofnetworktrainingbyfirst giving a probabilistic interpretation to the network outputs.
We have already seen manyadvantagesofusingprobabilisticpredictionsin Section1.5.4.
Hereitwillalso provide us with a clearer motivation both for the choice of output unit nonlinearity andthechoiceoferrorfunction.
We start by discussing regression problems, and for the moment we consider a single target variable t that can take any real value.
Following the discussions in Section 1.2.5 and 3.1, we assume that t has a Gaussian distribution with an x- dependentmean, whichisgivenbytheoutputoftheneuralnetwork, sothat p(t|x, w)=N t|y(x, w),β −1 (5.12) where β is the precision (inverse variance) of the Gaussian noise.
Of course this isasomewhatrestrictiveassumption, andin Section5.6weshallseehowtoextend thisapproachtoallowformoregeneralconditionaldistributions.
Fortheconditional distributiongivenby(5.12), itissufficienttotaketheoutputunitactivationfunction tobetheidentity, becausesuchanetworkcanapproximateanycontinuousfunction fromxtoy.
Givenadatasetof N independent, identicallydistributedobservations canconstructthecorrespondinglikelihoodfunction N p(t|X, w,β)= p(tn |xn, w,β).
n=1 Takingthenegativelogarithm, weobtaintheerrorfunction N β N N {y(xn, w)−tn }2− lnβ+ ln(2π) (5.13) 2 2 2 n=1 which can be used to learn the parameters w and β.
In Section 5.7, we shall dis- cussthe Bayesiantreatmentofneuralnetworks, whilehereweconsideramaximum likelihood approach.
Note that in the neural networks literature, it is usual to con- sidertheminimizationofanerrorfunctionratherthanthemaximizationofthe(log) likelihood, and so here we shall follow this convention.
Consider first the determi- nation of w.
Maximizing the likelihood function is equivalent to minimizing the sum-of-squareserrorfunctiongivenby N 1 E(w)= {y(xn, w)−tn }2 (5.14) 2 n=1 234 5.
NEURALNETWORKS wherewehavediscardedadditiveandmultiplicativeconstants.
Thevalueofwfound byminimizing E(w)willbedenotedw ML becauseitcorrespondstothemaximum likelihood solution.
In practice, the nonlinearity of the network function y(xn, w) causes the error E(w) to be nonconvex, and so in practice local maxima of the likelihood may be found, corresponding to local minima of the error function, as discussedin Section5.2.1.
Havingfoundw ML, thevalueofβ canbefoundbyminimizingthenegativelog likelihoodtogive N 1 1 = {y(xn, w ML )−tn }2.
(5.15) β N ML n=1 Notethatthiscanbeevaluatedoncetheiterativeoptimizationrequiredtofindw ML iscompleted.
Ifwehavemultipletargetvariables, andweassumethattheyareinde- pendentconditionalonxandwwithsharednoiseprecisionβ, thentheconditional distributionofthetargetvaluesisgivenby p(t|x, w)=N t|y(x, w),β −1I .
(5.16) Followingthesameargumentasforasingletargetvariable, weseethatthemaximum likelihoodweightsaredeterminedbyminimizingthesum-of-squareserrorfunction Exercise 5.2 (5.11).
Thenoiseprecisionisthengivenby N 1 1 = y(xn, w ML )−tn 2 (5.17) β NK ML n=1 where K isthenumberoftargetvariables.
Theassumptionofindependencecanbe Exercise 5.3 droppedattheexpenseofaslightlymorecomplexoptimizationproblem.
Recall from Section 4.3.6 that there is a natural pairing of the error function (givenbythenegativeloglikelihood)andtheoutputunitactivationfunction.
Inthe regressioncase, wecanviewthenetworkashavinganoutputactivationfunctionthat is the identity, so that yk = ak.
The corresponding sum-of-squares error function hastheproperty ∂E =yk −tk (5.18) ∂ak whichweshallmakeuseofwhendiscussingerrorbackpropagationin Section5.3.
Nowconsiderthecaseofbinaryclassificationinwhichwehaveasingletarget variable t such that t = 1 denotes class C 1 and t = 0 denotes class C 2.
Following the discussion of canonical link functions in Section 4.3.6, we consider a network havingasingleoutputwhoseactivationfunctionisalogisticsigmoid 1 y =σ(a)≡ (5.19) 1+exp(−a) so that 0 y(x, w) 1.
We can interpret y(x, w) as the conditional probability p(C 1 |x), withp(C 2 |x)givenby1−y(x, w).
Theconditionaldistributionoftargets giveninputsisthena Bernoullidistributionoftheform p(t|x, w)=y(x, w) t{1−y(x, w)}1−t .
(5.20) 5.2.
Network Training 235 If we consider a training set of independent observations, then the error function, whichisgivenbythenegativeloglikelihood, isthenacross-entropyerrorfunction oftheform N E(w)=− {tnlnyn+(1−tn)ln(1−yn)} (5.21) n=1 whereyn denotesy(xn, w).
Notethatthereisnoanalogueofthenoiseprecisionβ becausethetargetvaluesareassumedtobecorrectlylabelled.
However, themodel Exercise 5.4 iseasilyextendedtoallowforlabellingerrors.
Simardetal.
(2003)foundthatusing the cross-entropy error function instead of the sum-of-squares for a classification problemleadstofastertrainingaswellasimprovedgeneralization.
Ifwehave K separatebinaryclassificationstoperform, thenwecanuseanet- work having K outputs each of which has a logistic sigmoid activation function.
Associatedwitheachoutputisabinaryclasslabeltk ∈{0,1}, wherek =1,..., K.
If we assume that the class labels are independent, given the input vector, then the conditionaldistributionofthetargetsis K p(t|x, w)= yk(x, w) t k[1−yk(x, w)] 1−t k.
(5.22) k=1 Taking the negative logarithm of the corresponding likelihood function then gives Exercise 5.5 thefollowingerrorfunction N K E(w)=− {tnklnynk +(1−tnk)ln(1−ynk)} (5.23) n=1k=1 where ynk denotes yk(xn, w).
Again, the derivative of the error function with re- Exercise 5.6 specttotheactivationforaparticularoutputunittakestheform(5.18)justasinthe regressioncase.
Itisinterestingtocontrasttheneuralnetworksolutiontothisproblemwiththe correspondingapproachbasedonalinearclassificationmodelofthekinddiscussed in Chapter 4.
Suppose that we are using a standard two-layer network of the kind shown in Figure 5.1.
We see that the weight parameters in the first layer of the network are shared between the various outputs, whereas in the linear model each classification problem is solved independently.
The first layer of the network can be viewed as performing a nonlinear feature extraction, and the sharing of features betweenthedifferentoutputscansaveoncomputationandcanalsoleadtoimproved generalization.
Finally, weconsiderthestandardmulticlassclassificationprobleminwhicheach inputisassignedtooneof K mutuallyexclusiveclasses.
Thebinarytargetvariables tk ∈ {0,1} have a 1-of-K coding scheme indicating the class, and the network outputs are interpreted as yk(x, w) = p(tk = 1|x), leading to the following error function N K E(w)=− tknlnyk(xn, w).
(5.24) n=1k=1 236 5.
NEURALNETWORKS Figure5.5 Geometrical view of the error function E(w) as E(w) asurfacesittingoverweightspace.
Pointw A is alocalminimumandw B istheglobalminimum.
At any point w C, the local gradient of the error surfaceisgivenbythevector∇E.
w1 w A w B w C w2 ∇E Following the discussion of Section 4.3.4, we see that the output unit activation function, whichcorrespondstothecanonicallink, isgivenbythesoftmaxfunction yk(x, w)= exp(ak(x, w)) (5.25) exp(aj(x, w)) j whichsatisfies0 yk 1and k yk = 1.
Notethattheyk(x, w)areunchanged ifaconstantisaddedtoalloftheak(x, w), causingtheerrorfunctiontobeconstant for some directions in weight space.
This degeneracy is removed if an appropriate regularizationterm(Section5.5)isaddedtotheerrorfunction.
Onceagain, thederivativeoftheerrorfunctionwithrespecttotheactivationfor Exercise 5.7 aparticularoutputunittakesthefamiliarform(5.18).
In summary, there is a natural choice of both output unit activation function andmatchingerrorfunction, accordingtothetypeofproblembeingsolved.
Forre- gressionweuselinearoutputsandasum-of-squareserror, for(multipleindependent) binaryclassificationsweuselogisticsigmoidoutputsandacross-entropyerrorfunc- tion, andformulticlassclassificationweusesoftmaxoutputswiththecorresponding multiclass cross-entropy error function.
For classification problems involving two classes, we can use a single logistic sigmoid output, or alternatively we can use a networkwithtwooutputshavingasoftmaxoutputactivationfunction.
5.2.1 Parameter optimization We turn next to the task of finding a weight vector w which minimizes the chosenfunction E(w).
Atthispoint, itisusefultohaveageometricalpictureofthe errorfunction, whichwecanviewasasurfacesittingoverweightspaceasshownin Figure5.5.
Firstnotethatifwemakeasmallstepinweightspacefromwtow+δw thenthechangeintheerrorfunctionisδE δw T∇E(w), wherethevector∇E(w) pointsinthedirectionofgreatestrateofincreaseoftheerrorfunction.
Becausethe error E(w)isasmoothcontinuousfunctionofw, itssmallestvaluewilloccurata 5.2.
Network Training 237 pointinweightspacesuchthatthegradientoftheerrorfunctionvanishes, sothat ∇E(w)=0 (5.26) as otherwise we could make a small step in the direction of −∇E(w) and thereby further reduce the error.
Points at which the gradient vanishes are called stationary points, andmaybefurtherclassifiedintominima, maxima, andsaddlepoints.
Our goal is to find a vector w such that E(w) takes its smallest value.
How- ever, the error function typically has a highly nonlinear dependence on the weights andbiasparameters, andsotherewillbemanypointsinweightspaceatwhichthe gradientvanishes(orisnumericallyverysmall).
Indeed, fromthediscussionin Sec- tion 5.1.1 we see that for any point w that is a local minimum, there will be other pointsinweightspacethatareequivalentminima.
Forinstance, inatwo-layernet- work of the kind shown in Figure 5.1, with M hidden units, each point in weight Section5.1.1 spaceisamemberofafamilyof M!2M equivalentpoints.
Furthermore, therewilltypicallybemultipleinequivalentstationarypointsand in particular multiple inequivalent minima.
A minimum that corresponds to the smallest value of the error function for any weight vector is said to be a global minimum.
Any other minima corresponding to higher values of the error function aresaidtobelocalminima.
Forasuccessfulapplicationofneuralnetworks, itmay not be necessary to find the global minimum (and in general it will not be known whether the global minimum has been found) but it may be necessary to compare severallocalminimainordertofindasufficientlygoodsolution.
Because there is clearly no hope of finding an analytical solution to the equa- tion ∇E(w) = 0 we resort to iterative numerical procedures.
The optimization of continuous nonlinear functions is a widely studied problem and there exists an ex- tensive literature on how to solve it efficiently.
Most techniques involve choosing someinitialvaluew(0) fortheweightvectorandthenmovingthroughweightspace inasuccessionofstepsoftheform w(τ+1) =w(τ)+∆w(τ) (5.27) where τ labels the iteration step.
Different algorithms involve different choices for theweightvectorupdate∆w(τ).
Manyalgorithmsmakeuseofgradientinformation and therefore require that, after each update, the value of ∇E(w) is evaluated at the new weight vector w(τ+1).
In order to understand the importance of gradient information, itisusefultoconsideralocalapproximationtotheerrorfunctionbased ona Taylorexpansion.
5.2.2 Local quadratic approximation Insightintotheoptimizationproblem, andintothevarioustechniquesforsolv- ing it, can be obtained by considering a local quadratic approximation to the error function.
Considerthe Taylorexpansionof E(w)aroundsomepointw inweightspace 1 E(w) E(w )+(w−w )Tb+ (w−w )TH(w−w ) (5.28) 2 238 5.
NEURALNETWORKS wherecubicandhighertermshavebeenomitted.
Herebisdefinedtobethegradient of E evaluatedatw b≡ ∇E| (5.29) w=wb andthe Hessianmatrix H=∇∇E haselements ∂E (H)ij ≡ .
(5.30) ∂wi∂wj w=wb From(5.28), thecorrespondinglocalapproximationtothegradientisgivenby ∇E b+H(w−w ).
(5.31) Forpointsw thataresufficientlyclosetow , theseexpressionswillgivereasonable approximationsfortheerroranditsgradient.
Consider the particular case of a local quadratic approximation around a point w that is a minimum of the error function.
In this case there is no linear term, because∇E =0atw , and(5.28)becomes 1 E(w)=E(w )+ (w−w )TH(w−w ) (5.32) 2 where the Hessian H is evaluated at w .
In order to interpret this geometrically, considertheeigenvalueequationforthe Hessianmatrix Hui =λiui (5.33) wheretheeigenvectorsui formacompleteorthonormalset(Appendix C)sothat u T i uj =δij.
(5.34) Wenowexpand(w−w )asalinearcombinationoftheeigenvectorsintheform w−w = αiui.
(5.35) i Thiscanberegardedasatransformationofthecoordinatesysteminwhichtheorigin istranslatedtothepointw , andtheaxesarerotatedtoalignwiththeeigenvectors (throughtheorthogonalmatrixwhosecolumnsaretheui), andisdiscussedinmore allowstheerrorfunctiontobewrittenintheform 1 E(w)=E(w )+ λiα i 2.
(5.36) 2 i Amatrix Hissaidtobepositivedefiniteif, andonlyif, v THv >0 forallv.
(5.37) 5.2.
Network Training 239 Figure5.6 Intheneighbourhoodofamin- w2 imum w , the error function can be approximated by a u2 quadratic.
Contours of con- u1 stant error are then ellipses whose axes are aligned with theeigenvectorsu i ofthe Hes- −1/2 w sian matrix, with lengths that λ 2 areinverselyproportionaltothe −1/2 λ squarerootsofthecorrespond- 1 ingeigenvectorsλ i.
w1 Because the eigenvectors {ui } form a complete set, an arbitrary vector v can be writtenintheform v = ciui.
(5.38) i From(5.33)and(5.34), wethenhave v THv = c2 i λi (5.39) i Exercise 5.10 and so H will be positive definite if, and only if, all of its eigenvalues are positive.
In the new coordinate system, whose basis vectors are given by the eigenvectors Exercise 5.11 {ui }, the contours of constant E are ellipses centred on the origin, as illustrated in Figure 5.6.
For a one-dimensional weight space, a stationary point w will be a minimumif ∂2E >0.
(5.40) ∂w2 w The corresponding result in D-dimensions is that the Hessian matrix, evaluated at Exercise 5.12 w , shouldbepositivedefinite.
5.2.3 Use of gradient information Asweshallseein Section5.3, itispossibletoevaluatethegradientofanerror function efficiently by means of the backpropagation procedure.
The use of this gradient information can lead to significant improvements in the speed with which theminimaoftheerrorfunctioncanbelocated.
Wecanseewhythisisso, asfollows.
In the quadratic approximation to the error function, given in (5.28), the error surface is specified by the quantities b and H, which contain a total of W(W + Exercise 5.13 3)/2 independent elements (because the matrix H is symmetric), where W is the dimensionality of w (i.
e., the total number of adaptive parameters in the network).
Thelocationoftheminimumofthisquadraticapproximationthereforedependson O(W2)parameters, andweshouldnotexpecttobeabletolocatetheminimumuntil we have gathered O(W2) independent pieces of information.
If we do not make use of gradient information, we would expect to have to perform O(W2) function 240 5.
NEURALNETWORKS evaluations, each of which would require O(W) steps.
Thus, the computational effortneededtofindtheminimumusingsuchanapproachwouldbe O(W3).
Nowcomparethiswithanalgorithmthatmakesuseofthegradientinformation.
Because each evaluation of ∇E brings W items of information, we might hope to find the minimum of the function in O(W) gradient evaluations.
As we shall see, byusingerrorbackpropagation, eachsuchevaluationtakesonly O(W)stepsandso theminimumcannowbefoundin O(W2)steps.
Forthisreason, theuseofgradient informationformsthebasisofpracticalalgorithmsfortrainingneuralnetworks.
5.2.4 Gradient descent optimization The simplest approach to using gradient information is to choose the weight updatein(5.27)tocompriseasmallstepinthedirectionofthenegativegradient, so that w(τ+1) =w(τ)−η∇E(w(τ)) (5.41) wheretheparameterη >0isknownasthelearningrate.
Aftereachsuchupdate, the gradientisre-evaluatedforthenewweightvectorandtheprocessrepeated.
Notethat theerrorfunctionisdefinedwithrespecttoatrainingset, andsoeachsteprequires that the entire training set be processed in order to evaluate ∇E.
Techniques that use the whole data set at once are called batch methods.
At each step the weight vectorismovedinthedirectionofthegreatestrateofdecreaseoftheerrorfunction, and so this approach is known as gradient descent or steepest descent.
Although suchanapproachmightintuitivelyseemreasonable, infactitturnsouttobeapoor algorithm, forreasonsdiscussedin Bishopand Nabney(2008).
Forbatchoptimization, therearemoreefficientmethods, suchasconjugategra- dients and quasi-Newton methods, which are much more robust and much faster thansimplegradientdescent(Gilletal.,1981; Fletcher,1987; Nocedaland Wright, 1999).
Unlike gradient descent, these algorithms have the property that the error functionalwaysdecreasesateachiterationunlesstheweightvectorhasarrivedata localorglobalminimum.
In order to find a sufficiently good minimum, it may be necessary to run a gradient-basedalgorithmmultipletimes, eachtimeusingadifferentrandomlycho- senstartingpoint, andcomparingtheresultingperformanceonanindependentvali- dationset.
Thereis, however, anon-lineversionofgradientdescentthathasproveduseful in practice for training neural networks on large data sets (Le Cun et al., 1989).
Errorfunctionsbasedonmaximumlikelihoodforasetofindependentobservations compriseasumofterms, oneforeachdatapoint N E(w)= En(w).
(5.42) n=1 On-line gradient descent, also known as sequential gradient descent or stochastic gradientdescent, makesanupdatetotheweightvectorbasedononedatapointata time, sothat w(τ+1) =w(τ)−η∇En(w(τ)).
(5.43) 5.3.
Error Backpropagation 241 Thisupdateisrepeatedbycyclingthroughthedataeitherinsequenceorbyselecting points at random with replacement.
There are of course intermediate scenarios in whichtheupdatesarebasedonbatchesofdatapoints.
Oneadvantageofon-linemethodscomparedtobatchmethodsisthattheformer handle redundancy in the data much more efficiently.
To see, this consider an ex- tremeexampleinwhichwetakeadatasetanddoubleitssizebyduplicating every datapoint.
Notethatthissimplymultipliestheerrorfunctionbyafactorof2andso isequivalenttousingtheoriginalerrorfunction.
Batchmethodswillrequiredouble the computational effort to evaluate the batch error function gradient, whereas on- linemethodswillbeunaffected.
Anotherpropertyofon-linegradientdescentisthe possibility of escaping from local minima, since a stationary point with respect to the error function for the whole data set will generally not be a stationary point for eachdatapointindividually.
Nonlinearoptimizationalgorithms, andtheirpracticalapplicationtoneuralnet- worktraining, arediscussedindetailin Bishopand Nabney(2008).
5.3.
Error Backpropagation Our goal in this section is to find an efficient technique for evaluating the gradient of an error function E(w) for a feed-forward neural network.
We shall see that thiscanbeachievedusingalocalmessagepassingschemeinwhichinformationis sentalternatelyforwardsandbackwardsthroughthenetworkandisknownaserror backpropagation, orsometimessimplyasbackprop.
It should be noted that the term backpropagation is used in the neural com- puting literature to mean a variety of different things.
For instance, the multilayer perceptron architecture is sometimes called a backpropagation network.
The term backpropagation is also used to describe the training of a multilayer perceptron us- ing gradient descent applied to a sum-of-squares error function.
In order to clarify theterminology, itisusefultoconsiderthenatureofthetrainingprocessmorecare- fully.
Mosttrainingalgorithmsinvolveaniterativeprocedureforminimizationofan errorfunction, withadjustmentstotheweightsbeingmadeinasequenceofsteps.
At eachsuchstep, wecandistinguishbetweentwodistinctstages.
Inthefirststage, the derivatives of the error function with respect to the weights must be evaluated.
As weshallsee, theimportantcontributionofthebackpropagationtechniqueisinpro- viding a computationally efficient method for evaluating such derivatives.
Because itisatthisstagethaterrorsarepropagatedbackwardsthroughthenetwork, weshall use the term backpropagation specifically to describe the evaluation of derivatives.
In the second stage, the derivatives are then used to compute the adjustments to be madetotheweights.
Thesimplestsuchtechnique, andtheoneoriginallyconsidered by Rumelhart et al.
(1986), involves gradient descent.
It is important to recognize that the two stages are distinct.
Thus, the first stage, namely the propagation of er- rorsbackwardsthroughthenetworkinordertoevaluatederivatives, canbeapplied tomanyotherkindsofnetworkandnotjustthemultilayerperceptron.
Itcanalsobe appliedtoerrorfunctionsotherthatjustthesimplesum-of-squares, andtotheeval- 242 5.
NEURALNETWORKS uation of other derivatives such as the Jacobian and Hessian matrices, as we shall seelaterinthischapter.
Similarly, thesecondstageofweightadjustmentusingthe calculatedderivativescanbetackledusingavarietyofoptimizationschemes, many ofwhicharesubstantiallymorepowerfulthansimplegradientdescent.
5.3.1 Evaluation of error-function derivatives Wenowderivethebackpropagationalgorithmforageneralnetworkhavingar- bitraryfeed-forwardtopology, arbitrarydifferentiablenonlinearactivationfunctions, and a broad class of error function.
The resulting formulae will then be illustrated using a simple layered network structure having a single layer of sigmoidal hidden unitstogetherwithasum-of-squareserror.
Many error functions of practical interest, for instance those defined by maxi- mum likelihood for a set of i.
i.
d.
data, comprise a sum of terms, one for each data pointinthetrainingset, sothat N E(w)= En(w).
(5.44) n=1 Hereweshallconsidertheproblemofevaluating∇En(w)foronesuchterminthe errorfunction.
Thismaybeuseddirectlyforsequentialoptimization, ortheresults canbeaccumulatedoverthetrainingsetinthecaseofbatchmethods.
Considerfirstasimplelinearmodelinwhichtheoutputsyk arelinearcombina- tionsoftheinputvariablesxi sothat yk = wkixi (5.45) i togetherwithanerrorfunctionthat, foraparticularinputpatternn, takestheform 1 En = (ynk −tnk)2 (5.46) 2 k whereynk =yk(xn, w).
Thegradientofthiserrorfunctionwithrespecttoaweight wji isgivenby ∂En =(ynj −tnj)xni (5.47) ∂wji whichcanbeinterpretedasa‘local’computationinvolvingtheproductofan‘error signal’ynj −tnj associatedwiththeoutputendofthelinkwji andthevariablexni associated with the input end of the link.
In Section 4.3.2, we saw how a similar formula arises with the logistic sigmoid activation function together with the cross entropy error function, and similarly for the softmax activation function together with its matching cross-entropy error function.
We shall now see how this simple resultextendstothemorecomplexsettingofmultilayerfeed-forwardnetworks.
In a general feed-forward network, each unit computes a weighted sum of its inputsoftheform aj = wjizi (5.48) i 5.3.
Error Backpropagation 243 whereziistheactivationofaunit, orinput, thatsendsaconnectiontounitj, andwji istheweightassociatedwiththatconnection.
In Section5.1, wesawthatbiasescan be included in this sum by introducing an extra unit, or input, with activation fixed at+1.
Wethereforedonotneedtodealwithbiasesexplicitly.
Thesumin(5.48)is transformedbyanonlinearactivationfunctionh(·)togivetheactivationzj ofunitj intheform zj =h(aj).
(5.49) Notethatoneormoreofthevariableszi inthesumin(5.48)couldbeaninput, and similarly, theunitj in(5.49)couldbeanoutput.
Foreachpatterninthetrainingset, weshallsupposethatwehavesuppliedthe corresponding input vector to the network and calculated the activations of all of the hidden and output units in the network by successive application of (5.48) and (5.49).
Thisprocessisoftencalledforwardpropagationbecauseitcanberegarded asaforwardflowofinformationthroughthenetwork.
Now consider the evaluation of the derivative of En with respect to a weight wji.
The outputs of the various units will depend on the particular input pattern n.
However, in order to keep the notation uncluttered, we shall omit the subscript n from the network variables.
First we note that En depends on the weight wji only viathesummedinputaj tounitj.
Wecanthereforeapplythechainruleforpartial derivativestogive ∂En ∂En ∂aj = .
(5.50) ∂wji ∂aj ∂wji Wenowintroduceausefulnotation δj ≡ ∂En (5.51) ∂aj wheretheδ’sareoftenreferredtoaserrorsforreasonsweshallseeshortly.
Using (5.48), wecanwrite ∂aj =zi.
(5.52) ∂wji Substituting(5.51)and(5.52)into(5.50), wethenobtain ∂En =δjzi.
(5.53) ∂wji Equation(5.53)tellsusthattherequiredderivativeisobtainedsimplybymultiplying thevalueofδfortheunitattheoutputendoftheweightbythevalueofzfortheunit attheinputendoftheweight(wherez =1inthecaseofabias).
Notethatthistakes the same form as for the simple linear model considered at the start of this section.
Thus, in order to evaluate the derivatives, we need only to calculate the value ofδj foreachhiddenandoutputunitinthenetwork, andthenapply(5.53).
Aswehaveseenalready, fortheoutputunits, wehave δk =yk −tk (5.54) 244 5.
NEURALNETWORKS Figure5.7 Illustration of the calculation of δ j for hidden unit j by backpropagation of the δ’s from those units k to which zi δk unit j sends connections.
The blue arrow denotes the δj directionofinformationflowduringforwardpropagation, wji wkj and the red arrows indicate the backward propagation oferrorinformation.
zj δ1 provided we are using the canonical link as the output-unit activation function.
To evaluate the δ’s for hidden units, we again make use of the chain rule for partial derivatives, δj ≡ ∂En = ∂En∂ak (5.55) ∂aj ∂ak ∂aj k wherethesumrunsoverallunitsktowhichunitj sendsconnections.
Thearrange- mentofunitsandweightsisillustratedin Figure5.7.
Notethattheunitslabelledk couldincludeotherhiddenunitsand/oroutputunits.
Inwritingdown(5.55), weare making use of the fact that variations in aj give rise to variations in the error func- tion only through variations in the variablesak.
If we now substitute the definition of δ given by (5.51) into (5.55), and make use of (5.48) and (5.49), we obtain the followingbackpropagationformula δj =h(aj) wkjδk (5.56) k which tells us that the value of δ for a particular hidden unit can be obtained by propagating the δ’s backwards from units higher up in the network, as illustrated in Figure 5.7.
Note that the summation in (5.56) is taken over the first index on wkj (corresponding to backward propagation of information through the network), whereasintheforwardpropagationequation(5.10)itistakenoverthesecondindex.
Because we already know the values of the δ’s for the output units, it follows that byrecursivelyapplying(5.56)wecanevaluatetheδ’sforallofthehiddenunitsina feed-forwardnetwork, regardlessofitstopology.
Thebackpropagationprocedurecanthereforebesummarizedasfollows.
Error Backpropagation 1.
Apply an input vector xn to the network and forward propagate through thenetworkusing(5.48)and(5.49)tofindtheactivationsofallthehidden andoutputunits.
2.
Evaluatetheδk foralltheoutputunitsusing(5.54).
3.
Backpropagatetheδ’susing(5.56)toobtainδj foreachhiddenunitinthe network.
4.
Use(5.53)toevaluatetherequiredderivatives.
5.3.
Error Backpropagation 245 For batch methods, the derivative of the total error E can then be obtained by repeatingtheabovestepsforeachpatterninthetrainingsetandthensummingover allpatterns: ∂E ∂En = .
(5.57) ∂wji ∂wji n Intheabovederivationwehaveimplicitlyassumedthateachhiddenoroutputunitin thenetworkhasthesameactivationfunctionh(·).
Thederivationiseasilygeneral- ized, however, toallowdifferentunitstohaveindividualactivationfunctions, simply bykeepingtrackofwhichformofh(·)goeswithwhichunit.
5.3.2 A simple example The above derivation of the backpropagation procedure allowed for general forms for the error function, the activation functions, and the network topology.
In order to illustrate the application of this algorithm, we shall consider a particular example.
Thisischosenbothforitssimplicityandforitspracticalimportance, be- cause many applications of neural networks reported in the literature make use of thistypeofnetwork.
Specifically, weshallconsideratwo-layernetworkoftheform illustrated in Figure 5.1, together with a sum-of-squares error, in which the output units have linear activation functions, so that yk = ak, while the hidden units have logisticsigmoidactivationfunctionsgivenby h(a)≡tanh(a) (5.58) where ea−e−a tanh(a)= .
(5.59) ea+e−a Ausefulfeatureofthisfunctionisthatitsderivativecanbeexpressedinapar- ticularlysimpleform: h (a)=1−h(a)2.
(5.60) Wealsoconsiderastandardsum-of-squareserrorfunction, sothatforpatternnthe errorisgivenby K 1 En = (yk −tk)2 (5.61) 2 k=1 whereyk istheactivationofoutputunitk, andtk isthecorrespondingtarget, fora particularinputpatternxn.
Foreachpatterninthetrainingsetinturn, wefirstperformaforwardpropagation using D (1) aj = w ji xi (5.62) i=0 zj = tanh(aj) (5.63) M (2) yk = w kj zj.
(5.64) j=0 246 5.
NEURALNETWORKS Nextwecomputetheδ’sforeachoutputunitusing δk =yk −tk.
(5.65) Thenwebackpropagatethesetoobtainδsforthehiddenunitsusing K δj =(1−z j 2) wkjδk.
(5.66) k=1 Finally, the derivatives with respect to the first-layer and second-layer weights are givenby ∂En ∂En (1) =δjxi, (2) =δkzj.
(5.67) ∂w ∂w ji kj 5.3.3 Efficiency of backpropagation Oneofthemostimportantaspectsofbackpropagationisitscomputationaleffi- ciency.
To understand this, let us examine how the number of computer operations requiredtoevaluatethederivativesoftheerrorfunctionscaleswiththetotalnumber W of weights and biases in the network.
A single evaluation of the error function (foragiveninputpattern)wouldrequire O(W)operations, forsufficientlylarge W.
This follows from the fact that, except for a network with very sparse connections, thenumberofweightsistypicallymuchgreaterthanthenumberofunits, andsothe bulk of the computational effort in forward propagation is concerned with evaluat- ingthesumsin(5.48), withtheevaluationoftheactivationfunctionsrepresentinga smalloverhead.
Eachterminthesumin(5.48)requiresonemultiplicationandone addition, leadingtoanoverallcomputationalcostthatis O(W).
Analternativeapproachtobackpropagationforcomputingthederivativesofthe errorfunctionistousefinitedifferences.
Thiscanbedonebyperturbingeachweight inturn, andapproximatingthederivativesbytheexpression ∂En En(wji+ )−En(wji) = +O( ) (5.68) ∂wji where 1.
In a software simulation, the accuracy of the approximation to the derivativescanbeimprovedbymaking smaller, untilnumericalroundoffproblems arise.
The accuracy of the finite differences method can be improved significantly byusingsymmetricalcentraldifferencesoftheform ∂En = En(wji+ )−En(wji − ) +O( 2).
(5.69) ∂wji 2 Exercise 5.14 Inthis case, the O( )corrections cancel, ascan beverifiedby Taylor expansion on theright-handsideof(5.69), andsotheresidualcorrectionsare O( 2).
Thenumber ofcomputationalstepsis, however, roughlydoubledcomparedwith(5.68).
The main problem with numerical differentiation is that the highly desirable O(W) scaling has been lost.
Each forward propagation requires O(W) steps, and 5.3.
Error Backpropagation 247 Figure5.8 Illustration of a modular pattern recognitionsysteminwhichthe u Jacobian matrix can be used v to backpropagate error signals fromtheoutputsthroughtoear- y liermodulesinthesystem.
z x w thereare W weightsinthenetworkeachofwhichmustbeperturbedindividually, so thattheoverallscalingis O(W2).
However, numericaldifferentiationplaysanimportantroleinpractice, becausea comparisonofthederivativescalculatedbybackpropagationwiththoseobtainedus- ingcentraldifferencesprovidesapowerfulcheckonthecorrectnessofanysoftware implementationofthebackpropagationalgorithm.
Whentrainingnetworksinprac- tice, derivatives should be evaluated using backpropagation, because this gives the greatestaccuracyandnumericalefficiency.
However, theresultsshouldbecompared withnumericaldifferentiationusing(5.69)forsometestcasesinordertocheckthe correctnessoftheimplementation.
5.3.4 The Jacobian matrix Wehaveseenhowthederivativesofanerrorfunctionwithrespecttotheweights can be obtained by the propagation of errors backwards through the network.
The techniqueofbackpropagationcanalsobeappliedtothecalculationofotherderiva- tives.
Here we consider the evaluation of the Jacobian matrix, whose elements are givenbythederivativesofthenetworkoutputswithrespecttotheinputs Jki ≡ ∂yk (5.70) ∂xi where each such derivative is evaluated with all other inputs held fixed.
Jacobian matrices play a useful role in systems built from a number of distinct modules, as illustrated in Figure 5.8.
Each module can comprise a fixed or adaptive function, which can be linear or nonlinear, so long as it is differentiable.
Suppose we wish tominimizeanerrorfunction E withrespecttotheparameterw in Figure5.8.
The derivativeoftheerrorfunctionisgivenby ∂E ∂E ∂yk ∂zj = (5.71) ∂w ∂yk ∂zj ∂w k, j inwhichthe Jacobianmatrixfortheredmodulein Figure5.8appearsinthemiddle term.
Because the Jacobian matrix provides a measure of the local sensitivity of the outputstochangesineachoftheinputvariables, italsoallowsanyknownerrors∆xi 248 5.
NEURALNETWORKS associated with the inputs to be propagated through the trained network in order to estimatetheircontribution∆yk totheerrorsattheoutputs, throughtherelation ∆yk ∂yk ∆xi (5.72) ∂xi i which is valid provided the |∆xi | are small.
In general, the network mapping rep- resented by a trained neural network will be nonlinear, and so the elements of the Jacobianmatrixwillnotbeconstantsbutwilldependontheparticularinputvector used.
Thus(5.72)isvalidonlyforsmallperturbationsoftheinputs, andthe Jacobian itselfmustbere-evaluatedforeachnewinputvector.
The Jacobianmatrixcanbeevaluatedusingabackpropagationprocedurethatis similar to the one derived earlier for evaluating the derivatives of an error function withrespecttotheweights.
Westartbywritingtheelement Jki intheform ∂yk ∂yk ∂aj Jki = = ∂xi ∂aj ∂xi j ∂yk = wji (5.73) ∂aj j wherewehavemadeuseof(5.48).
Thesumin(5.73)runsoverallunitsj towhich the input unit i sends connections (for example, over all units in the first hidden layer in the layered topology considered earlier).
We now write down a recursive backpropagationformulatodeterminethederivatives∂yk/∂aj ∂yk ∂yk ∂al = ∂aj ∂al ∂aj l ∂yk = h(aj) wlj (5.74) ∂al l wherethesumrunsoverallunitsltowhichunitjsendsconnections(corresponding to the first index of wlj).
Again, we have made use of (5.48) and (5.49).
This backpropagation starts at the output units for which the required derivatives can be found directly from the functional form of the output-unit activation function.
For instance, if we have individual sigmoidal activation functions at each output unit, then ∂yk =δkjσ (aj) (5.75) ∂aj whereasforsoftmaxoutputswehave ∂yk =δkjyk −ykyj.
(5.76) ∂aj Wecansummarizetheprocedureforevaluatingthe Jacobianmatrixasfollows.
Apply the input vector corresponding to the point in input space at which the Ja- cobian matrix is to be found, and forward propagate in the usual way to obtain the 5.4.
The Hessian Matrix 249 activations of all of the hidden and output units in the network.
Next, for each row k of the Jacobian matrix, corresponding to the output unit k, backpropagate using therecursiverelation(5.74), startingwith(5.75)or(5.76), forallofthehiddenunits in the network.
Finally, use (5.73) to do the backpropagation to the inputs.
The Jacobiancanalsobeevaluatedusinganalternativeforward propagationformalism, which can be derived in an analogous way to the backpropagation approach given Exercise 5.15 here.
Again, theimplementationofsuchalgorithmscanbecheckedbyusingnumeri- caldifferentiationintheform ∂yk = yk(xi+ )−yk(xi − ) +O( 2) (5.77) ∂xi 2 whichinvolves2Dforwardpropagationsforanetworkhaving Dinputs.
5.4.
The Hessian Matrix Wehaveshownhowthetechniqueofbackpropagationcanbeusedtoobtainthefirst derivatives of an error function with respect to the weights in the network.
Back- propagation can also be used to evaluate the second derivatives of the error, given by ∂2E .
(5.78) ∂wji∂wlk Notethatitissometimesconvenienttoconsideralloftheweightandbiasparameters as elements wi of a single vector, denoted w, in which case the second derivatives formtheelements Hij ofthe Hessianmatrix H, wherei, j ∈ {1,..., W}and W is thetotalnumberofweightsandbiases.
The Hessianplaysanimportantroleinmany aspectsofneuralcomputing, includingthefollowing: 1.
Several nonlinear optimization algorithms used for training neural networks arebasedonconsiderationsofthesecond-orderpropertiesoftheerrorsurface, whicharecontrolledbythe Hessianmatrix(Bishopand Nabney,2008).
2.
The Hessianformsthebasisofafastprocedureforre-trainingafeed-forward networkfollowingasmallchangeinthetrainingdata(Bishop,1991).
3.
Theinverseofthe Hessianhasbeenusedtoidentifytheleastsignificantweights inanetworkaspartofnetwork‘pruning’algorithms(Le Cunetal.,1990).
4.
The Hessianplaysacentralroleinthe Laplaceapproximationfora Bayesian neural network (see Section 5.7).
Its inverse is used to determine the predic- tivedistributionforatrainednetwork, itseigenvaluesdeterminethevaluesof hyperparameters, anditsdeterminantisusedtoevaluatethemodelevidence.
Various approximation schemes have been used to evaluate the Hessian matrix foraneuralnetwork.
However, the Hessiancanalsobecalculatedexactlyusingan extensionofthebackpropagationtechnique.
250 5.
NEURALNETWORKS Animportantconsiderationformanyapplicationsofthe Hessianistheefficiency withwhichitcanbeevaluated.
Ifthereare W parameters(weightsandbiases)inthe network, thenthe Hessianmatrixhasdimensions W ×W andsothecomputational effort needed to evaluate the Hessian will scale like O(W2) for each pattern in the data set.
As we shall see, there are efficient methods for evaluating the Hessian whosescalingisindeed O(W2).
5.4.1 Diagonal approximation Some of the applications for the Hessian matrix discussed above require the inverse of the Hessian, rather than the Hessian itself.
For this reason, there has beensomeinterestinusingadiagonalapproximationtothe Hessian, inotherwords onethatsimplyreplacestheoff-diagonalelementswithzeros, becauseitsinverseis trivialtoevaluate.
Again, weshallconsideranerrorfunct ionthatconsistsofasum ofterms, oneforeachpatterninthedataset, sothat E = n En.
The Hessiancan thenbeobtainedbyconsideringonepatternatatime, andthensummingtheresults over all patterns.
From (5.48), the diagonal elements of the Hessian, for pattern n, canbewritten ∂2En = ∂2En z2.
(5.79) ∂w2 ∂a2 i ji j Using(5.48)and(5.49), thesecondderivativesontheright-handsideof(5.79)can befoundrecursivelyusingthechainruleofdifferentialcalculustogiveabackprop- agationequationoftheform ∂ ∂ 2 a E 2 j n =h (aj)2 k k wkjwk j ∂a ∂ k 2 ∂ E a n k +h (aj) k wkj ∂ ∂ E ak n .
(5.80) If we now neglect off-diagonal elements in the second-derivative terms, we obtain (Beckerand Le Cun,1989; Le Cunetal.,1990) ∂ ∂ 2 a E 2 j n =h (aj)2 k w k 2 j ∂ ∂ 2 a E 2 k n +h (aj) k wkj ∂ ∂ E ak n .
(5.81) Notethatthenumberofcomputationalstepsrequiredtoevaluatethisapproximation is O(W), where W isthetotalnumberofweightandbiasparametersinthenetwork, comparedwith O(W2)forthefull Hessian.
Ricotti et al.
(1988) also used the diagonal approximation to the Hessian, but theyretainedalltermsintheevaluationof∂2En/∂a2 j andsoobtainedexactexpres- sionsforthediagonalterms.
Notethatthisnolongerhas O(W)scaling.
Themajor problem with diagonal approximations, however, is that in practice the Hessian is typicallyfoundtobestronglynondiagonal, andsotheseapproximations, whichare drivenmainlybecomputationalconvenience, mustbetreatedwithcare.
5.4.
The Hessian Matrix 251 5.4.2 Outer product approximation Whenneural networks areapplied toregression problems, itiscommon to use asum-of-squareserrorfunctionoftheform N 1 E = (yn −tn)2 (5.82) 2 n=1 where we have considered the case of a single output in order to keep the notation Exercise 5.16 simple (the extension to several outputs is straightforward).
We can then write the Hessianmatrixintheform N N H=∇∇E = ∇yn ∇yn+ (yn −tn)∇∇yn.
(5.83) n=1 n=1 Ifthenetworkhasbeentrainedonthedataset, anditsoutputsyn happentobevery close to the target values tn, then the second term in (5.83) will be small and can be neglected.
More generally, however, it may be appropriate to neglect this term bythefollowingargument.
Recallfrom Section1.5.5thattheoptimalfunctionthat minimizes a sum-of-squares loss is the conditional average of the target data.
The quantity(yn −tn)isthenarandomvariablewithzeromean.
Ifweassumethatits valueisuncorrelatedwiththevalueofthesecondderivativetermontheright-hand Exercise 5.17 sideof(5.83), thenthewholetermwillaveragetozerointhesummationovern.
Byneglectingthesecondtermin(5.83), wearriveatthe Levenberg–Marquardt approximation orouter product approximation (because the Hessian matrix is built upfromasumofouterproductsofvectors), givenby N H bnb T n (5.84) n=1 where bn = ∇yn = ∇an because the activation function for the output units is simply the identity.
Evaluation of the outer product approximation for the Hessian is straightforward as it only involves first derivatives of the error function, which can be evaluated efficiently in O(W) steps using standard backpropagation.
The elementsofthematrixcanthenbefoundin O(W2)stepsbysimplemultiplication.
It is important to emphasize that this approximation is only likely to be valid for a networkthathasbeentrainedappropriately, andthatforageneralnetworkmapping the second derivative terms on the right-hand side of (5.83) will typically not be negligible.
Inthecaseofthecross-entropyerrorfunctionforanetworkwithlogisticsigmoid Exercise 5.19 output-unitactivationfunctions, thecorrespondingapproximationisgivenby N H yn(1−yn)bnb T n .
(5.85) n=1 Ananalogousresultcanbeobtainedformulticlassnetworkshavingsoftmaxoutput- Exercise 5.20 unitactivationfunctions.
252 5.
NEURALNETWORKS 5.4.3 Inverse Hessian We can use the outer-product approximation to develop a computationally ef- ficient procedure for approximating the inverse of the Hessian (Hassibi and Stork, 1993).
Firstwewritetheouter-productapproximationinmatrixnotationas N HN = bnb T n (5.86) n=1 where bn ≡ ∇ w an is the contribution to the gradient of the output unit activation arisingfromdatapointn.
Wenowderiveasequentialprocedureforbuildingupthe Hessian by including data points one at a time.
Suppose we have already obtained theinverse Hessianusingthefirst Ldatapoints.
Byseparatingoffthecontribution fromdatapoint L+1, weobtain HL+1 =HL+b L+1 b T L+1 .
(5.87) Inordertoevaluatetheinverseofthe Hessian, wenowconsiderthematrixidentity M+vv T −1 =M −1− (M−1v) v TM−1 (5.88) 1+v TM−1v where Iistheunit matrix, whichissimply aspecial caseof the Woodbury identity (C.7).
Ifwenowidentify HL with Mandb L+1 withv, weobtain H −1 =H −1− H − L 1b L+1 b T L+1 H − L 1 .
(5.89) L+1 L 1+b T L+1 H − L 1b L+1 Inthisway, datapointsaresequentiallyabsorbeduntil L+1=N andthewholedata set has been processed.
This result therefore represents a procedure for evaluating theinverseofthe Hessianusingasinglepassthroughthedataset.
Theinitialmatrix H 0 is chosen to be αI, where α is a small quantity, so that the algorithm actually findstheinverseof H+αI.
Theresultsarenotparticularlysensitivetotheprecise valueofα.
Extensionofthisalgorithmtonetworkshavingmorethanoneoutputis Exercise 5.21 straightforward.
Wenoteherethatthe Hessianmatrixcansometimesbecalculatedindirectlyas part of the network training algorithm.
In particular, quasi-Newton nonlinear opti- mizationalgorithmsgraduallybuildupanapproximationtotheinverseofthe Hes- sianduringtraining.
Suchalgorithmsarediscussedindetailin Bishopand Nabney (2008).
5.4.4 Finite differences Asinthecaseofthefirstderivativesoftheerrorfunction, wecanfindthesecond derivativesbyusingfinitedifferences, withaccuracylimitedbynumericalprecision.
Ifweperturbeachpossiblepairofweightsinturn, weobtain ∂2E 1 ∂wji∂wlk = 4 2 {E(wji+ , wlk + ) −E(wji+ , wlk − ) −E(wji − , wlk+ )+ E(wji − , wlk − )}+O( 2).
(5.90) 5.4.
The Hessian Matrix 253 Again, by using a symmetrical central differences formulation, we ensure that the residual errors are O( 2) rather than O( ).
Because there are W2 elements in the Hessian matrix, and because the evaluation of each element requires four forward propagationseachneeding O(W)operations(perpattern), weseethatthisapproach will require O(W3) operations to evaluate the complete Hessian.
It therefore has poorscalingproperties, althoughinpracticeitisveryusefulasacheckonthesoft- wareimplementationofbackpropagationmethods.
A more efficient version of numerical differentiation can be found by applying centraldifferencestothefirstderivativesoftheerrorfunction, whicharethemselves calculatedusingbackpropagation.
Thisgives ∂2E 1 ∂E ∂E = (wlk + )− (wlk − ) +O( 2).
(5.91) ∂wji∂wlk 2 ∂wji ∂wji Because there are now only W weights to be perturbed, and because the gradients canbeevaluatedin O(W)steps, weseethatthismethodgivesthe Hessianin O(W2) operations.
5.4.5 Exact evaluation of the Hessian So far, we have considered various approximation schemes for evaluating the Hessian matrix or its inverse.
The Hessian can also be evaluated exactly, for a net- work of arbitrary feed-forward topology, using extension of the technique of back- propagation used to evaluate first derivatives, which shares many of its desirable featuresincludingcomputationalefficiency(Bishop,1991; Bishop,1992).
Itcanbe applied to any differentiable error function that can be expressed as a function of thenetworkoutputsandtonetworkshavingarbitrarydifferentiableactivationfunc- tions.
The number of computational steps needed to evaluate the Hessian scales like O(W2).
Similaralgorithmshavealsobeenconsideredby Buntineand Weigend (1993).
Here we consider the specific case of a network having two layers of weights, Exercise 5.22 for which the required equations are easily derived.
We shall use indices i and i to denote inputs, indices j and j to denoted hidden units, and indices k and k to denoteoutputs.
Wefirstdefine δk = ∂En , Mkk ≡ ∂2En (5.92) ∂ak ∂ak∂ak where En isthecontributiontotheerrorfromdatapointn.
The Hessianmatrixfor thisnetworkcanthenbeconsideredinthreeseparateblocksasfollows.
1.
Bothweightsinthesecondlayer: ∂2En (2) (2) =zjzj Mkk .
(5.93) ∂w ∂w kj k j 254 5.
NEURALNETWORKS 2.
Bothweightsinthefirstlayer: ∂2En (2) (1) (1) =xixi h (aj )Ijj w kj δk ∂w ∂w ji j i k (2) (2) +xixi h(aj )h(aj) w k j w kj Mkk .
(5.94) k k 3.
Oneweightineachlayer: ∂2En (2) (1) (2) =xih(aj ) δk Ijj +zj w k j Hkk .
(5.95) ∂w ∂w ji kj k Here Ijj isthej, j elementoftheidentitymatrix.
Ifoneorbothoftheweightsis a bias term, then the corresponding expressions are obtained simply by setting the Exercise 5.23 appropriateactivation(s)to1.
Inclusionofskip-layerconnectionsisstraightforward.
5.4.6 Fast multiplication by the Hessian Formanyapplicationsofthe Hessian, thequantityofinterestisnotthe Hessian matrix H itself but the product of H with some vector v.
We have seen that the evaluationofthe Hessiantakes O(W2)operations, anditalsorequiresstoragethatis O(W2).
Thevectorv THthatwewishtocalculate, however, hasonly W elements, so instead of computing the Hessian as an intermediate step, we can instead try to find an efficient approach to evaluating v TH directly in a way that requires only O(W)operations.
Todothis, wefirstnotethat v TH=v T∇(∇E) (5.96) where ∇ denotes the gradient operator in weight space.
We can then write down thestandardforward-propagationandbackpropagationequationsfortheevaluation of∇E andapply(5.96)totheseequationstogiveasetofforward-propagationand backpropagation equations for the evaluation of v TH (Møller, 1993; Pearlmutter, 1994).
This corresponds to acting on the original forward-propagation and back- propagationequationswithadifferentialoperatorv T∇.
Pearlmutter(1994)usedthe notation R{·}todenotetheoperatorv T∇, andweshallfollowthisconvention.
The analysisisstraightforwardandmakesuseoftheusualrulesofdifferentialcalculus, togetherwiththeresult R{w}=v.
(5.97) Thetechniqueisbestillustratedwithasimpleexample, andagainwechoosea two-layer network of the form shown in Figure 5.1, with linear output units and a sum-of-squares error function.
As before, we consider the contribution to the error function from one pattern in the data set.
The required vector is then obtained as 5.4.
The Hessian Matrix 255 usual by summing over the contributions from each of the patterns separately.
For thetwo-layernetwork, theforward-propagationequationsaregivenby aj = wjixi (5.98) i zj = h(aj) (5.99) yk = wkjzj.
(5.100) j We now act on these equations using the R{·} operator to obtain a set of forward propagationequationsintheform R{aj } = vjixi (5.101) i R{zj } = h (aj)R{aj } (5.102) R{yk } = wkj R{zj }+ vkjzj (5.103) j j where vji is the element of the vector v that corresponds to the weight wji.
Quan- tities of the form R{zj }, R{aj } and R{yk } are to be regarded as new variables whosevaluesarefoundusingtheaboveequations.
Because we are considering a sum-of-squares error function, we have the fol- lowingstandardbackpropagationexpressions: δk = yk −tk (5.104) δj = h(aj) wkjδk.
(5.105) k Again, weactontheseequationswiththe R{·}operatortoobtainasetofbackprop- agationequationsintheform R{δk } = R{yk } (5.106) R{δj } = h (aj)R{aj } wkjδk k +h (aj) vkjδk+h (aj) wkj R{δk }.
(5.107) k k Finally, wehavetheusualequationsforthefirstderivativesoftheerror ∂E = δkzj (5.108) ∂wkj ∂E = δjxi (5.109) ∂wji 256 5.
NEURALNETWORKS andactingonthesewiththe R{·}operator, weobtainexpressionsfortheelements ofthevectorv TH ∂E R = R{δk }zj +δk R{zj } (5.110) ∂wkj ∂E R = xi R{δj }.
(5.111) ∂wji The implementation of this algorithm involves the introduction of additional variables R{aj }, R{zj } and R{δj } for the hidden units and R{δk } and R{yk } for the output units.
For each input pattern, the values of these quantities can be found using the above results, and the elements of v TH are then given by (5.110) and(5.111).
Anelegantaspectofthistechniqueisthattheequationsforevaluating v THmirrorcloselythoseforstandardforwardandbackwardpropagation, andsothe extensionofexistingsoftwaretocomputethisproductistypicallystraightforward.
If desired, the technique can be used to evaluate the full Hessian matrix by choosing the vector v to be given successively by a series of unit vectors of the leadstoaformalismthatisanalyticallyequivalenttothebackpropagationprocedure of Bishop(1992), asdescribedin Section5.4.5, thoughwithsomelossofefficiency duetoredundantcalculations.
5.5.
Regularization in Neural Networks The number of input and outputs units in a neural network is generally determined bythedimensionalityofthedataset, whereasthenumber M ofhiddenunitsisafree parameterthatcanbeadjustedtogivethebestpredictiveperformance.
Notethat M controls the number of parameters (weights and biases) in the network, and so we might expect that in a maximum likelihood setting there will be an optimum value of M thatgivesthebestgeneralizationperformance, correspondingtotheoptimum balance between under-fitting and over-fitting.
Figure 5.9 shows an example of the effectofdifferentvaluesof M forthesinusoidalregressionproblem.
The generalization error, however, is not a simple function of M due to the presence of local minima in the error function, as illustrated in Figure 5.10.
Here we see the effect of choosing multiple random initializations for the weight vector for a range of values of M.
The overall best validation set performance in this caseoccurredforaparticularsolutionhaving M = 8.
Inpractice, oneapproachto choosing M is in fact to plot a graph of the kind shown in Figure 5.10 and then to choosethespecificsolutionhavingthesmallestvalidationseterror.
There are, however, other ways to control the complexity of a neural network modelinordertoavoidover-fitting.
Fromourdiscussionofpolynomialcurvefitting in Chapter1, weseethatanalternativeapproachistochoosearelativelylargevalue for M andthentocontrolcomplexitybytheadditionofaregularizationtermtothe error function.
The simplest regularizer is the quadratic, giving a regularized error 5.5.
Regularizationin Neural Networks 257 1 M =1 1 M =3 1 M =10 0 0 0 −1 −1 −1 0 1 0 1 0 1 Figure5.9 Examplesoftwo-layernetworkstrainedon10datapointsdrawnfromthesinusoidaldataset.
The graphs show the result of fitting networks having M = 1, 3 and 10 hidden units, respectively, by minimizing a sum-of-squareserrorfunctionusingascaledconjugate-gradientalgorithm.
oftheform E (w)=E(w)+ λ w Tw.
(5.112) 2 This regularizer is also known as weight decay and has been discussed at length in Chapter 3.
The effective model complexity is then determined by the choice of theregularizationcoefficientλ.
Aswehaveseenpreviously, thisregularizercanbe interpretedasthenegativelogarithmofazero-mean Gaussianpriordistributionover theweightvectorw.
5.5.1 Consistent Gaussian priors One of the limitations of simple weight decay in the form (5.112) is that is inconsistentwithcertainscalingpropertiesofnetworkmappings.
Toillustratethis, consider a multilayer perceptron network having two layers of weights and linear output units, which performs a mapping from a set of input variables {xi } to a set ofoutputvariables{yk }.
Theactivationsofthehiddenunitsinthefirsthiddenlayer Figure5.10 Plot of the sum-of-squares test-set errorforthepolynomialdatasetver- susthenumberofhiddenunitsinthe 160 network, with 30 random starts for each network size, showing the ef- 140 fect of local minima.
For each new start, the weight vector was initial- 120 ized by sampling from an isotropic Gaussiandistributionhavingamean 100 ofzeroandavarianceof10.
80 60 0 2 4 6 8 10 258 5.
NEURALNETWORKS taketheform zj =h wjixi+wj0 (5.113) i whiletheactivationsoftheoutputunitsaregivenby yk = wkjzj +wk0 .
(5.114) j Supposeweperformalineartransformationoftheinputdataoftheform xi → xi =axi+b.
(5.115) Then we can arrange for the mapping performed by the network to be unchanged bymakingacorrespondinglineartransformationoftheweightsandbiasesfromthe Exercise 5.24 inputstotheunitsinthehiddenlayeroftheform 1 wji →w ji = wji (5.116) a b wj0 →w j0 = wj0 − wji.
(5.117) a i Similarly, alineartransformationoftheoutputvariablesofthenetworkoftheform yk → yk =cyk+d (5.118) canbeachievedbymakingatransformationofthesecond-layerweightsandbiases using wkj →w kj = cwkj (5.119) wk0 →w k0 = cwk0 +d.
(5.120) Ifwetrainonenetworkusingtheoriginaldataandonenetworkusingdataforwhich theinputand/ortargetvariablesaretransformedbyoneoftheabovelineartransfor- mations, then consistency requires that we should obtain equivalent networks that differ only by the linear transformation of the weights as given.
Any regularizer shouldbeconsistentwiththisproperty, otherwiseitarbitrarilyfavoursonesolution over another, equivalent one.
Clearly, simple weight decay (5.112), that treats all weightsandbiasesonanequalfooting, doesnotsatisfythisproperty.
We therefore look for a regularizer which is invariant under the linear trans- should be invariant to re-scaling of the weights and to shifts of the biases.
Such a regularizerisgivenby λ λ 1 w2+ 2 w2 (5.121) 2 2 w∈W w∈W 1 2 where W 1 denotesthesetofweightsinthefirstlayer, W 2 denotesthesetofweights inthesecondlayer, andbiasesareexcludedfromthesummations.
Thisregularizer 5.5.
Regularizationin Neural Networks 259 willremainunchangedundertheweighttransformationsprovidedtheregularization parametersarere-scaledusingλ 1 →a1/2λ 1 andλ 2 →c−1/2λ 2.
Theregularizer(5.121)correspondstoaprioroftheform α α p(w|α 1 ,α 2 )∝exp − 1 w2− 2 w2 .
(5.122) 2 2 w∈W w∈W 1 2 Note that priors of this form areimproper (they cannot be normalized) because the biasparametersareunconstrained.
Theuseofimproperpriorscanleadtodifficulties inselectingregularizationcoefficientsandinmodelcomparisonwithinthe Bayesian framework, because the corresponding evidence is zero.
It is therefore common to includeseparatepriorsforthebiases(whichthenbreakshiftinvariance)havingtheir own hyperparameters.
We can illustrate the effect of the resulting four hyperpa- rametersbydrawingsamplesfromthepriorandplottingthecorrespondingnetwork functions, asshownin Figure5.11.
More generally, we can consider priors in which the weights are divided into anynumberofgroups W k sothat 1 p(w)∝exp − αk w 2 k (5.123) 2 k where w 2 = w2.
(5.124) k j j∈W k As a special case of this prior, if we choose the groups to correspond to the sets of weights associated with each of the input units, and we optimize the marginal likelihood with respect to the corresponding parameters αk, we obtain automatic relevancedeterminationasdiscussedin Section7.2.2.
5.5.2 Early stopping Analternativetoregularizationasawayofcontrollingtheeffectivecomplexity of a network is the procedure of early stopping.
The training of nonlinear network models corresponds to an iterative reduction of the error function defined with re- spect to a set of training data.
For many of the optimization algorithms used for network training, such as conjugate gradients, the error is a nonincreasing function oftheiterationindex.
However, theerrormeasuredwithrespecttoindependentdata, generally called a validation set, often shows a decrease at first, followed by an in- creaseasthenetworkstartstoover-fit.
Trainingcanthereforebestoppedatthepoint ofsmallesterrorwithrespecttothevalidationdataset, asindicatedin Figure5.12, inordertoobtainanetworkhavinggoodgeneralizationperformance.
The behaviour of the network in this case is sometimes explained qualitatively intermsoftheeffectivenumberofdegreesoffreedominthenetwork, inwhichthis numberstartsoutsmallandthentogrowsduringthetrainingprocess, corresponding toasteadyincreaseintheeffectivecomplexityofthemodel.
Haltingtrainingbefore 260 5.
NEURALNETWORKS αw =1,αb =1,αw =1,αb =1 αw =1,αb =1,αw =10,αb =1 1 1 2 2 1 1 2 2 4 40 2 20 0 0 −2 −20 −4 −40 −6 −60 −1 −0.5 0 0.5 1 −1 −0.5 0 0.5 1 αw =1000,αb =100,αw =1,αb =1 αw =1000,αb =1000,αw =1,αb =1 1 1 2 2 1 1 2 2 5 5 0 0 −5 −5 −10 −10 −1 −0.5 0 0.5 1 −1 −0.5 0 0.5 1 Figure5.11 Illustrationoftheeffectofthehyperparametersgoverningthepriordistributionoverweightsand biases in a two-layer network having a single input, a single linear output, and 12 hidden units having ‘tanh’ activation functions.
The priors are governed by four hyperparameters α 1 b, α 1 w, α 2 b, and α 2 w, which represent theprecisionsofthe Gaussiandistributionsofthefirst-layerbiases, first-layerweights, second-layerbiases, and second-layerweights, respectively.
Weseethattheparameterα 2 w governstheverticalscaleoffunctions(note thedifferentverticalaxisrangesonthetoptwodiagrams), α 1 w governsthehorizontalscaleofvariationsinthe function values, and α 1 b governs the horizontal range over which variations occur.
The parameter α 2 b, whose effectisnotillustratedhere, governstherangeofverticaloffsetsofthefunctions.
aminimumofthetrainingerrorhasbeenreachedthenrepresentsawayoflimiting theeffectivenetworkcomplexity.
In the case of a quadratic error function, we can verify this insight, and show that early stopping should exhibit similar behaviour to regularization using a sim- pleweight-decayterm.
Thiscanbeunderstoodfrom Figure5.13, inwhichtheaxes in weight space have been rotated to be parallel to the eigenvectors of the Hessian matrix.
If, intheabsenceofweightdecay, theweightvectorstartsattheoriginand proceeds during training along a path that follows the local negative gradient vec- tor, thentheweightvectorwillmoveinitiallyparalleltothew 2 axisthroughapoint correspondingroughlytow andthenmovetowardstheminimumoftheerrorfunc- tionw ML.
Thisfollowsfromtheshapeoftheerrorsurfaceandthewidelydiffering eigenvaluesofthe Hessian.
Stoppingatapointnearw isthereforesimilartoweight decay.
Therelationshipbetweenearlystoppingandweightdecaycanbemadequan- Exercise 5.25 titative, thereby showing that the quantity τη (where τ is the iteration index, and η is the learning rate parameter) plays the role of the reciprocal of the regularization 5.5.
Regularizationin Neural Networks 261 0.45 0.25 0.4 0.2 0.15 0.35 0 10 20 30 40 50 0 10 20 30 40 50 Figure 5.12 An illustration of the behaviour of training set error (left) and validation set error (right) during a typical training session, as a function of the iteration step, for the sinusoidal data set.
The goal of achieving thebestgeneralizationperformancesuggeststhattrainingshouldbestoppedatthepointshownbythevertical dashedlines, correspondingtotheminimumofthevalidationseterror.
parameter λ.
The effective number of parameters in the network therefore grows duringthecourseoftraining.
5.5.3 Invariances Inmanyapplicationsofpatternrecognition, itisknownthatpredictionsshould be unchanged, or invariant, under one or more transformations of the input vari- ables.
Forexample, intheclassificationofobjectsintwo-dimensionalimages, such as handwritten digits, a particular object should be assigned the same classification irrespective of its position within the image (translation invariance) or of its size (scale invariance).
Such transformations produce significant changes in the raw data, expressed in terms of the intensities at each of the pixels in the image, and yet should give rise to the same output from the classification system.
Similarly inspeechrecognition, smalllevelsofnonlinearwarpingalongthetimeaxis, which preservetemporalordering, shouldnotchangetheinterpretationofthesignal.
Ifsufficientlylargenumbersoftrainingpatternsareavailable, thenanadaptive modelsuchasaneuralnetworkcanlearntheinvariance, atleastapproximately.
This involvesincludingwithinthetrainingsetasufficientlylargenumberofexamplesof theeffectsofthevarioustransformations.
Thus, fortranslationinvarianceinanim- age, thetrainingsetshouldincludeexamplesofobjectsatmanydifferentpositions.
Thisapproachmaybeimpractical, however, ifthenumberoftrainingexamples islimited, orifthereareseveralinvariants(becausethenumberofcombinationsof transformationsgrowsexponentiallywiththenumberofsuchtransformations).
We therefore seek alternative approaches for encouraging an adaptive model to exhibit therequiredinvariances.
Thesecanbroadlybedividedintofourcategories: 1.
The training set is augmented using replicas of the training patterns, trans- formed according to the desired invariances.
For instance, in our digit recog- nitionexample, wecouldmakemultiplecopiesofeachexampleinwhichthe 262 5.
NEURALNETWORKS Figure5.13 A schematic illustration of why w2 early stopping can give similar results to weight decay in the case of a quadratic error func- tion.
The ellipse shows a con- tour of constant error, and w ML w ML denotes the minimum of the er- ror function.
If the weight vector w startsattheoriginandmovesac- cordingtothelocalnegativegra- dient direction, then it will follow thepathshownbythecurve.
By stopping training early, a weight vector we is found that is qual- w1 itatively similar to that obtained with a simple weight-decay reg- ularizer and training to the mini- mum of the regularized error, as can be seen by comparing with Figure3.15.
digitisshiftedtoadifferentpositionineachimage.
2.
A regularization term is added to the error function that penalizes changes in themodeloutputwhentheinputistransformed.
Thisleadstothetechniqueof tangentpropagation, discussedin Section5.5.4.
3.
Invarianceisbuiltintothepre-processingbyextractingfeaturesthatareinvari- ant under the required transformations.
Any subsequent regression or classi- fication system that uses such features as inputs will necessarily also respect theseinvariances.
4.
Thefinaloptionistobuildtheinvariancepropertiesintothestructureofaneu- ralnetwork(orintothedefinitionofakernelfunctioninthecaseoftechniques suchastherelevancevectormachine).
Onewaytoachievethisisthroughthe useoflocalreceptivefieldsandsharedweights, asdiscussedinthecontextof convolutionalneuralnetworksin Section5.5.6.
Approach1isoftenrelativelyeasytoimplementandcanbeusedtoencouragecom- plex invariances such as those illustrated in Figure 5.14.
For sequential training algorithms, thiscanbedonebytransformingeachinputpatternbeforeitispresented to the model so that, if the patterns are being recycled, a different transformation (drawn from an appropriate distribution) is added each time.
For batch methods, a similareffectcanbeachievedbyreplicatingeachdatapointanumberoftimesand transformingeachcopyindependently.
Theuseofsuchaugmenteddatacanleadto significantimprovementsingeneralization(Simardetal.,2003), althoughitcanalso becomputationallycostly.
Approach2leavesthedatasetunchangedbutmodifiestheerrorfunctionthrough the addition of a regularizer.
In Section 5.5.5, we shall show that this approach is closelyrelatedtoapproach2.
5.5.
Regularizationin Neural Networks 263 Figure 5.14 Illustration of the synthetic warping of a handwritten digit.
The original image is shown on the left.
On the right, the top row shows three examples of warped digits, with the corresponding displacement fieldsshownonthebottomrow.
Thesedisplacementfieldsaregeneratedbysamplingrandomdisplacements ∆x,∆y ∈ (0,1) at each pixel and then smoothing by convolution with Gaussians of width 0.01, 30 and 60 respectively.
Oneadvantageofapproach3isthatitcancorrectlyextrapolatewellbeyondthe range of transformations included in the training set.
However, it can be difficult to find hand-crafted features with the required invariances that do not also discard informationthatcanbeusefulfordiscrimination.
5.5.4 Tangent propagation Wecanuseregularizationtoencouragemodelstobeinvarianttotransformations of the input through the technique of tangent propagation (Simard et al., 1992).
Considertheeffectofatransformationonaparticularinputvectorxn.
Providedthe transformationiscontinuous(suchastranslationorrotation, butnotmirrorreflection forinstance), thenthetransformedpatternwillsweepoutamanifold Mwithinthe D-dimensional input space.
This is illustrated in Figure 5.15, for the case of D = 2 for simplicity.
Suppose the transformation is governed by a single parameter ξ (whichmightberotationangleforinstance).
Thenthesubspace Msweptoutbyxn Figure5.15 Illustration of a two-dimensional input space x2 showingtheeffectofacontinuoustransforma- M τ tion on a particular input vector x n.
A one- n dimensionaltransformation, parameterizedby ξ thecontinuousvariableξ, appliedtox ncauses xn ittosweepoutaone-dimensionalmanifold M.
Locally, theeffectofthetransformationcanbe approximatedbythetangentvectorτ n.
x1 264 5.
NEURALNETWORKS willbeone-dimensional, andwillbeparameterizedbyξ.
Letthevectorthatresults from acting on xn by this transformation be denoted by s(xn,ξ), which is defined so that s(x,0) = x.
Then the tangent to the curve M is given by the directional derivativeτ =∂s/∂ξ, andthetangentvectoratthepointxn isgivenby τ n = ∂s(xn,ξ) .
(5.125) ∂ξ ξ=0 Underatransformationoftheinputvector, thenetworkoutputvectorwill, ingeneral, change.
Thederivativeofoutputkwithrespecttoξ isgivenby D D ∂ ∂ y ξ k ξ=0 = i=1 ∂ ∂ y x k i ∂ ∂ x ξ i ξ=0 = i=1 Jkiτi (5.126) where Jkiisthe(k, i)elementofthe Jacobianmatrix J, asdiscussedin Section5.3.4.
Theresult(5.126)canbeusedtomodifythestandarderrorfunction, soastoencour- age local invariance in the neighbourhood of the data points, by the addition to the originalerrorfunction E ofaregularizationfunctionΩtogiveatotalerrorfunction oftheform E =E+λΩ (5.127) whereλisaregularizationcoefficientand 2 D 2 1 ∂ynk 1 Ω= = Jnkiτni .
(5.128) 2 ∂ξ 2 n k ξ=0 n k i=1 The regularization function will be zero when the network mapping function is in- variant under the transformation in the neighbourhood of each pattern vector, and thevalueoftheparameterλdeterminesthebalancebetweenfittingthetrainingdata andlearningtheinvarianceproperty.
In a practical implementation, the tangent vector τ n can be approximated us- ing finite differences, by subtracting the original vector xn from the corresponding vectoraftertransformationusingasmallvalueofξ, andthendividingbyξ.
Thisis illustratedin Figure5.16.
The regularization function depends on the network weights through the Jaco- bian J.
A backpropagation formalism for computing the derivatives of the regu- Exercise 5.26 larizer with respect to the network weights is easily obtained by extension of the techniquesintroducedin Section5.3.
If the transformation is governed by L parameters (e.
g., L = 3 for the case of translationscombinedwithin-planerotationsinatwo-dimensionalimage), thenthe manifold Mwillhavedimensionality L, andthecorrespondingregularizerisgiven by the sum of terms of the form (5.128), one for each transformation.
If several transformationsareconsideredatthesametime, andthenetworkmappingismade invarianttoeachseparately, thenitwillbe(locally)invarianttocombinationsofthe transformations(Simardetal.,1992).
5.5.
Regularizationin Neural Networks 265 Figure5.16 Illustration showing (a) the original image x of a hand- (a) (b) written digit, (b) the tangent vector τ corresponding to an infinitesimal clockwise rotation, (c) the result of addingasmallcontributionfromthe tangent vector to the original image giving x+ τ with = 15 degrees, and (d) the true image rotated for comparison.
(c) (d) A related technique, called tangent distance, can be used to build invariance propertiesintodistance-basedmethodssuchasnearest-neighbourclassifiers(Simard etal.,1993).
5.5.5 Training with transformed data Wehaveseenthatonewaytoencourageinvarianceofamodeltoasetoftrans- formations is to expand the training set using transformed versions of the original inputpatterns.
Hereweshowthatthisapproachiscloselyrelatedtothetechniqueof tangentpropagation(Bishop,1995b; Leen,1995).
As in Section 5.5.4, we shall consider a transformation governed by a single parameter ξ and described by the function s(x,ξ), with s(x,0) = x.
We shall alsoconsiderasum-of-squareserrorfunction.
Theerrorfunctionforuntransformed inputscanbewritten(intheinfinitedatasetlimit)intheform 1 E = {y(x)−t}2p(t|x)p(x)dxdt (5.129) 2 as discussed in Section 1.5.5.
Here we have considered a network having a single output, in order to keep the notation uncluttered.
If we now consider an infinite numberofcopiesofeachdatapoint, eachofwhichisperturbedbythetransformation 266 5.
NEURALNETWORKS in which the parameter ξ is drawn from a distribution p(ξ), then the error function definedoverthisexpandeddatasetcanbewrittenas E = 1 {y(s(x,ξ))−t}2p(t|x)p(x)p(ξ)dxdtdξ.
(5.130) 2 Wenowassumethatthedistributionp(ξ)haszeromeanwithsmallvariance, sothat weareonlyconsideringsmalltransformationsoftheoriginalinputvectors.
Wecan thenexpandthetransformationfunctionasa Taylorseriesinpowersofξ togive s(x,ξ) = s(x,0)+ξ ∂ s(x,ξ) + ξ2 ∂2 s(x,ξ) +O(ξ3) ∂ξ 2 ∂ξ2 ξ=0 ξ=0 1 = x+ξτ + ξ2τ +O(ξ3) 2 whereτ denotesthesecondderivativeofs(x,ξ)withrespecttoξevaluatedatξ =0.
Thisallowsustoexpandthemodelfunctiontogive - .
y(s(x,ξ))=y(x)+ξτT∇y(x)+ ξ2 (τ ) T∇y(x)+τT∇∇y(x)τ +O(ξ3).
2 Substitutingintothemeanerrorfunction(5.130)andexpanding, wethenhave E = 1 {y(x)−t}2p(t|x)p(x)dxdt 2 + E[ξ] {y(x)−t}τT∇y(x)p(t|x)p(x)dxdt + E[ξ2] {y(x)−t} 1 (τ ) T∇y(x)+τT∇∇y(x)τ 2 .
+ τT∇y(x) 2 p(t|x)p(x)dxdt+O(ξ3).
Becausethedistributionoftransformationshaszeromeanwehave E[ξ] = 0.
Also, weshalldenote E[ξ2]byλ.
Omittingtermsof O(ξ3), theaverageerrorfunctionthen becomes E =E+λΩ (5.131) where Eistheoriginalsum-of-squareserror, andtheregularizationtermΩtakesthe form Ω = {y(x)−E[t|x]} 1 (τ ) T∇y(x)+τT∇∇y(x)τ 2 + τT∇y(x) 2 p(x)dx (5.132) inwhichwehaveperformedtheintegrationovert.
5.5.
Regularizationin Neural Networks 267 Wecanfurthersimplifythisregularizationtermasfollows.
In Section1.5.5we sawthatthefunctionthatminimizesthesum-of-squareserrorisgivenbythecondi- tionalaverage E[t|x]ofthetargetvaluest.
From(5.131)weseethattheregularized errorwillequaltheunregularizedsum-of-squaresplustermswhichare O(ξ), andso thenetworkfunctionthatminimizesthetotalerrorwillhavetheform y(x)=E[t|x]+O(ξ).
(5.133) Thus, toleadingorderinξ, thefirsttermintheregularizervanishesandweareleft with Ω= 1 τT∇y(x) 2 p(x)dx (5.134) 2 whichisequivalenttothetangentpropagationregularizer(5.128).
Ifweconsiderthespecialcaseinwhichthetransformationoftheinputssimply consists of the addition of random noise, so that x → x+ξ, then the regularizer Exercise 5.27 takestheform 1 Ω= ∇y(x) 2 p(x)dx (5.135) 2 which is known as Tikhonov regularization (Tikhonov and Arsenin, 1977; Bishop, 1995b).
Derivatives of this regularizer with respect to the network weights can be foundusinganextendedbackpropagationalgorithm(Bishop,1993).
Weseethat, for smallnoiseamplitudes, Tikhonovregularizationisrelatedtotheadditionofrandom noisetotheinputs, whichhasbeenshowntoimprovegeneralizationinappropriate circumstances(Sietsmaand Dow,1991).
5.5.6 Convolutional networks Anotherapproachtocreatingmodelsthatareinvarianttocertaintransformation of the inputs is to build the invariance properties into the structure of a neural net- work.
This is the basis for the convolutional neural network (Le Cun et al., 1989; Le Cunetal.,1998), whichhasbeenwidelyappliedtoimagedata.
Consider the specific task of recognizing handwritten digits.
Each input image comprisesasetofpixelintensityvalues, andthedesiredoutputisaposteriorproba- bilitydistributionoverthetendigitclasses.
Weknowthattheidentityofthedigitis invariantundertranslationsandscalingaswellas(small)rotations.
Furthermore, the network mustalsoexhibitinvariancetomoresubtle transformations suchaselastic deformationsofthekindillustratedin Figure5.14.
Onesimpleapproachwouldbeto treattheimageastheinputtoafullyconnectednetwork, suchasthekindshownin Figure5.1.
Givenasufficientlylargetrainingset, suchanetworkcouldinprinciple yieldagoodsolutiontothisproblemandwouldlearntheappropriateinvariancesby example.
However, this approach ignores a key property of images, which is that nearby pixels are more strongly correlated than more distant pixels.
Many of the modern approachestocomputervisionexploitthispropertybyextractinglocalfeaturesthat dependonlyonsmallsubregionsoftheimage.
Informationfromsuchfeaturescan thenbemergedinlaterstagesofprocessinginordertodetecthigher-orderfeatures 268 5.
NEURALNETWORKS Sub-sampling Input image Convolutional layer layer Figure 5.17 Diagram illustrating part of a convolutional neural network, showing a layer of convolu- tional units followed by a layer of subsampling units.
Several successive pairs of such layersmaybeused.
and ultimately to yield information about the image as whole.
Also, local features that are useful inone region of the image are likely to be useful in other regions of theimage, forinstanceiftheobjectofinterestistranslated.
Thesenotionsareincorporatedintoconvolutionalneuralnetworksthroughthree mechanisms: (i)localreceptivefields,(ii)weightsharing, and(iii)subsampling.
The structureofaconvolutionalnetworkisillustratedin Figure5.17.
Intheconvolutional layertheunitsareorganizedintoplanes, eachofwhichiscalledafeaturemap.
Units inafeaturemapeachtakeinputsonlyfromasmallsubregionoftheimage, andall of the units in a feature map are constrained to share the same weight values.
For instance, a feature map might consist of 100 units arranged in a10×10 grid, with eachunittakinginputsfroma5×5pixelpatchoftheimage.
Thewholefeaturemap therefore has 25 adjustable weight parameters plus one adjustable bias parameter.
Inputvaluesfromapatcharelinearlycombinedusingtheweightsandthebias, and theresulttransformedbyasigmoidalnonlinearityusing(5.1).
Ifwethinkoftheunits asfeaturedetectors, thenalloftheunitsinafeaturemapdetectthesamepatternbut at different locations in the input image.
Due to the weight sharing, the evaluation of the activations of these units is equivalent to a convolution of the image pixel intensities with a ‘kernel’ comprising the weight parameters.
If the input image is shifted, theactivationsofthefeaturemapwillbeshiftedbythesameamountbutwill otherwisebeunchanged.
Thisprovidesthebasisforthe(approximate)invarianceof 5.5.
Regularizationin Neural Networks 269 the network outputs to translations and distortions of the input image.
Because we will typically need to detect multiple features in order to build an effective model, therewillgenerallybemultiplefeaturemapsintheconvolutionallayer, eachhaving itsownsetofweightandbiasparameters.
Theoutputsoftheconvolutionalunitsformtheinputstothesubsamplinglayer of the network.
For each feature map in the convolutional layer, there is a plane of unitsinthesubsamplinglayerandeachunittakesinputsfromasmallreceptivefield in the corresponding feature map of the convolutional layer.
These units perform subsampling.
For instance, each subsampling unit might take inputs from a 2×2 unit region in the corresponding feature map and would compute the average of thoseinputs, multipliedbyanadaptiveweightwiththeadditionofanadaptivebias parameter, and then transformed using a sigmoidal nonlinear activation function.
The receptive fields are chosen to be contiguous and nonoverlapping so that there are half the number of rows and columns in the subsampling layer compared with theconvolutionallayer.
Inthisway, theresponseofaunitinthesubsamplinglayer willberelativelyinsensitivetosmallshiftsoftheimageinthecorrespondingregions oftheinputspace.
Inapracticalarchitecture, theremaybeseveralpairsofconvolutionalandsub- sampling layers.
At each stage there is a larger degree of invariance to input trans- formationscomparedtothepreviouslayer.
Theremaybeseveralfeaturemapsina givenconvolutionallayerforeachplaneofunitsintheprevioussubsamplinglayer, sothatthegradualreductioninspatialresolutionisthencompensatedbyanincreas- ing number of features.
The final layer of the network would typically be a fully connected, fully adaptive layer, with a softmax output nonlinearity in the case of multiclassclassification.
Thewholenetworkcanbetrainedbyerrorminimizationusingbackpropagation to evaluate the gradient of the error function.
This involves a slight modification oftheusualbackpropagationalgorithmtoensurethattheshared-weightconstraints Exercise 5.28 are satisfied.
Due to the use of local receptive fields, the number of weights in the network is smaller than if the network were fully connected.
Furthermore, the numberofindependentparameterstobelearnedfromthedataismuchsmallerstill, duetothesubstantialnumbersofconstraintsontheweights.
5.5.7 Soft weight sharing One way to reduce the effective complexity of a network with a large number of weights is to constrain weights within certain groups to be equal.
This is the techniqueofweightsharingthatwasdiscussedin Section5.5.6asawayofbuilding translation invariance into networks used for image interpretation.
It is only appli- cable, however, to particular problems in which the form of the constraints can be specified in advance.
Here we consider a form of soft weight sharing (Nowlan and Hinton, 1992) in which the hard constraint of equal weights is replaced by a form ofregularizationinwhichgroupsofweightsareencouragedtohavesimilarvalues.
Furthermore, the division of weights into groups, the mean weight value for each group, and the spread of values within the groups are all determined as part of the learningprocess.
270 5.
NEURALNETWORKS Recallthatthesimpleweightdecayregularizer, givenin(5.112), canbeviewed asthenegativelogofa Gaussianpriordistributionovertheweights.
Wecanencour- agetheweightvaluestoformseveralgroups, ratherthanjustonegroup, byconsid- Section2.3.9 ering instead a probability distribution that is a mixture of Gaussians.
The centres andvariancesofthe Gaussiancomponents, aswellasthemixingcoefficients, willbe consideredasadjustableparameterstobedeterminedaspartofthelearningprocess.
Thus, wehaveaprobabilitydensityoftheform p(w)= p(wi) (5.136) i where M p(wi)= πj N(wi |µj,σ j 2) (5.137) j=1 and πj are the mixing coefficients.
Taking the negative logarithm then leads to a regularizationfunctionoftheform M Ω(w)=− ln πj N(wi |µj,σ j 2) .
(5.138) i j=1 Thetotalerrorfunctionisthengivenby E(w)=E(w)+λΩ(w) (5.139) where λ is the regularization coefficient.
This error is minimized both with respect to the weights wi and with respect to the parameters {πj,µj,σj } of the mixture model.
Iftheweightswereconstant, thentheparametersofthemixturemodelcould bedeterminedbyusingthe EMalgorithmdiscussedin Chapter9.
However, thedis- tributionofweightsisitselfevolvingduringthelearningprocess, andsotoavoidnu- mericalinstability, ajointoptimizationisperformedsimultaneouslyovertheweights andthemixture-modelparameters.
Thiscanbedoneusingastandardoptimization algorithmsuchasconjugategradientsorquasi-Newtonmethods.
Inordertominimizethetotalerrorfunction, itisnecessarytobeabletoevaluate itsderivativeswithrespecttothevariousadjustableparameters.
Todothisitiscon- venienttoregardthe{πj }asprior probabilitiesandtointroducethecorresponding posteriorprobabilitieswhich, following(2.192), aregivenby Bayes’theoreminthe form πj N(w|µj,σ j 2) γj(w)= k πk N(w|µk,σ k 2) .
(5.140) Thederivativesofthetotalerrorfunctionwithrespecttotheweightsarethengiven Exercise 5.29 by ∂E ∂E (wi −µj) ∂wi = ∂wi +λ j γj(wi) σ j 2 .
(5.141) 5.5.
Regularizationin Neural Networks 271 The effect of the regularization term is therefore to pull each weight towards the centre of the jth Gaussian, with a force proportional to the posterior probability of that Gaussian for the given weight.
This is precisely the kind of effect that we are seeking.
Derivatives of the error with respect to the centres of the Gaussians are also Exercise 5.30 easilycomputedtogive ∂E (µi −wj) ∂µj =λ i γj(wi) σ j 2 (5.142) which has a simple intuitive interpretation, because it pushes µj towards an aver- ageoftheweightvalues, weightedbytheposteriorprobabilitiesthattherespective weight parameters were generated by component j.
Similarly, the derivatives with Exercise 5.31 respecttothevariancesaregivenby ∂ ∂ σ E j =λ i γj(wi) σ 1 j − (wi − σ j 3 µj)2 (5.143) whichdrivesσjtowardstheweightedaverageofthesquareddeviationsoftheweights aroundthecorrespondingcentreµj, wheretheweightingcoefficientsareagaingiven bytheposteriorprobabilitythateachweightisgeneratedbycomponentj.
Notethat inapracticalimplementation, newvariablesηj definedby σ j 2 =exp(ηj) (5.144) are introduced, and the minimization is performed with respect to the ηj.
This en- sures that the parameters σj remain positive.
It also has the effect of discouraging pathological solutions in which one or more of the σj goes to zero, corresponding toa Gaussiancomponentcollapsingontooneoftheweightparametervalues.
Such solutionsarediscussedinmoredetailinthecontextof Gaussianmixturemodelsin Section9.2.1.
For the derivatives with respect to the mixing coefficients πj, we need to take accountoftheconstraints πj =1, 0 πi 1 (5.145) j which follow from the interpretation of the πj as prior probabilities.
This can be done by expressing the mixing coefficients in terms of a set of auxiliary variables {ηj }usingthesoftmaxfunctiongivenby exp(ηj) πj = M .
(5.146) k=1 exp(ηk) The derivatives of the regularized error function with respect to the {ηj } then take Exercise 5.32 theform 272 5.
NEURALNETWORKS F in ig w u h r i e ch 5.
t 1 h 8 e C T a h rt e es le ia f n t fi c g o u o r r e di s n h a o te w s s ( a x 1 t , w x o 2 - ) lin o k ft r h o e bo e t nd ar e m f- , L2 (x1, x2) (x1, x2) fector aredetermined uniquely by the two joint angles θ 1 andθ 2andthe(fixed)lengths L 1and L 2ofthearms.
This θ2 is know as the forwardkinematicsof the arm.
In prac- elbow tice, wehavetofindthejointanglesthatwillgiverisetoa up desiredendeffectorpositionand, asshownintherightfig- L1 elbow ure, thisinversekinematicshastwosolutionscorrespond- θ1 down ingto‘elbowup’and‘elbowdown’.
∂E = {πj −γj(wi)}.
(5.147) ∂ηj i Weseethatπj isthereforedriventowardstheaverageposteriorprobabilityforcom- ponentj.
5.6.
Mixture Density Networks Thegoalofsupervisedlearningistomodelaconditionaldistributionp(t|x), which for many simple regression problems is chosen to be Gaussian.
However, practical machinelearningproblemscanoftenhavesignificantlynon-Gaussiandistributions.
Thesecanarise, forexample, withinverseproblemsinwhichthedistributioncanbe multimodal, in which case the Gaussian assumption can lead to very poor predic- tions.
Asasimpleexampleofaninverseproblem, considerthekinematicsofarobot Exercise 5.33 arm, asillustratedin Figure5.18.
Theforwardprobleminvolvesfindingtheendef- fectorpositiongiventhejointanglesandhasauniquesolution.
However, inpractice wewishtomovetheendeffectoroftherobottoaspecificposition, andtodothiswe must set appropriate joint angles.
We therefore need to solve the inverse problem, whichhastwosolutionsasseenin Figure5.18.
Forwardproblemsoftencorrespondstocausalityinaphysicalsystemandgen- erally have a unique solution.
For instance, a specific pattern of symptoms in the humanbodymaybecausedbythepresenceofaparticulardisease.
Inpatternrecog- nition, however, we typically have to solve an inverse problem, such as trying to predict the presence of a disease given a set of symptoms.
If the forward problem involvesamany-to-onemapping, thentheinverseproblemwillhavemultiplesolu- tions.
Forinstance, severaldifferentdiseasesmayresultinthesamesymptoms.
Intheroboticsexample, thekinematicsisdefinedbygeometricalequations, and themultimodalityisreadilyapparent.
However, inmanymachinelearningproblems thepresenceofmultimodality, particularlyinproblemsinvolvingspacesofhighdi- mensionality, canbelessobvious.
Fortutorialpurposes, however, weshallconsider asimpletoyproblemforwhichwecaneasilyvisualizethemultimodality.
Datafor thisproblemisgeneratedbysamplingavariablexuniformlyovertheinterval(0,1), to give a set of values {xn }, and the corresponding target values tn are obtained 5.6.
Mixture Density Networks 273 Figure5.19 Ontheleftisthedata setforasimple‘forwardproblem’in 1 1 whichtheredcurveshowstheresult of fitting a two-layer neural network by minimizing the sum-of-squares error function.
The corresponding inverseproblem, shownontheright, is obtained by exchanging the roles of x and t.
Here the same net- worktrainedagainbyminimizingthe sum-of-squares error function gives averypoorfittothedataduetothe 0 0 multimodalityofthedataset.
0 1 0 1 bycomputingthefunctionxn +0.3sin(2πxn)andthenaddinguniformnoiseover theinterval(−0.1,0.1).
Theinverseproblemisthenobtainedbykeepingthesame datapointsbutexchangingtherolesofxandt.
Figure5.19showsthedatasetsfor the forward and inverse problems, along with the results of fitting two-layer neural networkshaving6hiddenunitsandasinglelinearoutputunitbyminimizingasum- of-squares error function.
Leastsquares corresponds tomaximum likelihood under a Gaussian assumption.
We see that this leads to a very poor model for the highly non-Gaussianinverseproblem.
We therefore seek a general framework for modelling conditional probability distributions.
This can be achieved by using a mixture model for p(t|x) in which boththemixingcoefficientsaswellasthecomponentdensitiesareflexiblefunctions oftheinputvectorx, givingrisetothemixturedensitynetwork.
Foranygivenvalue of x, the mixture model provides a general formalism for modelling an arbitrary conditional density function p(t|x).
Provided we consider a sufficiently flexible network, we then have a framework for approximating arbitrary conditional distri- butions.
Hereweshalldevelopthemodelexplicitlyfor Gaussiancomponents, sothat K p(t|x)= πk(x)N t|µ k (x),σ k 2(x) .
(5.148) k=1 This is an example of a heteroscedastic model since the noise variance on the data isafunctionoftheinputvectorx.
Insteadof Gaussians, wecanuseotherdistribu- tions for the components, such as Bernoulli distributions if the target variables are binaryratherthancontinuous.
Wehavealsospecializedtothecaseofisotropicco- variances for the components, although the mixture density network can readily be extended to allow for general covariance matrices by representing the covariances using a Cholesky factorization (Williams, 1996).
Even with isotropic components, theconditionaldistributionp(t|x)doesnotassumefactorizationwithrespecttothe componentsoft(incontrasttothestandardsum-of-squaresregressionmodel)asa consequenceofthemixturedistribution.
We now take the various parameters of the mixture model, namely the mixing coefficients πk(x), the means µ k (x), and the variances σ k 2(x), to be governed by 274 5.
NEURALNETWORKS p(t|x) x D θM θ x1 θ1 t Figure5.20 Themixturedensitynetworkcanrepresentgeneralconditionalprobabilitydensitiesp(t|x) byconsideringaparametricmixturemodelforthedistributionoftwhoseparametersare determinedbytheoutputsofaneuralnetworkthattakesxasitsinputvector.
theoutputsofaconventionalneuralnetworkthattakesxasitsinput.
Thestructure of this mixture density network is illustrated in Figure 5.20.
The mixture density networkiscloselyrelatedtothemixtureofexpertsdiscussedin Section14.5.3.
The principledifferenceisthatinthemixturedensitynetworkthesamefunctionisused topredicttheparametersofallofthecomponentdensitiesaswellasthemixingco- efficients, andsothenonlinearhiddenunitsaresharedamongsttheinput-dependent functions.
The neural network in Figure 5.20 can, for example, be a two-layer network having sigmoidal (‘tanh’) hidden units.
If there are L components in the mixture model(5.148), andifthas K components, thenthenetworkwillhave Loutputunit activations denoted by aπ k that determine the mixing coefficients πk(x), K outputs denotedbyaσ k thatdeterminethekernelwidthsσk(x), and L×K outputsdenoted bya µ kj thatdeterminethecomponentsµkj(x)ofthekernelcentresµ k (x).
Thetotal number of network outputs is given by (K +2)L, as compared with the usual K outputs for a network, which simply predicts the conditional means of the target variables.
Themixingcoefficientsmustsatisfytheconstraints K πk(x)=1, 0 πk(x) 1 (5.149) k=1 whichcanbeachievedusingasetofsoftmaxoutputs exp(aπ) πk(x)= K exp k (aπ) .
(5.150) l=1 l Similarly, thevariancesmustsatisfyσ2(x) 0andsocanberepresentedinterms k oftheexponentialsofthecorrespondingnetworkactivationsusing σ σk(x)=exp(a k ).
(5.151) Finally, because the means µ (x) have real components, they can be represented k 5.6.
Mixture Density Networks 275 directlybythenetworkoutputactivations µ µkj(x)=a kj .
(5.152) Theadaptiveparametersofthemixturedensitynetworkcomprisethevectorw ofweightsandbiasesintheneuralnetwork, thatcanbesetbymaximumlikelihood, orequivalentlybyminimizinganerrorfunctiondefinedtobethenegativelogarithm ofthelikelihood.
Forindependentdata, thiserrorfunctiontakestheform N k E(w)=− ln πk(xn, w)N tn |µ k (xn, w),σ k 2(xn, w) (5.153) n=1 k=1 wherewehavemadethedependenciesonwexplicit.
In order to minimize the error function, we need to calculate the derivatives of the error E(w) with respect to the components of w.
These can be evaluated by using the standard backpropagation procedure, provided we obtain suitable expres- sionsforthederivativesoftheerrorwithrespecttotheoutput-unitactivations.
These representerrorsignalsδ foreachpatternandforeachoutputunit, andcanbeback- propagated to the hidden units and the error function derivatives evaluated in the usual way.
Because the error function (5.153) is composed of a sum of terms, one for each training data point, we can consider the derivatives for a particular pattern nandthenfindthederivativesof E bysummingoverallpatterns.
Because we are dealing with mixture distributions, it is convenient to view the mixing coefficients πk(x) as x-dependent prior probabilities and to introduce the correspondingposteriorprobabilitiesgivenby γk(t|x)= π K l= k 1 N π n l N k nl (5.154) where N nk denotes N (tn |µ k (xn),σ k 2(xn)).
Thederivativeswithrespecttothenetworkoutputactivationsgoverningthemix- Exercise 5.34 ingcoefficientsaregivenby ∂ ∂ E aπ n =πk −γk.
(5.155) k Similarly, thederivativeswithrespecttotheoutputactivationscontrollingthecom- Exercise 5.35 ponentmeansaregivenby ∂En µkl −tl ∂a µ =γk σ2 .
(5.156) kl k Finally, thederivativeswithrespecttotheoutputactivationscontrollingthecompo- Exercise 5.36 nentvariancesaregivenby ∂ ∂ E aσ k n =−γk t− σ µ k 3 k 2 − σ 1 k .
(5.157) 276 5.
NEURALNETWORKS Figure5.21 (a) Plot of the mixing coefficients π k (x) as a function of 1 1 x for the three kernel functions in a mixture density network trained on thedatashownin Figure5.19.
The model has three Gaussian compo- nents, and uses a two-layer multi- layerperceptronwithfive‘tanh’sig- moidalunitsinthehiddenlayer, and nineoutputs(correspondingtothe3 0 0 meansand3variancesofthe Gaus- sian components and the 3 mixing 0 1 0 1 coefficients).
Atbothsmallandlarge values of x, where the conditional (a) (b) probabilitydensityofthetargetdata is unimodal, only one of the ker- nels has a high value for its prior 1 1 probability, whileatintermediateval- uesofx, wheretheconditionalden- sityistrimodal, thethreemixingco- efficients have comparable values.
(b) Plots of the means µ k (x) using the same colour coding as for the mixing coefficients.
(c) Plot of the contours of the corresponding con- ditionalprobabilitydensityofthetar- 0 0 get data for the same mixture den- 0 1 0 1 sity network.
(d) Plot of the ap- proximate conditional mode, shown (c) (d) by the red points, of the conditional density.
We illustrate the use of a mixture density network by returning to the toy ex- ample of an inverse problem shown in Figure 5.19.
Plots of the mixing coeffi- cientsπk(x), themeansµk(x), andtheconditional densitycontourscorresponding top(t|x), areshownin Figure5.21.
Theoutputsoftheneuralnetwork, andhencethe parametersinthemixturemodel, arenecessarilycontinuoussingle-valuedfunctions oftheinputvariables.
However, weseefrom Figure5.21(c)thatthemodelisableto produceaconditionaldensitythatisunimodalforsomevaluesofxandtrimodalfor othervaluesbymodulatingtheamplitudesofthemixingcomponentsπk(x).
Onceamixturedensitynetworkhasbeentrained, itcanpredicttheconditional density function of the target data for any given value of the input vector.
This conditionaldensityrepresentsacompletedescriptionofthegeneratorofthedata, so far as the problem of predicting the value of the output vector is concerned.
From thisdensityfunctionwecancalculatemorespecificquantitiesthatmaybeofinterest indifferentapplications.
Oneofthesimplestoftheseisthemean, correspondingto theconditionalaverageofthetargetdata, andisgivenby K E[t|x]= tp(t|x)dt= πk(x)µ k (x) (5.158) k=1 5.7.
Bayesian Neural Networks 277 where we have used (5.148).
Because a standard network trained by least squares is approximating the conditional mean, we see that a mixture density network can reproduce the conventional least-squares result as a special case.
Of course, as we havealreadynoted, foramultimodaldistributiontheconditionalmeanisoflimited value.
Wecan similarly evaluate thevariance of the density function about thecondi- Exercise 5.37 tionalaverage, togive s2(x) = E t−E[t|x] 2|x (5.159) ⎧ ⎫ ’ ’ K ⎨ ’ K ’2⎬ ’ ’ = πk(x) ⎩ σ k 2(x)+’ ’ µ k (x)− πl(x)µ l (x)’ ’ ⎭ (5.160) k=1 l=1 wherewehaveused(5.148)and(5.158).
Thisismoregeneralthanthecorresponding least-squaresresultbecausethevarianceisafunctionofx.
We have seen that for multimodal distributions, the conditional mean can give a poor representation of the data.
For instance, in controlling the simple robot arm shown in Figure 5.18, we need to pick one of the two possible joint angle settings inordertoachievethedesiredend-effectorlocation, whereastheaverageofthetwo solutions is not itself a solution.
In such cases, the conditional mode may be of morevalue.
Becausetheconditionalmodeforthemixturedensitynetworkdoesnot have a simple analytical solution, this would require numerical iteration.
A simple alternativeistotakethemeanofthemostprobablecomponent(i.
e., theonewiththe largest mixing coefficient) at each value of x.
This is shown for the toy data set in Figure5.21(d).
5.7.
Bayesian Neural Networks Sofar, ourdiscussionofneuralnetworkshasfocussedontheuseofmaximumlike- lihoodtodeterminethenetworkparameters(weightsandbiases).
Regularizedmax- imum likelihood can be interpreted as a MAP (maximum posterior) approach in which the regularizer can be viewed as the logarithm of a prior parameter distribu- tion.
However, ina Bayesiantreatmentweneedtomarginalizeoverthedistribution ofparametersinordertomakepredictions.
In Section3.3, wedevelopeda Bayesiansolutionforasimplelinearregression model under the assumption of Gaussian noise.
We saw that the posterior distribu- tion, whichis Gaussian, couldbeevaluatedexactlyandthatthepredictivedistribu- tion could also be found in closed form.
In the case of a multilayered network, the highlynonlineardependenceofthenetworkfunctionontheparametervaluesmeans thatanexact Bayesiantreatmentcannolongerbefound.
Infact, thelogofthepos- teriordistributionwillbenonconvex, correspondingtothemultiplelocalminimain theerrorfunction.
Thetechniqueofvariationalinference, tobediscussedin Chapter10, hasbeen applied to Bayesian neural networks using a factorized Gaussian approximation 278 5.
NEURALNETWORKS to the posterior distribution (Hinton and van Camp, 1993) and also using a full- covariance Gaussian (Barber and Bishop, 1998a; Barber and Bishop, 1998b).
The most complete treatment, however, has been based on the Laplace approximation (Mac Kay,1992c; Mac Kay,1992b)andformsthebasisforthediscussiongivenhere.
We will approximate the posterior distribution by a Gaussian, centred at a mode of the true posterior.
Furthermore, we shall assume that the covariance of this Gaus- sianissmallsothatthenetworkfunctionisapproximatelylinearwithrespecttothe parametersovertheregionofparameterspaceforwhichtheposteriorprobabilityis significantly nonzero.
With these two approximations, we will obtain models that are analogous to the linear regression and classification models discussed in earlier chaptersandsowecanexploittheresultsobtainedthere.
Wecanthenmakeuseof the evidence framework to provide point estimates for the hyperparameters and to comparealternativemodels(forexample, networkshavingdifferentnumbersofhid- denunits).
Tostartwith, weshalldiscusstheregressioncaseandthenlaterconsider themodificationsneededforsolvingclassificationtasks.
5.7.1 Posterior parameter distribution Consider the problem of predicting a single continuous target variable t from a vector x of inputs (the extension to multiple targets is straightforward).
We shall suppose that the conditional distribution p(t|x) is Gaussian, with an x-dependent mean given by the output of a neural network model y(x, w), and with precision (inversevariance)β p(t|x, w,β)=N(t|y(x, w),β −1).
(5.161) Similarly, weshallchooseapriordistributionovertheweightswthatis Gaussianof theform p(w|α)=N(w|0,α −1I).
(5.162) Forani.
i.
d.
datasetof N observationsx 1 ,..., x N, withacorrespondingsetoftarget values D ={t 1 ,..., t N }, thelikelihoodfunctionisgivenby N p(D|w,β)= N(tn |y(xn, w),β −1) (5.163) n=1 andsotheresultingposteriordistributionisthen p(w|D,α,β)∝p(w|α)p(D|w,β).
(5.164) which, asaconsequenceofthenonlineardependenceofy(x, w)onw, willbenon- Gaussian.
Wecanfinda Gaussianapproximationtotheposteriordistributionbyusingthe Laplace approximation.
To do this, we must first find a (local) maximum of the posterior, andthismustbedoneusingiterativenumericaloptimization.
Asusual, it isconvenienttomaximizethelogarithmoftheposterior, whichcanbewritteninthe 5.7.
Bayesian Neural Networks 279 form N α β lnp(w|D)=− w Tw− {y(xn, w)−tn }2 +const (5.165) 2 2 n=1 which corresponds to a regularized sum-of-squares error function.
Assuming for themomentthatαandβ arefixed, wecanfindamaximumoftheposterior, which wedenotew MAP, bystandardnonlinearoptimizationalgorithmssuchasconjugate gradients, usingerrorbackpropagationtoevaluatetherequiredderivatives.
Havingfoundamodew MAP, wecanthenbuildalocal Gaussianapproximation byevaluatingthematrixofsecondderivativesofthenegativelogposteriordistribu- tion.
From(5.165), thisisgivenby A=−∇∇lnp(w|D,α,β)=αI+βH (5.166) where H is the Hessian matrix comprising the second derivatives of the sum-of- squareserrorfunctionwithrespecttothecomponentsofw.
Algorithmsforcomput- ingandapproximatingthe Hessianwerediscussedin Section5.4.
Thecorresponding Gaussianapproximationtotheposterioristhengivenfrom(4.134)by q(w|D)=N(w|w MAP , A −1).
(5.167) Similarly, the predictive distribution is obtained by marginalizing with respect tothisposteriordistribution p(t|x, D)= p(t|x, w)q(w|D)dw.
(5.168) However, even with the Gaussian approximation to the posterior, this integration is stillanalyticallyintractableduetothenonlinearityofthenetworkfunctiony(x, w) asafunctionofw.
Tomakeprogress, wenowassumethattheposteriordistribution hassmallvariancecomparedwiththecharacteristicscalesofwoverwhichy(x, w) isvarying.
Thisallowsustomakea Taylorseriesexpansionofthenetworkfunction aroundw MAP andretainonlythelinearterms y(x, w) y(x, w MAP )+g T(w−w MAP ) (5.169) wherewehavedefined g = ∇ w y(x, w)| w=w .
(5.170) MAP With this approximation, we now have a linear-Gaussian model with a Gaussian distributionforp(w)anda Gaussianforp(t|w)whosemeanisalinearfunctionof woftheform p(t|x, w,β) N t|y(x, w MAP )+g T(w−w MAP ),β −1 .
(5.171) Exercise 5.38 Wecanthereforemakeuseofthegeneralresult(2.115)forthemarginalp(t)togive p(t|x, D,α,β)=N t|y(x, w MAP ),σ2(x) (5.172) 280 5.
NEURALNETWORKS wheretheinput-dependentvarianceisgivenby σ2(x)=β −1+g TA −1g.
(5.173) Weseethatthepredictivedistributionp(t|x, D)isa Gaussianwhosemeanisgiven bythenetworkfunctiony(x, w MAP )withtheparametersettotheir MAPvalue.
The variancehastwoterms, thefirstofwhicharisesfromtheintrinsicnoiseonthetarget variable, whereas the second is an x-dependent term that expresses the uncertainty in the interpolant due to the uncertainty in the model parameters w.
This should becomparedwiththecorrespondingpredictivedistributionforthelinearregression model, givenby(3.58)and(3.59).
5.7.2 Hyperparameter optimization Sofar, wehaveassumedthatthehyperparametersαandβ arefixedandknown.
Wecanmakeuseoftheevidenceframework, discussedin Section3.5, togetherwith the Gaussianapproximationtotheposteriorobtainedusingthe Laplaceapproxima- tion, toobtainapracticalprocedureforchoosingthevaluesofsuchhyperparameters.
The marginal likelihood, or evidence, for the hyperparameters is obtained by integratingoverthenetworkweights p(D|α,β)= p(D|w,β)p(w|α)dw.
(5.174) Exercise 5.39 Thisiseasilyevaluatedbymakinguseofthe Laplaceapproximationresult(4.135).
Takinglogarithmsthengives 1 W N N lnp(D|α,β) −E(w MAP )− ln|A|+ lnα+ lnβ− ln(2π) (5.175) 2 2 2 2 where W isthetotalnumberofparametersinw, andtheregularizederrorfunction isdefinedby N β α E(w MAP )= 2 {y(xn, w MAP )−tn }2 + 2 w M T AP w MAP .
(5.176) n=1 Weseethatthistakesthesameformasthecorrespondingresult(3.86)forthelinear regressionmodel.
Intheevidenceframework, wemakepointestimatesforαandβbymaximizing lnp(D|α,β).
Considerfirstthemaximizationwithrespecttoα, whichcanbedone byanalogywiththelinearregressioncasediscussedin Section3.5.2.
Wefirstdefine theeigenvalueequation βHui =λiui (5.177) where H is the Hessian matrix comprising the second derivatives of the sum-of- squareserrorfunction, evaluatedatw =w MAP.
Byanalogywith(3.92), weobtain γ α= (5.178) w T w MAP MAP 5.7.
Bayesian Neural Networks 281 Section3.5.3 whereγ representstheeffectivenumberofparametersandisdefinedby W λi γ = .
(5.179) α+λi i=1 Notethatthisresultwasexactforthelinearregressioncase.
Forthenonlinearneural network, however, it ignores the fact that changes in α will cause changes in the Hessian H, whichinturnwillchangetheeigenvalues.
Wehavethereforeimplicitly ignoredtermsinvolvingthederivativesofλi withrespecttoα.
Similarly, from (3.95) we see that maximizing the evidence with respect to β givesthere-estimationformula N 1 1 β = N −γ {y(xn, w MAP )−tn }2.
(5.180) n=1 As with the linear model, we need to alternate between re-estimation of the hyper- parameters α and β and updating of the posterior distribution.
The situation with aneuralnetworkmodelismorecomplex, however, duetothemultimodalityofthe posteriordistribution.
Asaconsequence, thesolutionforw MAP foundbymaximiz- ingthelogposteriorwilldependontheinitializationofw.
Solutionsthatdifferonly Section5.1.1 asaconsequenceoftheinterchangeandsignreversalsymmetriesinthehiddenunits are identical so far as predictions are concerned, and it is irrelevant which of the equivalentsolutionsisfound.
However, theremaybeinequivalentsolutionsaswell, andthesewillgenerallyyielddifferentvaluesfortheoptimizedhyperparameters.
Inordertocomparedifferentmodels, forexampleneuralnetworkshavingdiffer- entnumbersofhiddenunits, weneedtoevaluatethemodelevidencep(D).
Thiscan be approximated by taking (5.175) and substituting the values of α and β obtained fromtheiterativeoptimizationofthesehyperparameters.
Amorecarefulevaluation isobtainedbymarginalizingoverαandβ, againbymakinga Gaussianapproxima- tion(Mac Kay,1992c; Bishop,1995a).
Ineithercase, itisnecessarytoevaluatethe determinant|A|ofthe Hessianmatrix.
Thiscanbeproblematicinpracticebecause thedeterminant, unlikethetrace, issensitivetothesmalleigenvaluesthatareoften difficulttodetermineaccurately.
The Laplace approximation is based on a local quadratic expansion around a modeoftheposteriordistributionoverweights.
Wehaveseenin Section5.1.1that any given mode in a two-layer network is a member of a set of M!2M equivalent modesthatdifferbyinterchangeandsign-changesymmetries, where M isthenum- ber of hidden units.
When comparing networks having different numbers of hid- denunits, thiscanbetakenintoaccountbymultiplying theevidencebyafactor of M!2M.
5.7.3 Bayesian neural networks for classification So far, we have used the Laplace approximation to develop a Bayesian treat- ment of neural network regression models.
We now discuss the modifications to 282 5.
NEURALNETWORKS this framework that arise when it is applied to classification.
Here we shall con- sideranetworkhavingasinglelogisticsigmoidoutputcorrespondingtoatwo-class classification problem.
The extension to networks with multiclass softmax outputs Exercise 5.40 is straightforward.
We shall build extensively on the analogous results for linear classification models discussed in Section 4.5, and so we encourage the reader to familiarizethemselveswiththatmaterialbeforestudyingthissection.
Theloglikelihoodfunctionforthismodelisgivenby lnp(D|w)= =1 N {tnlnyn+(1−tn)ln(1−yn)} (5.181) n where tn ∈ {0,1} are the target values, and yn ≡ y(xn, w).
Note that there is no hyperparameterβ, becausethedatapointsareassumedtobecorrectlylabelled.
As before, theprioristakentobeanisotropic Gaussianoftheform(5.162).
The first stage in applying the Laplace framework to this model is to initialize thehyperparameterα, andthentodeterminetheparametervectorwbymaximizing thelogposteriordistribution.
Thisisequivalenttominimizingtheregularizederror function α E(w)=−lnp(D|w)+ w Tw (5.182) 2 andcanbeachievedusingerrorbackpropagationcombinedwithstandardoptimiza- tionalgorithms, asdiscussedin Section5.3.
Having found a solution w MAP for the weight vector, the next step is to eval- uate the Hessian matrix H comprising the second derivatives of the negative log likelihoodfunction.
Thiscanbedone, forinstance, usingtheexactmethodof Sec- tion 5.4.5, or using the outer product approximation given by (5.85).
The second derivativesofthenegativelogposteriorcanagainbewrittenintheform(5.166), and the Gaussianapproximationtotheposterioristhengivenby(5.167).
Tooptimizethehyperparameterα, weagainmaximizethemarginallikelihood, Exercise 5.41 whichiseasilyshowntotaketheform 1 W lnp(D|α) −E(w MAP )− ln|A|+ lnα+const (5.183) 2 2 wheretheregularizederrorfunctionisdefinedby N α E(w MAP )=− {tnlnyn+(1−tn)ln(1−yn)}+ 2 w M T AP w MAP (5.184) n=1 inwhichyn ≡ y(xn, w MAP ).
Maximizingthisevidencefunctionwithrespecttoα againleadstothere-estimationequationgivenby(5.178).
The use of the evidence procedure to determine α is illustrated in Figure 5.22 forthesynthetictwo-dimensionaldatadiscussedin Appendix A.
Finally, weneedthepredictivedistribution, whichisdefinedby(5.168).
Again, this integration is intractable due to the nonlinearity of the network function.
The 5.7.
Bayesian Neural Networks 283 Figure5.22 Illustration of the evidence framework 3 appliedtoasynthetictwo-classdataset.
The green curve shows the optimal de- 2 cision boundary, the black curve shows the result of fitting a two-layer network with 8 hidden units by maximum likeli- 1 hood, and the red curve shows the re- sult of including a regularizer in which 0 α is optimized using the evidence pro- cedure, starting from the initial value −1 α = 0.
Note that the evidence proce- dure greatly reduces the over-fitting of −2 thenetwork.
−2 −1 0 1 2 simplest approximation is to assume that the posterior distribution is very narrow andhencemaketheapproximation p(t|x, D) p(t|x, w MAP ).
(5.185) Wecanimproveonthis, however, bytakingaccountofthevarianceoftheposterior distribution.
Inthiscase, alinearapproximationforthenetworkoutputs, aswasused inthecaseofregression, wouldbeinappropriateduetothelogisticsigmoidoutput- unitactivationfunctionthatconstrainstheoutputtolieintherange(0,1).
Instead, wemakealinearapproximationfortheoutputunitactivationintheform a(x, w) a MAP (x)+b T(w−w MAP ) (5.186) wherea MAP (x)=a(x, w MAP ), andthevectorb≡∇a(x, w MAP )canbefoundby backpropagation.
Because we now have a Gaussian approximation for the posterior distribution over w, and a model for a that is a linear function of w, we can now appeal to the resultsof Section4.5.2.
Thedistributionofoutputunitactivationvalues, inducedby thedistributionovernetworkweights, isgivenby p(a|x, D)= δ a−a MAP (x)−b T(x)(w−w MAP ) q(w|D)dw (5.187) where q(w|D) is the Gaussian approximation to the posterior distribution given by (5.167).
From Section 4.5.2, we see that this distribution is Gaussian with mean a MAP ≡a(x, w MAP ), andvariance σ2(x)=b T(x)A −1b(x).
(5.188) a Finally, toobtainthepredictivedistribution, wemustmarginalizeoverausing p(t=1|x, D)= σ(a)p(a|x, D)da.
(5.189) 284 5.
NEURALNETWORKS 3 3 2 2 1 1 0 0 −1 −1 −2 −2 −2 −1 0 1 2 −2 −1 0 1 2 Figure5.23 Anillustrationofthe Laplaceapproximationfora Bayesianneuralnetworkhaving8hiddenunits with‘tanh’activationfunctionsandasinglelogistic-sigmoidoutputunit.
Theweightparameterswerefoundusing scaledconjugategradients, andthehyperparameterαwasoptimizedusingtheevidenceframework.
Ontheleft is the result of using the simple approximation (5.185) based on a point estimate w MAP of the parameters, in which the green curve shows the y = 0.5 decision boundary, and the other contours correspond to output that the effect of marginalization is to spread out the contours and to make the predictions less confident, so thatateachinputpointx, theposteriorprobabilitiesareshiftedtowards0.5, whilethey = 0.5contouritselfis unaffected.
The convolution of a Gaussian with a logistic sigmoid is intractable.
We therefore applytheapproximation(4.153)to(5.189)giving p(t=1|x, D)=σ κ(σ a 2)b Tw MAP (5.190) whereκ(·)isdefinedby(4.154).
Recallthatbothσ2 andbarefunctionsofx.
a Figure5.23showsanexampleofthisframeworkappliedtothesyntheticclassi- ficationdatasetdescribedin Appendix A.
Exercises 5.1 ( ) Consideratwo-layernetworkfunctionoftheform(5.7)inwhichthehidden- unitnonlinearactivationfunctionsg(·)aregivenbylogisticsigmoidfunctionsofthe form σ(a)={1+exp(−a)}−1 .
(5.191) Showthatthereexistsanequivalentnetwork, whichcomputesexactlythesamefunc- tion, butwithhiddenunitactivationfunctionsgivenbytanh(a)wherethetanhfunc- tionisdefinedby(5.59).
Hint: firstfindtherelationbetweenσ(a)andtanh(a), and thenshowthattheparametersofthetwonetworksdifferbylineartransformations.
5.2 ( ) www Show that maximizing the likelihood function under the conditional distribution(5.16)foramultioutputneuralnetworkisequivalenttominimizingthe sum-of-squareserrorfunction(5.11).
Exercises 285 5.3 ( ) Consideraregressionprobleminvolvingmultipletargetvariablesinwhichit isassumedthatthedistributionofthetargets, conditionedontheinputvectorx, isa Gaussianoftheform p(t|x, w)=N(t|y(x, w),Σ) (5.192) where y(x, w) is the output of a neural network with input vector x and weight vector w, and Σ is the covariance of the assumed Gaussian noise on the targets.
Given a set of independent observations of x and t, write down the error function that must be minimized in order to find the maximum likelihood solution for w, if weassumethatΣisfixedandknown.
NowassumethatΣisalsotobedetermined from the data, and write down an expression for the maximum likelihood solution for Σ.
Note that the optimizations of w and Σ are now coupled, in contrast to the caseofindependenttargetvariablesdiscussedin Section5.2.
5.4 ( ) Consider a binary classification problem in which the target values are t ∈ {0,1}, withanetworkoutputy(x, w)thatrepresentsp(t = 1|x), andsupposethat thereisaprobability thattheclasslabelonatrainingdatapointhasbeenincorrectly set.
Assuming independent and identically distributed data, write down the error functioncorrespondingtothenegativeloglikelihood.
Verifythattheerrorfunction (5.21)isobtainedwhen = 0.
Notethatthiserrorfunctionmakesthemodelrobust toincorrectlylabelleddata, incontrasttotheusualerrorfunction.
5.5 ( ) www Showthatmaximizinglikelihoodforamulticlassneuralnetworkmodel in which the network outputs have the interpretation yk(x, w) = p(tk = 1|x) is equivalenttotheminimizationofthecross-entropyerrorfunction(5.24).
5.6 ( ) www Show the derivative of the error function (5.21) with respect to the activationak foranoutputunithavingalogisticsigmoidactivationfunctionsatisfies (5.18).
5.7 ( ) Showthederivativeoftheerrorfunction(5.24)withrespecttotheactivationak foroutputunitshavingasoftmaxactivationfunctionsatisfies(5.18).
5.8 ( ) Wesawin(4.88)thatthederivativeofthelogisticsigmoidactivationfunction canbeexpressedintermsofthefunctionvalueitself.
Derivethecorrespondingresult forthe‘tanh’activationfunctiondefinedby(5.59).
5.9 ( ) www The error function (5.21) for binary classification problems was de- rived for a network having a logistic-sigmoid output activation function, so that 0 y(x, w) 1, anddatahavingtargetvaluest ∈ {0,1}.
Derivethecorrespond- ing error function if we consider a network having an output −1 y(x, w) 1 and target values t = 1 for class C 1 and t = −1 for class C 2.
What would be the appropriatechoiceofoutputunitactivationfunction? 5.10 ( ) www Consider a Hessian matrix H with eigenvector equation (5.33).
By settingthevectorv in(5.39)equaltoeachoftheeigenvectorsui inturn, showthat Hispositivedefiniteif, andonlyif, allofitseigenvaluesarepositive.
286 5.
NEURALNETWORKS 5.11 ( ) www Consider a quadratic error function defined by (5.32), in which the Hessian matrix H has an eigenvalue equation given by (5.33).
Show that the con- toursofconstanterrorareellipseswhoseaxesarealignedwiththeeigenvectorsui, with lengths that are inversely proportional to the square root of the corresponding eigenvaluesλi.
5.12 ( ) www Byconsideringthelocal Taylorexpansion(5.32)ofanerrorfunction aboutastationarypointw , showthatthenecessaryandsufficientconditionforthe stationarypointtobealocalminimumoftheerrorfunctionisthatthe Hessianmatrix H, definedby(5.30)withw =w , bepositivedefinite.
5.13 ( ) Show that as a consequence of the symmetry of the Hessian matrix H, the number of independent elements in the quadratic error function (5.28) is given by W(W +3)/2.
5.14 ( ) Bymakinga Taylorexpansion, verifythatthetermsthatare O( )cancelonthe right-handsideof(5.69).
5.15 ( ) In Section5.3.4, wederivedaprocedureforevaluatingthe Jacobianmatrixofa neuralnetworkusingabackpropagationprocedure.
Deriveanalternativeformalism forfindingthe Jacobianbasedonforwardpropagationequations.
5.16 ( ) The outer product approximation to the Hessian matrix for a neural network using a sum-of-squares error function is given by (5.84).
Extend this result to the caseofmultipleoutputs.
5.17 ( ) Considerasquaredlossfunctionoftheform 1 E = {y(x, w)−t}2 p(x, t)dxdt (5.193) 2 where y(x, w) is a parametric function such as a neural network.
The result (1.89) showsthatthefunctiony(x, w)thatminimizesthiserrorisgivenbytheconditional expectationoftgivenx.
Usethisresulttoshowthatthesecondderivativeof E with respecttotwoelementswr andws ofthevectorw, isgivenby ∂2E ∂y ∂y = p(x)dx.
(5.194) ∂wr∂ws ∂wr ∂ws Notethat, forafinitesamplefromp(x), weobtain(5.84).
5.18 ( ) Consideratwo-layernetworkoftheformshownin Figure5.1withtheaddition of extra parameters corresponding to skip-layer connections that go directly from the inputs to theoutputs.
By extending thediscussion of Section 5.3.2, write down theequationsforthederivativesoftheerrorfunctionwithrespecttotheseadditional parameters.
5.19 ( ) www Derive the expression (5.85) for the outer product approximation to the Hessian matrix for a network having a single output with a logistic sigmoid output-unitactivationfunctionandacross-entropyerrorfunction, correspondingto theresult(5.84)forthesum-of-squareserrorfunction.
Exercises 287 5.20 ( ) Deriveanexpressionfortheouterproductapproximationtothe Hessianmatrix for a network having K outputs with a softmax output-unit activation function and a cross-entropy error function, corresponding to the result (5.84) for the sum-of- squareserrorfunction.
5.21 ( ) Extendtheexpression(5.86)fortheouterproductapproximationofthe Hes- sianmatrixtothecaseof K > 1outputunits.
Hence, derivearecursiveexpression analogousto(5.87)forincrementingthenumber N ofpatternsandasimilarexpres- sionforincrementingthenumber K ofoutputs.
Usetheseresults, togetherwiththe identity(5.88), tofindsequentialupdateexpressionsanalogousto(5.89)forfinding the inverse of the Hessian by incrementally including both extra patterns and extra outputs.
5.22 ( ) Derive the results (5.93), (5.94), and (5.95) for the elements of the Hessian matrix of a two-layer feed-forward network by application of the chain rule of cal- culus.
5.23 ( ) Extendtheresultsof Section5.4.5fortheexact Hessianofatwo-layernetwork toincludeskip-layerconnectionsthatgodirectlyfrominputstooutputs.
5.24 ( ) Verifythatthenetworkfunctiondefinedby(5.113)and(5.114)isinvariantun- derthetransformation(5.115)appliedtotheinputs, providedtheweightsandbiases are simultaneously transformed using (5.116) and (5.117).
Similarly, show that the network outputs can be transformed according (5.118) by applying the transforma- tion(5.119)and(5.120)tothesecond-layerweightsandbiases.
5.25 ( ) www Consideraquadraticerrorfunctionoftheform 1 E =E 0 + (w−w )TH(w−w ) (5.195) 2 wherew representstheminimum, andthe Hessianmatrix Hispositivedefiniteand constant.
Supposetheinitialweightvectorw(0) ischosentobeattheoriginandis updatedusingsimplegradientdescent w(τ) =w(τ−1)−ρ∇E (5.196) whereτ denotesthestepnumber, andρisthelearningrate(whichisassumedtobe small).
Showthat, afterτ steps, thecomponentsoftheweightvectorparalleltothe eigenvectorsof Hcanbewritten w j (τ) ={1−(1−ρηj) τ}w j (5.197) wherewj =w Tuj, anduj andηj aretheeigenvectorsandeigenvalues, respectively, of Hsothat Huj =ηjuj.
(5.198) Showthat asτ → ∞, this givesw(τ) → w asexpected, provided|1−ρηj | < 1.
Now suppose that training is halted after a finite number τ of steps.
Show that the 288 5.
NEURALNETWORKS componentsoftheweightvectorparalleltotheeigenvectorsofthe Hessiansatisfy w j (τ) w j when ηj (ρτ) −1 (5.199) |w j (τ)| |w j | when ηj (ρτ) −1.
(5.200) Comparethisresultwiththediscussionin Section3.5.3ofregularizationwithsimple weightdecay, andhenceshowthat(ρτ)−1 isanalogoustotheregularizationparam- eter λ.
The above results also show that the effective number of parameters in the network, asdefinedby(3.91), growsasthetrainingprogresses.
5.26 ( ) Consideramultilayerperceptronwitharbitraryfeed-forwardtopology, which is to be trained by minimizing the tangent propagation error function (5.127) in which the regularizing function is given by (5.128).
Show that the regularization termΩcanbewrittenasasumoverpatternsoftermsoftheform 1 Ωn = (Gyk) 2 (5.201) 2 k where G isadifferentialoperatordefinedby ∂ G ≡ τi .
(5.202) ∂xi i Byactingontheforwardpropagationequations zj =h(aj), aj = wjizi (5.203) i with the operator G, show that Ωn can be evaluated by forward propagation using thefollowingequations: αj =h(aj)βj, βj = wjiαi.
(5.204) i wherewehavedefinedthenewvariables αj ≡Gzj, βj ≡Gaj.
(5.205) NowshowthatthederivativesofΩnwithrespecttoaweightwrsinthenetworkcan bewrittenintheform ∂Ωn = αk {φkrzs+δkrαs } (5.206) ∂wrs k wherewehavedefined δkr ≡ ∂yk , φkr ≡Gδkr.
(5.207) ∂ar Write down the backpropagation equations forδkr, and hence derive a set of back- propagationequationsfortheevaluationoftheφkr.
Exercises 289 5.27 ( ) www Consider the framework for training with transformed data in the special case in which the transformation consists simply of the addition of random noise x → x + ξ where ξ has a Gaussian distribution with zero mean and unit covariance.
Byfollowinganargumentanalogoustothatof Section5.5.5, showthat theresultingregularizerreducestothe Tikhonovform(5.135).
5.28 ( ) www Consideraneuralnetwork, suchastheconvolutionalnetworkdiscussed in Section5.5.6, inwhich multipleweights areconstrainedtohavethesamevalue.
Discuss how the standard backpropagation algorithm must be modified in order to ensurethatsuchconstraintsaresatisfiedwhenevaluatingthederivativesofanerror functionwithrespecttotheadjustableparametersinthenetwork.
5.29 ( ) www Verifytheresult(5.141).
5.30 ( ) Verifytheresult(5.142).
5.31 ( ) Verifytheresult(5.143).
5.32 ( ) Showthatthederivativesofthemixingcoefficients{πk }, definedby(5.146), withrespecttotheauxiliaryparameters{ηj }aregivenby ∂πk =δjkπj −πjπk.
(5.208) ∂ηj Hence, bymakinguseoftheconstraint k πk =1, derivetheresult(5.147).
5.33 ( ) Writedownapairofequationsthatexpressthe Cartesiancoordinates(x 1 , x 2 ) for the robot arm shown in Figure 5.18 in terms of the joint angles θ 1 and θ 2 and the lengths L 1 and L 2 of the links.
Assume the origin of the coordinate system is givenbytheattachmentpointofthelowerarm.
Theseequationsdefinethe‘forward kinematics’oftherobotarm.
5.34 ( ) www Derive the result (5.155) for the derivative of the error function with respect to the network output activations controlling the mixing coefficients in the mixturedensitynetwork.
5.35 ( ) Derive the result (5.156) for the derivative of the error function with respect to the network output activations controlling the component means in the mixture densitynetwork.
5.36 ( ) Derivetheresult(5.157)forthederivativeoftheerrorfunctionwithrespectto the network output activations controlling the component variances in the mixture densitynetwork.
5.37 ( ) Verifytheresults(5.158)and(5.160)fortheconditionalmeanandvarianceof themixturedensitynetworkmodel.
5.38 ( ) Using the general result (2.115), derive the predictive distribution (5.172) for the Laplaceapproximationtothe Bayesianneuralnetworkmodel.
290 5.
NEURALNETWORKS 5.39 ( ) www Makeuseofthe Laplaceapproximationresult(4.135)toshowthatthe evidence function for the hyperparameters α and β in the Bayesian neural network modelcanbeapproximatedby(5.175).
5.40 ( ) www Outlinethemodificationsneededtotheframeworkfor Bayesianneural networks, discussedin Section5.7.3, tohandlemulticlassproblemsusingnetworks havingsoftmaxoutput-unitactivationfunctions.
5.41 ( ) By following analogous steps to those given in Section 5.7.1 for regression networks, derive the result (5.183) for the marginal likelihood in the case of a net- work having a cross-entropy error function and logistic-sigmoid output-unit activa- tionfunction.
6 Kernel Methods In Chapters 3 and 4, we considered linear parametric models for regression and classification in which the form of the mapping y(x, w) from input x to output y is governed by a vector w of adaptive parameters.
During the learning phase, a set of training data is used either to obtain a point estimate of the parameter vector or to determine a posterior distribution over this vector.
The training data is then discarded, andpredictionsfornewinputsarebasedpurelyonthelearnedparameter vectorw.
Thisapproachisalsousedinnonlinearparametricmodelssuchasneural Chapter5 networks.
However, thereisaclassofpatternrecognitiontechniques, inwhichthetraining datapoints, orasubsetofthem, arekeptandusedalsoduringthepredictionphase.
Section2.5.1 For instance, the Parzen probability density model comprised a linear combination of‘kernel’functionseachonecentredononeofthetrainingdatapoints.
Similarly, in Section 2.5.2 we introduced a simple technique for classification called nearest neighbours, which involved assigning to each new test vector the same label as the 291 292 6.
KERNELMETHODS closestexamplefromthetrainingset.
Theseareexamplesofmemory-basedmethods thatinvolvestoringtheentiretrainingsetinordertomakepredictionsforfuturedata points.
Theytypicallyrequireametrictobedefinedthatmeasuresthesimilarityof any two vectors in input space, and are generally fast to ‘train’ but slow at making predictionsfortestdatapoints.
Manylinearparametricmodelscanbere-castintoanequivalent‘dualrepresen- tation’ in which the predictions are also based on linear combinations of a kernel functionevaluatedatthetrainingdatapoints.
Asweshallsee, formodelswhichare basedonafixednonlinearfeaturespacemappingφ(x), thekernelfunctionisgiven bytherelation k(x, x )=φ(x)Tφ(x ).
(6.1) Fromthisdefinition, weseethatthekernelisasymmetricfunctionofitsarguments sothatk(x, x )=k(x , x).
Thekernelconceptwasintroducedintothefieldofpat- ternrecognitionby Aizermanetal.
(1964)inthecontextofthemethodofpotential functions, so-called because of an analogy with electrostatics.
Although neglected for many years, it was re-introduced into machine learning in the context of large- margin classifiers by Boser et al.
(1992) giving rise to the technique of support Chapter7 vectormachines.
Sincethen, therehasbeenconsiderableinterestinthistopic, both in terms of theory and applications.
One of the most significant developments has beentheextensionofkernelstohandlesymbolicobjects, therebygreatlyexpanding therangeofproblemsthatcanbeaddressed.
Thesimplestexampleofakernelfunctionisobtainedbyconsideringtheidentity mapping for the feature space in (6.1) so that φ(x) = x, in which case k(x, x ) = x Tx .
Weshallrefertothisasthelinearkernel.
Theconceptofakernelformulatedasaninnerproductinafeaturespaceallows ustobuildinterestingextensionsofmanywell-knownalgorithmsbymakinguseof thekerneltrick, alsoknownaskernelsubstitution.
Thegeneralideaisthat, ifwehave analgorithmformulatedinsuchawaythattheinputvectorxentersonlyintheform ofscalarproducts, thenwecanreplacethatscalarproductwithsomeotherchoiceof kernel.
Forinstance, thetechniqueofkernelsubstitutioncanbeappliedtoprincipal Section12.3 componentanalysisinordertodevelopanonlinearvariantof PCA(Scho¨lkopfetal., 1998).
Other examples of kernel substitution include nearest-neighbour classifiers and the kernel Fisher discriminant (Mika et al., 1999; Roth and Steinhage, 2000; Baudatand Anouar,2000).
Therearenumerousformsofkernelfunctionsincommonuse, andweshallen- counterseveralexamplesinthischapter.
Manyhavethepropertyofbeingafunction onlyofthedifferencebetweenthearguments, sothatk(x, x ) = k(x−x ), which are known as stationary kernels because they are invariant to translations in input space.
A further specialization involves homogeneous kernels, also known as ra- Section6.3 dialbasisfunctions, whichdependonlyonthemagnitudeofthedistance(typically Euclidean)betweentheargumentssothatk(x, x )=k( x−x ).
Forrecenttextbooksonkernelmethods, see Scho¨lkopfand Smola(2002), Her- brich(2002), and Shawe-Taylorand Cristianini(2004).
6.1.
Dual Representations 293 6.1.
Dual Representations Manylinearmodelsforregressionandclassificationcanbereformulatedintermsof adualrepresentationinwhichthekernelfunctionarisesnaturally.
Thisconceptwill playanimportantrolewhenweconsidersupportvectormachinesinthenextchapter.
Here we consider a linear regression model whose parameters are determined by minimizingaregularizedsum-of-squareserrorfunctiongivenby N J(w)= 1 w Tφ(xn)−tn 2 + λ w Tw (6.2) 2 2 n=1 whereλ 0.
Ifwesetthegradientof J(w)withrespecttowequaltozero, wesee thatthesolutionforwtakestheformofalinearcombinationofthevectorsφ(xn), withcoefficientsthatarefunctionsofw, oftheform N N 1 w =− w Tφ(xn)−tn φ(xn)= an φ(xn)=ΦTa (6.3) λ n=1 n=1 where Φ is the design matrix, whose nth row is given by φ(xn)T.
Here the vector a=(a 1 ,..., a N)T, andwehavedefined 1 an =− w Tφ(xn)−tn .
(6.4) λ Insteadofworkingwiththeparametervectorw, wecannowreformulatetheleast- squaresalgorithmintermsoftheparametervectora, givingrisetoadualrepresen- tation.
Ifwesubstitutew =ΦTainto J(w), weobtain 1 1 λ J(a)= a TΦΦTΦΦTa−a TΦΦT t+ t T t+ a TΦΦTa (6.5) 2 2 2 wheret = (t 1 ,..., t N)T.
Wenowdefinethe Grammatrix K = ΦΦT , whichisan N ×N symmetricmatrixwithelements Knm =φ(xn)Tφ(xm)=k(xn, xm) (6.6) wherewehaveintroducedthekernelfunctionk(x, x )definedby(6.1).
Intermsof the Grammatrix, thesum-of-squareserrorfunctioncanbewrittenas 1 1 λ J(a)= a TKKa−a TKt+ t T t+ a TKa.
(6.7) 2 2 2 Settingthegradientof J(a)withrespecttoatozero, weobtainthefollowingsolu- tion −1 a=(K+λIN) t.
(6.8) 294 6.
KERNELMETHODS If we substitute this back into the linear regression model, we obtain the following predictionforanewinputx y(x)=w Tφ(x)=a TΦφ(x)=k(x)T(K+λIN) −1 t (6.9) wherewehavedefinedthevectork(x)withelementskn(x) = k(xn, x).
Thuswe see that the dual formulation allows the solution to the least-squares problem to be expressedentirelyintermsofthekernelfunctionk(x, x ).
Thisisknownasadual formulation because, by noting that the solution for a can be expressed as a linear combinationoftheelementsofφ(x), werecovertheoriginalformulationintermsof Exercise 6.1 theparametervectorw.
Notethatthepredictionatxisgivenbyalinearcombination ofthetargetvaluesfromthetrainingset.
Infact, wehavealreadyobtainedthisresult, usingaslightlydifferentnotation, in Section3.3.3.
In the dual formulation, we determine the parameter vector a by inverting an N×N matrix, whereasintheoriginalparameterspaceformulationwehadtoinvert an M × M matrix in order to determine w.
Because N is typically much larger than M, thedualformulationdoesnotseemtobeparticularlyuseful.
However, the advantageofthedualformulation, asweshallsee, isthatitisexpressedentirelyin terms of the kernel function k(x, x ).
We can therefore work directly in terms of kernelsandavoidtheexplicitintroductionofthefeaturevectorφ(x), whichallows usimplicitlytousefeaturespacesofhigh, eveninfinite, dimensionality.
Theexistenceofadualrepresentationbasedonthe Grammatrixisapropertyof Exercise 6.2 manylinearmodels, includingtheperceptron.
In Section6.4, wewilldevelopadual- itybetweenprobabilisticlinearmodelsforregressionandthetechniqueof Gaussian processes.
Dualitywillalsoplayanimportantrolewhenwediscusssupportvector machinesin Chapter7.
6.2.
Constructing Kernels In order to exploit kernel substitution, we need to be able to construct valid kernel functions.
One approach is to choose a feature space mapping φ(x) and then use thistofindthecorrespondingkernel, asisillustratedin Figure6.1.
Herethekernel functionisdefinedforaone-dimensionalinputspaceby M k(x, x )=φ(x)Tφ(x )= φi(x)φi(x ) (6.10) i=1 whereφi(x)arethebasisfunctions.
An alternative approach is to construct kernel functions directly.
In this case, we must ensure that the function we choose is a valid kernel, in other words that it correspondstoascalarproductinsome(perhapsinfinitedimensional)featurespace.
Asasimpleexample, considerakernelfunctiongivenby k(x, z)= x Tz 2 .
(6.11) 6.2.
Constructing Kernels 295 1 1 1 0.5 0.75 0.75 0 0.5 0.5 −0.5 0.25 0.25 −1 0 0 −1 0 1 −1 0 1 −1 0 1 0.04 0.04 0.04 0.02 0.02 0.02 0 0 0 −1 0 1 −1 0 1 −1 0 1 Figure 6.1 Illustration of the construction of kernel functions starting from a corresponding set of basis func- tions.
Ineachcolumnthelowerplotshowsthekernelfunctionk(x, x )definedby(6.10)plottedasafunctionof xforx = 0, whiletheupperplotshowsthecorrespondingbasisfunctionsgivenbypolynomials(leftcolumn), ‘Gaussians’(centrecolumn), andlogisticsigmoids(rightcolumn).
If we take the particular case of a two-dimensional input space x = (x 1 , x 2 ) we can expand out the terms and thereby identify the corresponding nonlinear feature mapping k(x, z) = x Tz 2 =(x z +x z )2 1 1 2 2 = x2z2+2x z x z +x2z2 1 1√ 1 1 2 2 √2 2 = (x2, 2x x , x2)(z2, 2z z , z2)T 1 1 2 2 1 1 2 2 = φ(x)Tφ(z).
(6.12) √ We see that the feature mapping takes the form φ(x) = (x2 1 , 2x 1 x 2 , x2 2 )T and therefore comprises all possible second order terms, with a specific weighting be- tweenthem.
Moregenerally, however, weneedasimplewaytotestwhetherafunctioncon- stitutes a valid kernel without having to construct the function φ(x) explicitly.
A necessaryandsufficientconditionforafunctionk(x, x )tobeavalidkernel(Shawe- Taylorand Cristianini,2004)isthatthe Grammatrix K, whoseelementsaregivenby k(xn, xm), shouldbepositivesemidefiniteforallpossiblechoicesoftheset{xn }.
Note that a positive semidefinite matrix is not the same thing as a matrix whose Appendix C elementsarenonnegative.
One powerful technique for constructing new kernels is to build them out of simplerkernelsasbuildingblocks.
Thiscanbedoneusingthefollowingproperties: 296 6.
KERNELMETHODS Techniquesfor Constructing New Kernels.
Givenvalidkernelsk 1 (x, x )andk 2 (x, x ), thefollowingnewkernelswillalso bevalid: k(x, x) = ck 1 (x, x) (6.13) k(x, x) = f(x)k 1 (x, x)f(x) (6.14) k(x, x) = q(k 1 (x, x)) (6.15) k(x, x) = exp(k 1 (x, x)) (6.16) k(x, x) = k 1 (x, x)+k 2 (x, x) (6.17) k(x, x) = k 1 (x, x)k 2 (x, x) (6.18) k(x, x ) = k 3 (φ(x),φ(x )) (6.19) k(x, x ) = x TAx (6.20) k(x, x) = ka(xa, x a )+kb(xb, x b ) (6.21) k(x, x) = ka(xa, x a )kb(xb, x b ) (6.22) wherec>0isaconstant, f(·)isanyfunction, q(·)isapolynomialwithnonneg- ativecoefficients, φ(x)isafunctionfromxto RM, k 3 (·,·)isavalidkernelin RM, Aisasymmetricpositivesemidefinitematrix, xaandxbarevariables(not necessarilydisjoint)withx=(xa, xb), andkaandkbarevalidkernelfunctions overtheirrespectivespaces.
Equippedwiththeseproperties, wecannowembarkontheconstructionofmore complex kernels appropriate to specific applications.
We require that the kernel k(x, x )besymmetricandpositivesemidefiniteandthatitexpressestheappropriate formofsimilaritybetweenxandx accordingtotheintendedapplication.
Herewe considerafewcommonexamplesofkernelfunctions.
Foramoreextensivediscus- sionof‘kernelengineering’, see Shawe-Taylorand Cristianin i(200 4).
We saw that the simple polynomial kernel k(x, x ) = x Tx 2 contains only terms of degree two.
If we consider the slightly generalized kernel k(x, x ) = x Tx +c 2 withc>0, thenthecorrespondingfeaturemappingφ(x)containscon- stantandlineartermsaswellastermsofordertwo.
Similarly, k(x, x )= x Tx M contains all monomials of order M.
For instance, if x and x are two images, then thekernelrepresentsaparticularweightedsumofallpossibleproductsof M pixels in the first image with M pixels in the second image.
This can similarly be gener- alizedtoincludealltermsuptodegree M byconsideringk(x, x ) = x Tx +c M with c > 0.
Using the results (6.17) and (6.18) for combining kernels we see that thesewillallbevalidkernelfunctions.
Anothercommonlyusedkerneltakestheform k(x, x )=exp − x−x 2/2σ2 (6.23) and is often called a ‘Gaussian’ kernel.
Note, however, that in this context it is not interpreted as a probability density, and hence the normalization coefficient is 6.2.
Constructing Kernels 297 omitted.
Wecanseethatthisisavalidkernelbyexpandingthesquare x−x 2 =x Tx+(x )Tx −2x Tx (6.24) togive k(x, x )=exp −x Tx/2σ2 exp x Tx /σ2 exp −(x )Tx /2σ2 (6.25) and then making use of (6.14) and (6.16), together with the validity of the linear kernelk(x, x )=x Tx .
Notethatthefeaturevectorthatcorrespondstothe Gaussian Exercise 6.11 kernelhasinfinitedimensionality.
The Gaussiankernelisnotrestrictedtotheuseof Euclideandistance.
Ifweuse kernel substitution in (6.24) to replace x Tx with a nonlinear kernel κ(x, x ), we obtain 1 k(x, x )=exp − (κ(x, x)+κ(x , x )−2κ(x, x )) .
(6.26) 2σ2 Animportantcontributiontoarisefromthekernelviewpointhasbeentheexten- siontoinputsthataresymbolic, ratherthansimplyvectorsofrealnumbers.
Kernel functionscanbedefinedoverobjectsasdiverseasgraphs, sets, strings, andtextdoc- uments.
Consider, forinstance, afixedsetanddefineanonvectorialspaceconsisting ofallpossiblesubsetsofthisset.
If A 1 and A 2 aretwosuchsubsetsthenonesimple choiceofkernelwouldbe |A ∩A | k(A 1 , A 2 )=2 1 2 (6.27) where A 1 ∩ A 2 denotes the intersection of sets A 1 and A 2, and |A| denotes the number of subsets in A.
This is a valid kernel function because it can be shown to Exercise 6.12 correspondtoaninnerproductinafeaturespace.
Onepowerfulapproachtotheconstructionofkernelsstartsfromaprobabilistic generativemodel(Haussler,1999), whichallowsustoapplygenerativemodelsina discriminative setting.
Generative models can deal naturally with missing data and in the case of hidden Markov models can handle sequences of varying length.
By contrast, discriminativemodelsgenerallygivebetterperformanceondiscriminative tasks than generative models.
It is therefore of some interest to combine these two approaches(Lasserreetal.,2006).
Onewaytocombinethemistouseagenerative modeltodefineakernel, andthenusethiskernelinadiscriminativeapproach.
Givenagenerativemodelp(x)wecandefineakernelby k(x, x)=p(x)p(x).
(6.28) Thisisclearlyavalidkernelfunctionbecausewecaninterpretitasaninnerproduct intheone-dimensionalfeaturespacedefinedbythemappingp(x).
Itsaysthattwo inputsxandx aresimilariftheybothhavehighprobabilities.
Wecanuse(6.13)and (6.17)toextendthisclassofkernelsbyconsideringsumsoverproductsofdifferent probabilitydistributions, withpositiveweightingcoefficientsp(i), oftheform k(x, x )= p(x|i)p(x |i)p(i).
(6.29) i 298 6.
KERNELMETHODS Thisisequivalent, uptoanoverallmultiplicativeconstant, toamixturedistribution in which the components factorize, with the index i playing the role of a ‘latent’ Section9.2 variable.
Two inputs x and x will give a large value for the kernel function, and hence appear similar, if they have significant probability under a range of different components.
Takingthelimitofaninfinitesum, wecanalsoconsiderkernelsofthe form k(x, x )= p(x|z)p(x |z)p(z)dz (6.30) wherezisacontinuouslatentvariable.
Now suppose that our data consists of ordered sequences of length L so that an observation is given by X = {x 1 ,..., x L }.
A popular generative model for Section13.2 sequences is the hidden Markov model, which expresses the distributionp(X) as a marginalizationoveracorrespondingsequenceofhiddenstates Z = {z 1 ,..., z L }.
Wecanusethisapproachtodefineakernelfunctionmeasuringthesimilarityoftwo sequences Xand X byextendingthemixturerepresentation(6.29)togive k(X, X )= p(X|Z)p(X |Z)p(Z) (6.31) Z sothatbothobservedsequencesaregeneratedbythesamehiddensequence Z.
This modelcaneasilybeextendedtoallowsequencesofdifferinglengthtobecompared.
Analternativetechniqueforusinggenerativemodelstodefinekernelfunctions isknownasthe Fisherkernel(Jaakkolaand Haussler,1999).
Consideraparametric generative model p(x|θ) where θ denotes the vector of parameters.
The goal is to findakernelthatmeasuresthesimilarityoftwoinputvectorsxandx inducedbythe generative model.
Jaakkola and Haussler (1999) consider the gradient with respect to θ, which defines a vector in a ‘feature’ space having the same dimensionality as θ.
Inparticular, theyconsiderthe Fisherscore g(θ, x)=∇ θlnp(x|θ) (6.32) fromwhichthe Fisherkernelisdefinedby k(x, x )=g(θ, x)TF −1g(θ, x ).
(6.33) Here Fisthe Fisherinformationmatrix, givenby F=E x g(θ, x)g(θ, x)T (6.34) where the expectation is with respect to x under the distribution p(x|θ).
This can be motivated from the perspective of information geometry (Amari, 1998), which considersthedifferentialgeometryofthespaceofmodelparameters.
Herewesim- ply note that the presence of the Fisher information matrix causes this kernel to be Exercise 6.13 invariantunderanonlinearre-parameterizationofthedensitymodelθ →ψ(θ).
Inpractice, itisofteninfeasibletoevaluatethe Fisherinformationmatrix.
One approachissimplytoreplacetheexpectationinthedefinitionofthe Fisherinforma- tionwiththesampleaverage, giving N 1 F g(θ, xn)g(θ, xn)T.
(6.35) N n=1 6.3.
Radial Basis Function Networks 299 This is the covariance matrix of the Fisher scores, and so the Fisher kernel corre- Section12.1.3 sponds to a whitening of these scores.
More simply, we can just omit the Fisher informationmatrixaltogetherandusethenoninvariantkernel k(x, x )=g(θ, x)Tg(θ, x ).
(6.36) Anapplicationof Fisherkernelstodocumentretrievalisgivenby Hofmann(2000).
Afinalexampleofakernelfunctionisthesigmoidalkernelgivenby k(x, x )=tanh ax Tx +b (6.37) whose Gram matrix in general is not positive semidefinite.
This form of kernel has, however, beenusedinpractice(Vapnik,1995), possiblybecauseitgiveskernel expansions such as the support vector machine a superficial resemblance to neural networkmodels.
Asweshallsee, inthelimitofaninfinitenumberofbasisfunctions, a Bayesianneuralnetworkwithanappropriatepriorreducestoa Gaussianprocess, Section6.4.7 therebyprovidingadeeperlinkbetweenneuralnetworksandkernelmethods.
6.3.
Radial Basis Function Networks In Chapter3, wediscussedregressionmodelsbasedonlinearcombinationsoffixed basisfunctions, althoughwedidnotdiscussindetailwhatformthosebasisfunctions might take.
One choice that has been widely used is that of radial basis functions, whichhavethepropertythateachbasisfunctiondependsonlyontheradialdistance (typically Euclidean)fromacentreµ j , sothatφj(x)=h( x−µ j ).
Historically, radialbasisfunctionswereintroducedforthepurposeofexactfunc- tion interpolation (Powell, 1987).
Given a set of input vectors {x 1 ,..., x N } along withcorrespondingtargetvalues{t 1 ,..., t N }, thegoalistofindasmoothfunction f(x)thatfitseverytargetvalueexactly, sothatf(xn) = tn forn = 1,..., N.
This isachievedbyexpressingf(x)asalinearcombinationofradialbasisfunctions, one centredoneverydatapoint N f(x)= wnh( x−xn ).
(6.38) n=1 The values of the coefficients {wn } are found by least squares, and because there are the same number of coefficients as there are constraints, the result is a function thatfitseverytargetvalueexactly.
Inpatternrecognitionapplications, however, the targetvaluesaregenerallynoisy, andexactinterpolationisundesirablebecausethis correspondstoanover-fittedsolution.
Expansionsinradialbasisfunctionsalsoarisefromregularizationtheory(Pog- gio and Girosi, 1990; Bishop, 1995a).
For a sum-of-squares error function with a regularizer defined in terms of a differential operator, the optimal solution is given byanexpansioninthe Green’sfunctionsoftheoperator(whichareanalogoustothe eigenvectorsofadiscretematrix), againwithonebasisfunctioncentredoneachdata 300 6.
KERNELMETHODS point.
Ifthedifferentialoperatorisisotropicthenthe Green’sfunctionsdependonly ontheradialdistancefromthecorrespondingdatapoint.
Duetothepresenceofthe regularizer, thesolutionnolongerinterpolatesthetrainingdataexactly.
Another motivation for radial basis functions comes from a consideration of theinterpolationproblemwhentheinput(ratherthanthetarget)variablesarenoisy (Webb, 1994; Bishop, 1995a).
If the noise on the input variable x is described by a variable ξ having a distribution ν(ξ), then the sum-of-squares error function becomes N 1 E = {y(xn+ξ)−tn }2 ν(ξ)dξ.
(6.39) 2 n=1 Appendix D Usingthecalculusofvariations, wecanoptimizewithrespecttothefunctionf(x) Exercise 6.17 togive N y(xn)= tnh(x−xn) (6.40) n=1 wherethebasisfunctionsaregivenby h(x−xn)= ν(x−xn) .
(6.41) N ν(x−xn) n=1 Weseethatthereisonebasisfunctioncentredoneverydatapoint.
Thisisknownas the Nadaraya-Watson model and will be derived again from a different perspective in Section 6.3.1.
If the noise distribution ν(ξ) is isotropic, so that it is a function onlyof ξ , thenthebasisfunctionswillberadial.
Notethatthebasisfunctions(6.41)arenormalized, sothat n h(x−xn) = 1 foranyvalueofx.
Theeffectofsuchnormalizationisshownin Figure6.2.
Normal- izationissometimesusedinpracticeasitavoidshavingregionsofinputspacewhere allofthebasisfunctionstakesmallvalues, whichwouldnecessarilyleadtopredic- tionsinsuchregionsthatareeithersmallorcontrolledpurelybythebiasparameter.
Anothersituationinwhichexpansionsinnormalizedradialbasisfunctionsarise isintheapplicationofkerneldensityestimationtotheproblemofregression, aswe shalldiscussin Section6.3.1.
Becausethereisonebasisfunctionassociatedwitheverydatapoint, thecorre- spondingmodelcanbecomputationallycostlytoevaluatewhenmakingpredictions for new data points.
Models have therefore been proposed (Broomhead and Lowe, 1988; Moodyand Darken,1989; Poggioand Girosi,1990), whichretaintheexpan- sioninradialbasisfunctionsbutwherethenumber M ofbasisfunctionsissmaller thanthenumber N ofdatapoints.
Typically, thenumberofbasisfunctions, andthe locationsµ i oftheircentres, aredeterminedbasedontheinputdata{xn }alone.
The basisfunctionsarethenkeptfixedandthecoefficients{wi }aredeterminedbyleast squaresbysolvingtheusualsetoflinearequations, asdiscussedin Section3.1.1.
6.3.
Radial Basis Function Networks 301 1 1 0.8 0.8 0.6 0.6 0.4 0.4 0.2 0.2 0 0 −1 −0.5 0 0.5 1 −1 −0.5 0 0.5 1 Figure 6.2 Plot of a set of Gaussian basis functions on the left, together with the corresponding normalized basisfunctionsontheright.
Oneofthesimplestwaysofchoosingbasisfunctioncentresistousearandomly chosen subset of the data points.
A more systematic approach is called orthogonal leastsquares(Chenetal., 1991).
Thisisasequentialselectionprocessinwhichat eachstep thenextdata pointto bechosen asabasis function centrecorresponds to theonethatgivesthegreatestreduction inthesum-of-squares error.
Valuesfor the expansioncoefficientsaredeterminedaspartofthealgorithm.
Clusteringalgorithms Section9.1 suchas K-meanshavealsobeenused, whichgiveasetofbasisfunctioncentresthat nolongercoincidewithtrainingdatapoints.
6.3.1 Nadaraya-Watson model In Section 3.3.3, we saw that the prediction of a linear regression model for a new input x takes the form of a linear combination of the training set target values withcoefficientsgivenbythe‘equivalentkernel’(3.62)wheretheequivalentkernel satisfiesthesummationconstraint(3.64).
Wecanmotivatethekernelregressionmodel(3.61)fromadifferentperspective, startingwithkerneldensityestimation.
Supposewehaveatrainingset{xn, tn }and Section2.5.1 weusea Parzendensityestimatortomodelthejointdistributionp(x, t), sothat N 1 p(x, t)= f(x−xn, t−tn) (6.42) N n=1 where f(x, t) is the component density function, and there is one such component centred on each data point.
We now find an expression for the regression function y(x), correspondingtotheconditionalaverageofthetargetvariableconditionedon 302 6.
KERNELMETHODS theinputvariable, whichisgivenby ∞ y(x) = E[t|x]= tp(t|x)dt −∞ tp(x, t)dt = p(x, t)dt tf(x−xn, t−tn)dt n = .
(6.43) f(x−xm, t−tm)dt m Wenowassumeforsimplicitythatthecomponentdensityfunctionshavezeromean sothat ∞ f(x, t)tdt=0 (6.44) −∞ forallvaluesofx.
Usingasimplechangeofvariable, wethenobtain g(x−xn)tn y(x) = n g(x−xm) m = k(x, xn)tn (6.45) n wheren, m=1,..., N andthekernelfunctionk(x, xn)isgivenby k(x, xn)= g(x−xn) (6.46) g(x−xm) m andwehavedefined ∞ g(x)= f(x, t)dt.
(6.47) −∞ The result (6.45) is known as the Nadaraya-Watson model, or kernel regression (Nadaraya, 1964; Watson, 1964).
For a localized kernel function, it has the prop- erty of giving more weight to the data points xn that are close to x.
Note that the kernel(6.46)satisfiesthesummationconstraint N k(x, xn)=1.
n=1 6.4.
Gaussian Processes 303 Figure6.3 Illustration of the Nadaraya-Watson kernel 1.5 regressionmodelusingisotropic Gaussiankernels, forthe sinusoidal data set.
The original sine function is shown 1 by the green curve, the data points are shown in blue, and each is the centre of an isotropic Gaussian kernel.
The resulting regression function, given by the condi- 0.5 tionalmean, isshownbytheredline, alongwiththetwo- standard-deviation region for the conditional distribution 0 p(t|x)shownbytheredshading.
Theblueellipsearound eachdatapointshowsonestandarddeviationcontourfor −0.5 thecorrespondingkernel.
Theseappearnoncirculardue tothedifferentscalesonthehorizontalandverticalaxes.
−1 −1.5 0 0.2 0.4 0.6 0.8 1 In fact, this model defines not only a conditional expectation but also a full conditionaldistributiongivenby f(x−xn, t−tn) p(t, x) p(t|x)= = n (6.48) p(t, x)dt f(x−xm, t−tm)dt m fromwhichotherexpectationscanbeevaluated.
As an illustration we consider the case of a single input variable x in which f(x, t)isgivenbyazero-meanisotropic Gaussianoverthevariablez = (x, t)with variance σ2.
The corresponding conditional distribution (6.48) is given by a Gaus- Exercise 6.18 sian mixture, and is shown, together with the conditional mean, for the sinusoidal syntheticdatasetin Figure6.3.
Anobviousextensionofthismodelistoallowformoreflexibleformsof Gaus- siancomponents, forinstancehavingdifferentvarianceparametersfortheinputand targetvariables.
Moregenerally, wecouldmodelthejointdistributionp(t, x)using a Gaussianmixturemodel, trainedusingtechniquesdiscussedin Chapter9(Ghahra- mani and Jordan, 1994), and then find the corresponding conditional distribution p(t|x).
Inthislattercasewenolongerhavearepresentationintermsofkernelfunc- tionsevaluatedatthetrainingsetdatapoints.
However, thenumberofcomponents inthemixturemodelcanbesmallerthanthenumberoftrainingsetpoints, resulting inamodelthatisfastertoevaluatefortestdatapoints.
Wehavetherebyacceptedan increasedcomputationalcostduringthetrainingphaseinordertohaveamodelthat isfasteratmakingpredictions.
6.4.
Gaussian Processes In Section 6.1, we introduced kernels by applying the concept of duality to a non- probabilisticmodelforregression.
Hereweextendtheroleofkernelstoprobabilis- 304 6.
KERNELMETHODS ticdiscriminativemodels, leadingtotheframeworkof Gaussianprocesses.
Weshall therebyseehowkernelsarisenaturallyina Bayesiansetting.
In Chapter 3, we considered linear regression models of the form y(x, w) = w Tφ(x)inwhichwisavectorofparametersandφ(x)isavectoroffixednonlinear basisfunctionsthatdependontheinputvectorx.
Weshowedthatapriordistribution over w induced a corresponding prior distribution over functions y(x, w).
Given a training data set, we then evaluated the posterior distribution over w and thereby obtained the corresponding posterior distribution over regression functions, which in turn (with the addition of noise) implies a predictive distribution p(t|x) for new inputvectorsx.
Inthe Gaussianprocessviewpoint, wedispensewiththeparametricmodeland insteaddefineapriorprobabilitydistributionoverfunctionsdirectly.
Atfirstsight, it mightseemdifficulttoworkwithadistributionovertheuncountablyinfinitespaceof functions.
However, asweshallsee, forafinitetrainingsetweonlyneedtoconsider thevaluesofthefunctionatthediscretesetofinputvaluesxn correspondingtothe trainingsetandtestsetdatapoints, andsoinpracticewecanworkinafinitespace.
Modelsequivalentto Gaussianprocesseshavebeenwidelystudiedinmanydif- ferentfields.
Forinstance, inthegeostatisticsliterature Gaussianprocessregression isknownaskriging(Cressie,1993).
Similarly, ARMA(autoregressivemovingaver- age)models, Kalmanfilters, andradialbasisfunctionnetworkscanallbeviewedas formsof Gaussianprocessmodels.
Reviewsof Gaussianprocessesfromamachine learningperspectivecanbefoundin Mac Kay(1998), Williams(1999), and Mac Kay (2003), andacomparisonof Gaussianprocessmodelswithalternativeapproachesis given in Rasmussen (1996).
See also Rasmussen and Williams (2006) for a recent textbookon Gaussianprocesses.
6.4.1 Linear regression revisited In order to motivate the Gaussian process viewpoint, let us return to the linear regression example and re-derive the predictive distribution by working in terms of distributions over functions y(x, w).
This will provide a specific example of a Gaussianprocess.
Consider a model defined in terms of a linear combination of M fixed basis functionsgivenbytheelementsofthevectorφ(x)sothat y(x)=w Tφ(x) (6.49) wherexistheinputvectorandwisthe M-dimensionalweightvector.
Nowconsider apriordistributionoverwgivenbyanisotropic Gaussianoftheform p(w)=N(w|0,α −1I) (6.50) governedbythehyperparameterα, whichrepresentstheprecision(inversevariance) of the distribution.
For any given value of w, the definition (6.49) defines a partic- ular function of x.
The probability distribution over w defined by (6.50) therefore inducesaprobabilitydistributionoverfunctionsy(x).
Inpractice, wewishtoeval- uate this function at specific values of x, for example at the training data points 6.4.
Gaussian Processes 305 x 1 ,..., x N.
Wearethereforeinterestedinthejointdistributionofthefunctionval- uesy(x 1 ),..., y(x N), whichwedenotebythevectorywithelementsyn = y(xn) y=Φw (6.51) whereΦisthedesignmatrixwithelementsΦnk = φk(xn).
Wecanfindtheproba- bilitydistributionofyasfollows.
Firstofallwenotethatyisalinearcombinationof Gaussiandistributedvariablesgivenbytheelementsofw andhenceisitself Gaus- Exercise 2.31 sian.
Wethereforeneedonlytofinditsmeanandcovariance, whicharegivenfrom (6.50)by E[y] = ΦE[w]=0 (6.52) 1 cov[y] = E yy T =ΦE ww T ΦT = ΦΦT =K (6.53) α where Kisthe Grammatrixwithelements 1 Knm =k(xn, xm)= φ(xn)Tφ(xm) (6.54) α andk(x, x )isthekernelfunction.
Thismodelprovidesuswithaparticularexampleofa Gaussianprocess.
Ingen- eral, a Gaussian process isdefined asa probability distribution over functionsy(x) suchthatthesetofvaluesofy(x)evaluatedatanarbitrarysetofpointsx 1 ,..., x N jointly have a Gaussian distribution.
In cases where the input vector x is two di- mensional, this may also be known as a Gaussian random field.
More generally, a stochastic process y(x) is specified by giving the joint probability distribution for anyfinitesetofvaluesy(x 1 ),..., y(x N)inaconsistentmanner.
A key point about Gaussian stochastic processes is that the joint distribution over N variables y 1 ,..., y N is specified completely by the second-order statistics, namely the mean and the covariance.
In most applications, we will not have any priorknowledgeaboutthemeanofy(x)andsobysymmetrywetakeittobezero.
This is equivalent to choosing the mean of the prior over weight values p(w|α) to bezerointhebasisfunctionviewpoint.
Thespecificationofthe Gaussianprocessis then completed by giving the covariance of y(x) evaluated at any two values of x, whichisgivenbythekernelfunction E[y(xn)y(xm)]=k(xn, xm).
(6.55) For the specific case of a Gaussian process defined by the linear regression model (6.49)withaweightprior(6.50), thekernelfunctionisgivenby(6.54).
Wecanalsodefinethekernelfunctiondirectly, ratherthanindirectlythrougha choice of basis function.
Figure 6.4 shows samples of functions drawn from Gaus- sian processes for two different choices of kernel function.
The first of these is a ‘Gaussian’kerneloftheform(6.23), andthesecondistheexponentialkernelgiven by k(x, x )=exp(−θ|x−x |) (6.56) whichcorrespondstothe Ornstein-Uhlenbeckprocessoriginallyintroducedby Uh- lenbeckand Ornstein(1930)todescribe Brownianmotion.
306 6.
KERNELMETHODS Figure6.4 Samples from Gaus- 3 3 sianprocessesfora‘Gaussian’ker- nel (left) and an exponential kernel (right).
1.5 1.5 0 0 −1.5 −1.5 −3 −3 −1 −0.5 0 0.5 1 −1 −0.5 0 0.5 1 6.4.2 Gaussian processes for regression Inordertoapply Gaussianprocessmodelstotheproblemofregression, weneed totakeaccountofthenoiseontheobservedtargetvalues, whicharegivenby tn =yn+ n (6.57) where yn = y(xn), and n is a random noise variable whose value is chosen inde- pendentlyforeachobservationn.
Hereweshallconsidernoiseprocessesthathave a Gaussiandistribution, sothat p(tn |yn)=N(tn |yn,β −1) (6.58) where β is a hyperparameter representing the precision of the noise.
Because the noise is independent for each data point, the joint distribution of the target values isotropic Gaussianoftheform p(t|y)=N(t|y,β −1IN) (6.59) where IN denotesthe N×N unitmatrix.
Fromthedefinitionofa Gaussianprocess, themarginaldistributionp(y)isgivenbya Gaussianwhosemeaniszeroandwhose covarianceisdefinedbya Grammatrix Ksothat p(y)=N(y|0, K).
(6.60) The kernel function that determines K is typically chosen to express the property that, for points xn and xm that are similar, the corresponding values y(xn) and y(xm) will be more strongly correlated than for dissimilar points.
Here the notion ofsimilaritywilldependontheapplication.
In order to find the marginal distribution p(t), conditioned on the input values x 1 ,..., x N, we need to integrate over y.
This can be done by making use of the resultsfrom Section2.3.3forthelinear-Gaussianmodel.
Using(2.115), weseethat themarginaldistributionoftisgivenby p(t)= p(t|y)p(y)dy=N(t|0, C) (6.61) 6.4.
Gaussian Processes 307 wherethecovariancematrix Chaselements C(xn, xm)=k(xn, xm)+β −1δnm.
(6.62) This result reflects the fact that the two Gaussian sources of randomness, namely that associated with y(x) and that associated with , are independent and so their covariancessimplyadd.
Onewidelyusedkernelfunctionfor Gaussianprocessregressionisgivenbythe exponential of a quadratic form, with the addition of constant and linear terms to give θ k(xn, xm)=θ 0 exp − 1 xn −xm 2 +θ 2 +θ 3 x T n xm.
(6.63) 2 Note that the term involving θ 3 corresponds to a parametric model that is a linear functionoftheinputvariables.
Samplesfromthispriorareplottedforvariousvalues pled from the joint distribution (6.60) along with the corresponding values defined by(6.61).
So far, we have used the Gaussian process viewpoint to build a model of the joint distribution over sets of data points.
Our goal in regression, however, is to makepredictionsofthetargetvariablesfornewinputs, givenasetoftrainingdata.
comprisetheobservedtrainingset, andourgoalistopredictthetargetvariablet N+1 for a new input vector x N+1.
This requires that we evaluate the predictive distri- bution p(t N+1 |t N).
Note that this distribution is conditioned also on the variables x 1 ,..., x N andx N+1.
However, tokeepthenotationsimplewewillnotshowthese conditioningvariablesexplicitly.
To find the conditional distribution p(t N+1 |t), we begin by writing down the jointdistributionp(t N+1 ), wheret N+1 denotesthevector(t 1 ,..., t N, t N+1 )T.
We thenapplytheresultsfrom Section2.3.1toobtaintherequiredconditionaldistribu- tion, asillustratedin Figure6.7.
From(6.61), thejointdistributionovert 1 ,..., t N+1 willbegivenby p(t N+1 )=N(t N+1 |0, CN+1 ) (6.64) where CN+1 is an (N +1)×(N +1) covariance matrix with elements given by (6.62).
Because this joint distribution is Gaussian, we can apply the results from Section 2.3.1 to find the conditional Gaussian distribution.
To do this, we partition thecovariancematrixasfollows CN k CN+1 = k T c (6.65) where CN isthe N×N covariancematrixwithelementsgivenby(6.62)forn, m= 308 6.
KERNELMETHODS 3 9 3 1.5 4.5 1.5 0 0 0 −1.5 −4.5 −1.5 −3 −9 −3 3 9 4 1.5 4.5 2 0 0 0 −1.5 −4.5 −2 −3 −9 −4 Figure6.5 Samplesfroma Gaussianprocesspriordefinedbythecovariancefunction(6.63).
Thetitleabove eachplotdenotes(θ 0 ,θ 1 ,θ 2 ,θ 3 ).
c=k(x N+1 , x N+1 )+β−1.
Usingtheresults(2.81)and(2.82), weseethatthecon- ditional distribution p(t N+1 |t) is a Gaussian distribution with mean and covariance givenby m(x N+1 ) = k TC − N 1 t (6.66) σ2(x N+1 ) = c−k TC − N 1k.
(6.67) Thesearethekeyresultsthatdefine Gaussianprocessregression.
Becausethevector kisafunctionofthetestpointinputvaluex N+1, weseethatthepredictivedistribu- tionisa Gaussianwhosemeanandvariancebothdependonx N+1.
Anexampleof Gaussianprocessregressionisshownin Figure6.8.
Theonlyrestrictiononthekernelfunctionisthatthecovariancematrixgivenby (6.62)mustbepositivedefinite.
Ifλi isaneigenvalueof K, thenthecorresponding eigenvalue of C will be λi +β−1.
It is therefore sufficient that the kernel matrix k(xn, xm)bepositivesemidefiniteforanypairofpointsxnandxm, sothatλi 0, because any eigenvalue λi that is zero will still give rise to a positive eigenvalue for C because β > 0.
This is the same restriction on the kernel function discussed earlier, andsowecanagainexploitallofthetechniquesin Section6.2toconstruct 6.4.
Gaussian Processes 309 Figure6.6 Illustration of the sampling of data 3 points{t n }froma Gaussianprocess.
Thebluecurveshowsasamplefunc- tionfromthe Gaussianprocessprior over functions, and the red points show the values of y n obtained by t evaluatingthefunctionatasetofin- put values {x n }.
The correspond- ing values of {t n }, shown in green, are obtained by adding independent 0 Gaussiannoisetoeachofthe{y n }.
−3 −1 0 x 1 suitablekernels.
Notethatthemean(6.66)ofthepredictivedistributioncanbewritten, asafunc- tionofx N+1, intheform N m(x N+1 )= ank(xn, x N+1 ) (6.68) n=1 where an is the nth component of C − N 1 t.
Thus, if the kernel function k(xn, xm) depends only on the distance xn − xm , then we obtain an expansion in radial basisfunctions.
Theresults(6.66)and(6.67)definethepredictivedistributionfor Gaussianpro- cessregressionwithanarbitrarykernelfunctionk(xn, xm).
Intheparticularcasein whichthekernelfunctionk(x, x )isdefinedintermsofafinitesetofbasisfunctions, we can derive the results obtained previously in Section 3.3.2 for linear regression Exercise 6.21 startingfromthe Gaussianprocessviewpoint.
For such models, we can therefore obtain the predictive distribution either by takingaparameterspaceviewpointandusingthelinearregressionresultorbytaking afunctionspaceviewpointandusingthe Gaussianprocessresult.
The central computational operation in using Gaussian processes will involve theinversionofamatrixofsize N×N, forwhichstandardmethodsrequire O(N3) computations.
By contrast, in the basis function model we have to invert a matrix SN of size M × M, which has O(M3) computational complexity.
Note that for bothviewpoints, thematrixinversionmustbeperformedonceforthegiventraining set.
For each new test point, both methods require a vector-matrix multiply, which has cost O(N2) in the Gaussian process case and O(M2) for the linear basis func- tion model.
If the number M of basis functions is smaller than the number N of data points, it will be computationally more efficient to work in the basis function 310 6.
KERNELMETHODS Figure6.7 Illustration of the mechanism of Gaussian process regression for t2 the case of one training point and one test point, in which the red el- lipsesshowcontoursofthejointdis- tribution p(t 1 , t 2 ).
Here t 1 is the 1 training data point, and condition- ing on the value of t 1, correspond- m(x2) ing to the vertical blue line, we ob- tain p(t 2 |t 1 ) shown as a function of 0 t 2bythegreencurve.
t1 −1 −1 0 1 framework.
However, an advantage of a Gaussian processes viewpoint is that we canconsidercovariancefunctionsthatcanonlybeexpressedintermsofaninfinite numberofbasisfunctions.
Forlargetrainingdatasets, however, thedirectapplicationof Gaussianprocess methodscanbecomeinfeasible, andsoarangeofapproximationschemeshavebeen developed that have better scaling with training set size than the exact approach (Gibbs, 1997; Tresp, 2001; Smola and Bartlett, 2001; Williams and Seeger, 2001; Csato´ and Opper, 2002; Seeger et al., 2003).
Practical issues in the application of Gaussianprocessesarediscussedin Bishopand Nabney(2008).
We have introduced Gaussian process regression for the case of a single tar- get variable.
The extension of this formalism to multiple target variables, known Exercise 6.23 asco-kriging(Cressie,1993), isstraightforward.
Variousotherextensionsof Gaus- Figure6.8 Illustrationof Gaussianprocessre- gression applied to the sinusoidal datasetin Figure A.6inwhichthe 1 three right-most data points have been omitted.
The green curve 0.5 showsthesinusoidalfunctionfrom which the data points, shown in blue, areobtainedbysamplingand 0 addition of Gaussian noise.
The red line shows the mean of the −0.5 Gaussianprocesspredictivedistri- bution, andtheshadedregioncor- −1 responds to plus and minus two standard deviations.
Notice how theuncertaintyincreasesinthere- 0 0.2 0.4 0.6 0.8 1 giontotherightofthedatapoints.
6.4.
Gaussian Processes 311 sian process regression have also been considered, for purposes such as modelling the distribution over low-dimensional manifolds for unsupervised learning (Bishop etal.,1998a)andthesolutionofstochasticdifferentialequations(Graepel,2003).
6.4.3 Learning the hyperparameters Thepredictionsofa Gaussianprocessmodelwilldepend, inpart, onthechoice of covariance function.
In practice, rather than fixing the covariance function, we may prefer to use a parametric family of functions and then infer the parameter valuesfromthedata.
Theseparametersgovernsuchthingsasthelengthscaleofthe correlationsandtheprecisionofthenoiseandcorrespondtothehyperparametersin astandardparametricmodel.
Techniquesforlearningthehyperparametersarebasedontheevaluationofthe likelihoodfunctionp(t|θ)whereθdenotesthehyperparametersofthe Gaussianpro- cessmodel.
Thesimplestapproachistomakeapointestimateofθ bymaximizing the log likelihood function.
Because θ represents a set of hyperparameters for the regression problem, this can be viewed as analogous to the type 2 maximum like- Section3.5 lihood procedure for linear regression models.
Maximization of the log likelihood canbedoneusingefficientgradient-basedoptimizationalgorithmssuchasconjugate gradients(Fletcher,1987; Nocedaland Wright,1999; Bishopand Nabney,2008).
The log likelihood function for a Gaussian process regression model is easily evaluatedusingthestandardformforamultivariate Gaussiandistribution, giving 1 1 N lnp(t|θ)=− 2 ln|CN |− 2 t TC − N 1 t− 2 ln(2π).
(6.69) For nonlinear optimization, we also need the gradient of the log likelihood func- tion with respect to the parameter vector θ.
We shall assume that evaluation of the derivatives of CN is straightforward, as would be the case for the covariance func- tionsconsideredinthischapter.
Makinguseoftheresult(C.21)forthederivativeof C − N 1 , togetherwiththeresult(C.22)forthederivativeofln|CN |, weobtain ∂ lnp(t|θ)=− 1 Tr C −1 ∂CN + 1 t TC −1 ∂CN C −1 t.
(6.70) ∂θi 2 N ∂θi 2 N ∂θi N Becauselnp(t|θ)willingeneralbeanonconvexfunction, itcanhavemultiplemax- ima.
Itisstraightforward tointroduce aprior overθ andtomaximize thelogposte- riorusinggradient-basedmethods.
Inafully Bayesiantreatment, weneedtoevaluate marginalsoverθ weightedbytheproductofthepriorp(θ)andthelikelihoodfunc- tion p(t|θ).
In general, however, exact marginalization will be intractable, and we mustresorttoapproximations.
The Gaussian process regression model gives a predictive distribution whose mean and variance are functions of the input vectorx.
However, we have assumed thatthecontributiontothepredictivevariancearisingfromtheadditivenoise, gov- ernedbytheparameterβ, isaconstant.
Forsomeproblems, knownasheteroscedas- tic, thenoisevarianceitselfwillalsodependonx.
Tomodelthis, wecanextendthe 312 6.
KERNELMETHODS Figure6.9 Samplesfromthe ARD prior for Gaussian processes, in whichthekernelfunctionisgivenby (6.71).
The left plot corresponds to η 1 = η 2 = 1, and the right plot cor- respondstoη 1 =1,η 2 =0.01.
Gaussianprocessframeworkbyintroducingasecond Gaussianprocesstorepresent thedependenceofβ ontheinputx(Goldbergetal.,1998).
Becauseβ isavariance, andhencenonnegative, weusethe Gaussianprocesstomodellnβ(x).
6.4.4 Automatic relevance determination Intheprevioussection, wesawhowmaximumlikelihood couldbeusedtode- termine a value for the correlation length-scale parameter in a Gaussian process.
This technique can usefully be extended by incorporating a separate parameter for eachinputvariable(Rasmussenand Williams,2006).
Theresult, asweshallsee, is thattheoptimizationoftheseparametersbymaximumlikelihoodallowstherelative importanceofdifferentinputstobeinferredfromthedata.
Thisrepresentsanexam- ple in the Gaussian process context of automatic relevance determination, or ARD, which was originally formulated in the framework of neural networks (Mac Kay, 1994; Neal, 1996).
The mechanism by which appropriate inputs are preferred is discussedin Section7.2.2.
Considera Gaussianprocesswithatwo-dimensionalinputspacex = (x 1 , x 2 ), havingakernelfunctionoftheform 2 1 k(x, x )=θ 0 exp − ηi(xi −x i )2 .
(6.71) 2 i=1 Samples from the resulting prior over functions y(x) are shown for two different settings of the precision parameters ηi in Figure 6.9.
We see that, as a particu- lar parameter ηi becomes small, the function becomes relatively insensitive to the corresponding input variable xi.
By adapting these parameters to a data set using maximum likelihood, it becomes possible to detect input variables that have little effect on the predictive distribution, because the corresponding values ofηi will be small.
This can be useful in practice because it allows such inputs to be discarded.
ARDisillustratedusingasimplesyntheticdatasethavingthreeinputsx 1, x 2andx 3 (Nabney, 2002) in Figure 6.10.
The target variablet, is generated bysampling100 values of x 1 from a Gaussian, evaluating the function sin(2πx 1 ), and then adding 6.4.
Gaussian Processes 313 Figure6.10 Illustration of automatic rele- vance determination in a Gaus- 102 sianprocessforasyntheticprob- lem having three inputs x 1, x 2, and x 3, for which the curves showthecorrespondingvaluesof 100 thehyperparametersη 1 (red), η 2 (green), and η 3 (blue) as a func- tion of the number of iterations when optimizing the marginal 10−2 likelihood.
Details are given in the text.
Note the logarithmic scaleontheverticalaxis.
10−4 0 20 40 60 80 100 Gaussian noise.
Values of x 2 are given by copying the corresponding values of x 1 andaddingnoise, andvaluesofx 3 aresampledfromanindependent Gaussiandis- tribution.
Thusx 1 isagoodpredictoroft, x 2 isamorenoisypredictoroft, andx 3 hasonlychancecorrelationswitht.
Themarginallikelihoodfora Gaussianprocess with ARD parameters η 1 ,η 2 ,η 3 is optimized using the scaled conjugate gradients algorithm.
Weseefrom Figure6.10thatη 1 convergestoarelativelylargevalue,η 2 convergestoamuchsmallervalue, andη 3 becomesverysmallindicatingthatx 3 is irrelevantforpredictingt.
The ARDframeworkiseasilyincorporatedintotheexponential-quadratickernel (6.63)togivethefollowingformofkernelfunction, whichhasbeenfoundusefulfor applicationsof Gaussianprocessestoarangeofregressionproblems D D 1 k(xn, xm)=θ 0 exp − ηi(xni −xmi)2 +θ 2 +θ 3 xnixmi (6.72) 2 i=1 i=1 where Disthedimensionalityoftheinputspace.
6.4.5 Gaussian processes for classification In a probabilistic approach to classification, our goal is to model the posterior probabilities of the target variable for a new input vector, given a set of training data.
Theseprobabilitiesmustlieintheinterval(0,1), whereasa Gaussianprocess model makes predictions that lie on the entire real axis.
However, we can easily adapt Gaussian processes to classification problems by transforming the output of the Gaussianprocessusinganappropriatenonlinearactivationfunction.
Considerfirstthetwo-classproblemwithatargetvariablet ∈ {0,1}.
Ifwede- fine a Gaussian process over a function a(x) and then transform the function using a logistic sigmoid y = σ(a), given by (4.59), then we will obtain a non-Gaussian stochastic process over functions y(x) where y ∈ (0,1).
This is illustrated for the caseofaone-dimensionalinputspacein Figure6.11inwhichtheprobabilitydistri- 314 6.
KERNELMETHODS 10 1 5 0.75 0 0.5 −5 0.25 −10 0 −1 −0.5 0 0.5 1 −1 −0.5 0 0.5 1 Figure6.11 Theleftplotshowsasamplefroma Gaussianprocessprioroverfunctionsa(x), andtherightplot showstheresultoftransformingthissampleusingalogisticsigmoidfunction.
butionoverthetargetvariabletisthengivenbythe Bernoullidistribution p(t|a)=σ(a) t (1−σ(a))1−t .
(6.73) As usual, we denote the training set inputs by x 1 ,..., x N with corresponding observed target variables t = (t 1 ,..., t N)T.
We also consider a single test point x N+1 with target value t N+1.
Our goal is to determine the predictive distribution p(t N+1 |t), wherewehavelefttheconditioningontheinputvariablesimplicit.
Todo thisweintroducea Gaussianprocessprioroverthevectora N+1, whichhascompo- nentsa(x 1 ),..., a(x N+1 ).
Thisinturndefinesanon-Gaussianprocessovert N+1, andbyconditioningonthetrainingdatat N weobtaintherequiredpredictivedistri- bution.
The Gaussianprocesspriorfora N+1 takestheform p(a N+1 )=N(a N+1 |0, CN+1 ).
(6.74) Unlike the regression case, the covariance matrix no longer includes a noise term because we assume that all of the training data points are correctly labelled.
How- ever, for numerical reasons it is convenient to introduce a noise-like term governed by a parameter ν that ensures that the covariance matrix is positive definite.
Thus thecovariancematrix CN+1 haselementsgivenby C(xn, xm)=k(xn, xm)+νδnm (6.75) wherek(xn, xm)isanypositivesemidefinitekernelfunctionofthekindconsidered in Section6.2, andthevalueofν istypicallyfixedinadvance.
Weshallassumethat the kernel function k(x, x ) is governed by a vector θ of parameters, and we shall laterdiscusshowθmaybelearnedfromthetrainingdata.
For two-class problems, it is sufficient to predict p(t N+1 = 1|t N) because the value of p(t N+1 = 0|t N) is then given by 1 − p(t N+1 = 1|t N).
The required 6.4.
Gaussian Processes 315 predictivedistributionisgivenby p(t N+1 =1|t N)= p(t N+1 =1|a N+1 )p(a N+1 |t N)da N+1 (6.76) wherep(t N+1 =1|a N+1 )=σ(a N+1 ).
Thisintegralisanalyticallyintractable, andsomaybeapproximatedusingsam- pling methods (Neal, 1997).
Alternatively, we can consider techniques based on an analytical approximation.
In Section 4.5.2, we derived the approximate formula (4.153) for the convolution of a logistic sigmoid with a Gaussian distribution.
We can use this result to evaluate the integral in (6.76) provided we have a Gaussian approximationtotheposteriordistributionp(a N+1 |t N).
Theusualjustificationfora Gaussianapproximationtoaposteriordistributionisthatthetrueposteriorwilltend toa Gaussianasthenumberofdatapointsincreasesasaconsequenceofthecentral Section2.3 limittheorem.
Inthecaseof Gaussianprocesses, thenumberofvariablesgrowswith thenumberofdatapoints, andsothisargumentdoesnotapplydirectly.
However, if weconsiderincreasingthenumberofdatapointsfallinginafixedregionofxspace, thenthecorrespondinguncertaintyinthefunctiona(x)willdecrease, againleading asymptoticallytoa Gaussian(Williamsand Barber,1998).
Three different approaches to obtaining a Gaussian approximation have been Section10.1 considered.
One technique is based on variational inference (Gibbs and Mac Kay, 2000)andmakesuseofthelocalvariationalbound(10.144)onthelogisticsigmoid.
This allows the product of sigmoid functions to be approximated by a product of Gaussians thereby allowing the marginalization over a N to be performed analyti- cally.
The approach also yields a lower bound on the likelihood function p(t N |θ).
Thevariationalframeworkfor Gaussianprocessclassificationcanalsobeextended tomulticlass(K > 2)problemsbyusinga Gaussianapproximationtothesoftmax function(Gibbs,1997).
Section10.7 A second approach uses expectation propagation (Opper and Winther, 2000b; Minka,2001b; Seeger,2003).
Becausethetrueposteriordistributionisunimodal, as weshallseeshortly, theexpectationpropagationapproachcangivegoodresults.
6.4.6 Laplace approximation The third approach to Gaussian process classification is based on the Laplace Section4.4 approximation, whichwenowconsiderindetail.
Inordertoevaluatethepredictive distribution (6.76), we seek a Gaussian approximation to the posterior distribution overa N+1, which, using Bayes’theorem, isgivenby p(a N+1 |t N) = p(a N+1 , a N |t N)da N 1 = p(a N+1 , a N)p(t N |a N+1 , a N)da N p(t N) 1 = p(a N+1 |a N)p(a N)p(t N |a N)da N p(t N) = p(a N+1 |a N)p(a N |t N)da N (6.77) 316 6.
KERNELMETHODS where we have used p(t N |a N+1 , a N) = p(t N |a N).
The conditional distribution p(a N+1 |a N)isobtainedbyinvokingtheresults(6.66)and(6.67)for Gaussianpro- cessregression, togive p(a N+1 |a N)=N(a N+1 |k TC − N 1a N, c−k TC − N 1k).
(6.78) Wecanthereforeevaluatetheintegralin(6.77)byfindinga Laplaceapproximation for the posterior distribution p(a N |t N), and then using the standard result for the convolutionoftwo Gaussiandistributions.
Thepriorp(a N)isgivenbyazero-mean Gaussianprocesswithcovariancema- trix CN, andthedataterm(assumingindependenceofthedatapoints)isgivenby N N p(t N |a N)= σ(an) t n(1−σ(an))1−t n = e a n t nσ(−an).
(6.79) n=1 n=1 We then obtain the Laplace approximation by Taylor expanding the logarithm of p(a N |t N), whichuptoanadditivenormalizationconstantisgivenbythequantity Ψ(a N) = lnp(a N)+lnp(t N |a N) 1 N 1 = − 2 a T N C − N 1a N − 2 ln(2π)− 2 ln|CN |+t T N a N N − ln(1+e a n)+const.
(6.80) n=1 Firstweneedtofindthemodeoftheposteriordistribution, andthisrequiresthatwe evaluatethegradientofΨ(a N), whichisgivenby ∇Ψ(a N)=t N −σ N −C − N 1a N (6.81) where σ N is a vector with elements σ(an).
We cannot simply find the mode by setting this gradient to zero, because σ N depends nonlinearly on a N, and so we resorttoaniterativeschemebasedonthe Newton-Raphsonmethod, whichgivesrise Section4.3.3 to an iterative reweighted least squares (IRLS) algorithm.
This requires the second derivativesofΨ(a N), whichwealsorequireforthe Laplaceapproximationanyway, andwhicharegivenby ∇∇Ψ(a N)=−WN −C − N 1 (6.82) where WN isadiagonalmatrixwithelementsσ(an)(1−σ(an)), andwehaveused the result (4.88) for the derivative of the logistic sigmoid function.
Note that these diagonal elements lie in the range (0,1/4), and hence WN is a positive definite matrix.
Because CN (andhenceitsinverse)ispositivedefinitebyconstruction, and Exercise 6.24 because the sum of two positive definite matrices is also positive definite, we see that the Hessian matrix A = −∇∇Ψ(a N) is positive definite and so the posterior distributionp(a N |t N)islogconvexandthereforehasasinglemodethatistheglobal 6.4.
Gaussian Processes 317 maximum.
Theposteriordistributionisnot Gaussian, however, becausethe Hessian isafunctionofa N.
Usingthe Newton-Raphsonformula(4.92), theiterativeupdateequationfora N Exercise 6.25 isgivenby an N ew =CN(I+WNCN) −1{t N −σ N +WNa N }.
(6.83) These equations are iterated until they converge to the mode which we denote by a N .
Atthemode, thegradient∇Ψ(a N)willvanish, andhencea N willsatisfy a N =CN(t N −σ N).
(6.84) Oncewehavefoundthemodea oftheposterior, wecanevaluatethe Hessian N matrixgivenby H=−∇∇Ψ(a N)=WN +C − N 1 (6.85) where the elements of WN are evaluated using a N .
This defines our Gaussian ap- proximationtotheposteriordistributionp(a N |t N)givenby q(a N)=N(a N |a N , H −1).
(6.86) Wecannowcombinethiswith(6.78)andhenceevaluatetheintegral(6.77).
Because thiscorrespondstoalinear-Gaussianmodel, wecanusethegeneralresult(2.115)to Exercise 6.26 give E[a N+1 |t N] = k T(t N −σ N) (6.87) var[a N+1 |t N] = c−k T(W N −1+CN) −1k.
(6.88) Nowthatwehavea Gaussiandistributionforp(a N+1 |t N), wecanapproximate theintegral(6.76)usingtheresult(4.153).
Aswiththe Bayesianlogisticregression modelof Section4.5, ifweareonlyinterestedinthedecisionboundarycorrespond- ing to p(t N+1 |t N) = 0.5, then we need only consider the mean and we can ignore theeffectofthevariance.
We also need to determine the parameters θ of the covariance function.
One approachistomaximizethelikelihoodfunctiongivenbyp(t N |θ)forwhichweneed expressionsfortheloglikelihoodanditsgradient.
Ifdesired, suitableregularization terms can also be added, leading to a penalized maximum likelihood solution.
The likelihoodfunctionisdefinedby p(t N |θ)= p(t N |a N)p(a N |θ)da N.
(6.89) Thisintegralisanalyticallyintractable, soagainwemakeuseofthe Laplaceapprox- imation.
Usingtheresult(4.135), weobtainthefollowingapproximationforthelog ofthelikelihoodfunction 1 N lnp(t N |θ)=Ψ(a N )− 2 ln|WN +C − N 1|+ 2 ln(2π) (6.90) 318 6.
KERNELMETHODS where Ψ(a N ) = lnp(a N |θ)+lnp(t N |a N ).
We also need to evaluate the gradient of lnp(t N |θ) with respect to the parameter vector θ.
Note that changes in θ will cause changes in a , leading to additional terms in the gradient.
Thus, when we N differentiate (6.90) with respect to θ, we obtain two sets of terms, the first arising from the dependence of the covariance matrix CN on θ, and the rest arising from dependenceofa onθ.
N The terms arising from the explicit dependence on θ can be found by using (6.80)togetherwiththeresults(C.21)and(C.22), andaregivenby ∂lnp(t N |θ) = 1 a TC −1 ∂CN C −1a ∂θj 2 N N ∂θj N N − 1 Tr (I+CNWN) −1WN ∂CN .
(6.91) 2 ∂θj To compute the terms arising from the dependence of a on θ, we note that N the LaplaceapproximationhasbeenconstructedsuchthatΨ(a N)haszerogradient at a N = a N , and so Ψ(a N ) gives no contribution to the gradient as a result of its dependence on a .
This leaves the following contribution to the derivative with N respecttoacomponentθj ofθ − 1 N ∂ln|WN +C − N 1|∂a n 2 n=1 ∂a n ∂θj 1 N ∂a =− 2 (I+CNWN) −1CN nn σ n (1−σ n )(1−2σ n ) ∂θ n j (6.92) n=1 where σ = σ(a ), and again we have used the result (C.22) together with the n n definitionof WN.
Wecanevaluatethederivativeofa N withrespecttoθj bydiffer- entiatingtherelation(6.84)withrespecttoθj togive ∂a n = ∂CN (t N −σ N)−CNWN ∂a n .
(6.93) ∂θj ∂θj ∂θj Rearrangingthengives ∂a n =(I+WNCN) −1 ∂CN (t N −σ N).
(6.94) ∂θj ∂θj Combining (6.91), (6.92), and (6.94), we can evaluate the gradient of the log likelihood function, which can be used with standard nonlinear optimization algo- rithmsinordertodetermineavalueforθ.
Wecanillustratetheapplicationofthe Laplaceapproximationfor Gaussianpro- Appendix A cessesusingthesynthetictwo-classdatasetshownin Figure6.12.
Extensionofthe Laplace approximation to Gaussian processes involving K > 2 classes, using the softmaxactivationfunction, isstraightforward(Williamsand Barber,1998).
6.4.
Gaussian Processes 319 2 0 −2 −2 0 2 Figure6.12 Illustrationoftheuseofa Gaussianprocessforclassification, showingthedataonthelefttogether with the optimal decision boundary from the true distribution in green, and the decision boundary from the Gaussian process classifier in black.
On the right is the predicted posterior probability for the blue and red classestogetherwiththe Gaussianprocessdecisionboundary.
6.4.7 Connection to neural networks We have seen that the range of functions which can be represented by a neural network is governed by the number M of hidden units, and that, for sufficiently large M, a two-layer network can approximate any given function with arbitrary accuracy.
In the framework of maximum likelihood, the number of hidden units needs to be limited (to a level dependent on the size of the training set) in order to avoid over-fitting.
However, from a Bayesian perspective it makes little sense to limit the number of parameters in the network according to the size of the training set.
In a Bayesian neural network, the prior distribution over the parameter vector w, in conjunction with the network function f(x, w), produces a prior distribution over functions from y(x) where y is the vector of network outputs.
Neal (1996) has shown that, for a broad class of prior distributions over w, the distribution of functionsgeneratedbyaneuralnetworkwilltendtoa Gaussianprocessinthelimit M → ∞.
It should be noted, however, that in this limit the output variables of the neural network become independent.
One of the great merits of neural networks is that the outputs share the hidden units and so they can ‘borrow statistical strength’ fromeachother, thatis, theweightsassociatedwitheachhiddenunitareinfluenced byalloftheoutputvariablesnotjustbyoneofthem.
Thispropertyisthereforelost inthe Gaussianprocesslimit.
We have seen that a Gaussian process is determined by its covariance (kernel) function.
Williams(1998)hasgivenexplicitformsforthecovarianceinthecaseof two specific choices for the hidden unit activation function (probit and Gaussian).
These kernel functions k(x, x ) are nonstationary, i.
e.
they cannot be expressed as a function of the difference x−x , as a consequence of the Gaussian weight prior beingcentredonzerowhichbreakstranslationinvarianceinweightspace.
320 6.
KERNELMETHODS By working directly with the covariance function we have implicitly marginal- ized over the distribution of weights.
If the weight prior is governed by hyperpa- rameters, then their values will determine the length scales of the distribution over functions, ascanbeunderstoodbystudyingtheexamplesin Figure5.11forthecase ofafinitenumberofhiddenunits.
Notethatwecannotmarginalizeoutthehyperpa- rametersanalytically, andmustinsteadresorttotechniquesofthekinddiscussedin Section6.4.
Exercises 6.1 ( ) www Consider the dual formulation of the least squares linear regression problem given in Section 6.1.
Show that the solution for the components an of the vector a can be expressed as a linear combination of the elements of the vector φ(xn).
Denoting these coefficients by the vectorw, show that the dual of the dual formulation is given by the original representation in terms of the parameter vector w.
6.2 ( ) In this exercise, we develop a dual formulation of the perceptron learning algorithm.
Using the perceptron learning rule (4.55), show that the learned weight vectorwcanbewrittenasalinearcombinationofthevectorstn φ(xn)wheretn ∈ {−1,+1}.
Denote the coefficients of this linear combination by αn and derive a formulationoftheperceptronlearningalgorithm, andthepredictivefunctionforthe perceptron, intermsoftheαn.
Showthatthefeaturevectorφ(x)entersonlyinthe formofthekernelfunctionk(x, x )=φ(x)Tφ(x ).
6.3 ( ) The nearest-neighbour classifier (Section 2.5.2) assigns a new input vector x tothesameclassasthatofthenearestinputvectorxn fromthetrainingset, where inthesimplestcase, thedistanceisdefinedbythe Euclideanmetric x−xn 2.
By expressing this rule in terms of scalar products and then making use of kernel sub- stitution, formulatethenearest-neighbourclassifierforageneralnonlinearkernel.
6.4 ( ) In Appendix C, wegiveanexampleofamatrixthathaspositiveelementsbut thathasanegativeeigenvalueandhencethatisnotpositivedefinite.
Findanexample of the converse property, namely a 2×2 matrix with positive eigenvalues yet that hasatleastonenegativeelement.
6.5 ( ) www Verifytheresults(6.13)and(6.14)forconstructingvalidkernels.
6.6 ( ) Verifytheresults(6.15)and(6.16)forconstructingvalidkernels.
6.7 ( ) www Verifytheresults(6.17)and(6.18)forconstructingvalidkernels.
6.8 ( ) Verifytheresults(6.19)and(6.20)forconstructingvalidkernels.
6.9 ( ) Verifytheresults(6.21)and(6.22)forconstructingvalidkernels.
6.10 ( ) Show that an excellent choice of kernel for learning a function f(x) is given by k(x, x ) = f(x)f(x ) by showing that a linear learning machine based on this kernelwillalwaysfindasolutionproportionaltof(x).
Exercises 321 6.11 ( ) By making use of the expansion (6.25), and then expanding the middle factor asapowerseries, showthatthe Gaussiankernel(6.23)canbeexpressedastheinner productofaninfinite-dimensionalfeaturevector.
6.12 ( ) www Consider the space of all possible subsets A of a given fixed set D.
Show that the kernel function (6.27) corresponds to an inner product in a feature spaceofdimensionality2|D|definedbythemappingφ(A)where Aisasubsetof D andtheelementφU(A), indexedbythesubset U, isgivenby 1, if U ⊆A; φU(A)= 0, otherwise.
(6.95) Here U ⊆Adenotesthat U iseitherasubsetof Aorisequalto A.
6.13 ( ) Show that the Fisher kernel, defined by (6.33), remains invariant if we make a nonlinear transformation of the parameter vector θ → ψ(θ), where the function ψ(·)isinvertibleanddifferentiable.
6.14 ( ) www Write down the form of the Fisher kernel, defined by (6.33), for the caseofadistributionp(x|µ) = N(x|µ, S)thatis Gaussianwithmeanµandfixed covariance S.
6.15 ( ) By considering the determinant of a 2×2 Gram matrix, show that a positive- definitekernelfunctionk(x, x )satisfiesthe Cauchy-Schwartzinequality k(x 1 , x 2 )2 k(x 1 , x 1 )k(x 2 , x 2 ).
(6.96) 6.16 ( ) Consider a parametric model governed by the parameter vector w together with a data set of input values x 1 ,..., x N and a nonlinear feature mapping φ(x).
Supposethatthedependenceoftheerrorfunctiononwtakestheform J(w)=f(w Tφ(x 1 ),..., w Tφ(x N))+g(w Tw) (6.97) whereg(·)isamonotonicallyincreasingfunction.
Bywritingwintheform N w = αn φ(xn)+w⊥ (6.98) n=1 showthatthevalueofwthatminimizes J(w)takestheformofalinearcombination ofthebasisfunctionsφ(xn)forn=1,..., N.
6.17 ( ) www Consider the sum-of-squares error function (6.39) for data having noisy inputs, where ν(ξ) is the distribution of the noise.
Use the calculus of vari- ations to minimize this error function with respect to the function y(x), and hence showthattheoptimalsolutionisgivenbyanexpansionoftheform(6.40)inwhich thebasisfunctionsaregivenby(6.41).
322 6.
KERNELMETHODS 6.18 ( ) Consider a Nadaraya-Watson model with one input variable x and one target variable t having Gaussian components with isotropic covariances, so that the co- variancematrixisgivenbyσ2Iwhere Iistheunitmatrix.
Writedownexpressions for the conditional density p(t|x) and for the conditional mean E[t|x] and variance var[t|x], intermsofthekernelfunctionk(x, xn).
6.19 ( ) Another viewpoint on kernel regression comes from a consideration of re- gression problems in which the input variables as well as the target variables are corrupted with additive noise.
Suppose each target value tn is generated as usual bytakingafunctiony(zn)evaluatedatapointzn, andadding Gaussiannoise.
The value of zn is not directly observed, however, but only a noise corrupted version xn = zn +ξ n wheretherandomvariableξ isgovernedbysomedistributiong(ξ).
Consider a set of observations {xn, tn }, where n = 1,..., N, together with a cor- respondingsum-of-squareserrorfunctiondefinedbyaveragingoverthedistribution ofinputnoisetogive N 1 E = {y(xn −ξ n )−tn }2 g(ξ n )dξ n .
(6.99) 2 n=1 By minimizing E with respect to the function y(z) using the calculus of variations (Appendix D), show that optimal solution for y(x) is given by a Nadaraya-Watson kernelregressionsolutionoftheform(6.45)withakerneloftheform(6.46).
6.20 ( ) www Verifytheresults(6.66)and(6.67).
6.21 ( ) www Consider a Gaussian process regression model in which the kernel functionisdefinedintermsofafixedsetofnonlinearbasisfunctions.
Showthatthe predictivedistributionisidenticaltotheresult(3.58)obtainedin Section3.3.2forthe Bayesian linear regression model.
To do this, note that both models have Gaussian predictivedistributions, andsoitisonlynecessarytoshowthattheconditionalmean andvariancearethesame.
Forthemean, makeuseofthematrixidentity(C.6), and forthevariance, makeuseofthematrixidentity(C.7).
6.22 ( ) Consider a regression problem with N training set input vectors x 1 ,..., x N and L test set input vectors x N+1 ,..., x N+L, and suppose we define a Gaussian process prior over functions t(x).
Derive an expression for the joint predictive dis- marginalofthisdistributionforoneofthetestobservationstj where N +1 j N +Lisgivenbytheusual Gaussianprocessregressionresult(6.66)and(6.67).
6.23 ( ) www Consider a Gaussian process regression model in which the target variable t has dimensionality D.
Write down the conditional distribution of t N+1 foratestinputvectorx N+1, givenatrainingsetofinputvectorsx 1 ,..., x N+1 and correspondingtargetobservationst 1 ,..., t N.
6.24 ( ) Showthatadiagonalmatrix Wwhoseelementssatisfy0<Wii <1ispositive definite.
Showthatthesumoftwopositivedefinitematricesisitselfpositivedefinite.
Exercises 323 6.25 ( ) www Using the Newton-Raphson formula (4.92), derive the iterative update formula(6.83)forfindingthemodea oftheposteriordistributioninthe Gaussian N processclassificationmodel.
6.26 ( ) Usingtheresult(2.115), derivetheexpressions(6.87)and(6.88)forthemean andvarianceoftheposteriordistributionp(a N+1 |t N)inthe Gaussianprocessclas- sificationmodel.
6.27 ( ) Derivetheresult(6.90)fortheloglikelihoodfunctioninthe Laplaceapprox- imationframeworkfor Gaussianprocessclassification.
Similarly, derivetheresults (6.91),(6.92), and(6.94)forthetermsinthegradientoftheloglikelihood.
7 Sparse Kernel Machines Inthepreviouschapter, weexploredavarietyoflearningalgorithmsbasedonnon- linear kernels.
One of the significant limitations of many such algorithms is that the kernel function k(xn, xm) must be evaluated for all possible pairs xn and xm of training points, which can be computationally infeasible during training and can lead to excessive computation times when making predictions for new data points.
In this chapter we shall look at kernel-based algorithms that have sparse solutions, sothatpredictionsfornewinputsdependonlyonthekernelfunctionevaluatedata subsetofthetrainingdatapoints.
Webeginbylookinginsomedetailatthesupportvectormachine(SVM), which becamepopularinsomeyearsagoforsolvingproblemsinclassification, regression, andnoveltydetection.
Animportantpropertyofsupportvectormachinesisthatthe determination of the model parameters corresponds to a convex optimization prob- lem, and so any local solution is also a global optimum.
Because the discussion of supportvectormachinesmakesextensiveuseof Lagrangemultipliers, thereaderis 325 326 7.
SPARSEKERNELMACHINES encouraged to review the key concepts covered in Appendix E.
Additional infor- mation on support vector machines can be found in Vapnik (1995), Burges (1998), Cristianini and Shawe-Taylor (2000), Mu¨ller et al.
(2001), Scho¨lkopf and Smola (2002), and Herbrich(2002).
The SVMisadecisionmachineandsodoesnotprovideposteriorprobabilities.
Wehavealreadydiscussedsomeofthebenefitsofdeterminingprobabilitiesin Sec- tion 1.5.4.
An alternative sparse kernel technique, known as the relevance vector Section7.2 machine (RVM), is based on a Bayesian formulation and provides posterior proba- bilisticoutputs, aswellashavingtypicallymuchsparsersolutionsthanthe SVM.
7.1.
Maximum Margin Classifiers We begin our discussion of support vector machines by returning to the two-class classificationproblemusinglinearmodelsoftheform y(x)=w Tφ(x)+b (7.1) where φ(x) denotes a fixed feature-space transformation, and we have made the biasparameterbexplicit.
Notethatweshallshortlyintroduceadualrepresentation expressed in terms of kernel functions, which avoids having to work explicitly in feature space.
The training data set comprises N input vectors x 1 ,..., x N, with corresponding target values t 1 ,..., t N where tn ∈ {−1,1}, and new data points x areclassifiedaccordingtothesignofy(x).
Weshallassumeforthemomentthatthetrainingdatasetislinearlyseparablein featurespace, sothatbydefinitionthereexistsatleastonechoiceoftheparameters wandbsuchthatafunctionoftheform(7.1)satisfiesy(xn) > 0forpointshaving tn = +1 and y(xn) < 0 for points having tn = −1, so that tny(xn) > 0 for all trainingdatapoints.
Theremayofcourseexistmanysuchsolutionsthatseparatetheclassesexactly.
In Section 4.1.7, we described the perceptron algorithm that is guaranteed to find a solution in a finite number of steps.
The solution that it finds, however, will be dependent on the (arbitrary) initial values chosen for w and b as well as on the order in which the data points are presented.
If there are multiple solutions all of which classify the training data set exactly, then we should try to find the one that will give the smallest generalization error.
The support vector machine approaches thisproblemthroughtheconceptofthemargin, whichisdefinedtobethesmallest distance between the decision boundary and any of the samples, as illustrated in Figure7.1.
In support vector machines the decision boundary is chosen to be the one for whichthemarginismaximized.
Themaximummarginsolutioncanbemotivatedus- Section7.1.5 ingcomputationallearningtheory, alsoknownasstatisticallearningtheory.
How- ever, a simple insight into the origins of maximum margin has been given by Tong and Koller(2000)whoconsideraframeworkforclassificationbasedonahybridof generativeanddiscriminativeapproaches.
Theyfirstmodelthedistributionoverin- putvectorsxforeachclassusinga Parzendensityestimatorwith Gaussiankernels 7.1.
Maximum Margin Classifiers 327 y =1 y =−1 y =0 y =0 y =−1 y =1 margin Figure7.1 Themarginisdefinedastheperpendiculardistancebetweenthedecisionboundaryandtheclosest of the data points, as shown on the left figure.
Maximizing the margin leads to a particular choice of decision boundary, as shown on the right.
The location of this boundary is determined by a subset of the data points, knownassupportvectors, whichareindicatedbythecircles.
havingacommonparameterσ2.
Togetherwiththeclasspriors, thisdefinesanopti- malmisclassification-ratedecisionboundary.
However, insteadofusingthisoptimal boundary, theydeterminethebesthyperplanebyminimizingtheprobabilityoferror relative to the learned density model.
In the limit σ2 → 0, the optimal hyperplane isshowntobetheonehavingmaximummargin.
Theintuitionbehindthisresultis thatasσ2isreduced, thehyperplaneisincreasinglydominatedbynearbydatapoints relative to more distant ones.
In the limit, the hyperplane becomes independent of datapointsthatarenotsupportvectors.
Weshallseein Figure10.13thatmarginalizationwithrespecttothepriordistri- butionoftheparametersina Bayesianapproachforasimplelinearlyseparabledata set leads to a decision boundary that lies in the middle of the region separating the datapoints.
Thelargemarginsolutionhassimilarbehaviour.
Recallfrom Figure4.1thattheperpendiculardistanceofapointxfromahyper- planedefinedbyy(x)=0wherey(x)takestheform(7.1)isgivenby|y(x)|/ w .
Furthermore, we are only interested in solutions for which all data points are cor- rectlyclassified, sothattny(xn)>0foralln.
Thusthedistanceofapointxntothe decisionsurfaceisgivenby tny(xn) tn(w Tφ(xn)+b) = .
(7.2) w w The margin is given by the perpendicular distance to the closest point xn from the dataset, andwewishtooptimizetheparametersw andbinordertomaximizethis distance.
Thusthemaximummarginsolutionisfoundbysolving 1 arg w m , b ax w m n in tn w Tφ(xn)+b (7.3) where we have taken the factor 1/ w outside the optimization over n because w 328 7.
SPARSEKERNELMACHINES does not depend on n.
Direct solution of this optimization problem would be very complex, and so we shall convert it into an equivalent problem that is much easier to solve.
To do this we note that if we make the rescaling w → κw and b → κb, thenthedistancefromanypointxn tothedecisionsurface, givenbytny(xn)/ w , isunchanged.
Wecanusethisfreedomtoset tn w Tφ(xn)+b =1 (7.4) forthepointthatisclosesttothesurface.
Inthiscase, alldatapointswillsatisfythe constraints This is known as the canonical representation of the decision hyperplane.
In the caseofdatapointsforwhichtheequalityholds, theconstraintsaresaidtobeactive, whereas for the remainder they are said to be inactive.
By definition, there will alwaysbeatleastoneactiveconstraint, becausetherewillalwaysbeaclosestpoint, andoncethemarginhasbeenmaximizedtherewillbeatleasttwoactiveconstraints.
Theoptimizationproblemthensimplyrequiresthatwemaximize w −1, whichis equivalenttominimizing w 2, andsowehavetosolvetheoptimizationproblem 1 argmin w 2 (7.6) 2 w, b subject to the constraints given by (7.5).
The factor of 1/2 in (7.6) is included for laterconvenience.
Thisisanexampleofaquadraticprogrammingprobleminwhich we are trying to minimize a quadratic function subject to a set of linear inequality constraints.
Itappearsthatthebiasparameterbhasdisappearedfromtheoptimiza- tion.
However, itisdeterminedimplicitlyviatheconstraints, becausetheserequire thatchangesto w becompensatedbychangestob.
Weshallseehowthisworks shortly.
Inordertosolvethisconstrainedoptimizationproblem, weintroduce Lagrange Appendix E multipliersan 0, withonemultiplieranforeachoftheconstraintsin(7.5), giving the Lagrangianfunction N 1 L(w, b, a)= w 2− an tn(w Tφ(xn)+b)−1 (7.7) 2 n=1 wherea = (a 1 ,..., a N)T.
Notetheminussigninfrontofthe Lagrangemultiplier term, because we are minimizing with respect to w and b, and maximizing with respect to a.
Setting the derivatives of L(w, b, a) with respect to w and b equal to zero, weobtainthefollowingtwoconditions N w = antn φ(xn) (7.8) n=1 N 0 = antn.
(7.9) n=1 7.1.
Maximum Margin Classifiers 329 Eliminating w and b from L(w, b, a) using these conditions then gives the dual representationofthemaximummarginprobleminwhichwemaximize N N N L (a)= an − 1 anamtntmk(xn, xm) (7.10) 2 n=1 n=1m=1 withrespecttoasubjecttotheconstraints an 0, n=1,..., N, (7.11) N antn = 0.
(7.12) n=1 Herethekernelfunctionisdefinedbyk(x, x )=φ(x)Tφ(x ).
Again, thistakesthe formofaquadraticprogrammingprobleminwhichweoptimizeaquadraticfunction ofasubjecttoasetofinequalityconstraints.
Weshalldiscusstechniquesforsolving suchquadraticprogrammingproblemsin Section7.1.1.
Thesolutiontoaquadraticprogrammingproblemin M variablesingeneralhas computationalcomplexitythatis O(M3).
Ingoingtothedualformulationwehave turnedtheoriginaloptimizationproblem, whichinvolvedminimizing(7.6)over M variables, into the dual problem (7.10), which has N variables.
For a fixed set of basis functions whose number M is smaller than the number N of data points, the movetothedualproblemappearsdisadvantageous.
However, itallowsthemodelto bereformulatedusingkernels, andsothemaximummarginclassifiercanbeapplied efficientlytofeaturespaceswhosedimensionalityexceedsthenumberofdatapoints, including infinite feature spaces.
The kernel formulation also makes clear the role of the constraint that the kernel function k(x, x ) be positive definite, because this ensures that the Lagrangian function L(a) is bounded below, giving rise to a well- definedoptimizationproblem.
Inordertoclassifynewdatapointsusingthetrainedmodel, weevaluatethesign ofy(x)definedby(7.1).
Thiscanbeexpressedintermsoftheparameters{an }and thekernelfunctionbysubstitutingforwusing(7.8)togive N y(x)= antnk(x, xn)+b.
(7.13) n=1 Joseph-Louis Lagrange years, Euler worked hard to persuade Lagrange to 1736–1813 moveto Berlin, whichheeventuallydidin1766where he succeeded Euler as Director of Mathematics at Although widely considered to be the Berlin Academy.
Later he moved to Paris, nar- a French mathematician, Lagrange rowly escaping with his life during the French revo- wasbornin Turinin Italy.
Bytheage lution thanks tothe personalinterventionof Lavoisier of nineteen, he had already made (the Frenchchemistwhodiscoveredoxygen)whohim- important contributions mathemat- self was later executed at the guillotine.
Lagrange icsandhadbeenappointedas Pro- made key contributions to the calculus of variations fessoratthe Royal Artillery Schoolin Turin.
Formany andthefoundationsofdynamics.
330 7.
SPARSEKERNELMACHINES In Appendix E, weshowthataconstrainedoptimizationofthisformsatisfiesthe Karush-Kuhn-Tucker(KKT)conditions, whichinthiscaserequirethatthefollowing threepropertieshold an 0 (7.14) tny(xn)−1 0 (7.15) an {tny(xn)−1} = 0.
(7.16) Thus for every data point, either an = 0 or tny(xn) = 1.
Any data point for whichan =0willnotappearinthesumin(7.13)andhenceplaysnoroleinmaking predictionsfornewdatapoints.
Theremainingdatapointsarecalledsupportvectors, and because they satisfy tny(xn) = 1, they correspond to points that lie on the maximum margin hyperplanes in feature space, as illustrated in Figure 7.1.
This property is central to the practical applicability of support vector machines.
Once themodelistrained, asignificantproportionofthedatapointscanbediscardedand onlythesupportvectorsretained.
Havingsolvedthequadraticprogrammingproblemandfoundavaluefora, we canthendeterminethevalueofthethresholdparameterbbynotingthatanysupport vectorxn satisfiestny(xn)=1.
Using(7.13)thisgives tn amtmk(xn, xm)+b =1 (7.17) m∈S where S denotes the set of indices of the support vectors.
Although we can solve thisequationforbusinganarbitrarilychosensupportvectorxn, anumericallymore stablesolutionisobtainedbyfirstmultiplyingthroughbytn, makinguseoft2 n =1, andthenaveragingtheseequationsoverallsupportvectorsandsolvingforbtogive 1 b= tn − amtmk(xn, xm) (7.18) NS n∈S m∈S where NS isthetotalnumberofsupportvectors.
For later comparison with alternative models, we can express the maximum- margin classifier in terms of the minimization of an error function, with a simple quadraticregularizer, intheform N E∞(y(xn)tn −1)+λ w 2 (7.19) n=1 where E∞(z) is a function that is zero if z 0 and ∞ otherwise and ensures that the constraints (7.5) are satisfied.
Note that as long as the regularization parameter satisfiesλ>0, itsprecisevalueplaysnorole.
Figure7.2showsanexampleoftheclassificationresultingfromtrainingasup- port vector machine on a simple synthetic data set using a Gaussian kernel of the 7.1.
Maximum Margin Classifiers 331 Figure7.2 Example of synthetic data from two classes in two dimensions showing contours of constant y(x) obtained from a support vector machine having a Gaus- siankernelfunction.
Alsoshown are the decision boundary, the marginboundaries, andthesup- portvectors.
form (6.23).
Although the data set is not linearly separable in the two-dimensional dataspacex, itislinearlyseparableinthenonlinearfeaturespacedefinedimplicitly bythenonlinearkernelfunction.
Thusthetrainingdatapointsareperfectlyseparated intheoriginaldataspace.
This example also provides a geometrical insight into the origin of sparsity in the SVM.
Themaximummarginhyperplaneisdefinedbythelocationofthesupport vectors.
Otherdatapointscanbemovedaroundfreely(solongastheyremainout- sidethemarginregion)withoutchangingthedecisionboundary, andsothesolution willbeindependentofsuchdatapoints.
7.1.1 Overlapping class distributions Sofar, wehaveassumedthatthetrainingdatapointsarelinearlyseparableinthe featurespaceφ(x).
Theresultingsupportvectormachinewillgiveexactseparation ofthetrainingdataintheoriginalinputspacex, althoughthecorrespondingdecision boundarywillbenonlinear.
Inpractice, however, theclass-conditionaldistributions may overlap, in which case exact separation of the training data can lead to poor generalization.
We therefore need a way to modify the support vector machine so as to allow some of the training points to be misclassified.
From (7.19) we see that in the case of separable classes, we implicitly used an error function that gave infinite error if a data point was misclassified and zero error if it was classified correctly, and thenoptimizedthemodelparameterstomaximizethemargin.
Wenowmodifythis approach so that data points are allowed to be on the ‘wrong side’ of the margin boundary, butwithapenaltythatincreaseswiththedistancefromthatboundary.
For the subsequent optimization problem, it is convenient to make this penalty a linear function of this distance.
To do this, we introduce slack variables, ξn 0 where n = 1,..., N, with one slack variable for each training data point (Bennett, 1992; Cortesand Vapnik,1995).
Thesearedefinedbyξn =0fordatapointsthatareonor insidethecorrectmarginboundaryandξn = |tn −y(xn)|forotherpoints.
Thusa datapointthatisonthedecisionboundaryy(xn) = 0willhaveξn = 1, andpoints 332 7.
SPARSEKERNELMACHINES Figure7.3 Illustration of the slack variables ξ n 0.
Data points with circles around them are y =−1 supportvectors.
y =0 ξ>1 y =1 ξ<1 ξ=0 ξ=0 withξn > 1willbemisclassified.
Theexactclassificationconstraints(7.5)arethen replacedwith tny(xn) 1−ξn, n=1,..., N (7.20) inwhichtheslackvariablesareconstrainedtosatisfyξn 0.
Datapointsforwhich ξn = 0 are correctly classified and are either on the margin or on the correct side of the margin.
Points for which 0 < ξn 1 lie inside the margin, but on the cor- rect side of the decision boundary, and those data points for which ξn > 1 lie on thewrongsideofthedecisionboundaryandaremisclassified, asillustratedin Fig- ure7.3.
Thisissometimesdescribedasrelaxingthehardmarginconstrainttogivea softmarginandallowssomeofthetrainingsetdatapointstobemisclassified.
Note thatwhileslackvariablesallowforoverlappingclassdistributions, thisframeworkis still sensitive to outliers because the penalty for misclassification increases linearly withξ.
Our goal is now to maximize the margin while softly penalizing points that lie onthewrongsideofthemarginboundary.
Wethereforeminimize N 1 C ξn+ w 2 (7.21) 2 n=1 wheretheparameter C >0controlsthetrade-offbetweentheslackvariablepenalty and the margin.
Because any point that is misclassified has ξn > 1, it follows that n ξnisanupperboundonthenumberofmisclassifiedpoints.
Theparameter C is thereforeanalogousto(theinverseof)aregularizationcoefficientbecauseitcontrols thetrade-offbetweenminimizingtrainingerrorsandcontrollingmodelcomplexity.
Inthelimit C →∞, wewillrecovertheearliersupportvectormachineforseparable data.
Wenowwishtominimize(7.21)subjecttotheconstraints(7.20)togetherwith ξn 0.
Thecorresponding Lagrangianisgivenby N N N 1 L(w, b, a)= w 2+C ξn − an {tny(xn)−1+ξn }− µnξn (7.22) 2 n=1 n=1 n=1 7.1.
Maximum Margin Classifiers 333 where{an 0}and{µn 0}are Lagrangemultipliers.
Thecorrespondingsetof Appendix E KKTconditionsaregivenby an 0 (7.23) tny(xn)−1+ξn 0 (7.24) an(tny(xn)−1+ξn) = 0 (7.25) µn 0 (7.26) ξn 0 (7.27) µnξn = 0 (7.28) wheren=1,..., N.
Wenowoptimizeoutw, b, and{ξn }makinguseofthedefinition(7.1)ofy(x) togive N ∂L =0 ⇒ w = antn φ(xn) (7.29) ∂w n=1 N ∂L =0 ⇒ antn =0 (7.30) ∂b n=1 ∂L =0 ⇒ an =C −µn.
(7.31) ∂ξn Usingtheseresultstoeliminatew, b, and{ξn }fromthe Lagrangian, weobtainthe dual Lagrangianintheform N N N L (a)= an − 1 anamtntmk(xn, xm) (7.32) 2 n=1 n=1m=1 which is identical to the separable case, except that the constraints are somewhat different.
Toseewhattheseconstraintsare, wenotethatan 0isrequiredbecause these are Lagrange multipliers.
Furthermore, (7.31) together with µn 0 implies an C.
We therefore have to minimize (7.32) with respect to the dual variables {an }subjectto 0 an C (7.33) N antn =0 (7.34) n=1 a quadratic programming problem.
If we substitute (7.29) into (7.1), we see that predictionsfornewdatapointsareagainmadebyusing(7.13).
We can now interpret the resulting solution.
As before, a subset of the data points may have an = 0, in which case they do not contribute to the predictive 334 7.
SPARSEKERNELMACHINES model(7.13).
Theremainingdatapointsconstitutethesupportvectors.
Thesehave an >0andhencefrom(7.25)mustsatisfy tny(xn)=1−ξn.
(7.35) Ifan < C, then(7.31)impliesthatµn > 0, whichfrom(7.28)requiresξn = 0and hence such points lie on the margin.
Points withan = C can lie inside the margin andcaneitherbecorrectlyclassifiedifξn 1ormisclassifiedifξn >1.
To determine the parameter b in (7.1), we note that those support vectors for which0<an <C haveξn =0sothattny(xn)=1andhencewillsatisfy tn amtmk(xn, xm)+b =1.
(7.36) m∈S Again, anumericallystablesolutionisobtainedbyaveragingtogive 1 b= tn − amtmk(xn, xm) (7.37) NM n∈M m∈S where Mdenotesthesetofindicesofdatapointshaving0<an <C.
Analternative, equivalentformulationofthesupportvectormachine, knownas theν-SVM, hasbeenproposedby Scho¨lkopfetal.
(2000).
Thisinvolvesmaximizing N N L (a)=− 1 anamtntmk(xn, xm) (7.38) 2 n=1m=1 subjecttotheconstraints 0 an 1/N (7.39) N antn =0 (7.40) n=1 N an ν.
(7.41) n=1 This approach has the advantage that the parameter ν, which replaces C, can be interpretedasbothanupperboundonthefractionofmarginerrors(pointsforwhich ξn >0andhencewhichlieonthewrongsideofthemarginboundaryandwhichmay ormaynotbemisclassified)andalowerboundonthefractionofsupportvectors.
An exampleof the ν-SVMapplied toasynthetic data setisshown in Figure 7.4.
Here Gaussiankernelsoftheformexp(−γ x−x 2)havebeenused, withγ =0.45.
Although predictions for new inputs are made using only the support vectors, the training phase (i.
e., the determination of the parameters a and b) makes use of the whole data set, and so it is important to have efficient algorithms for solving 7.1.
Maximum Margin Classifiers 335 Figure7.4 Illustration of the ν-SVM applied toanonseparabledatasetintwo dimensions.
The support vectors 2 areindicatedbycircles.
0 −2 −2 0 2 the quadratic programming problem.
We first note that the objective function L(a) givenby(7.10)or(7.32)isquadraticandsoanylocaloptimumwillalsobeaglobal optimumprovidedtheconstraintsdefineaconvexregion(whichtheydoasaconse- quence of being linear).
Direct solution of the quadratic programming problem us- ingtraditionaltechniquesisofteninfeasibleduetothedemandingcomputationand memoryrequirements, andsomorepracticalapproachesneedtobefound.
Thetech- nique of chunking (Vapnik, 1982) exploits the fact that the value of the Lagrangian isunchangedifweremovetherowsandcolumnsofthekernelmatrixcorresponding to Lagrange multipliers that have value zero.
This allows the full quadratic pro- gramming problem to be broken down into a series of smaller ones, whose goal is eventuallytoidentifyallofthenonzero Lagrangemultipliersanddiscardtheothers.
Chunkingcanbeimplementedusingprotectedconjugategradients(Burges,1998).
Althoughchunkingreducesthesizeofthematrixinthequadraticfunctionfromthe number of data points squared to approximately the number of nonzero Lagrange multiplierssquared, eventhismaybetoobigtofitinmemoryforlarge-scaleappli- cations.
Decomposition methods (Osuna et al., 1996) also solve a series of smaller quadraticprogrammingproblemsbutaredesignedsothateachoftheseisofafixed size, and so the technique can be applied to arbitrarily large data sets.
However, it still involves numerical solution of quadratic programming subproblems and these can be problematic and expensive.
One of the most popular approaches to training support vector machines is called sequential minimal optimization, or SMO (Platt, 1999).
It takes the concept of chunking to the extreme limit and considers just two Lagrange multipliers at a time.
In this case, the subproblem can be solved analyti- cally, therebyavoidingnumericalquadraticprogrammingaltogether.
Heuristicsare given for choosing the pair of Lagrange multipliers to be considered at each step.
In practice, SMO is found to have a scaling with the number of data points that is somewherebetweenlinearandquadraticdependingontheparticularapplication.
Wehaveseenthatkernelfunctionscorrespondtoinnerproductsinfeaturespaces thatcanhavehigh, oreveninfinite, dimensionality.
Byworkingdirectlyintermsof the kernel function, without introducing the feature space explicitly, it might there- fore seem that support vector machines somehow manage to avoid the curse of di- 336 7.
SPARSEKERNELMACHINES Section1.4 mensionality.
This is not the case, however, because there are constraints amongst the feature values that restrict the effective dimensionality of feature space.
To see thisconsiderasimplesecond-order polynomialkernelthatwecanexpandinterms ofitscomponents k(x, z) = 1+x Tz 2 =(1+x z +x z )2 1 1 2 2 = 1+2x z +2x z +x2z2+2x z x z +x2z2 √ 1 1√ 2 2 √ 1 1 1 1 √2 2 √2 2 √ = (1, 2x , 2x , x2, 2x x , x2)(1, 2z , 2z , z2, 2z z , z2)T 1 2 1 1 2 2 1 2 1 1 2 2 = φ(x)Tφ(z).
(7.42) Thiskernelfunctionthereforerepresentsaninnerproductinafeaturespacehaving sixdimensions, inwhichthemappingfrominputspacetofeaturespaceisdescribed by the vector function φ(x).
However, the coefficients weighting these different featuresareconstrainedtohavespecificforms.
Thusanysetofpointsintheoriginal two-dimensional spacex would be constrained to lie exactly on a two-dimensional nonlinearmanifoldembeddedinthesix-dimensionalfeaturespace.
We have already highlighted the fact that the support vector machine does not provide probabilistic outputs but instead makes classification decisions for new in- put vectors.
Veropoulos et al.
(1999) discuss modifications to the SVM to allow thetrade-offbetweenfalsepositiveandfalsenegativeerrorstobecontrolled.
How- ever, if we wish to use the SVM as a module in a larger probabilistic system, then probabilisticpredictionsoftheclasslabeltfornewinputsxarerequired.
Toaddressthisissue, Platt(2000)hasproposedfittingalogisticsigmoidtothe outputs of a previously trained support vector machine.
Specifically, the required conditionalprobabilityisassumedtobeoftheform p(t=1|x)=σ(Ay(x)+B) (7.43) where y(x) is defined by (7.1).
Values for the parameters A and B are found by minimizing the cross-entropy error function defined by a training set consisting of pairsofvaluesy(xn)andtn.
Thedatausedtofitthesigmoidneedstobeindependent ofthatusedtotraintheoriginal SVMinordertoavoidsevereover-fitting.
Thistwo- stage approach is equivalent to assuming that the outputy(x) of the support vector machine represents the log-odds of x belonging to class t = 1.
Because the SVM training procedure is not specifically intended to encourage this, the SVM can give apoorapproximationtotheposteriorprobabilities(Tipping,2001).
7.1.2 Relation to logistic regression As with the separable case, we can re-cast the SVM for nonseparable distri- butions in terms of the minimization of a regularized error function.
This will also allowustohighlightsimilarities, anddifferences, comparedtothelogisticregression Section4.3.2 model.
We have seen that for data points that are on the correct side of the margin boundary, and which therefore satisfy yntn 1, we have ξn = 0, and for the 7.1.
Maximum Margin Classifiers 337 Figure7.5 Plotofthe‘hinge’errorfunctionused E(z) in support vector machines, shown in blue, along with the error function for logistic regression, rescaled by a factor of 1/ln(2) so that it passes throughthepoint(0,1), showninred.
Also shown are the misclassification error in black and the squared error ingreen.
z −2 −1 0 1 2 remainingpointswehaveξn =1−yntn.
Thustheobjectivefunction(7.21)canbe written(uptoanoverallmultiplicativeconstant)intheform N E SV (yntn)+λ w 2 (7.44) n=1 whereλ=(2C)−1, and E SV (·)isthehingeerrorfunctiondefinedby E SV (yntn)=[1−yntn] + (7.45) where [·] + denotes the positive part.
The hinge error function, so-called because of its shape, is plotted in Figure 7.5.
It can be viewed as an approximation to the misclassificationerror, i.
e., theerrorfunctionthatideallywewouldliketominimize, whichisalsoshownin Figure7.5.
Whenweconsideredthelogisticregressionmodelin Section4.3.2, wefoundit convenienttoworkwithtargetvariablet∈{0,1}.
Forcomparisonwiththesupport vector machine, we first reformulate maximum likelihood logistic regression using the target variable t ∈ {−1,1}.
To do this, we note that p(t = 1|y) = σ(y) where y(x)isgivenby(7.1), andσ(y)isthelogisticsigmoidfunctiondefinedby(4.59).
It followsthatp(t = −1|y) = 1−σ(y) = σ(−y), wherewehaveusedtheproperties ofthelogisticsigmoidfunction, andsowecanwrite p(t|y)=σ(yt).
(7.46) Fromthiswecanconstructanerrorfunctionbytakingthenegativelogarithmofthe Exercise 7.6 likelihoodfunctionthat, withaquadraticregularizer, takestheform N E LR (yntn)+λ w 2.
(7.47) n=1 where E LR (yt)=ln(1+exp(−yt)).
(7.48) 338 7.
SPARSEKERNELMACHINES For comparison with other error functions, we can divide byln(2) so that the error functionpassesthroughthepoint(0,1).
Thisrescalederrorfunctionisalsoplotted in Figure7.5andweseethatithasasimilarformtothesupportvectorerrorfunction.
Thekeydifferenceisthattheflatregionin E SV (yt)leadstosparsesolutions.
Both the logistic error and the hinge loss can be viewed as continuous approx- imations to the misclassification error.
Another continuous error function that has sometimes been used to solve classification problems is the squared error, which is again plotted in Figure 7.5.
It has the property, however, of placing increasing emphasis on data points that are correctly classified but that are a long way from thedecisionboundaryonthecorrectside.
Suchpointswillbestronglyweightedat the expense of misclassified points, and so if the objective is to minimize the mis- classificationrate, thenamonotonicallydecreasingerrorfunctionwouldbeabetter choice.
7.1.3 Multiclass SVMs Thesupportvectormachineisfundamentallyatwo-classclassifier.
Inpractice, however, weoftenhavetotackleproblemsinvolving K > 2classes.
Variousmeth- ods have therefore been proposed for combining multiple two-class SVMs in order tobuildamulticlassclassifier.
Onecommonlyusedapproach(Vapnik,1998)istoconstruct K separate SVMs, inwhichthekth modelyk(x)istrainedusingthedatafromclass C k asthepositive examplesandthedatafromtheremaining K −1classesasthenegativeexamples.
This is known as the one-versus-the-rest approach.
However, in Figure 4.2 we saw that using the decisions of the individual classifiers can lead to inconsistent results in which an input is assigned to multiple classes simultaneously.
This problem is sometimesaddressedbymakingpredictionsfornewinputsxusing y(x)=maxyk(x).
(7.49) k Unfortunately, this heuristic approach suffers from the problem that the different classifiers were trained on different tasks, and there is no guarantee that the real- valuedquantitiesyk(x)fordifferentclassifierswillhaveappropriatescales.
Another problem with the one-versus-the-rest approach is that the training sets are imbalanced.
For instance, if we have ten classes each with equal numbers of trainingdatapoints, thentheindividualclassifiersaretrainedondatasetscomprising 90% negative examples and only 10% positive examples, and the symmetry of the original problem is lost.
A variant of the one-versus-the-rest scheme was proposed by Leeetal.
(2001)whomodifythetargetvaluessothatthepositiveclasshastarget +1andthenegativeclasshastarget−1/(K −1).
Weston and Watkins (1999) define a single objective function for training all K SVMssimultaneously, basedonmaximizingthemarginfromeachtoremaining classes.
However, thiscanresultinmuchslowertrainingbecause, insteadofsolving K separate optimization problems each over N data points with an overall cost of O(KN2), asingleoptimizationproblemofsize(K−1)N mustbesolvedgivingan overallcostof O(K2N2).
7.1.
Maximum Margin Classifiers 339 Anotherapproachistotrain K(K−1)/2different2-class SVMsonallpossible pairsofclasses, andthentoclassifytestpointsaccordingtowhichclasshasthehigh- estnumberof‘votes’, anapproachthatissometimescalledone-versus-one.
Again, wesawin Figure4.2thatthiscanleadtoambiguitiesintheresultingclassification.
Also, for large K this approach requires significantly more training time than the one-versus-the-rest approach.
Similarly, to evaluate test points, significantly more computationisrequired.
The latter problem can be alleviated by organizing the pairwise classifiers into a directed acyclic graph (not to be confused with a probabilistic graphical model) leadingtothe DAGSVM(Plattetal.,2000).
For K classes, the DAGSVMhasatotal of K(K − 1)/2 classifiers, and to classify a new test point only K − 1 pairwise classifiers need to be evaluated, with the particular classifiers used depending on whichpaththroughthegraphistraversed.
Adifferentapproachtomulticlassclassification, basedonerror-correctingout- put codes, was developed by Dietterich and Bakiri (1995) and applied to support vectormachinesby Allweinetal.
(2000).
Thiscanbeviewedasageneralizationof thevotingschemeoftheone-versus-oneapproachinwhichmoregeneralpartitions of the classes are used to train the individual classifiers.
The K classes themselves arerepresentedasparticularsetsofresponsesfromthetwo-classclassifierschosen, andtogetherwithasuitabledecodingscheme, thisgivesrobustnesstoerrorsandto ambiguity in the outputs of the individual classifiers.
Although the application of SVMs to multiclass classification problems remains an open issue, in practice the one-versus-the-restapproachisthemostwidelyusedinspiteofitsad-hocformula- tionanditspracticallimitations.
There are also single-class support vector machines, which solve an unsuper- vised learning problem related to probability density estimation.
Instead of mod- elling the density of data, however, these methods aim to find a smooth boundary enclosingaregionofhighdensity.
Theboundaryischosentorepresentaquantileof thedensity, thatis, theprobabilitythatadatapointdrawnfromthedistributionwill landinsidethatregionisgivenbyafixednumberbetween0and1thatisspecifiedin advance.
Thisisamorerestrictedproblemthanestimatingthefulldensitybutmay besufficientinspecificapplications.
Twoapproachestothisproblemusingsupport vectormachineshavebeenproposed.
Thealgorithmof Scho¨lkopfetal.
(2001)tries tofindahyperplanethatseparatesallbutafixedfractionν ofthetrainingdatafrom theoriginwhileatthesametimemaximizingthedistance(margin)ofthehyperplane from the origin, while Tax and Duin (1999) look for the smallest sphere in feature space that contains all but a fraction ν of the data points.
For kernels k(x, x ) that arefunctionsonlyofx−x , thetwoalgorithmsareequivalent.
7.1.4 SVMs for regression We now extend support vector machines to regression problems while at the Section3.1.4 same time preserving the property of sparseness.
In simple linear regression, we 340 7.
SPARSEKERNELMACHINES Figure7.6 Plotofan -insensitiveerrorfunction(in E(z) red) in which the error increases lin- early with distance beyond the insen- sitive region.
Also shown for compar- ison is the quadratic error function (in green).
− 0 z minimizearegularizederrorfunctiongivenby N 1 λ {yn −tn }2 + w 2.
(7.50) 2 2 n=1 Toobtainsparsesolutions, thequadraticerrorfunctionisreplacedbyan -insensitive error function (Vapnik, 1995), which gives zero error if the absolute difference be- tween the prediction y(x) and the target t is less than where > 0.
A simple exampleofan -insensitiveerrorfunction, havingalinearcostassociatedwitherrors outsidetheinsensitiveregion, isgivenby 0, if|y(x)−t|< ; E (y(x)−t)= |y(x)−t|− , otherwise (7.51) andisillustratedin Figure7.6.
Wethereforeminimizearegularizederrorfunctiongivenby N 1 C E (y(xn)−tn)+ w 2 (7.52) 2 n=1 wherey(x)isgivenby(7.1).
Byconventionthe(inverse)regularizationparameter, denoted C, appearsinfrontoftheerrorterm.
As before, we can re-express the optimization problem by introducing slack variables.
For each data point xn, we now need two slack variables ξn 0 and ξn 0, whereξn >0correspondstoapointforwhichtn >y(xn)+ , andξn >0 correspondstoapointforwhichtn <y(xn)− , asillustratedin Figure7.7.
The condition for a target point to lie inside the -tube is that yn − tn yn+ , whereyn =y(xn).
Introducingtheslackvariablesallowspointstolieoutside thetubeprovidedtheslackvariablesarenonzero, andthecorrespondingconditions are tn y(xn)+ +ξn (7.53) tn y(xn)− −ξn.
(7.54) 7.1.
Maximum Margin Classifiers 341 Figure7.7 Illustration of SVM regression, showing the regression curve together with the - y(x) y+ insensitive ‘tube’.
Also shown are exam- ξ >0 y ples of the slack variables ξ and bξ.
Points above the -tube have ξ > 0 and bξ = 0, y− points below the -tube have ξ = 0 and bξ > 0, and points inside the -tube have ξ=bξ=0.
ξ >0 x Theerrorfunctionforsupportvectorregressioncanthenbewrittenas N C (ξn+ ξn)+ 1 w 2 (7.55) 2 n=1 which must be minimized subject to the constraints ξn 0 and ξn 0 as well as (7.53)and(7.54).
Thiscanbeachievedbyintroducing Lagrangemultipliersan 0, an 0,µn 0, andµ n 0andoptimizingthe Lagrangian N N L = C (ξn+ ξn)+ 1 w 2− (µnξn+µ n ξn) 2 n=1 n=1 N N − an( +ξn+yn −tn)− an( + ξn −yn+tn).
(7.56) n=1 n=1 We now substitute for y(x) using (7.1) and then set the derivatives of the La- grangianwithrespecttow, b,ξn, andξn tozero, giving N ∂L =0 ⇒ w = (an − an)φ(xn) (7.57) ∂w n=1 N ∂L =0 ⇒ (an − an)=0 (7.58) ∂b n=1 ∂L =0 ⇒ an+µn =C (7.59) ∂ξn ∂L =0 ⇒ an+µ n =C.
(7.60) ∂ξn Usingtheseresultstoeliminatethecorrespondingvariablesfromthe Lagrangian, we Exercise 7.7 seethatthedualprobleminvolvesmaximizing 342 7.
SPARSEKERNELMACHINES N N L (a, a) = − 1 (an − an)(am − am)k(xn, xm) 2 n=1m=1 N N − (an+ an)+ (an − an)tn (7.61) n=1 n=1 with respect to {an } and { an }, where we have introduced the kernel k(x, x ) = φ(x)Tφ(x ).
Again, thisisaconstrained maximization, and tofindthe constraints we note that an 0 and an 0 are both required because these are Lagrange multipliers.
Also µn 0 and µ n 0 together with (7.59) and (7.60), require an C and an C, andsoagainwehavetheboxconstraints 0 an C (7.62) 0 an C (7.63) togetherwiththecondition(7.58).
Substituting(7.57)into(7.1), weseethatpredictionsfornewinputscanbemade using N y(x)= (an − an)k(x, xn)+b (7.64) n=1 whichisagainexpressedintermsofthekernelfunction.
The corresponding Karush-Kuhn-Tucker (KKT) conditions, which state that at the solution the product of the dual variables and the constraints must vanish, are givenby an( +ξn+yn −tn) = 0 (7.65) an( + ξn −yn+tn) = 0 (7.66) (C −an)ξn = 0 (7.67) (C − an) ξn = 0.
(7.68) Fromthesewecanobtainseveralusefulresults.
Firstofall, wenotethatacoefficient an canonlybenonzeroif +ξn +yn −tn = 0, whichimpliesthatthedatapoint either lies on the upper boundary of the -tube (ξn = 0) or lies above the upper boundary(ξn >0).
Similarly, anonzerovaluefor an implies + ξn −yn+tn =0, andsuchpointsmustlieeitheronorbelowthelowerboundaryofthe -tube.
Furthermore, thetwoconstraints +ξn+yn −tn =0and +ξn −yn+tn =0 are incompatible, as is easily seen by adding them together and noting that ξn and ξn arenonnegativewhile isstrictlypositive, andsoforeverydatapointxn, either an or an (orboth)mustbezero.
Thesupportvectorsarethosedatapointsthatcontributetopredictionsgivenby (7.64), inotherwordsthoseforwhicheitheran =0or an =0.
Thesearepointsthat lieontheboundaryofthe -tubeoroutsidethetube.
Allpointswithinthetubehave 7.1.
Maximum Margin Classifiers 343 an = an = 0.
We again have a sparse solution, and the only terms that have to be evaluatedinthepredictivemodel(7.64)arethosethatinvolvethesupportvectors.
Theparameterbcanbefoundbyconsideringadatapointforwhich0 < an < C, which from (7.67) must have ξn = 0, and from (7.65) must therefore satisfy +yn −tn =0.
Using(7.1)andsolvingforb, weobtain b = tn − −w Tφ(xn) N = tn − − (am − am)k(xn, xm) (7.69) m=1 wherewehaveused(7.57).
Wecanobtainananalogousresultbyconsideringapoint forwhich0 < an < C.
Inpractice, itisbettertoaverageoverallsuchestimatesof b.
As with the classification case, there is an alternative formulation of the SVM for regression in which the parameter governing complexity has a more intuitive interpretation(Scho¨lkopfetal.,2000).
Inparticular, insteadoffixingthewidth of theinsensitiveregion, wefixinsteadaparameterν thatboundsthefractionofpoints lyingoutsidethetube.
Thisinvolvesmaximizing N N L (a, a) = − 1 (an − an)(am − am)k(xn, xm) 2 n=1m=1 N + (an − an)tn (7.70) n=1 subjecttotheconstraints 0 an C/N (7.71) 0 an C/N (7.72) N (an − an)=0 (7.73) n=1 N (an+ an) νC.
(7.74) n=1 ItcanbeshownthatthereareatmostνN datapointsfallingoutsidetheinsensitive tube, whileatleastνN datapointsaresupportvectorsandsolieeitheronthetube oroutsideit.
Theuseofasupportvectormachinetosolvearegressionproblemisillustrated Appendix A usingthesinusoidaldatasetin Figure7.8.
Heretheparametersν and C havebeen chosen by hand.
In practice, their values would typically be determined by cross- validation.
344 7.
SPARSEKERNELMACHINES Figure7.8 Illustration of the ν-SVM for re- gressionappliedtothesinusoidal syntheticdatasetusing Gaussian 1 kernels.
Thepredictedregression curveisshownbytheredline, and t the -insensitivetubecorresponds to the shaded region.
Also, the data points are shown in green, 0 and those with support vectors areindicatedbybluecircles.
−1 0 1 x 7.1.5 Computational learning theory Historically, supportvectormachineshavelargelybeenmotivatedandanalysed usingatheoreticalframeworkknownascomputationallearningtheory, alsosome- timescalledstatisticallearningtheory(Anthonyand Biggs,1992; Kearnsand Vazi- rani, 1994; Vapnik, 1995; Vapnik, 1998).
This has its origins with Valiant (1984) who formulated the probably approximately correct, or PAC, learning framework.
Thegoalofthe PACframeworkistounderstandhowlargeadatasetneedstobein ordertogivegoodgeneralization.
Italsogivesboundsforthecomputationalcostof learning, althoughwedonotconsiderthesehere.
Supposethatadataset Dofsize N isdrawnfromsomejointdistributionp(x, t) where x is the input variable and t represents the class label, and that we restrict attentionto‘noisefree’situationsinwhichtheclasslabelsaredeterminedbysome (unknown)deterministicfunctiont = g(x).
In PAClearningwesaythatafunction f(x; D), drawn from a space F of such functions on the basis of the training set D, has good generalization if its expected error rate is below some pre-specified threshold , sothat E x, t [I(f(x; D) =t)]< (7.75) where I(·) is the indicator function, and the expectation is with respect to the dis- tribution p(x, t).
The quantity on the left-hand side is a random variable, because itdependsonthetrainingset D, andthe PACframeworkrequiresthat(7.75)holds, with probability greater than 1−δ, for a data set D drawn randomly from p(x, t).
Here δ is another pre-specified parameter, and the terminology ‘probably approxi- matelycorrect’comesfromtherequirementthatwithhighprobability(greaterthan 1−δ), theerrorratebesmall(lessthan ).
Foragivenchoiceofmodelspace F, and forgivenparameters andδ, PAClearningaimstoprovideboundsontheminimum size N of data set needed to meet this criterion.
A key quantity in PAC learning is the Vapnik-Chervonenkisdimension, or VCdimension, whichprovidesameasureof thecomplexityofaspaceoffunctions, andwhichallowsthe PACframeworktobe extendedtospacescontaininganinfinitenumberoffunctions.
The bounds derived within the PAC framework are often described as worst- 7.2.
Relevance Vector Machines 345 case, because they apply to any choice for the distribution p(x, t), so long as both thetrainingandthetestexamplesaredrawn(independently)fromthesamedistribu- tion, andforanychoiceforthefunctionf(x)solongasitbelongsto F.
Inreal-world applicationsofmachinelearning, wedealwithdistributionsthathavesignificantreg- ularity, forexampleinwhichlargeregionsofinputspacecarrythesameclasslabel.
Asaconsequenceofthelackofanyassumptionsabouttheformofthedistribution, the PAC bounds are very conservative, in other words they strongly over-estimate thesizeofdatasetsrequiredtoachieveagivengeneralizationperformance.
Forthis reason, PACboundshavefoundfew, ifany, practicalapplications.
One attempt to improve the tightness of the PAC bounds is the PAC-Bayesian framework (Mc Allester, 2003), which considers a distribution over the space F of functions, somewhat analogous tothe prior in a Bayesian treatment.
This stillcon- siders any possible choice for p(x, t), and so although the bounds are tighter, they arestillveryconservative.
7.2.
Relevance Vector Machines Support vector machines have been used in a variety of classification and regres- sion applications.
Nevertheless, they suffer from a number of limitations, several ofwhichhavebeenhighlightedalreadyinthischapter.
Inparticular, theoutputsof an SVMrepresentdecisionsratherthanposteriorprobabilities.
Also, the SVMwas originally formulated for two classes, and the extension to K > 2 classes is prob- lematic.
Thereisacomplexityparameter C, orν(aswellasaparameter inthecase ofregression), thatmustbefoundusingahold-outmethodsuchascross-validation.
Finally, predictionsareexpressedaslinearcombinationsofkernelfunctionsthatare centredontrainingdatapointsandthatarerequiredtobepositivedefinite.
Therelevancevectormachineor RVM(Tipping,2001)isa Bayesiansparseker- neltechniqueforregressionandclassificationthatsharesmanyofthecharacteristics ofthe SVMwhilstavoidingitsprincipallimitations.
Additionally, ittypicallyleads tomuchsparsermodelsresultingincorrespondinglyfasterperformanceontestdata whilstmaintainingcomparablegeneralizationerror.
Incontrasttothe SVMweshallfinditmoreconvenienttointroducetheregres- sionformofthe RVMfirstandthenconsidertheextensiontoclassificationtasks.
7.2.1 RVM for regression Therelevancevectormachineforregressionisalinearmodeloftheformstudied in Chapter 3 but with a modified prior that results in sparse solutions.
The model defines a conditional distribution for a real-valued target variable t, given an input vectorx, whichtakestheform p(t|x, w,β)=N(t|y(x),β −1) (7.76) 346 7.
SPARSEKERNELMACHINES whereβ =σ−2isthenoiseprecision(inversenoisevariance), andthemeanisgiven byalinearmodeloftheform M y(x)= wiφi(x)=w Tφ(x) (7.77) i=1 with fixed nonlinear basis functions φi(x), which will typically include a constant termsothatthecorrespondingweightparameterrepresentsa‘bias’.
The relevance vector machine is a specific instance of this model, which is in- tendedtomirrorthestructureofthesupportvectormachine.
Inparticular, thebasis functions are given by kernels, with one kernel associated with each of the data pointsfromthetrainingset.
Thegeneralexpression(7.77)thentakesthe SVM-like form N y(x)= wnk(x, xn)+b (7.78) n=1 wherebisabiasparameter.
Thenumberofparametersinthiscaseis M = N +1, andy(x)hasthesameformasthepredictivemodel(7.64)forthe SVM, exceptthat thecoefficientsanareheredenotedwn.
Itshouldbeemphasizedthatthesubsequent analysis is valid for arbitrary choices of basis function, and for generality we shall workwiththeform(7.77).
Incontrasttothe SVM, thereisnorestrictiontopositive- definite kernels, nor are the basis functions tied in either number or location to the trainingdatapoints.
Suppose we are given a set of N observations of the input vector x, which we denotecollectivelybyadatamatrix Xwhosenthrowisx Twithn=1,..., N.
The n corresponding target values are given by t = (t 1 ,..., t N)T.
Thus, the likelihood functionisgivenby N p(t|X, w,β)= p(tn |xn, w,β −1).
(7.79) n=1 Next we introduce a prior distribution over the parameter vector w and as in Chapter 3, we shall consider a zero-mean Gaussian prior.
However, the key differ- ence in the RVM is that we introduce a separate hyperparameter αi for each of the weight parameters wi instead of a single shared hyperparameter.
Thus the weight priortakestheform M p(w|α)= N(wi |0,α i −1) (7.80) i=1 whereαirepresentstheprecisionofthecorrespondingparameterwi, andαdenotes (α 1 ,...,αM)T.
We shall see that, when we maximize the evidence with respect to these hyperparameters, a significant proportion of them go to infinity, and the corresponding weight parameters have posterior distributions that are concentrated atzero.
Thebasisfunctionsassociatedwiththeseparametersthereforeplaynorole 7.2.
Relevance Vector Machines 347 inthepredictionsmadebythemodelandsoareeffectivelyprunedout, resultingin asparsemodel.
Using the result (3.49) for linear regression models, we see that the posterior distributionfortheweightsisagain Gaussianandtakestheform p(w|t, X,α,β)=N(w|m,Σ) (7.81) wherethemeanandcovariancearegivenby m = βΣΦT t (7.82) Σ = A+βΦTΦ −1 (7.83) where Φ is the N × M design matrix with elements Φni = φi(xn), and A = diag(αi).
Notethatinthespecificcaseofthemodel(7.78), wehaveΦ=K, where Kisthesymmetric(N +1)×(N +1)kernelmatrixwithelementsk(xn, xm).
The values of α and β are determined using type-2 maximum likelihood, also Section3.5 known as the evidence approximation, in which we maximize the marginal likeli- hoodfunctionobtainedbyintegratingouttheweightparameters p(t|X,α,β)= p(t|X, w,β)p(w|α)dw.
(7.84) Exercise 7.10 Because this represents the convolution of two Gaussians, it is readily evaluated to givethelogmarginallikelihoodintheform lnp(t|X,α,β) = ln N(t|0, C) 1 = − Nln(2π)+ln|C|+t TC −1t (7.85) 2 wheret=(t 1 ,..., t N)T, andwehavedefinedthe N ×N matrix Cgivenby C=β −1I+ΦA −1ΦT.
(7.86) Ourgoalisnowtomaximize(7.85)withrespecttothehyperparametersαand β.
Thisrequiresonlyasmallmodificationtotheresultsobtainedin Section3.5for the evidence approximation in the linear regression model.
Again, we can identify two approaches.
In the first, we simply set the required derivatives of the marginal Exercise 7.12 likelihoodtozeroandobtainthefollowingre-estimationequations αnew = γi (7.87) i m2 i t−Φm 2 (βnew) −1 = (7.88) N − i γi where mi is the ith component of the posterior mean m defined by (7.82).
The quantityγi measureshowwellthecorrespondingparameterwi isdeterminedbythe Section3.5.3 dataandisdefinedby 348 7.
SPARSEKERNELMACHINES γi =1−αiΣii (7.89) in which Σii is the ith diagonal component of the posterior covariance Σ given by (7.83).
Learning therefore proceeds by choosing initial values for α and β, evalu- atingthemeanandcovarianceoftheposteriorusing(7.82)and(7.83), respectively, andthenalternatelyre-estimatingthehyperparameters, using(7.87)and(7.88), and re-estimatingtheposteriormeanandcovariance, using(7.82)and(7.83), untilasuit- ableconvergencecriterionissatisfied.
The second approach is to use the EM algorithm, and is discussed in Sec- tion 9.3.4.
These two approaches to finding the values of the hyperparameters that Exercise 9.23 maximize the evidence are formally equivalent.
Numerically, however, it is found thatthedirectoptimizationapproachcorrespondingto(7.87)and(7.88)givessome- whatfasterconvergence(Tipping,2001).
Asaresultoftheoptimization, wefindthataproportionofthehyperparameters Section7.2.2 {αi } are driven to large (in principle infinite) values, and so the weight parameters wi corresponding to these hyperparameters have posterior distributions with mean and variance both zero.
Thus those parameters, and the corresponding basis func- tionsφi(x), areremovedfromthemodelandplaynoroleinmakingpredictionsfor newinputs.
Inthecaseofmodelsoftheform(7.78), theinputsxn correspondingto the remaining nonzero weights are called relevance vectors, because they are iden- tified through the mechanism of automatic relevance determination, and are analo- gous to the support vectors of an SVM.
It is worth emphasizing, however, that this mechanism for achieving sparsity in probabilistic models through automatic rele- vance determination is quite general and can be applied to any model expressed as anadaptivelinearcombinationofbasisfunctions.
Having found values α and β for the hyperparameters that maximize the marginal likelihood, we can evaluate the predictive distribution over t for a new Exercise 7.14 inputx.
Using(7.76)and(7.81), thisisgivenby p(t|x, X, t,α ,β ) = p(t|x, w,β )p(w|X, t,α ,β )dw = N t|m Tφ(x),σ2(x) .
(7.90) Thus the predictive mean is given by (7.76) with w set equal to the posterior mean m, andthevarianceofthepredictivedistributionisgivenby σ2(x)=(β ) −1+φ(x)TΣφ(x) (7.91) whereΣisgivenby(7.83)inwhichαandβaresettotheiroptimizedvaluesα and β .
Thisisjustthefamiliarresult(3.59)obtainedinthecontextoflinearregression.
Recallthatforlocalizedbasisfunctions, thepredictivevarianceforlinearregression modelsbecomessmallinregionsofinputspacewheretherearenobasisfunctions.
Inthecaseofan RVMwiththebasisfunctionscentredondatapoints, themodelwill therefore become increasingly certain of its predictions when extrapolating outside thedomainofthedata(Rasmussenand Quin˜onero-Candela,2005), whichofcourse Section6.4.2 is undesirable.
The predictive distribution in Gaussian process regression does not 7.2.
Relevance Vector Machines 349 Figure7.9 Illustration of RVM regression us- ing the same data set, and the same Gaussian kernel functions, 1 as used in Figure 7.8 for the ν-SVM regression model.
The t mean of the predictive distribu- tion for the RVM is shown by the red line, and the one standard- 0 deviation predictive distribution is shown by the shaded region.
Also, the data points are shown in green, and the relevance vec- −1 tors are indicated by blue circles.
Note that there are only 3 rele- vancevectorscomparedto7sup- 0 1 portvectorsfortheν-SVMin Fig- x ure7.8.
suffer from this problem.
However, the computational cost of making predictions witha Gaussianprocessesistypicallymuchhigherthanwithan RVM.
Figure 7.9 shows an example of the RVM applied to the sinusoidal regression dataset.
Herethenoiseprecisionparameterβ isalsodeterminedthroughevidence maximization.
We see that the number of relevance vectors in the RVM is signif- icantly smaller than the number of support vectors used by the SVM.
For a wide range of regression and classification tasks, the RVM is found to give models that are typically an order of magnitude more compact than the corresponding support vectormachine, resultinginasignificantimprovementinthespeedofprocessingon testdata.
Remarkably, thisgreatersparsityisachievedwithlittleornoreductionin generalizationerrorcomparedwiththecorresponding SVM.
The principal disadvantage of the RVM compared to the SVM is that training involvesoptimizinganonconvexfunction, andtrainingtimescanbelongerthanfora comparable SVM.
Foramodelwith M basisfunctions, the RVMrequiresinversion ofamatrixofsize M ×M, whichingeneralrequires O(M3)computation.
Inthe specificcaseofthe SVM-likemodel(7.78), wehave M =N+1.
Aswehavenoted, there are techniques for training SVMs whose cost is roughly quadratic in N.
Of course, inthecaseofthe RVMwealwayshavetheoptionofstartingwithasmaller number of basis functions than N +1.
More significantly, in the relevance vector machine the parameters governing complexity and noise variance are determined automaticallyfromasingletrainingrun, whereasinthesupportvectormachinethe parameters C and (orν)aregenerallyfoundusingcross-validation, whichinvolves multipletrainingruns.
Furthermore, inthenextsectionweshallderiveanalternative procedure for training the relevance vector machine that improves training speed significantly.
7.2.2 Analysis of sparsity Wehavenotedearlierthatthemechanismofautomaticrelevancedetermination causesasubsetofparameterstobedriventozero.
Wenowexamineinmoredetail 350 7.
SPARSEKERNELMACHINES t2 t2 t ϕ t C C t1 t1 Figure7.10 Illustrationofthemechanismforsparsityina Bayesianlinearregressionmodel, showingatraining set vector of target values given by t = (t 1 , t 2 )T, indicated by the cross, for a model with one basis vector ϕ = (φ(x 1 ),φ(x 2 ))T , which is poorly aligned with the target data vector t.
On the left we see a model having only isotropic noise, so that C = β−1I, corresponding to α = ∞, with β set to its most probable value.
On the right we see the same model but with a finite value of α.
In each case the red ellipse corresponds to unit Mahalanobis distance, with |C| taking the same value for both plots, while the dashed green circle shows the contrition arising from the noise term β−1.
We see that any finite value of α reduces the probability of the observeddata, andsoforthemostprobablesolutionthebasisvectorisremoved.
the mechanism of sparsity in the context of the relevance vector machine.
In the process, we will arrive at a significantly faster procedure for optimizing the hyper- parameterscomparedtothedirecttechniquesgivenabove.
Before proceeding with a mathematical analysis, we first give some informal insight into the origin of sparsity in Bayesian linear models.
Consider a data set comprising N = 2 observations t 1 and t 2, together with a model having a single basisfunctionφ(x), withhyperparameterα, alongwithisotropicnoisehavingpre- cisionβ.
From(7.85), themarginallikelihoodisgivenbyp(t|α,β) =N(t|0, C)in whichthecovariancematrixtakestheform 1 1 C= I+ ϕϕT (7.92) β α where ϕ denotes the N-dimensional vector (φ(x 1 ),φ(x 2 ))T, and similarly t = (t 1 , t 2 )T.
Notice that this is just a zero-mean Gaussian process model over t with covariance C.
Givenaparticularobservationfort, ourgoalistofindα andβ by maximizingthemarginallikelihood.
Weseefrom Figure7.10that, ifthereisapoor alignmentbetweenthedirectionofϕandthatofthetrainingdatavectort, thenthe corresponding hyperparameter α will be driven to ∞, and the basis vector will be prunedfromthemodel.
Thisarisesbecauseanyfinitevalueforαwillalwaysassign alowerprobabilitytothedata, therebydecreasingthevalueofthedensityatt, pro- videdthatβissettoitsoptimalvalue.
Weseethatanyfinitevalueforαwouldcause thedistributiontobeelongatedinadirectionawayfromthedata, therebyincreasing theprobabilitymassinregionsawayfromtheobserveddataandhencereducingthe valueofthedensityatthetargetdatavectoritself.
Forthemoregeneralcaseof M 7.2.
Relevance Vector Machines 351 basisvectorsϕ ,...,ϕ asimilarintuitionholds, namelythatifaparticularbasis 1 M vectorispoorlyalignedwiththedatavectort, thenitislikelytobeprunedfromthe model.
Wenowinvestigatethemechanismforsparsityfromamoremathematicalper- spective, for a general case involving M basis functions.
To motivate this analysis wefirstnotethat, intheresult(7.87)forre-estimatingtheparameterαi, thetermson theright-handsidearethemselvesalsofunctionsofαi.
Theseresultsthereforerep- resentimplicitsolutions, anditerationwouldberequiredeventodetermineasingle αi withallotherαj forj =ifixed.
This suggests a different approach to solving the optimization problem for the RVM, in which we make explicit all of the dependence of the marginal likelihood (7.85)onaparticularαiandthendetermineitsstationarypointsexplicitly(Fauland Tipping,2002; Tippingand Faul,2003).
Todothis, wefirstpulloutthecontribution fromαi inthematrix Cdefinedby(7.86)togive C = β −1I+ α −1ϕ ϕT+α −1ϕ ϕT j j j i i i j= i = C−i+α i −1ϕ i ϕT i (7.93) whereϕ denotestheithcolumnofΦ, inotherwordsthe N-dimensionalvectorwith i elements(φi(x 1 ),...,φi(x N)), incontrasttoφ n , whichdenotesthenth rowofΦ.
Thematrix C−i representsthematrix Cwiththecontributionfrombasisfunctioni removed.
Using the matrix identities (C.7) and (C.15), the determinant and inverse of Ccanthenbewritten |C| = |C−i ||1+α i −1ϕT i C − − 1 i ϕ i | (7.94) C −1ϕ ϕTC −1 C −1 = C −1− −i i i −i .
(7.95) −i αi+ϕT i C − −i 1ϕ i Usingtheseresults, wecanthenwritethelogmarginallikelihoodfunction(7.85)in Exercise 7.15 theform L(α)=L(α −i)+λ(αi) (7.96) where L(α −i)issimplythelogmarginallikelihoodwithbasisfunctionϕ i omitted, andthequantityλ(αi)isdefinedby 1 q2 λ(αi)= lnαi −ln(αi+si)+ i (7.97) 2 αi+si andcontainsallofthedependenceonαi.
Herewehaveintroducedthetwoquantities si = ϕT i C − −i 1ϕ i (7.98) qi = ϕT i C − −i 1 t.
(7.99) Here si is called the sparsity and qi is known as the quality of ϕ i , and as we shall see, a large value of si relative to the value of qi means that the basis function ϕ i 352 7.
SPARSEKERNELMACHINES Figure7.11 Plots of the log 2 2 marginal likelihood λ(α i ) versus lnα i showing on the left, the single maximum at a finite α i for q i 2 = 4 0 0 ands i = 1(sothatq i 2 > s i)andon the right, the maximum at α i = ∞ −2 −2 for q i 2 = 1 and s i = 2 (so that q i 2 <s i).
−4 −4 −5 0 5 −5 0 5 is more likely to be pruned from the model.
The ‘sparsity’ measures the extent to whichbasisfunctionϕ overlapswiththeotherbasisvectorsinthemodel, andthe i ‘quality’representsameasureofthealignmentofthebasisvectorϕ withtheerror n betweenthetrainingsetvaluest = (t 1 ,..., t N)T andthevectory −i ofpredictions that would result from the model with the vector ϕ excluded (Tipping and Faul, i 2003).
The stationary points of the marginal likelihood with respect to αi occur when thederivative dλ(αi) = α i −1s2 i −(q i 2−si) (7.100) dαi 2(αi+si)2 isequaltozero.
Therearetwopossibleformsforthesolution.
Recallingthatαi 0, weseethatifq i 2 <si, thenαi →∞providesasolution.
Conversely, ifq i 2 >si, we cansolveforαi toobtain s2 αi = q i 2− i si .
(7.101) These two solutions are illustrated in Figure 7.11.
We see that the relative size of the quality and sparsity terms determines whether a particular basis vector will be prunedfromthemodelornot.
Amorecompleteanalysis(Fauland Tipping,2002), basedonthesecondderivativesofthemarginallikelihood, confirmsthesesolutions Exercise 7.16 areindeedtheuniquemaximaofλ(αi).
Note that this approach has yielded a closed-form solution for αi, for given valuesoftheotherhyperparameters.
Aswellasprovidinginsightintotheoriginof sparsityinthe RVM, thisanalysisalsoleadstoapracticalalgorithmforoptimizing the hyperparameters that has significant speed advantages.
This uses a fixed set of candidate basis vectors, and then cycles through them in turn to decide whether eachvectorshouldbeincludedinthemodelornot.
Theresultingsequentialsparse Bayesianlearningalgorithmisdescribedbelow.
Sequential Sparse Bayesian Learning Algorithm 1.
Ifsolvingaregressionproblem, initializeβ.
2.
Initialize using one basis function ϕ 1 , with hyperparameter α 1 set using (7.101), with the remaining hyperparameters αj for j = i initialized to infinity, sothatonlyϕ isincludedinthemodel.
1 7.2.
Relevance Vector Machines 353 3.
EvaluateΣandm, alongwithqi andsi forallbasisfunctions.
4.
Selectacandidatebasisfunctionϕ .
i 5.
Ifq i 2 > si, andαi < ∞, sothatthebasisvectorϕ i isalreadyincludedin themodel, thenupdateαi using(7.101).
6.
Ifq i 2 > si, andαi = ∞, thenaddϕ i tothemodel, andevaluatehyperpa- rameterαi using(7.101).
7.
If q i 2 si, and αi < ∞ then remove basis function ϕ i from the model, andsetαi =∞.
8.
Ifsolvingaregressionproblem, updateβ.
9.
Ifconvergedterminate, otherwisegoto3.
Note that if q i 2 si and αi = ∞, then the basis function ϕ i is already excluded fromthemodelandnoactionisrequired.
Inpractice, itisconvenienttoevaluatethequantities Qi = ϕT i C −1t (7.102) Si = ϕT i C −1ϕ i .
(7.103) Thequalityandsparsenessvariablescanthenbeexpressedintheform αi Qi qi = αi −Si (7.104) αi Si si = αi −Si .
(7.105) Exercise 7.17 Notethatwhenαi =∞, wehaveqi =Qi andsi =Si.
Using(C.7), wecanwrite Qi = βϕT i t−β2ϕT i ΦΣΦT t (7.106) Si = βϕT i ϕ i −β2ϕT i ΦΣΦTϕ i (7.107) where Φ and Σ involve only those basis vectors that correspond to finite hyperpa- rameters αi.
At each stage the required computations therefore scale like O(M3), where M is the number of active basis vectors in the model and is typically much smallerthanthenumber N oftrainingpatterns.
7.2.3 RVM for classification We can extend the relevance vector machine framework to classification prob- lemsbyapplyingthe ARDprioroverweightstoaprobabilisticlinearclassification model of the kind studied in Chapter 4.
To start with, we consider two-class prob- lems with a binary target variable t ∈ {0,1}.
The model now takes the form of a linearcombinationofbasisfunctionstransformedbyalogisticsigmoidfunction y(x, w)=σ w Tφ(x) (7.108) 354 7.
SPARSEKERNELMACHINES where σ(·) is the logistic sigmoid function defined by (4.59).
If we introduce a Gaussian prior over the weight vector w, then we obtain the model that has been consideredalreadyin Chapter4.
Thedifferencehereisthatinthe RVM, thismodel uses the ARD prior (7.80) in which there is a separate precision hyperparameter associatedwitheachweightparameter.
Incontrasttotheregressionmodel, wecannolongerintegrateanalyticallyover the parameter vector w.
Here we follow Tipping (2001) and use the Laplace ap- Section4.4 proximation, whichwasappliedtothecloselyrelatedproblemof Bayesianlogistic regressionin Section4.5.1.
We begin by initializing the hyperparameter vector α.
For this given value of α, wethenbuilda Gaussianapproximationtotheposteriordistributionandthereby obtain an approximation to the marginal likelihood.
Maximization of this approxi- matemarginallikelihoodthenleadstoare-estimatedvalueforα, andtheprocessis repeateduntilconvergence.
Let us consider the Laplace approximation for this model in more detail.
For a fixed value of α, the mode of the posterior distribution over w is obtained by maximizing lnp(w|t,α)=ln{p(t|w)p(w|α)}−lnp(t|α) N 1 = {tnlnyn+(1−tn)ln(1−yn)}− w TAw+const (7.109) 2 n=1 where A = diag(αi).
This can be done using iterative reweighted least squares (IRLS) as discussed in Section 4.3.3.
For this, we need the gradient vector and Exercise 7.18 Hessianmatrixofthelogposteriordistribution, whichfrom(7.109)aregivenby ∇lnp(w|t,α) = ΦT(t−y)−Aw (7.110) ∇∇lnp(w|t,α) = − ΦTBΦ+A (7.111) where B is an N ×N diagonal matrix with elementsbn = yn(1−yn), the vector y = (y 1 ,..., y N)T, andΦisthedesignmatrixwithelementsΦni = φi(xn).
Here wehaveusedtheproperty(4.88)forthederivativeofthelogisticsigmoidfunction.
At convergence of the IRLS algorithm, the negative Hessian represents the inverse covariancematrixforthe Gaussianapproximationtotheposteriordistribution.
The mode of the resulting approximation to the posterior distribution, corre- sponding tothemeanof the Gaussianapproximation, isobtained setting (7.110) to zero, givingthemeanandcovarianceofthe Laplaceapproximationintheform w = A −1ΦT(t−y) (7.112) Σ = ΦTBΦ+A −1 .
(7.113) Wecannowusethis Laplaceapproximationtoevaluatethemarginallikelihood.
Usingthegeneralresult(4.135)foranintegralevaluatedusingthe Laplaceapproxi- 7.2.
Relevance Vector Machines 355 mation, wehave p(t|α) = p(t|w)p(w|α)dw p(t|w )p(w |α)(2π) M/2|Σ|1/2.
(7.114) Ifwesubstituteforp(t|w )andp(w |α)andthensetthederivativeofthemarginal Exercise 7.19 likelihoodwithrespecttoαi equaltozero, weobtain 1 1 1 − (w i )2+ − Σii =0.
(7.115) 2 2αi 2 Definingγi =1−αiΣii andrearrangingthengives αnew = γi (7.116) i (w )2 i which is identical to the re-estimation formula (7.87) obtained for the regression RVM.
Ifwedefine t=Φw +B −1(t−y) (7.117) wecanwritetheapproximatelogmarginallikelihoodintheform lnp(t|α,β)=− 1 Nln(2π)+ln|C|+( t)TC −1 t (7.118) 2 where C=B+ΦAΦT.
(7.119) This takes the same form as (7.85) in the regression case, and so we can apply the same analysis of sparsity and obtain the same fast learning algorithm in which we fullyoptimizeasinglehyperparameterαi ateachstep.
Figure 7.12 shows the relevance vector machine applied to a synthetic classifi- Appendix A cationdataset.
Weseethattherelevancevectorstendnottolieintheregionofthe decisionboundary, incontrasttothesupportvectormachine.
Thisisconsistentwith ourearlierdiscussionofsparsityinthe RVM, becauseabasisfunctionφi(x)centred on a data point near the boundary will have a vector ϕ that is poorly aligned with i thetrainingdatavectort.
Oneofthepotentialadvantagesoftherelevancevectormachinecomparedwith the SVMisthatitmakesprobabilisticpredictions.
Forexample, thisallowsthe RVM tobeusedtohelpconstructanemissiondensityinanonlinearextensionofthelinear Section13.3 dynamicalsystemfortrackingfacesinvideosequences(Williamsetal.,2005).
So far, we have considered the RVM for binary classification problems.
For K > 2classes, weagainmakeuseoftheprobabilisticapproachin Section4.3.4in whichthereare K linearmodelsoftheform ak =w k Tx (7.120) 356 7.
SPARSEKERNELMACHINES 2 0 −2 −2 0 2 Figure7.12 Exampleoftherelevancevectormachineappliedtoasyntheticdataset, inwhichtheleft-handplot showsthedecisionboundaryandthedatapoints, withtherelevancevectorsindicatedbycircles.
Comparison withtheresultsshownin Figure7.4forthecorrespondingsupportvectormachineshowsthatthe RVMgivesa muchsparsermodel.
Theright-handplotshowstheposteriorprobabilitygivenbythe RVMoutputinwhichthe proportionofred(blue)inkindicatestheprobabilityofthatpointbelongingtothered(blue)class.
whicharecombinedusingasoftmaxfunctiontogiveoutputs yk(x)= exp(ak) .
(7.121) exp(aj) j Theloglikelihoodfunctionisthengivenby N K lnp(T|w 1 ,..., w K)= y n t n k k (7.122) n=1k=1 where the target valuestnk have a1-of-K coding for each data pointn, and T is a matrixwithelementstnk.
Again, the Laplaceapproximationcanbeusedtooptimize thehyperparameters(Tipping,2001), inwhichthemodelandits Hessianarefound using IRLS.
Thisgivesamoreprincipledapproachtomulticlassclassificationthan thepairwisemethodusedinthesupportvectormachineandalsoprovidesprobabilis- tic predictions for new data points.
The principal disadvantage is that the Hessian matrixhassize MK×MK, where M isthenumberofactivebasisfunctions, which givesanadditionalfactorof K3inthecomputationalcostoftrainingcomparedwith thetwo-class RVM.
Theprincipaldisadvantageoftherelevancevectormachineistherelativelylong trainingtimescomparedwiththe SVM.
Thisisoffset, however, bytheavoidanceof cross-validationrunstosetthemodelcomplexityparameters.
Furthermore, because it yields sparser models, the computation time on test points, which is usually the moreimportantconsiderationinpractice, istypicallymuchless.
Exercises 357 Exercises 7.1 ( ) www Supposewehaveadatasetofinputvectors{xn }withcorresponding target values tn ∈ {−1,1}, and suppose that we model the density of input vec- tors within each class separately using a Parzen kernel density estimator (see Sec- tion 2.5.1) with a kernel k(x, x ).
Write down the minimum misclassification-rate decisionruleassumingthetwoclasseshaveequalpriorprobability.
Showalsothat, ifthekernelischosentobek(x, x ) = x Tx , thentheclassificationrulereducesto simply assigning a new input vector to the class having the closest mean.
Finally, showthat, ifthekerneltakestheformk(x, x )=φ(x)Tφ(x ), thattheclassification isbasedontheclosestmeaninthefeaturespaceφ(x).
7.2 ( ) Showthat, ifthe1ontheright-handsideoftheconstraint(7.5)isreplacedby somearbitraryconstantγ > 0, thesolutionforthemaximummarginhyperplaneis unchanged.
7.3 ( ) Show that, irrespective of the dimensionality of the data space, a data set consistingofjusttwodatapoints, onefromeachclass, issufficienttodeterminethe locationofthemaximum-marginhyperplane.
7.4 ( ) www Showthatthevalueρofthemarginforthemaximum-marginhyper- planeisgivenby N 1 ρ2 = an (7.123) n=1 where {an } are given by maximizing (7.10) subject to the constraints (7.11) and (7.12).
7.5 ( ) Showthatthevaluesofρand{an }inthepreviousexercisealsosatisfy 1 =2L(a) (7.124) ρ2 where L(a)isdefinedby(7.10).
Similarly, showthat 1 = w 2.
(7.125) ρ2 7.6 ( ) Consider the logistic regression model with a target variable t ∈ {−1,1}.
If we define p(t = 1|y) = σ(y) where y(x) is given by (7.1), show that the negative log likelihood, with the addition of a quadratic regularization term, takes the form (7.47).
7.7 ( ) Considerthe Lagrangian(7.56)fortheregressionsupportvectormachine.
By settingthederivativesofthe Lagrangianwithrespecttow, b,ξn, andξn tozeroand then back substituting to eliminate the corresponding variables, show that the dual Lagrangianisgivenby(7.61).
358 7.
SPARSEKERNELMACHINES 7.8 ( ) www Fortheregressionsupportvectormachineconsideredin Section7.1.4, showthatalltrainingdatapointsforwhichξn > 0willhavean = C, andsimilarly allpointsforwhich ξn >0willhave an =C.
7.9 ( ) Verifytheresults(7.82)and(7.83)forthemeanandcovarianceoftheposterior distributionoverweightsintheregression RVM.
7.10 ( ) www Derive the result (7.85) for the marginal likelihood function in the regression RVM, by performing the Gaussian integral over w in (7.84) using the techniqueofcompletingthesquareintheexponential.
7.11 ( ) Repeattheaboveexercise, butthistimemakeuseofthegeneralresult(2.115).
7.12 ( ) www Showthatdirectmaximizationofthelogmarginallikelihood(7.85)for the regression relevance vector machine leads to the re-estimation equations (7.87) and(7.88)whereγi isdefinedby(7.89).
7.13 ( ) Intheevidenceframeworkfor RVMregression, weobtainedthere-estimation formulae (7.87) and (7.88) by maximizing the marginal likelihood given by (7.85).
Extend this approach by inclusion of hyperpriors given by gamma distributions of theform(B.26)andobtainthecorrespondingre-estimationformulaeforαandβby maximizing the corresponding posterior probability p(t,α,β|X) with respect to α andβ.
7.14 ( ) Derivetheresult(7.90)forthepredictivedistributionintherelevancevector machineforregression.
Showthatthepredictivevarianceisgivenby(7.91).
7.15 ( ) www Usingtheresults(7.94)and(7.95), showthatthemarginallikelihood (7.85) can be written in the form (7.96), where λ(αn) is defined by (7.97) and the sparsityandqualityfactorsaredefinedby(7.98)and(7.99), respectively.
7.16 ( ) By taking the second derivative of the log marginal likelihood (7.97) for the regression RVM with respect to the hyperparameter αi, show that the stationary pointgivenby(7.101)isamaximumofthemarginallikelihood.
7.17 ( ) Using (7.83) and (7.86), together with the matrix identity (C.7), show that thequantities Sn and Qn definedby(7.102)and(7.103)canbewrittenintheform (7.106)and(7.107).
7.18 ( ) www Show that the gradient vector and Hessian matrix of the log poste- riordistribution(7.109)fortheclassificationrelevancevectormachinearegivenby (7.110)and(7.111).
7.19 ( ) Verifythatmaximizationoftheapproximatelogmarginallikelihoodfunction (7.114)fortheclassificationrelevancevectormachineleadstotheresult(7.116)for re-estimationofthehyperparameters.
8 Graphical Models Probabilities play a central role in modern pattern recognition.
We have seen in Chapter1thatprobabilitytheorycanbeexpressedintermsoftwosimpleequations corresponding to the sum rule and the product rule.
All of the probabilistic infer- ence and learning manipulations discussed in this book, no matter how complex, amounttorepeatedapplicationofthesetwoequations.
Wecouldthereforeproceed to formulate and solve complicated probabilistic models purely by algebraic ma- nipulation.
However, we shall find it highly advantageous to augment the analysis usingdiagrammaticrepresentationsofprobabilitydistributions, calledprobabilistic graphicalmodels.
Theseofferseveralusefulproperties: 1.
They provide a simple way to visualize the structure of a probabilistic model andcanbeusedtodesignandmotivatenewmodels.
2.
Insights into the properties of the model, including conditional independence properties, canbeobtainedbyinspectionofthegraph.
359 360 8.
GRAPHICALMODELS 3.
Complexcomputations, requiredtoperforminferenceandlearninginsophis- ticatedmodels, canbeexpressedintermsofgraphicalmanipulations, inwhich underlyingmathematicalexpressionsarecarriedalongimplicitly.
Agraphcomprisesnodes(alsocalledvertices)connectedbylinks(alsoknown asedgesorarcs).
Inaprobabilisticgraphicalmodel, eachnoderepresentsarandom variable(orgroupofrandomvariables), andthelinksexpressprobabilisticrelation- ships between these variables.
The graph then captures the way in which the joint distribution over all of the random variables can be decomposed into a product of factors each depending only on a subset of the variables.
We shall begin by dis- cussing Bayesian networks, also known as directed graphical models, in which the links of the graphs have a particular directionality indicated by arrows.
The other majorclassofgraphicalmodelsare Markovrandomfields, alsoknownasundirected graphical models, in which the links do not carry arrows and have no directional significance.
Directedgraphsareusefulforexpressingcausalrelationshipsbetween randomvariables, whereasundirectedgraphsarebettersuitedtoexpressingsoftcon- straintsbetweenrandomvariables.
Forthepurposesofsolvinginferenceproblems, itisoftenconvenienttoconvertbothdirectedandundirectedgraphsintoadifferent representationcalledafactorgraph.
Inthischapter, weshallfocusonthekeyaspectsofgraphicalmodelsasneeded for applications in pattern recognition and machine learning.
More general treat- mentsofgraphicalmodelscanbefoundinthebooksby Whittaker(1990), Lauritzen (1996), Jensen (1996), Castillo et al.
(1997), Jordan (1999), Cowell et al.
(1999), and Jordan(2007).
8.1.
Bayesian Networks Inordertomotivatetheuseofdirectedgraphstodescribeprobabilitydistributions, considerfirstanarbitraryjointdistributionp(a, b, c)overthreevariablesa, b, andc.
Notethatatthisstage, wedonotneedtospecifyanythingfurtheraboutthesevari- ables, suchaswhethertheyarediscreteorcontinuous.
Indeed, oneofthepowerful aspectsofgraphicalmodelsisthataspecificgraphcanmakeprobabilisticstatements for a broad class of distributions.
By application of the product rule of probability (1.11), wecanwritethejointdistributionintheform p(a, b, c)=p(c|a, b)p(a, b).
(8.1) A second application of the product rule, this time to the second term on the right- handsideof(8.1), gives p(a, b, c)=p(c|a, b)p(b|a)p(a).
(8.2) Notethatthisdecompositionholdsforanychoiceofthejointdistribution.
Wenow representtheright-handsideof(8.2)intermsofasimplegraphicalmodelasfollows.
Firstweintroduceanodeforeachoftherandomvariablesa, b, andcandassociate eachnodewiththecorrespondingconditionaldistributionontheright-handsideof 8.1.
Bayesian Networks 361 Figure8.1 Adirectedgraphicalmodelrepresentingthejointprobabil- a itydistributionoverthreevariablesa, b, andc, correspond- b ingtothedecompositionontheright-handsideof(8.2).
c (8.2).
Then, for each conditional distribution we add directed links (arrows) to the graph from the nodes corresponding to the variables on which the distribution is conditioned.
Thusforthefactorp(c|a, b), therewillbelinksfromnodesaandbto nodec, whereasforthefactorp(a)therewillbenoincominglinks.
Theresultisthe graphshownin Figure8.1.
Ifthereisalinkgoingfromanodeatoanodeb, thenwe saythatnodeaistheparentofnodeb, andwesaythatnodebisthechildofnodea.
Notethatweshallnotmakeanyformaldistinctionbetweenanodeandthevariable towhichitcorrespondsbutwillsimplyusethesamesymboltorefertoboth.
Aninterestingpointtonoteabout(8.2)isthattheleft-handsideissymmetrical with respect to the three variables a, b, and c, whereas the right-hand side is not.
Indeed, inmakingthedecompositionin(8.2), wehaveimplicitlychosenaparticular ordering, namely a, b, c, and had we chosen a different ordering we would have obtained a different decomposition and hence a different graphical representation.
Weshallreturntothispointlater.
Forthemomentletusextendtheexampleof Figure8.1byconsideringthejoint distribution over K variables given by p(x 1 ,..., x K).
By repeated application of the product rule of probability, this joint distribution can be written as a product of conditionaldistributions, oneforeachofthevariables For a given choice of K, we can again represent this as a directed graph having K nodes, oneforeachconditionaldistributionontheright-handsideof(8.3), witheach nodehavingincominglinksfromalllowernumberednodes.
Wesaythatthisgraph isfullyconnectedbecausethereisalinkbetweeneverypairofnodes.
So far, we have worked with completely general joint distributions, so that the decompositions, andtheirrepresentationsasfullyconnectedgraphs, willbeapplica- ble to any choice of distribution.
As we shall see shortly, it is the absence of links inthegraphthatconveysinterestinginformationaboutthepropertiesoftheclassof distributionsthatthegraphrepresents.
Considerthegraphshownin Figure8.2.
This isnotafullyconnectedgraphbecause, forinstance, thereisnolinkfromx 1 tox 2 or fromx 3 tox 7.
Weshallnowgofromthisgraphtothecorrespondingrepresentationofthejoint probability distribution written in terms of the product of a set of conditional dis- tributions, one for each node in the graph.
Each such conditional distribution will be conditioned only on the parents of the corresponding node in the graph.
For in- stance, x 5 willbeconditionedonx 1 andx 3.
Thejointdistributionofall7variables 362 8.
GRAPHICALMODELS Figure8.2 E di x s a tr m ib p u l t e ion of o a ve d r ir v e a c r t i e a d ble a s cy x c 1 li , c ..
g .
r , a x p 7 h .
d T e h s e cr c ib o i r n re g s t p h o e nd jo in in g t x1 decompositionofthejointdistributionisgivenby(8.4).
x2 x3 x4 x5 x6 x7 isthereforegivenby p(x 1 )p(x 2 )p(x 3 )p(x 4 |x 1 , x 2 , x 3 )p(x 5 |x 1 , x 3 )p(x 6 |x 4 )p(x 7 |x 4 , x 5 ).
(8.4) The reader should take a moment to study carefully the correspondence between (8.4)and Figure8.2.
We can now state in general terms the relationship between a given directed graph and the corresponding distribution over the variables.
The joint distribution defined by a graph is given by the product, over all of the nodes of the graph, of aconditionaldistributionforeachnodeconditionedonthevariablescorresponding to the parents of that node in the graph.
Thus, for a graph with K nodes, the joint distributionisgivenby K p(x)= p(xk |pa k ) (8.5) k=1 where pa k denotes the set of parents of xk, and x = {x 1 ,..., x K }.
This key equationexpressesthefactorizationpropertiesofthejointdistributionforadirected graphicalmodel.
Althoughwehaveconsideredeachnodetocorrespondtoasingle variable, wecanequallywellassociatesetsofvariablesandvector-valuedvariables with the nodes of a graph.
It is easy to show that the representation on the right- handsideof(8.5)isalwayscorrectlynormalizedprovidedtheindividualconditional Exercise 8.1 distributionsarenormalized.
Thedirectedgraphsthatweareconsideringaresubjecttoanimportantrestric- tionnamelythattheremustbenodirectedcycles, inotherwordstherearenoclosed pathswithinthegraphsuchthatwecanmovefromnodetonodealonglinksfollow- ingthedirectionofthearrowsandendupbackatthestartingnode.
Suchgraphsare Exercise 8.2 alsocalleddirectedacyclicgraphs, or DAGs.
Thisisequivalenttothestatementthat there exists an ordering of the nodes such that there are no links that go from any nodetoanylowernumberednode.
8.1.1 Example: Polynomial regression As an illustration of the use of directed graphs to describe probability distri- butions, we consider the Bayesian polynomial regression model introduced in Sec- 8.1.
Bayesian Networks 363 Figure8.3 Directed graphical model representing the joint w distribution (8.6) corresponding to the Bayesian polynomial regression model introduced in Sec- tion1.2.6.
t1 t N tion 1.2.6.
The random variables in this model are the vector of polynomial coeffi- cientsw andtheobserveddatat = (t 1 ,..., t N)T.
Inaddition, thismodelcontains theinputdatax = (x 1 ,..., x N)T, thenoisevarianceσ2, andthehyperparameterα representingtheprecisionofthe Gaussianprioroverw, allofwhichareparameters of the model rather than random variables.
Focussing just on the random variables forthemoment, weseethatthejointdistributionisgivenbytheproductoftheprior p(w)and N conditionaldistributionsp(tn |w)forn=1,..., N sothat N p(t, w)=p(w) p(tn |w).
(8.6) n=1 Thisjointdistributioncanberepresentedbyagraphicalmodelshownin Figure8.3.
Whenwestarttodealwithmorecomplexmodelslaterinthebook, weshallfind itinconvenienttohavetowriteoutmultiplenodesoftheformt 1 ,..., t N explicitlyas in Figure8.3.
Wethereforeintroduceagraphicalnotationthatallowssuchmultiple nodes to be expressed more compactly, in which we draw a single representative nodetn andthensurroundthiswithabox, calledaplate, labelledwith N indicating thatthereare N nodesofthiskind.
Re-writingthegraphof Figure8.3inthisway, weobtainthegraphshownin Figure8.4.
Weshallsometimesfindithelpfultomaketheparametersofamodel, aswellas itsstochasticvariables, explicit.
Inthiscase,(8.6)becomes N p(t, w|x,α,σ2)=p(w|α) p(tn |w, xn,σ2).
n=1 Correspondingly, we can make x and α explicit in the graphical representation.
To dothis, weshalladopttheconventionthatrandomvariableswillbedenotedbyopen circles, anddeterministicparameterswillbedenotedbysmallersolidcircles.
Ifwe takethegraphof Figure8.4andincludethedeterministicparameters, weobtainthe graphshownin Figure8.5.
When we apply a graphical model to a problem in machine learning or pattern recognition, wewilltypicallysetsomeoftherandomvariablestospecificobserved Figure8.4 Analternative, morecompact, representationofthegraph w shown in Figure 8.3 in which we have introduced aplate (theboxlabelled N)thatrepresents Nnodesofwhichonly tn asingleexamplet nisshownexplicitly.
N 364 8.
GRAPHICALMODELS Figure8.5 This shows the same model as in Figure 8.4 but xn α with the deterministic parameters shown explicitly bythesmallersolidnodes.
w σ2 tn N values, forexamplethevariables{tn }fromthetrainingsetinthecaseofpolynomial curvefitting.
Inagraphicalmodel, wewilldenotesuchobservedvariablesbyshad- ing the corresponding nodes.
Thus the graph corresponding to Figure 8.5 in which the variables {tn } are observed is shown in Figure 8.6.
Note that the value of w is not observed, and so w is an example of a latent variable, also known as a hidden variable.
Such variables play a crucial role in many probabilistic models and will formthefocusof Chapters9and12.
Having observed the values {tn } we can, if desired, evaluate the posterior dis- tribution of the polynomial coefficients w as discussed in Section 1.2.5.
For the moment, wenotethatthisinvolvesastraightforwardapplicationof Bayes’theorem N p(w|T)∝p(w) p(tn |w) (8.7) n=1 whereagainwehaveomittedthedeterministicparametersinordertokeepthenota- tionuncluttered.
Ingeneral, modelparameterssuchaswareoflittledirectinterestinthemselves, becauseourultimategoalistomakepredictionsfornewinputvalues.
Supposewe aregivenanewinputvalue xandwewishtofindthecorrespondingprobabilitydis- tributionfortconditionedontheobserveddata.
Thegraphicalmodelthatdescribes this problem is shown in Figure 8.7, and the corresponding joint distribution of all oftherandomvariablesinthismodel, conditionedonthedeterministicparameters, isthengivenby N p( t, t, w| x, x,α,σ2)= p(tn |xn, w,σ2) p(w|α)p( t| x, w,σ2).
(8.8) n=1 Figure8.6 As in Figure 8.5 but with the nodes {t n } shaded xn α to indicate that the corresponding random vari- ableshavebeensettotheirobserved(trainingset) values.
w σ2 tn N 8.1.
Bayesian Networks 365 Figure8.7 The polynomial regression model, corresponding xn α to Figure 8.6, showing also a new input value xb together with the corresponding model prediction bt.
w tn N xˆ σ2 tˆ The required predictive distribution for t is then obtained, from the sum rule of probability, byintegratingoutthemodelparameterswsothat p( t| x, x, t,α,σ2)∝ p( t, t, w| x, x,α,σ2)dw where we are implicitly setting the random variables in t to the specific values ob- servedinthedataset.
Thedetailsofthiscalculationwerediscussedin Chapter3.
8.1.2 Generative models Therearemanysituationsinwhichwewishtodrawsamplesfromagivenprob- abilitydistribution.
Althoughweshalldevotethewholeof Chapter11toadetailed discussionofsamplingmethods, itisinstructivetooutlinehereonetechnique, called ancestral sampling, which is particularly relevant to graphical models.
Consider a joint distribution p(x 1 ,..., x K) over K variables that factorizes according to (8.5) correspondingtoadirectedacyclicgraph.
Weshallsupposethatthevariableshave beenorderedsuchthattherearenolinksfromanynodetoanylowernumberednode, inotherwordseachnodehasahighernumberthananyofitsparents.
Ourgoalisto drawasample x 1 ,..., x K fromthejointdistribution.
Todothis, westartwiththelowest-numberednodeanddrawasamplefromthe distributionp(x 1 ), whichwecall x 1.
Wethenworkthrougheachofthenodesinor- der, sothatfornodenwedrawasamplefromtheconditionaldistributionp(xn |pa n ) inwhichtheparentvariableshavebeensettotheirsampledvalues.
Notethatateach stage, theseparentvalueswillalwaysbeavailablebecausetheycorrespondtolower- numbered nodes that have already been sampled.
Techniques for sampling from specific distributions will be discussed in detail in Chapter 11.
Once we have sam- pledfromthefinalvariablex K, wewillhaveachievedourobjectiveofobtaininga samplefromthejointdistribution.
Toobtainasamplefromsomemarginaldistribu- tion corresponding to a subset of the variables, we simply take the sampled values for the required nodes and ignore the sampled values for the remaining nodes.
For example, to draw a sample from the distribution p(x 2 , x 4 ), we simply sample from thefulljointdistributionandthenretainthevalues x 2 , x 4 anddiscardtheremaining values{ xj= 2,4 }.
366 8.
GRAPHICALMODELS Figure8.8 Agraphicalmodelrepresentingtheprocessbywhich Object Position Orientation images of objects are created, in which the identity ofanobject(adiscretevariable)andthepositionand orientationofthatobject(continuousvariables)have independentpriorprobabilities.
Theimage(avector ofpixelintensities)hasaprobabilitydistributionthat isdependentontheidentityoftheobjectaswellas onitspositionandorientation.
Image Forpracticalapplicationsofprobabilisticmodels, itwilltypicallybethehigher- numberedvariablescorrespondingtoterminalnodesofthegraphthatrepresentthe observations, with lower-numbered nodes corresponding to latent variables.
The primary role of the latent variables is to allow a complicated distribution over the observed variables to be represented in terms of a model constructed from simpler (typicallyexponentialfamily)conditionaldistributions.
Wecaninterpretsuchmodelsasexpressingtheprocessesbywhichtheobserved dataarose.
Forinstance, consideranobjectrecognitiontaskinwhicheachobserved datapointcorrespondstoanimage(comprisingavectorofpixelintensities)ofone of the objects.
In this case, the latent variables might have an interpretation as the positionandorientationoftheobject.
Givenaparticularobservedimage, ourgoalis tofindtheposteriordistributionoverobjects, inwhichweintegrateoverallpossible positions and orientations.
We can represent this problem using a graphical model oftheformshowin Figure8.8.
Thegraphicalmodelcapturesthecausalprocess(Pearl,1988)bywhichtheob- serveddatawasgenerated.
Forthisreason, suchmodelsareoftencalledgenerative models.
By contrast, the polynomial regression model described by Figure 8.5 is not generative because there is no probability distribution associated with the input variablex, andsoitisnotpossibletogeneratesyntheticdatapointsfromthismodel.
Wecouldmakeitgenerativebyintroducingasuitablepriordistributionp(x), atthe expenseofamorecomplexmodel.
The hidden variables in a probabilistic model need not, however, have any ex- plicitphysicalinterpretationbutmaybeintroducedsimplytoallowamorecomplex joint distribution to be constructed from simpler components.
In either case, the technique of ancestral sampling applied to a generative model mimics the creation oftheobserveddataandwouldthereforegiveriseto‘fantasy’datawhoseprobability distribution(ifthemodelwereaperfectrepresentationofreality)wouldbethesame as that of the observed data.
In practice, producing synthetic observations from a generativemodelcanproveinformativeinunderstandingtheformoftheprobability distributionrepresentedbythatmodel.
8.1.3 Discrete variables Wehavediscussedtheimportanceofprobabilitydistributionsthataremembers Section2.4 of the exponential family, and we have seen that this family includes many well- known distributions as particular cases.
Although such distributions are relatively simple, theyformusefulbuildingblocksforconstructingmorecomplexprobability 8.1.
Bayesian Networks 367 Figure8.9 (a)Thisfully-connectedgraphdescribesageneraldistribu- x1 x2 tion over two K-state discrete variables having a total of (a) K2 −1 parameters.
(b) By dropping the link between the nodes, thenumberofparametersisreducedto2(K−1).
x1 x2 (b) distributions, andtheframeworkofgraphicalmodelsisveryusefulinexpressingthe wayinwhichthesebuildingblocksarelinkedtogether.
Suchmodelshaveparticularlynicepropertiesifwechoosetherelationshipbe- tween each parent-child pair in a directed graph to be conjugate, and we shall ex- plore several examples of this shortly.
Two cases are particularly worthy of note, namely when the parent and child node each correspond to discrete variables and when they each correspond to Gaussian variables, because in these two cases the relationshipcanbeextendedhierarchicallytoconstructarbitrarilycomplexdirected acyclicgraphs.
Webeginbyexaminingthediscretecase.
The probability distribution p(x|µ) for a single discrete variable x having K possiblestates(usingthe1-of-K representation)isgivenby K p(x|µ)= µ x k (8.9) k k=1 a nd is governed by the parameters µ = (µ 1 ,...,µK)T.
Due to the constraint k µk = 1, only K −1 values for µk need to be specified in order to define the distribution.
Nowsupposethatwehavetwodiscretevariables, x 1 andx 2, eachofwhichhas K states, andwewishtomodeltheirjointdistribution.
Wedenotetheprobabilityof observing both x 1k = 1 and x 2l = 1 by the parameter µkl, where x 1k denotes the kth componentofx 1, andsimilarlyforx 2l.
Thejointdistributioncanbewritten K K p(x , x |µ)= µ x 1k x 2l.
1 2 kl k=1l=1 Becausetheparametersµkl aresubjecttotheconstraint k l µkl =1, thisdistri- butionisgovernedby K2 −1parameters.
Itiseasilyseenthatthetotalnumber of parametersthatmustbespecifiedforanarbitraryjointdistributionover M variables is KM −1andthereforegrowsexponentiallywiththenumber M ofvariables.
Usingtheproductrule, wecanfactorthejointdistributionp(x 1 , x 2 )intheform p(x 2 |x 1 )p(x 1 ), which corresponds to a two-node graph with a link going from the x 1 node to the x 2 node as shown in Figure 8.9(a).
The marginal distribution p(x 1 ) is governed by K −1 parameters, as before, Similarly, the conditional distribution p(x 2 |x 1 )requiresthespecificationof K −1parametersforeachofthe K possible values of x 1.
The total number of parameters that must be specified in the joint distributionistherefore(K −1)+K(K −1)=K2−1asbefore.
Now suppose that the variables x 1 and x 2 were independent, corresponding to the graphical model shown in Figure 8.9(b).
Each variable is then described by 368 8.
GRAPHICALMODELS Figure8.10 This chain of M discrete nodes, each x1 x2 x M having K states, requiresthespecificationof K−1+ (M −1)K(K −1) parameters, which grows linearly withthelength M ofthechain.
Incontrast, afullycon- nectedgraphof M nodeswouldhave KM −1param- eters, whichgrowsexponentiallywith M.
a separate multinomial distribution, and the total number of parameters would be 2(K −1).
Foradistributionover M independentdiscretevariables, eachhaving K states, thetotalnumberofparameterswouldbe M(K −1), whichthereforegrows linearlywiththenumberofvariables.
Fromagraphicalperspective, wehavereduced thenumberofparametersbydroppinglinksinthegraph, attheexpenseofhavinga restrictedclassofdistributions.
More generally, if we have M discrete variables x 1 ,..., x M, we can model thejointdistributionusingadirectedgraphwithonevariablecorrespondingtoeach node.
Theconditionaldistributionateachnodeisgivenbyasetofnonnegativepa- rameterssubjecttotheusualnormalizationconstraint.
Ifthegraphisfullyconnected thenwehaveacompletelygeneraldistributionhaving KM −1parameters, whereas if there are no links in the graph the joint distribution factorizes into the product of themarginals, andthetotalnumberofparametersis M(K −1).
Graphshavingin- termediatelevelsofconnectivityallowformoregeneraldistributionsthanthefully factorized one while requiring fewer parameters than the general joint distribution.
As an illustration, consider the chain of nodes shown in Figure 8.10.
The marginal distribution p(x 1 ) requires K −1 parameters, whereas each of the M −1 condi- tional distributions p(xi |xi−1 ), for i = 2,..., M, requires K(K −1) parameters.
Thisgivesatotalparametercountof K−1+(M−1)K(K−1), whichisquadratic in K andwhichgrowslinearly(ratherthanexponentially)withthelength M ofthe chain.
Analternativewaytoreducethenumberofindependentparametersinamodel is by sharing parameters (also known as tying of parameters).
For instance, in the chainexampleof Figure8.10, wecanarrangethatalloftheconditionaldistributions p(xi |xi−1 ), fori=2,..., M, aregovernedbythesamesetof K(K−1)parameters.
Togetherwiththe K−1parametersgoverningthedistributionofx 1, thisgivesatotal of K2−1parametersthatmustbespecifiedinordertodefinethejointdistribution.
Wecanturnagraphoverdiscretevariablesintoa Bayesianmodelbyintroduc- ing Dirichlet priors for the parameters.
From a graphical point of view, each node thenacquiresanadditionalparentrepresentingthe Dirichletdistributionoverthepa- rametersassociatedwiththecorrespondingdiscretenode.
Thisisillustratedforthe chainmodelin Figure8.11.
Thecorrespondingmodelinwhichwetietheparame- tersgoverningtheconditionaldistributionsp(xi |xi−1 ), fori = 2,..., M, isshown in Figure8.12.
Anotherwayofcontrollingtheexponentialgrowthinthenumberofparameters in models of discrete variables is to use parameterized models for the conditional distributions instead of complete tables of conditional probability values.
To illus- tratethisidea, considerthegraphin Figure8.13inwhichallofthenodesrepresent binary variables.
Each of the parent variables xi is governed by a single parame- 8.1.
Bayesian Networks 369 µ µ µ Figure8.11 An extension of the model of 1 2 M Figure 8.10 to include Dirich- let priors over the param- eters governing the discrete distributions.
x1 x2 x M µ µ Figure8.12 Asin Figure8.11butwithasin- 1 glesetofparametersµshared amongst all of the conditional distributionsp(x i |x i−1 ).
x1 x2 x M ter µi representing the probability p(xi = 1), giving M parameters in total for the parentnodes.
Theconditionaldistributionp(y|x 1 ,..., x M), however, wouldrequire 2M parameters representing the probability p(y = 1) for each of the 2M possible settingsoftheparentvariables.
Thusingeneralthenumber ofparametersrequired tospecifythisconditionaldistributionwillgrowexponentiallywith M.
Wecanob- tain a more parsimonious form for the conditional distribution by using a logistic Section2.4 sigmoidfunctionactingonalinearcombinationoftheparentvariables, giving M p(y =1|x 1 ,..., x M)=σ w 0 + wixi =σ(w Tx) (8.10) i=1 whereσ(a)=(1+exp(−a))−1isthelogisticsigmoid, x=(x 0 , x 1 ,..., x M)Tisan (M +1)-dimensionalvectorofparentstatesaugmentedwithanadditionalvariable x 0 whosevalueisclampedto1, andw = (w 0 , w 1 ,..., w M)T isavectorof M +1 parameters.
Thisisamorerestrictedformofconditionaldistributionthanthegeneral casebutisnowgovernedbyanumberofparametersthatgrowslinearlywith M.
In thissense, itisanalogoustothechoiceofarestrictiveformofcovariancematrix(for example, adiagonalmatrix)inamultivariate Gaussiandistribution.
Themotivation forthelogisticsigmoidrepresentationwasdiscussedin Section4.2.
Figure8.13 Agraphcomprising M parentsx 1 ,..., x M andasin- x1 x M glechildy, usedtoillustratetheideaofparameterized conditionaldistributionsfordiscretevariables.
y 370 8.
GRAPHICALMODELS 8.1.4 Linear-Gaussian models In the previous section, we saw how to construct joint probability distributions over a set of discrete variables by expressing the variables as nodes in a directed acyclic graph.
Here we show how a multivariate Gaussian can be expressed as a directed graph corresponding to a linear-Gaussian model over the component vari- ables.
This allows us to impose interesting structure on the distribution, with the general Gaussian and the diagonal covariance Gaussian representing opposite ex- tremes.
Several widely used techniques are examples of linear-Gaussian models, such as probabilistic principal component analysis, factor analysis, and linear dy- namical systems (Roweis and Ghahramani, 1999).
We shall make extensive use of theresultsofthissectioninlaterchapterswhenweconsidersomeofthesetechniques indetail.
Consider an arbitrary directed acyclic graph over D variables in which node i represents a single continuous random variable xi having a Gaussian distribution.
The mean of this distribution is taken to be a linear combination of the states of its parentnodespa ofnodei i ⎛ ⎞ p(xi |pa i )=N ⎝ xi wijxj +bi, vi ⎠ (8.11) j∈pa i where wij and bi are parameters governing the mean, and vi is the variance of the conditionaldistributionforxi.
Thelogofthejointdistributionisthenthelogofthe productoftheseconditionalsoverallnodesinthegraphandhencetakestheform D lnp(x) = lnp(xi |pa i ) (8.12) i=1 ⎛ ⎞ 2 D = − 1 ⎝ xi − wijxj −bi ⎠ +const (8.13) 2vi i=1 j∈pa i wherex = (x 1 ,..., x D)T and‘const’denotestermsindependentofx.
Weseethat thisisaquadraticfunctionofthecomponentsofx, andhencethejointdistribution p(x)isamultivariate Gaussian.
Wecandeterminethemeanandcovarianceofthejointdistributionrecursively asfollows.
Eachvariablexi has(conditionalonthestatesofitsparents)a Gaussian distributionoftheform(8.11)andso √ xi = wijxj +bi+ vi i (8.14) j∈pa i where iisazeromean, unitvariance Gaussianrandomvariablesatisfying E[ i]=0 and E[ i j] = Iij, where Iij is the i, j element of the identity matrix.
Taking the expectationof(8.14), wehave E[xi]= wij E[xj]+bi.
(8.15) j∈pa i 8.1.
Bayesian Networks 371 Figure8.14 A directed graph over three Gaussian variables, x1 x2 x3 withonemissinglink.
Thuswecanfindthecomponentsof E[x] = (E[x 1 ],..., E[x D])T bystartingatthe lowest numbered node and working recursively through the graph (here we again assume that the nodes are numbered such that each node has a higher number than itsparents).
Similarly, wecanuse(8.14)and(8.15)toobtainthei, j elementofthe covariancematrixforp(x)intheformofarecursionrelation cov[xi, xj] = E[ ⎡ (xi −E[xi])(x ⎧j −E[xj])] ⎫⎤ ⎨ ⎬ √ = E⎣ (xi −E[xi]) ⎩ wjk(xk −E[xk])+ vj j⎭ ⎦ k∈pa j = wjkcov[xi, xk]+Iijvj (8.16) k∈pa j andsothecovariancecansimilarlybeevaluatedrecursivelystartingfromthelowest numberednode.
Let us consider two extreme cases.
First of all, suppose that there are no links inthegraph, whichthereforecomprises D isolatednodes.
Inthiscase, thereareno parameters wij and so there are just D parameters bi and D parameters vi.
From the recursion relations (8.15) and (8.16), we see that the mean of p(x) is given by The joint distribution has a total of 2D parameters and represents a set of D inde- pendentunivariate Gaussiandistributions.
Now consider a fully connected graph in which each node has all lower num- bered nodes as parents.
The matrix wij then has i−1 entries on the ith row and hence is a lower triangular matrix (with no entries on the leading diagonal).
Then thetotalnumberofparameterswij isobtainedbytakingthenumber D2ofelements ina D×Dmatrix, subtracting Dtoaccountfortheabsenceofelementsonthelead- ingdiagonal, andthendividingby2becausethematrixhaselementsonlybelowthe diagonal, givingatotalof D(D−1)/2.
Thetotalnumberofindependentparameters {wij }and{vi }inthecovariancematrixistherefore D(D+1)/2correspondingto Section2.3 ageneralsymmetriccovariancematrix.
Graphshavingsomeintermediatelevelofcomplexitycorrespondtojoint Gaus- sian distributions with partially constrained covariance matrices.
Consider for ex- ample the graph shown in Figure 8.14, which has a link missing between variables x 1 andx 3.
Usingtherecursionrelations(8.15)and(8.16), weseethatthemeanand Exercise 8.7 covarianceofthejointdistributionaregivenby µ = (b 1 , b 2 +w 21 b 1 , b 3 +w 32 b 2 +w 32 w 21 b 1 ) T (8.17) v w v w w v 1 21 1 32 21 1 Σ = w 21 v 1 v 2 +w 2 2 1 v 1 w 32 (v 2 +w 2 2 1 v 1 ) .(8.18) w w v w (v +w2 v ) v +w2 (v +w2 v ) 32 21 1 32 2 21 1 3 32 2 21 1 372 8.
GRAPHICALMODELS Wecanreadilyextendthelinear-Gaussiangraphicalmodeltothecaseinwhich thenodesofthegraphrepresentmultivariate Gaussianvariables.
Inthiscase, wecan writetheconditionaldistributionfornodeiintheform ⎛ ⎞ p(xi |pa i )=N ⎝ xi Wijxj +bi,Σi ⎠ (8.19) j∈pa i wherenow Wij isamatrix(whichisnonsquareifxi andxj havedifferentdimen- sionalities).
Again it is easy to verify that the joint distribution over all variables is Gaussian.
Notethatwehavealreadyencounteredaspecificexampleofthelinear-Gaussian Section2.3.6 relationship when we saw that the conjugate prior for the mean µ of a Gaussian variablexisitselfa Gaussiandistributionoverµ.
Thejointdistributionoverxand µ is therefore Gaussian.
This corresponds to a simple two-node graph in which the node representing µ is the parent of the node representing x.
The mean of the distribution over µ is a parameter controlling a prior, and so it can be viewed as a hyperparameter.
Because the value of this hyperparameter may itself be unknown, we can again treat it from a Bayesian perspective by introducing a prior over the hyperparameter, sometimescalledahyperprior, whichisagaingivenbya Gaussian distribution.
Thistypeofconstructioncanbeextendedinprincipletoanylevelandis anillustrationofahierarchical Bayesianmodel, ofwhichweshallencounterfurther examplesinlaterchapters.
8.2.
Conditional Independence Animportantconceptforprobabilitydistributionsovermultiplevariablesisthatof conditional independence (Dawid, 1980).
Consider three variables a, b, and c, and supposethattheconditionaldistributionofa, givenbandc, issuchthatitdoesnot dependonthevalueofb, sothat p(a|b, c)=p(a|c).
(8.20) Wesaythataisconditionallyindependentofbgivenc.
Thiscanbeexpressedina slightlydifferentwayifweconsiderthejointdistributionofaandbconditionedon c, whichwecanwriteintheform p(a, b|c) = p(a|b, c)p(b|c) = p(a|c)p(b|c).
(8.21) where we have used the product rule of probability together with (8.20).
Thus we see that, conditioned on c, the joint distribution of a and b factorizes into the prod- uct of the marginal distribution of a and the marginal distribution of b (again both conditionedonc).
Thissaysthatthevariablesaandbarestatisticallyindependent, givenc.
Notethatourdefinitionofconditionalindependencewillrequirethat(8.20), 8.2.
Conditional Independence 373 c Figure8.15 Thefirstofthreeexamplesofgraphsoverthreevariables a, b, and c used to discuss conditional independence propertiesofdirectedgraphicalmodels.
a b orequivalently(8.21), mustholdforeverypossiblevalueofc, andnotjustforsome values.
We shall sometimes use a shorthand notation for conditional independence (Dawid,1979)inwhich a⊥⊥b|c (8.22) denotesthataisconditionallyindependentofbgivencandisequivalentto(8.20).
Conditionalindependencepropertiesplayanimportantroleinusingprobabilis- tic models for pattern recognition by simplifying both the structure of a model and the computations needed to perform inference and learning under that model.
We shallseeexamplesofthisshortly.
Ifwearegivenanexpressionforthejointdistributionoverasetofvariablesin termsofaproductofconditionaldistributions(i.
e., themathematicalrepresentation underlying a directed graph), then we could in principle test whether any poten- tialconditionalindependencepropertyholdsbyrepeatedapplicationofthesumand productrulesofprobability.
Inpractice, suchanapproachwouldbeverytimecon- suming.
An important and elegant feature of graphical models is that conditional independencepropertiesofthejointdistributioncanbereaddirectlyfromthegraph without having to perform any analytical manipulations.
The general framework for achieving this is called d-separation, where the ‘d’ stands for ‘directed’ (Pearl, 1988).
Hereweshallmotivatetheconceptofd-separationandgiveageneralstate- mentofthed-separationcriterion.
Aformalproofcanbefoundin Lauritzen(1996).
8.2.1 Three example graphs Webeginourdiscussionoftheconditionalindependencepropertiesofdirected graphsbyconsideringthreesimpleexampleseachinvolvinggraphshavingjustthree nodes.
Together, thesewillmotivateandillustratethekeyconceptsofd-separation.
The first of the three examples is shown in Figure 8.15, and the joint distribution corresponding to this graph is easily written down using the general result (8.5) to give p(a, b, c)=p(a|c)p(b|c)p(c).
(8.23) If none of the variables are observed, then we can investigate whether a and b are independentbymarginalizingbothsidesof(8.23)withrespecttoctogive p(a, b)= p(a|c)p(b|c)p(c).
(8.24) c Ingeneral, thisdoesnotfactorizeintotheproductp(a)p(b), andso a⊥ ⊥b|∅ (8.25) 374 8.
GRAPHICALMODELS c Figure8.16 Asin Figure8.15butwherewehaveconditionedonthe valueofvariablec.
a b where ∅ denotes the empty set, and the symbol⊥ ⊥ means that the conditional inde- pendencepropertydoesnotholdingeneral.
Ofcourse, itmayholdforaparticular distribution by virtue of the specific numerical values associated with the various conditional probabilities, but it does not follow in general from the structure of the graph.
Now suppose we condition on the variable c, as represented by the graph of Figure8.16.
From(8.23), wecaneasilywritedowntheconditionaldistributionofa andb, givenc, intheform p(a, b, c) p(a, b|c) = p(c) = p(a|c)p(b|c) andsoweobtaintheconditionalindependenceproperty a⊥⊥b|c.
We can provide a simple graphical interpretation of this result by considering the path from node a to node b via c.
The node c is said to be tail-to-tail with re- spect to this path because the node is connected to the tails of the two arrows, and the presence of such a path connecting nodes a and b causes these nodes to be de- pendent.
However, whenweconditiononnodec, asin Figure8.16, theconditioned node ‘blocks’ the path from a to b and causes a and b to become (conditionally) independent.
Wecansimilarlyconsiderthegraphshownin Figure8.17.
Thejointdistribution correspondingtothisgraphisagainobtainedfromourgeneralformula(8.5)togive p(a, b, c)=p(a)p(c|a)p(b|c).
(8.26) First of all, suppose that none of the variables are observed.
Again, we can test to seeifaandbareindependentbymarginalizingoverctogive p(a, b)=p(a) p(c|a)p(b|c)=p(a)p(b|a).
c Figure8.17 The second of our three examples of 3-node a c b graphs used to motivate the conditional indepen- denceframeworkfordirectedgraphicalmodels.
8.2.
Conditional Independence 375 Figure8.18 Asin Figure8.17butnowconditioningonnodec.
a c b whichingeneraldoesnotfactorizeintop(a)p(b), andso a⊥ ⊥b|∅ (8.27) asbefore.
Now suppose we condition on node c, as shown in Figure 8.18.
Using Bayes’ theorem, togetherwith(8.26), weobtain p(a, b, c) p(a, b|c) = p(c) p(a)p(c|a)p(b|c) = p(c) = p(a|c)p(b|c) andsoagainweobtaintheconditionalindependenceproperty a⊥⊥b|c.
As before, we can interpret these results graphically.
The node c is said to be head-to-tail with respect to the path from node a to node b.
Such a path connects nodesaandbandrendersthemdependent.
Ifwenowobservec, asin Figure8.18, thenthisobservation‘blocks’thepathfromatobandsoweobtaintheconditional independencepropertya⊥⊥b|c.
Finally, we consider the third of our 3-node examples, shown by the graph in Figure8.19.
Asweshallsee, thishasamoresubtlebehaviourthanthetwoprevious graphs.
Thejointdistributioncanagainbewrittendownusingourgeneralresult(8.5)to give p(a, b, c)=p(a)p(b)p(c|a, b).
(8.28) Considerfirstthecasewherenoneofthevariablesareobserved.
Marginalizingboth sidesof(8.28)overcweobtain p(a, b)=p(a)p(b) Figure8.19 Thelastofourthreeexamplesof3-nodegraphsusedto a b explore conditional independence properties in graphi- cal models.
This graph has rather different properties fromthetwopreviousexamples.
c 376 8.
GRAPHICALMODELS Figure8.20 Asin Figure8.19butconditioningonthevalueofnode a b c.
Inthisgraph, theactofconditioninginducesadepen- dencebetweenaandb.
c and so a and b are independent with no variables observed, in contrast to the two previousexamples.
Wecanwritethisresultas a⊥⊥b|∅.
(8.29) Nowsupposeweconditiononc, asindicatedin Figure8.20.
Theconditionaldistri- butionofaandbisthengivenby p(a, b, c) p(a, b|c) = p(c) p(a)p(b)p(c|a, b) = p(c) whichingeneraldoesnotfactorizeintotheproductp(a)p(b), andso a⊥ ⊥b|c.
Thus our third example has the opposite behaviour from the first two.
Graphically, we say that node c is head-to-head with respect to the path from a to b because it connects to the heads of the two arrows.
When node c is unobserved, it ‘blocks’ the path, and the variables a and b are independent.
However, conditioning on c ‘unblocks’thepathandrendersaandbdependent.
There is one more subtlety associated with this third example that we need to consider.
First we introduce some more terminology.
We say that node y is a de- scendant of node x if there is a path from x to y in which each step of the path followsthedirectionsofthearrows.
Thenitcanbeshownthatahead-to-headpath Exercise 8.10 willbecomeunblockedifeitherthenode, oranyofitsdescendants, isobserved.
In summary, a tail-to-tail node or a head-to-tail node leaves a path unblocked unless it is observed in which case it blocks the path.
By contrast, a head-to-head node blocks a path if it is unobserved, but once the node, and/or at least one of its descendants, isobservedthepathbecomesunblocked.
Itisworthspendingamomenttounderstandfurthertheunusualbehaviourofthe graphof Figure8.20.
Consider aparticular instanceof suchagraphcorresponding toaproblemwiththreebinaryrandomvariablesrelatingtothefuelsystemonacar, as shown in Figure 8.21.
The variables are called B, representing the state of a battery that is either charged (B = 1) or flat (B = 0), F representing the state of the fuel tank that is either full of fuel (F = 1) or empty (F = 0), and G, which is thestateofanelectricfuelgaugeandwhichindicateseitherfull(G = 1)orempty 8.2.
Conditional Independence 377 B F B F B F G G G Figure8.21 Anexampleofa3-nodegraphusedtoillustratethephenomenonof‘explainingaway’.
Thethree nodes represent the state of the battery (B), the state of the fuel tank (F) and the reading on the electric fuel gauge(G).
Seethetextfordetails.
(G = 0).
The battery is either charged or flat, and independently the fuel tank is eitherfullorempty, withpriorprobabilities p(B =1) = 0.9 p(F =1) = 0.9.
Giventhestateofthefueltankandthebattery, thefuelgaugereadsfullwithproba- bilitiesgivenby p(G=1|B =1, F =1) = 0.8 p(G=1|B =1, F =0) = 0.2 p(G=1|B =0, F =1) = 0.2 p(G=1|B =0, F =0) = 0.1 sothisisaratherunreliablefuelgauge! Allremainingprobabilitiesaredetermined bytherequirementthatprobabilitiessumtoone, andsowehaveacompletespecifi- cationoftheprobabilisticmodel.
Before we observe any data, the prior probability of the fuel tank being empty isp(F = 0) = 0.1.
Nowsupposethatweobservethefuelgaugeanddiscoverthat it reads empty, i.
e., G = 0, corresponding to the middle graph in Figure 8.21.
We can use Bayes’ theorem to evaluate the posterior probability of the fuel tank being empty.
Firstweevaluatethedenominatorfor Bayes’theoremgivenby p(G=0)= p(G=0|B, F)p(B)p(F)=0.315 (8.30) B∈{0,1}F∈{0,1} andsimilarlyweevaluate p(G=0|F =0)= p(G=0|B, F =0)p(B)=0.81 (8.31) B∈{0,1} andusingtheseresultswehave p(G=0|F =0)p(F =0) p(F =0|G=0)= 0.257 (8.32) p(G=0) 378 8.
GRAPHICALMODELS and so p(F = 0|G = 0) > p(F = 0).
Thus observing that the gauge reads empty makes it more likely that the tank is indeed empty, as we would intuitively expect.
Next suppose that we also check the state of the battery and find that it is flat, i.
e., B = 0.
Wehavenowobservedthestatesofboththefuelgaugeandthebattery, as shownbytheright-handgraphin Figure8.21.
Theposteriorprobabilitythatthefuel tank is empty given the observations of both the fuel gauge and the battery state is thengivenby p(G=0|B =0, F =0)p(F =0) p(F =0|G=0, B =0)= 0.111 (8.33) p(G=0|B =0, F)p(F) F∈{0,1} wherethepriorprobabilityp(B = 0)hascancelledbetweennumeratoranddenom- inator.
Thus the probability that the tank is empty has decreased (from 0.257 to 0.111)asaresultoftheobservationofthestateofthebattery.
Thisaccordswithour intuitionthatfindingoutthatthebatteryisflatexplainsawaytheobservationthatthe fuelgaugereadsempty.
Weseethatthestateofthefueltankandthatofthebattery have indeed become dependent on each other as a result of observing the reading on the fuel gauge.
In fact, this would also be the case if, instead of observing the fuel gauge directly, we observed the state of some descendant of G.
Note that the probability p(F = 0|G = 0, B = 0) 0.111 is greater than the prior probability p(F =0)=0.1becausetheobservationthatthefuelgaugereadszerostillprovides someevidenceinfavourofanemptyfueltank.
8.2.2 D-separation Wenowgiveageneralstatementofthed-separationproperty(Pearl, 1988)for directed graphs.
Consider a general directed graph in which A, B, and C are arbi- trary nonintersecting sets of nodes (whose union may be smaller than the complete set of nodes in the graph).
We wish to ascertain whether a particular conditional independencestatement A⊥⊥B|C isimpliedbyagivendirectedacyclicgraph.
To doso, weconsiderallpossiblepathsfromanynodein Atoanynodein B.
Anysuch pathissaidtobeblockedifitincludesanodesuchthateither (a) thearrowsonthepathmeeteitherhead-to-tailortail-to-tailatthenode, andthe nodeisintheset C, or (b) the arrows meet head-to-head at the node, and neither the node, nor any of its descendants, isintheset C.
Ifallpathsareblocked, then Aissaidtobed-separatedfrom B by C, andthejoint distributionoverallofthevariablesinthegraphwillsatisfy A⊥⊥B |C.
The concept of d-separation is illustrated in Figure 8.22.
In graph (a), the path from a to b is not blocked by node f because it is a tail-to-tail node for this path and is not observed, nor is it blocked by node e because, although the latter is a head-to-head node, it has a descendant c because is in the conditioning set.
Thus the conditional independence statement a ⊥⊥ b | c does not follow from this graph.
In graph (b), the path from a to b is blocked by node f because this is a tail-to-tail nodethatisobserved, andsotheconditionalindependencepropertya⊥⊥b|f will 8.2.
Conditional Independence 379 Figure8.22 Illustrationofthecon- a f a f ceptofd-separation.
Seethetextfor details.
e b e b c c (a) (b) besatisfiedbyanydistributionthatfactorizesaccordingtothisgraph.
Notethatthis pathisalsoblockedbynodeebecauseeisahead-to-headnodeandneitheritnorits descendantareintheconditioningset.
For the purposes of d-separation, parameters such as α and σ2 in Figure 8.5, indicated by small filled circles, behave in the same was as observed nodes.
How- ever, there are no marginal distributions associated with such nodes.
Consequently parameternodesneverthemselveshaveparentsandsoallpathsthroughthesenodes will always be tail-to-tail and hence blocked.
Consequently they play no role in d-separation.
Another example of conditional independence and d-separation is provided by the concept of i.
i.
d.
(independent identically distributed) data introduced in Sec- tion 1.2.4.
Consider the problem of finding the posterior distribution for the mean Section2.3 ofaunivariate Gaussiandistribution.
Thiscanberepresentedbythedirectedgraph shown in Figure 8.23 in which the joint distribution is defined by a prior p(µ) to- getherwithasetofconditionaldistributionsp(xn |µ)forn = 1,..., N.
Inpractice, we observe D = {x 1 ,..., x N } and our goal is to infer µ.
Suppose, for a moment, thatweconditiononµandconsiderthejointdistributionoftheobservations.
Using d-separation, wenotethatthereisauniquepathfromanyxi toanyotherxj= i and that this path is tail-to-tail with respect to the observed node µ.
Every such path is blockedandsotheobservations D ={x 1 ,..., x N }areindependentgivenµ, sothat N p(D|µ)= p(xn |µ).
(8.34) n=1 µ Figure8.23 (a) Directed graph corre- sponding to the problem of inferring the mean µ of µ a univariate Gaussian dis- tribution from observations x 1 ,..., x N.
(b) The same N graph drawn using the plate notation.
x1 x N xn N (a) (b) 380 8.
GRAPHICALMODELS Figure8.24 A graphical representation of the ‘naive Bayes’ z model for classification.
Conditioned on the class label z, the components of the observed vector x = (x 1 ,..., x D )T are assumed to be independent.
x1 x D However, ifweintegrateoverµ, theobservationsareingeneralnolongerindepen- dent ∞ N p(D)= p(D|µ)p(µ)dµ = p(xn).
(8.35) 0 n=1 Hereµisalatentvariable, becauseitsvalueisnotobserved.
Another example of a model representing i.
i.
d.
data is the graph in Figure 8.7 correspondingto Bayesianpolynomialregression.
Herethestochasticnodescorre- spond to {tn }, w and t.
We see that the node for w is tail-to-tail with respect to thepathfromttoanyoneofthenodestn andsowehavethefollowingconditional independenceproperty t⊥⊥tn |w.
(8.36) Thus, conditioned on the polynomial coefficients w, the predictive distribution for t is independent of the training data {t 1 ,..., t N }.
We can therefore first use the trainingdatatodeterminetheposteriordistributionoverthecoefficientswandthen we can discard the training data and use the posterior distribution for w to make Section3.3 predictionsof tfornewinputobservations x.
A related graphical structure arises in an approach to classification called the naive Bayesmodel, inwhichweuseconditionalindependenceassumptionstosim- plifythemodelstructure.
Supposeourobservedvariableconsistsofa D-dimensional vectorx = (x 1 ,..., x D)T, andwewishtoassignobservedvaluesofxtooneof K classes.
Usingthe1-of-K encodingscheme, wecanrepresenttheseclassesbya K- dimensionalbinaryvectorz.
Wecanthendefineagenerativemodelbyintroducing amultinomialpriorp(z|µ)overtheclasslabels, wherethekth componentµk ofµ is the prior probability of class C k, together with a conditional distribution p(x|z) for the observed vector x.
The key assumption of the naive Bayes model is that, conditionedontheclassz, thedistributionsoftheinputvariablesx 1 ,..., x D arein- dependent.
Thegraphicalrepresentationofthismodelisshownin Figure8.24.
We seethatobservationofzblocksthepathbetweenxi andxj forj = i(becausesuch paths are tail-to-tail at the node z) and so xi and xj are conditionally independent given z.
If, however, we marginalize out z (so that z is unobserved) the tail-to-tail path from xi to xj is no longer blocked.
This tells us that in general the marginal densityp(x)willnotfactorizewithrespecttothecomponentsofx.
Weencountered a simple application of the naive Bayes model in the context of fusing data from differentsourcesformedicaldiagnosisin Section1.5.
Ifwearegivenalabelledtrainingset, comprisinginputs{x 1 ,..., x N }together with their class labels, then we can fit the naive Bayes model to the training data 8.2.
Conditional Independence 381 using maximum likelihood assuming that the data are drawn independently from the model.
The solution is obtained by fitting the model for each class separately usingthecorrespondinglylabelleddata.
Asanexample, supposethattheprobability density within each class is chosen to be Gaussian.
In this case, the naive Bayes assumption then implies that the covariance matrix for each Gaussian is diagonal, andthecontoursofconstantdensitywithineachclasswillbeaxis-alignedellipsoids.
The marginal density, however, is given by a superposition of diagonal Gaussians (withweightingcoefficientsgivenbytheclasspriors)andsowillnolongerfactorize withrespecttoitscomponents.
Thenaive Bayesassumptionishelpfulwhenthedimensionality D oftheinput spaceishigh, makingdensityestimationinthefull D-dimensionalspacemorechal- lenging.
It is also useful if the input vector contains both discrete and continuous variables, since each can be represented separately using appropriate models (e.
g., Bernoulli distributions for binary observations or Gaussians for real-valued vari- ables).
The conditional independence assumption of this model is clearly a strong one that may lead to rather poor representations of the class-conditional densities.
Nevertheless, even if this assumption is not precisely satisfied, the model may still givegoodclassificationperformanceinpracticebecausethedecisionboundariescan be insensitive to some of the details in the class-conditional densities, as illustrated in Figure1.27.
Wehaveseenthataparticulardirectedgraphrepresentsaspecificdecomposition of a joint probability distribution into a product of conditional probabilities.
The graphalsoexpressesasetofconditionalindependencestatementsobtainedthrough thed-separationcriterion, andthed-separationtheoremisreallyanexpressionofthe equivalenceofthesetwoproperties.
Inordertomakethisclear, itishelpfultothink of a directed graph as a filter.
Suppose we consider a particular joint probability distribution p(x) over the variables x corresponding to the (nonobserved) nodes of thegraph.
Thefilterwillallowthisdistributiontopassthroughif, andonlyif, itcan beexpressedintermsofthefactorization(8.5)impliedbythegraph.
Ifwepresentto thefilterthesetofallpossibledistributionsp(x)overthesetofvariablesx, thenthe subsetofdistributionsthatarepassedbythefilterwillbedenoted DF, fordirected factorization.
Thisisillustratedin Figure8.25.
Alternatively, wecanusethegraphas adifferentkindoffilterbyfirstlistingalloftheconditionalindependenceproperties obtained by applying the d-separation criterion to the graph, and then allowing a distributiontopassonlyifitsatisfiesalloftheseproperties.
Ifwepresentallpossible distributionsp(x)tothissecondkindoffilter, thenthed-separationtheoremtellsus thatthesetofdistributionsthatwillbeallowedthroughispreciselytheset DF.
It should be emphasized that the conditional independence properties obtained from d-separation apply to any probabilistic model described by that particular di- rected graph.
This will be true, for instance, whether the variables are discrete or continuous or a combination of these.
Again, we see that a particular graph is de- scribingawholefamilyofprobabilitydistributions.
Atoneextremewehaveafullyconnectedgraphthatexhibitsnoconditionalin- dependencepropertiesatall, andwhichcanrepresentanypossiblejointprobability distributionoverthegivenvariables.
Theset DF willcontainallpossibledistribu- 382 8.
GRAPHICALMODELS p(x) DF Figure8.25 Wecanviewagraphicalmodel(inthiscaseadirectedgraph)asafilterinwhichaprob- abilitydistributionp(x)isallowedthroughthefilterif, andonlyif, itsatisfiesthedirected factorizationproperty(8.5).
Thesetofallpossibleprobabilitydistributionsp(x)thatpass throughthefilterisdenoted DF.
Wecanalternativelyusethegraphtofilterdistributions according to whether they respect all of the conditional independencies implied by the d-separationpropertiesofthegraph.
Thed-separationtheoremsaysthatitisthesame setofdistributions DF thatwillbeallowedthroughthissecondkindoffilter.
tions p(x).
At the other extreme, we have the fully disconnected graph, i.
e., one havingnolinksatall.
Thiscorrespondstojointdistributionswhichfactorizeintothe productofthemarginaldistributionsoverthevariablescomprisingthenodesofthe graph.
Notethatforanygivengraph, thesetofdistributions DF willincludeanydis- tributions that have additional independence properties beyond those described by thegraph.
Forinstance, afullyfactorizeddistributionwillalwaysbepassedthrough thefilterimpliedbyanygraphoverthecorrespondingsetofvariables.
Weendourdiscussionofconditionalindependencepropertiesbyexploringthe concept of a Markov blanket or Markov boundary.
Consider a joint distribution p(x 1 ,..., x D) represented by a directed graph having D nodes, and consider the conditional distribution of a particular node with variables xi conditioned on all of theremainingvariablesxj= i.
Usingthefactorizationproperty(8.5), wecanexpress thisconditionaldistributionintheform p(xi |x {j= i} ) = p(x 1 ,..., x D) p(x 1 ,..., x D)dxi p(xk |pa k ) k = p(xk |pa k )dxi k inwhichtheintegralisreplacedbyasummationinthecaseofdiscretevariables.
We nowobservethatanyfactorp(xk |pa k )thatdoesnothaveanyfunctionaldependence on xi can be taken outside the integral over xi, and will therefore cancel between numerator and denominator.
The only factors that remain will be the conditional distribution p(xi |pa i ) for node xi itself, together with the conditional distributions foranynodesxk suchthatnodexi isintheconditioningsetofp(xk |pa k ), inother wordsforwhichxi isaparentofxk.
Theconditionalp(xi |pa i )willdependonthe parentsofnodexi, whereastheconditionalsp(xk |pa k )willdependonthechildren 8.3.
Markov Random Fields 383 Figure8.26 The Markovblanketofanodex i comprisestheset of parents, children and co-parents of the node.
It has the property that the conditional distribution of x i, conditionedonalltheremainingvariablesinthe graph, is dependent only on the variables in the Markovblanket.
xi ofxi aswellasontheco-parents, inotherwordsvariablescorrespondingtoparents ofnodexk otherthannodexi.
Thesetofnodescomprisingtheparents, thechildren andtheco-parentsiscalledthe Markovblanketandisillustratedin Figure8.26.
We canthinkofthe Markovblanketofanodexi asbeingtheminimalsetofnodesthat isolatesxifromtherestofthegraph.
Notethatitisnotsufficienttoincludeonlythe parentsandchildrenofnodexi becausethephenomenonofexplainingawaymeans thatobservationsofthechildnodeswillnotblockpathstotheco-parents.
Wemust thereforeobservetheco-parentnodesalso.
8.3.
Markov Random Fields Wehaveseenthatdirectedgraphicalmodelsspecifyafactorizationofthejointdis- tributionoverasetofvariablesintoaproductoflocalconditionaldistributions.
They alsodefineasetofconditionalindependencepropertiesthatmustbesatisfiedbyany distribution that factorizes according to the graph.
We turn now to the second ma- jorclassofgraphicalmodelsthataredescribedbyundirectedgraphsandthatagain specifybothafactorizationandasetofconditionalindependencerelations.
A Markov random field, also known as a Markov network or an undirected graphical model (Kindermann and Snell, 1980), has a set of nodes each of which corresponds to a variable or group of variables, as well as a set of links each of which connects a pair of nodes.
The links are undirected, that is they do not carry arrows.
Inthecaseofundirectedgraphs, itisconvenienttobeginwithadiscussion ofconditionalindependenceproperties.
8.3.1 Conditional independence properties Section8.2 Inthecaseofdirectedgraphs, wesawthatitwaspossibletotestwhetherapar- ticular conditional independence property holds by applying a graphical test called d-separation.
This involved testing whether or not the paths connecting two sets of nodes were ‘blocked’.
The definition of blocked, however, was somewhat subtle due to the presence of paths having head-to-head nodes.
We might ask whether it is possible to define an alternative graphical semantics for probability distributions suchthatconditionalindependenceisdeterminedbysimplegraphseparation.
This isindeedthecaseandcorrespondstoundirectedgraphicalmodels.
Byremovingthe 384 8.
GRAPHICALMODELS Figure8.27 An example of an undirected graph in which every path from any node in set Atoanynodeinset B passesthrough at least one node in set C.
Conse- quently the conditional independence property A ⊥⊥ B | C holds for any probabilitydistributiondescribedbythis graph.
C B A directionality from the links of the graph, the asymmetry between parent and child nodesisremoved, andsothesubtletiesassociatedwithhead-to-headnodesnolonger arise.
Supposethatinanundirectedgraphweidentifythreesetsofnodes, denoted A, B, and C, andthatweconsidertheconditionalindependenceproperty A⊥⊥B |C.
(8.37) To test whether this property is satisfied by a probability distribution defined by a graph we consider all possible paths that connect nodes in set A to nodes in set B.
Ifallsuchpathspassthroughoneormorenodesinset C, thenallsuchpathsare ‘blocked’andsotheconditionalindependencepropertyholds.
However, ifthereisat leastonesuchpaththatisnotblocked, thenthepropertydoesnotnecessarilyhold, or morepreciselytherewillexistatleastsomedistributionscorrespondingtothegraph thatdonotsatisfythisconditionalindependencerelation.
Thisisillustratedwithan examplein Figure8.27.
Notethatthisisexactlythesameasthed-separationcrite- rionexceptthatthereisno‘explainingaway’phenomenon.
Testingforconditional independenceinundirectedgraphsisthereforesimplerthanindirectedgraphs.
An alternative way to view the conditional independence test is to imagine re- moving all nodes in set C from the graph together with any links that connect to those nodes.
We then ask if there exists a path that connects any node in A to any node in B.
If there are no such paths, then the conditional independence property musthold.
The Markov blanket for an undirected graph takes a particularly simple form, becauseanodewillbeconditionallyindependentofallothernodesconditionedonly ontheneighbouringnodes, asillustratedin Figure8.28.
8.3.2 Factorization properties We now seek a factorization rule for undirected graphs that will correspond to theaboveconditionalindependencetest.
Again, thiswillinvolveexpressingthejoint distributionp(x)asaproductoffunctionsdefinedoversetsofvariablesthatarelocal tothegraph.
Wethereforeneedtodecidewhatistheappropriatenotionoflocality inthiscase.
8.3.
Markov Random Fields 385 Figure8.28 For an undirected graph, the Markov blanket of a node x i consists of the set of neighbouring nodes.
It has the propertythattheconditionaldistributionofx i, conditioned on all the remaining variables in the graph, is dependent onlyonthevariablesinthe Markovblanket.
Ifweconsidertwonodesxi andxj thatarenotconnectedbyalink, thenthese variablesmustbeconditionallyindependentgivenallothernodesinthegraph.
This followsfromthefactthatthereisnodirectpathbetweenthetwonodes, andallother pathspassthroughnodesthatareobserved, andhencethosepathsareblocked.
This conditionalindependencepropertycanbeexpressedas p(xi, xj |x \{i, j} )=p(xi |x \{i, j} )p(xj |x \{i, j} ) (8.38) wherex \{i, j} denotesthesetxofallvariableswithxi andxj removed.
Thefactor- izationofthejointdistributionmustthereforebesuchthatxi andxj donotappear inthesamefactorinorderfortheconditionalindependencepropertytoholdforall possibledistributionsbelongingtothegraph.
This leads us to consider a graphical concept called a clique, which is defined as a subset of the nodes in a graph such that there exists a link between all pairs of nodes in the subset.
In other words, the set of nodes in a clique is fully connected.
Furthermore, amaximalcliqueisacliquesuchthatitisnotpossibletoincludeany othernodesfromthegraphinthesetwithoutitceasingtobeaclique.
Theseconcepts areillustratedbytheundirectedgraphoverfourvariablesshownin Figure8.29.
This graphhasfivecliquesoftwonodesgivenby{x 1 , x 2 },{x 2 , x 3 },{x 3 , x 4 },{x 4 , x 2 }, and{x 1 , x 3 }, aswellastwomaximalcliquesgivenby{x 1 , x 2 , x 3 }and{x 2 , x 3 , x 4 }.
Theset{x 1 , x 2 , x 3 , x 4 }isnotacliquebecauseofthemissinglinkfromx 1 tox 4.
Wecanthereforedefinethefactorsinthedecompositionofthejointdistribution to be functions of the variables in the cliques.
In fact, we can consider functions of the maximal cliques, without loss of generality, because other cliques must be subsetsofmaximalcliques.
Thus, if{x 1 , x 2 , x 3 }isamaximalcliqueandwedefine an arbitrary function over this clique, then including another factor defined over a subsetofthesevariableswouldberedundant.
Letusdenoteacliqueby C andthesetofvariablesinthatcliquebyx C.
Then Figure8.29 Afour-nodeundirectedgraphshowingaclique(outlinedin green)andamaximalclique(outlinedinblue).
x1 x2 x3 x4 386 8.
GRAPHICALMODELS thejointdistributioniswrittenasaproductofpotentialfunctionsψC(x C)overthe maximalcliquesofthegraph 1 p(x)= ψC(x C).
(8.39) Z C Herethequantity Z, sometimescalledthepartitionfunction, isanormalizationcon- stantandisgivenby Z = ψC(x C) (8.40) x C which ensures that the distribution p(x) given by (8.39) is correctly normalized.
By considering only potential functions which satisfy ψC(x C) 0 we ensure that p(x) 0.
In (8.40) we have assumed that x comprises discrete variables, but the frameworkisequallyapplicabletocontinuousvariables, oracombinationofthetwo, in which the summation is replaced by the appropriate combination of summation andintegration.
Notethatwedonotrestrictthechoiceofpotentialfunctionstothosethathavea specific probabilistic interpretation as marginal or conditional distributions.
This is incontrasttodirectedgraphsinwhicheachfactorrepresentstheconditionaldistribu- tionofthecorrespondingvariable, conditionedonthestateofitsparents.
However, in special cases, for instance where the undirected graph is constructed by starting withadirectedgraph, thepotentialfunctionsmayindeedhavesuchaninterpretation, asweshallseeshortly.
One consequence of the generality of the potential functions ψC(x C) is that their product will in general not be correctly normalized.
We therefore have to in- troduce an explicit normalization factor given by (8.40).
Recall that for directed graphs, thejointdistributionwasautomaticallynormalizedasaconsequenceofthe normalizationofeachoftheconditionaldistributionsinthefactorization.
The presence of this normalization constant is one of the major limitations of undirectedgraphs.
Ifwehaveamodelwith M discretenodeseachhaving K states, thentheevaluationofthenormalizationterminvolvessummingover KM statesand so(intheworstcase)isexponentialinthesizeofthemodel.
Thepartitionfunction isneededforparameterlearningbecauseitwillbeafunctionofanyparametersthat governthepotentialfunctionsψC(x C).
However, forevaluationoflocalconditional distributions, the partition function is not needed because a conditional is the ratio oftwomarginals, andthepartitionfunctioncancelsbetweennumeratoranddenom- inator when evaluating this ratio.
Similarly, for evaluating local marginal probabil- ities we can work with the unnormalized joint distribution and then normalize the marginalsexplicitlyattheend.
Providedthemarginalsonlyinvolvesasmallnumber ofvariables, theevaluationoftheirnormalizationcoefficientwillbefeasible.
Sofar, wehavediscussedthenotionofconditionalindependencebasedonsim- ple graph separation and we have proposed a factorization of the joint distribution thatisintendedtocorrespondtothisconditionalindependencestructure.
However, we have not made any formal connection between conditional independence and factorization forundirectedgraphs.
Todosoweneedtorestrictattentiontopoten- tial functions ψC(x C) that are strictly positive (i.
e., never zero or negative for any 8.3.
Markov Random Fields 387 choice of x C).
Given this restriction, we can make a precise relationship between factorizationandconditionalindependence.
Todothisweagainreturntotheconceptofagraphicalmodelasafilter, corre- sponding to Figure 8.25.
Consider the set of all possible distributions defined over afixedsetofvariablescorrespondingtothenodesofaparticularundirectedgraph.
Wecandefine UI tobethesetofsuchdistributionsthatareconsistentwiththeset ofconditionalindependencestatementsthatcanbereadfromthegraphusinggraph separation.
Similarly, we candefine UF tobe theset of such distributions that can beexpressedasafactorizationoftheform(8.39)withrespecttothemaximalcliques ofthegraph.
The Hammersley-Clifford theorem(Clifford,1990)statesthatthesets UI and UF areidentical.
Because we are restricted to potential functions which are strictly positive it is convenienttoexpressthemasexponentials, sothat ψC(x C)=exp{−E(x C)} (8.41) where E(x C) is called an energy function, and the exponential representation is calledthe Boltzmanndistribution.
Thejointdistributionisdefinedastheproductof potentials, andsothetotalenergyisobtainedbyaddingtheenergiesofeachofthe maximalcliques.
In contrast to the factors in the joint distribution for a directed graph, the po- tentials in an undirected graph do not have a specific probabilistic interpretation.
Although this gives greater flexibility in choosing the potential functions, because there is no normalization constraint, it does raise the question of how to motivate a choiceofpotentialfunctionforaparticularapplication.
Thiscanbedonebyview- ing the potential function as expressing which configurations of the local variables arepreferredtoothers.
Globalconfigurationsthathavearelativelyhighprobability are those that find a good balance in satisfying the (possibly conflicting) influences of the clique potentials.
We turn now to a specific example to illustrate the use of undirectedgraphs.
8.3.3 Illustration: Image de-noising Wecanillustratetheapplicationofundirectedgraphsusinganexampleofnoise removalfromabinaryimage(Besag,1974; Gemanand Geman,1984; Besag,1986).
Although a very simple example, this is typical of more sophisticated applications.
Let the observed noisy image be described by an array of binary pixel values yi ∈ {−1,+1}, where the index i = 1,..., D runs over all pixels.
We shall suppose that the image is obtained by taking an unknown noise-free image, described by binary pixel values xi ∈ {−1,+1} and randomly flipping the sign of pixels with some small probability.
An example binary image, together with a noise corrupted imageobtainedbyflippingthesignofthepixelswithprobability10%, isshownin Figure 8.30.
Given the noisy image, our goal is to recover the original noise-free image.
Becausethenoiselevelissmall, weknowthattherewillbeastrongcorrelation between xi and yi.
We also know that neighbouring pixels xi and xj in an image are strongly correlated.
This prior knowledge can be captured using the Markov 388 8.
GRAPHICALMODELS Figure 8.30 Illustration of image de-noising using a Markov random field.
The top row shows the original binaryimageontheleftandthecorruptedimageafterrandomlychanging10%ofthepixelsontheright.
The bottom row shows the restored images obtained using iterated conditional models (ICM) on the left and using the graph-cut algorithm on the right.
ICM produces an image where 96% of the pixels agree with the original image, whereasthecorrespondingnumberforgraph-cutis99%.
randomfieldmodelwhoseundirectedgraphisshownin Figure8.31.
Thisgraphhas twotypesofcliques, eachofwhichcontainstwovariables.
Thecliquesoftheform {xi, yi } have an associated energy function that expresses the correlation between these variables.
We choose a very simple energy function for these cliques of the form −ηxiyi where η is a positive constant.
This has the desired effect of giving a lowerenergy(thusencouragingahigherprobability)whenxi andyi havethesame signandahigherenergywhentheyhavetheoppositesign.
The remaining cliques comprise pairs of variables {xi, xj } where i and j are indices of neighbouring pixels.
Again, we want the energy to be lower when the pixelshavethesamesignthanwhentheyhavetheoppositesign, andsowechoose anenergygivenby−βxixj whereβ isapositiveconstant.
Becauseapotentialfunctionisanarbitrary, nonnegativefunctionoveramaximal clique, we can multiply it by any nonnegative functions of subsets of the clique, or 8.3.
Markov Random Fields 389 Figure8.31 An undirected graphical model representing a Markov random field for image de-noising, in which x i is a binary variable denoting the state ofpixeliintheunknownnoise-freeimage, andy i denotes the corresponding value of pixel i in the yi observednoisyimage.
xi equivalentlywecanaddthecorrespondingenergies.
Inthisexample, thisallowsus to add an extra term hxi for each pixel i in the noise-free image.
Such a term has theeffectofbiasingthemodeltowardspixelvaluesthathaveoneparticularsignin preferencetotheother.
Thecompleteenergyfunctionforthemodelthentakestheform E(x, y)=h xi −β xixj −η xiyi (8.42) i {i, j} i whichdefinesajointdistributionoverxandygivenby 1 p(x, y)= exp{−E(x, y)}.
(8.43) Z Wenowfixthe elementsof y totheobserved valuesgivenbythepixels of the noisy image, which implicitly defines a conditional distribution p(x|y) over noise- freeimages.
Thisisanexampleofthe Isingmodel, whichhasbeenwidelystudiedin statisticalphysics.
Forthepurposesofimagerestoration, wewishtofindanimagex havingahighprobability(ideallythemaximumprobability).
Todothisweshalluse a simple iterative technique called iterated conditional modes, or ICM (Kittler and Fo¨glein, 1984), which is simply an application of coordinate-wise gradient ascent.
Theideaisfirsttoinitializethevariables{xi }, whichwedobysimplysettingxi = yi for all i.
Then we take one node xj at a time and we evaluate the total energy forthetwopossiblestatesxj = +1andxj = −1, keepingallothernodevariables fixed, and set xj to whichever state has the lower energy.
This will either leave the probability unchanged, if xj is unchanged, or will increase it.
Because only Exercise 8.13 one variable is changed, this is a simple local computation that can be performed efficiently.
Wethenrepeattheupdateforanothersite, andsoon, untilsomesuitable stopping criterion is satisfied.
The nodes may be updated in a systematic way, for instance by repeatedly raster scanning through the image, or by choosing nodes at random.
If we have a sequence of updates in which every site is visited at least once, andinwhichnochangestothevariablesaremade, thenbydefinitionthealgorithm 390 8.
GRAPHICALMODELS Figure8.32 (a) Example of a directed x1 x2 x N−1 x N graph.
(b) The equivalent undirected (a) graph.
x1 x2 x N x N−1 (b) willhaveconvergedtoalocalmaximumoftheprobability.
Thisneednot, however, correspondtotheglobalmaximum.
For the purposes of this simple illustration, we have fixed the parameters to be β = 1.0, η = 2.1andh = 0.
Notethatleavingh = 0simply meansthattheprior probabilitiesofthetwostatesofxiareequal.
Startingwiththeobservednoisyimage astheinitialconfiguration, werun ICMuntilconvergence, leadingtothede-noised image shown in the lower left panel of Figure 8.30.
Note that if we set β = 0, which effectively removes the links between neighbouring pixels, then the global most probable solution is given by xi = yi for all i, corresponding to the observed Exercise 8.14 noisyimage.
Laterweshalldiscussamoreeffectivealgorithmforfindinghighprobabilityso- Section8.4 lutions called the max-product algorithm, which typically leads to better solutions, althoughthisisstillnotguaranteedtofindtheglobalmaximumoftheposteriordis- tribution.
However, forcertainclassesofmodel, includingtheonegivenby(8.42), there exist efficient algorithms based on graph cuts that are guaranteed to find the global maximum (Greig et al., 1989; Boykov et al., 2001; Kolmogorov and Zabih, 2004).
Thelowerrightpanelof Figure8.30showstheresultofapplyingagraph-cut algorithmtothede-noisingproblem.
8.3.4 Relation to directed graphs Wehaveintroducedtwographicalframeworksforrepresentingprobabilitydis- tributions, corresponding to directed and undirected graphs, and it is instructive to discusstherelationbetweenthese.
Considerfirsttheproblemoftakingamodelthat isspecifiedusingadirectedgraphandtryingtoconvertittoanundirectedgraph.
In somecasesthisisstraightforward, asinthesimpleexamplein Figure8.32.
Herethe joint distribution for the directed graph is given as a product of conditionals in the form p(x)=p(x 1 )p(x 2 |x 1 )p(x 3 |x 2 )···p(x N |x N−1 ).
(8.44) Now let us convert this to an undirected graph representation, as shown in Fig- ure8.32.
Intheundirectedgraph, themaximalcliquesaresimplythepairsofneigh- bouringnodes, andsofrom(8.39)wewishtowritethejointdistributionintheform 1 p(x)= ψ 1,2 (x 1 , x 2 )ψ 2,3 (x 2 , x 3 )···ψN−1, N(x N−1 , x N).
(8.45) Z 8.3.
Markov Random Fields 391 Figure8.33 Example of a simple x1 x3 x1 x3 directed graph (a) and the corre- spondingmoralgraph(b).
x2 x2 x4 x4 (a) (b) Thisiseasilydonebyidentifying ψ 1,2 (x 1 , x 2 ) = p(x 1 )p(x 2 |x 1 ) ψ 2,3 (x 2 , x 3 ) = p(x 3 |x 2 ) .
.
.
ψN−1, N(x N−1 , x N) = p(x N |x N−1 ) wherewehaveabsorbedthemarginalp(x 1 )forthefirstnodeintothefirstpotential function.
Notethatinthiscase, thepartitionfunction Z =1.
Letusconsiderhowtogeneralizethisconstruction, sothatwecanconvertany distributionspecifiedbyafactorizationoveradirectedgraphintoonespecifiedbya factorizationoveranundirectedgraph.
Thiscanbeachievedifthecliquepotentials of the undirected graph are given by the conditional distributions of the directed graph.
In order for this to be valid, we must ensure that the set of variables that appearsineachoftheconditionaldistributionsisamemberofatleastonecliqueof theundirectedgraph.
Fornodesonthedirectedgraphhavingjustoneparent, thisis achievedsimplybyreplacingthedirectedlinkwithanundirectedlink.
However, for nodesinthedirectedgraphhavingmorethanoneparent, thisisnotsufficient.
These arenodesthathave‘head-to-head’pathsencounteredinourdiscussionofconditional independence.
Considerasimpledirectedgraphover4nodesshownin Figure8.33.
Thejointdistributionforthedirectedgraphtakestheform p(x)=p(x 1 )p(x 2 )p(x 3 )p(x 4 |x 1 , x 2 , x 3 ).
(8.46) We see that the factor p(x 4 |x 1 , x 2 , x 3 ) involves the four variables x 1, x 2, x 3, and x 4, and so these must all belong to a single clique if this conditional distribution is to be absorbed into a clique potential.
To ensure this, we add extra links between all pairs of parents of the node x 4.
Anachronistically, this process of ‘marrying theparents’hasbecomeknownasmoralization, andtheresultingundirectedgraph, afterdroppingthearrows, iscalledthemoralgraph.
Itisimportanttoobservethat the moral graph in this example is fully connected and so exhibits no conditional independenceproperties, incontrasttotheoriginaldirectedgraph.
Thusingeneraltoconvertadirectedgraphintoanundirectedgraph, wefirstadd additionalundirectedlinksbetweenallpairsofparentsforeachnodeinthegraphand 392 8.
GRAPHICALMODELS thendropthearrowsontheoriginallinkstogivethemoralgraph.
Thenweinitialize all of the clique potentials of the moral graph to 1.
We then take each conditional distributionfactorintheoriginaldirectedgraphandmultiplyitintooneoftheclique potentials.
There will always exist at least one maximal clique that contains all of thevariablesinthefactorasaresultofthemoralizationstep.
Notethatinallcases thepartitionfunctionisgivenby Z =1.
The process of converting a directed graph into an undirected graph plays an Section8.4 important role in exact inference techniques such as the junction tree algorithm.
Converting from an undirected to a directed representation is much less common andingeneralpresentsproblemsduetothenormalizationconstraints.
Wesawthatingoingfromadirectedtoanundirectedrepresentationwehadto discard some conditional independence properties from the graph.
Of course, we couldalwaystriviallyconvertanydistributionoveradirectedgraphintooneoveran undirected graph by simply using a fully connected undirected graph.
This would, however, discardallconditionalindependencepropertiesandsowouldbevacuous.
Theprocessofmoralizationaddsthefewestextralinksandsoretainsthemaximum numberofindependenceproperties.
We have seen that the procedure for determining the conditional independence properties is different between directed and undirected graphs.
It turns out that the two types of graph can express different conditional independence properties, and it is worth exploring this issue in more detail.
To do so, we return to the view of Section8.2 a specific (directed or undirected) graph as a filter, so that the set of all possible distributions over the given variables could be reduced to a subset that respects the conditional independencies implied by the graph.
A graph is said to be a D map (for‘dependencymap’)ofadistributionifeveryconditionalindependencestatement satisfiedbythedistributionisreflectedinthegraph.
Thusacompletelydisconnected graph(nolinks)willbeatrivial Dmapforanydistribution.
Alternatively, wecanconsideraspecificdistributionandaskwhichgraphshave the appropriate conditional independence properties.
If every conditional indepen- dence statement implied by a graph is satisfied by a specific distribution, then the graphissaidtobean Imap(for‘independencemap’)ofthatdistribution.
Clearlya fullyconnectedgraphwillbeatrivial Imapforanydistribution.
Ifitisthecasethateveryconditionalindependencepropertyofthedistribution isreflectedinthegraph, andviceversa, thenthegraphissaidtobeaperfectmapfor Figure8.34 Venndiagramillustratingthesetofalldistributions P over a given set of variables, together with the setofdistributions Dthatcanberepresentedasa perfectmapusingadirectedgraph, andtheset U thatcanberepresentedasaperfectmapusingan undirectedgraph.
D U P 8.4.
Inferencein Graphical Models 393 Figure8.35 A directed graph whose conditional independence A B properties cannot be expressed using an undirected graphoverthesamethreevariables.
C thatdistribution.
Aperfectmapisthereforebothan Imapanda Dmap.
Consider the set of distributions such that for each distribution there exists a directedgraphthatisaperfectmap.
Thissetisdistinctfromthesetofdistributions suchthatforeachdistributionthereexistsanundirectedgraphthatisaperfectmap.
In addition there are distributions for which neither directed nor undirected graphs offeraperfectmap.
Thisisillustratedasa Venndiagramin Figure8.34.
Figure 8.35 shows an example of a directed graph that is a perfect map for a distribution satisfying the conditional independence properties A ⊥⊥ B | ∅ and A⊥ ⊥ B | C.
There is no corresponding undirected graph over the same three vari- ablesthatisaperfectmap.
Conversely, consider the undirected graph over four variables shown in Fig- ure 8.36.
This graph exhibits the properties A⊥ ⊥ B | ∅, C ⊥⊥ D | A ∪ B and A⊥⊥B|C∪D.
Thereisnodirectedgraphoverfourvariablesthatimpliesthesame setofconditionalindependenceproperties.
The graphical framework can be extended in a consistent way to graphs that includebothdirectedandundirectedlinks.
Thesearecalledchaingraphs(Lauritzen and Wermuth, 1989; Frydenberg, 1990), and contain the directed and undirected graphs considered so far as special cases.
Although such graphs can represent a broader class of distributions than either directed or undirected alone, there remain distributions for which even a chain graph cannot provide a perfect map.
Chain graphsarenotdiscussedfurtherinthisbook.
Figure8.36 An undirected graph whose conditional independence C properties cannot be expressed in terms of a directed graphoverthesamevariables.
A B D 8.4.
Inference in Graphical Models We turn now to the problem of inference in graphical models, in which some of the nodes in a graph are clamped to observed values, and we wish to compute the posterior distributions of one or more subsets of other nodes.
As we shall see, we canexploitthegraphicalstructurebothtofindefficientalgorithmsforinference, and 394 8.
GRAPHICALMODELS Figure8.37 A graphical representation of Bayes’ theorem.
x x x Seethetextfordetails.
y y y (a) (b) (c) tomakethestructureofthosealgorithmstransparent.
Specifically, weshallseethat many algorithms can be expressed in terms of the propagation of local messages around the graph.
In this section, we shall focus primarily on techniques for exact inference, and in Chapter 10 we shall consider a number of approximate inference algorithms.
To start with, let us consider the graphical interpretation of Bayes’ theorem.
Supposewedecomposethejointdistributionp(x, y)overtwovariablesxandyinto a product of factors in the form p(x, y) = p(x)p(y|x).
This can be represented by the directed graph shown in Figure 8.37(a).
Now suppose we observe the value of y, as indicated by the shaded node in Figure 8.37(b).
We can view the marginal distribution p(x) as a prior over the latent variable x, and our goal is to infer the corresponding posterior distribution over x.
Using the sum and product rules of probabilitywecanevaluate p(y)= p(y|x )p(x ) (8.47) x whichcanthenbeusedin Bayes’theoremtocalculate p(y|x)p(x) p(x|y)= .
(8.48) p(y) Thus the joint distribution is now expressed in terms of p(y) and p(x|y).
From a graphicalperspective, thejointdistributionp(x, y)isnowrepresentedbythegraph shownin Figure8.37(c), inwhichthedirectionofthearrowisreversed.
Thisisthe simplestexampleofaninferenceproblemforagraphicalmodel.
8.4.1 Inference on a chain Nowconsideramorecomplexprobleminvolvingthechainofnodesoftheform shownin Figure8.32.
Thisexamplewilllaythefoundationforadiscussionofexact inferenceinmoregeneralgraphslaterinthissection.
Specifically, weshallconsidertheundirectedgraphin Figure8.32(b).
Wehave alreadyseenthatthedirectedchaincanbetransformedintoanequivalentundirected chain.
Because the directed graph does not have any nodes with more than one parent, this does not require the addition of any extra links, and the directed and undirected versions of this graph express exactly the same set of conditional inde- pendencestatements.
8.4.
Inferencein Graphical Models 395 Thejointdistributionforthisgraphtakestheform 1 p(x)= ψ 1,2 (x 1 , x 2 )ψ 2,3 (x 2 , x 3 )···ψN−1, N(x N−1 , x N).
(8.49) Z We shall consider the specific case in which the N nodes represent discrete vari- ableseachhaving K states, inwhichcaseeachpotentialfunctionψn−1, n(xn−1 , xn) comprisesan K×K table, andsothejointdistributionhas(N −1)K2 parameters.
Letusconsidertheinferenceproblemoffindingthemarginaldistributionp(xn) for a specific node xn that is part way along the chain.
Note that, for the moment, there are no observed nodes.
By definition, the required marginal is obtained by summingthejointdistributionoverallvariablesexceptxn, sothat p(xn)= ··· ··· p(x).
(8.50) x x x x 1 n−1 n+1 N In a naive implementation, we would first evaluate the joint distribution and thenperformthesummationsexplicitly.
Thejointdistributioncanberepresentedas a set of numbers, one for each possible value for x.
Because there are N variables eachwith K states, thereare KN valuesforxandsoevaluationandstorageofthe jointdistribution, aswellasmarginalizationtoobtainp(xn), allinvolvestorageand computationthatscaleexponentiallywiththelength N ofthechain.
Wecan, however, obtainamuchmoreefficientalgorithmbyexploitingthecon- ditionalindependencepropertiesofthegraphicalmodel.
Ifwesubstitutethefactor- izedexpression(8.49)forthejointdistributioninto(8.50), thenwecanrearrangethe orderofthesummationsandthemultiplicationstoallowtherequiredmarginaltobe evaluatedmuchmoreefficiently.
Considerforinstancethesummationoverx N.
The potential ψN−1, N(x N−1 , x N) is the only one that depends on x N, and so we can performthesummation ψN−1, N(x N−1 , x N) (8.51) x N first to give a function of x N−1.
We can then use this to perform the summation over x N−1, which will involve only this new function together with the potential ψN−2, N−1 (x N−2 , x N−1 ), because this is the only other place that x N−1 appears.
Similarly, the summation over x 1 involves only the potential ψ 1,2 (x 1 , x 2 ) and so can be performed separately to give a function of x 2, and so on.
Because each summation effectively removes a variable from the distribution, this can be viewed astheremovalofanodefromthegraph.
Ifwegroupthepotentialsandsummationstogetherinthisway, wecanexpress 396 8.
GRAPHICALMODELS thedesiredmarginalintheform 1 p(xn)= ⎡ Z ⎤ ⎣ ψn−1, n(xn−1 , xn)··· ψ 2,3 (x 2 , x 3 ) ψ 1,2 (x 1 , x 2 ) ···⎦ x x x ( n−1 2 )* 1 + ⎡ µα(xn) ⎤ ⎣ ψn, n+1 (xn, xn+1 )··· ψN−1, N(x N−1 , x N) ···⎦ .
(8.52) x x ( n+1 )*N + µβ(xn) The reader is encouraged to study this re-ordering carefully as the underlying idea formsthebasisforthelaterdiscussionofthegeneralsum-productalgorithm.
Here thekeyconceptthatweareexploitingisthatmultiplicationisdistributiveoveraddi- tion, sothat ab+ac=a(b+c) (8.53) in which the left-hand side involves three arithmetic operations whereas the right- handsidereducesthistotwooperations.
Letusworkoutthecomputationalcostofevaluatingtherequiredmarginalusing thisre-orderedexpression.
Wehavetoperform N −1summationseachofwhichis over K statesandeachofwhichinvolvesafunctionoftwovariables.
Forinstance, the summation over x 1 involves only the function ψ 1,2 (x 1 , x 2 ), which is a table of K ×K numbers.
Wehavetosumthistableoverx 1 foreachvalueofx 2 andsothis has O(K2) cost.
The resulting vector of K numbers is multiplied by the matrix of numbersψ 2,3 (x 2 , x 3 )andsoisagain O(K2).
Becausethereare N −1summations and multiplications of this kind, the total cost of evaluating the marginal p(xn) is O(NK2).
Thisislinearinthelengthofthechain, incontrasttotheexponentialcost of a naive approach.
We have therefore been able to exploit the many conditional independencepropertiesofthissimplegraphinordertoobtainanefficientcalcula- tion.
If the graph had been fully connected, there would have been no conditional independence properties, and we would have been forced to work directly with the fulljointdistribution.
Wenowgiveapowerfulinterpretationofthiscalculationintermsofthepassing oflocalmessagesaroundonthegraph.
From(8.52)weseethattheexpressionforthe marginalp(xn)decomposesintotheproductoftwofactorstimesthenormalization constant 1 p(xn)= µα(xn)µβ(xn).
(8.54) Z Weshallinterpretµα(xn)asamessagepassedforwardsalongthechainfromnode xn−1 tonodexn.
Similarly,µβ(xn)canbeviewedasamessagepassedbackwards 8.4.
Inferencein Graphical Models 397 Figure8.38 The marginal distribution µα(xn−1) µα(xn) µβ(xn) µβ(xn+1) p(x n )foranodex nalongthechainisob- tained by multiplying the two messages µ α (x n ) and µ β (x n ), and then normaliz- ing.
These messages can themselves x1 xn−1 xn xn+1 x N beevaluatedrecursivelybypassingmes- sages from both ends of the chain to- wardsnodex n.
along the chain to node xn from node xn+1.
Note that each of the messages com- prisesasetof K values, oneforeachchoiceofxn, andsotheproductoftwomes- sages should be interpreted as the point-wise multiplication of the elements of the twomessagestogiveanothersetof K values.
Themessageµα(xn)canbeevaluatedrecursivelybecause ⎡ ⎤ µα(xn) = ψn−1, n(xn−1 , xn) ⎣ ···⎦ x x n−1 n−2 = ψn−1, n(xn−1 , xn)µα(xn−1 ).
(8.55) x n−1 Wethereforefirstevaluate µα(x 2 )= ψ 1,2 (x 1 , x 2 ) (8.56) x 1 andthenapply(8.55)repeatedlyuntilwereachthedesirednode.
Notecarefullythe structureofthemessagepassingequation.
Theoutgoingmessageµα(xn)in(8.55) is obtained by multiplying the incoming message µα(xn−1 ) by the local potential involving the node variable and the outgoing variable and then summing over the nodevariable.
Similarly, the message µβ(xn) can be evaluated recursively by starting with nodex N andusing ⎡ ⎤ µβ(xn) = ψn+1, n(xn+1 , xn) ⎣ ···⎦ x x n+1 n+2 = ψn+1, n(xn+1 , xn)µβ(xn+1 ).
(8.57) x n+1 Thisrecursivemessagepassingisillustratedin Figure8.38.
Thenormalizationcon- stant Z is easily evaluated by summing the right-hand side of (8.54) over all states ofxn, anoperationthatrequiresonly O(K)computation.
Graphs of the form shown in Figure 8.38 are called Markov chains, and the corresponding message passing equations represent an example of the Chapman- Kolmogorovequationsfor Markovprocesses(Papoulis,1984).
398 8.
GRAPHICALMODELS Now suppose we wish to evaluate the marginals p(xn) for every node n ∈ {1,..., N} in the chain.
Simply applying the above procedure separately for each node will have computational cost that is O(N2M2).
However, such an approach wouldbeverywastefulofcomputation.
Forinstance, tofindp(x 1 )weneedtoprop- agateamessageµβ(·)fromnodex N backtonodex 2.
Similarly, toevaluatep(x 2 ) we need to propagate a messages µβ(·) from node x N back to node x 3.
This will involvemuchduplicatedcomputationbecausemostofthemessageswillbeidentical inthetwocases.
Suppose instead we first launch a message µβ(x N−1 ) starting from node x N andpropagatecorrespondingmessagesallthewaybacktonodex 1, andsupposewe similarly launch a message µα(x 2 ) starting from node x 1 and propagate the corre- sponding messages all the way forward to node x N.
Provided we store all of the intermediate messages along the way, then any node can evaluate its marginal sim- ply by applying (8.54).
The computational cost is only twice that for finding the marginal of a single node, rather than N times as much.
Observe that a message has passed once in each direction across each link in the graph.
Note also that the normalizationconstant Z needbeevaluatedonlyonce, usinganyconvenientnode.
Ifsomeofthenodesinthegraphareobserved, thenthecorrespondingvariables are simply clamped to their observed values and there is no summation.
To see this, note that the effect of clamping a variable xn to an observed value xn can be expressed by multiplying the joint distribution by (one or more copies of) an additionalfunction I(xn, xn), whichtakesthevalue1whenxn = xn andthevalue 0otherwise.
Onesuchfunctioncanthenbeabsorbedintoeachofthepotentialsthat containxn.
Summationsoverxn thencontainonlyoneterminwhichxn = xn.
Now suppose we wish to calculate the joint distribution p(xn−1 , xn) for two neighbouring nodes on the chain.
This is similar to the evaluation of the marginal for a single node, except that there are now two variables that are not summed out.
Exercise 8.15 Afewmomentsthoughtwillshowthattherequiredjointdistributioncanbewritten intheform 1 p(xn−1 , xn)= µα(xn−1 )ψn−1, n(xn−1 , xn)µβ(xn).
(8.58) Z Thus we can obtain the joint distributions over all of the sets of variables in each of the potentials directly once we have completed the message passing required to obtainthemarginals.
Thisisausefulresultbecauseinpracticewemaywishtouseparametricforms forthecliquepotentials, orequivalentlyfortheconditionaldistributionsifwestarted from a directed graph.
In order to learn the parameters of these potentials in situa- Chapter9 tionswherenotallofthevariablesareobserved, wecanemploythe EMalgorithm, and it turns out that the local joint distributions of the cliques, conditioned on any observed data, is precisely what is needed in the E step.
We shall consider some examplesofthisindetailin Chapter13.
8.4.2 Trees Wehaveseenthatexactinferenceonagraphcomprisingachainofnodescanbe performedefficientlyintimethatislinearinthenumberofnodes, usinganalgorithm 8.4.
Inferencein Graphical Models 399 Figure8.39 Examples of tree- structured graphs, showing (a) an undirected tree, (b) a directed tree, and(c)adirectedpolytree.
(a) (b) (c) thatcanbeinterpretedintermsofmessagespassedalongthechain.
Moregenerally, inference can be performed efficiently using local message passing on a broader class of graphs called trees.
In particular, we shall shortly generalize the message passingformalismderivedaboveforchainstogivethesum-productalgorithm, which providesanefficientframeworkforexactinferenceintree-structuredgraphs.
In the case of an undirected graph, a tree is defined as a graph in which there isone, andonlyone, pathbetweenanypairofnodes.
Suchgraphsthereforedonot haveloops.
Inthecaseofdirectedgraphs, atreeisdefinedsuchthatthereisasingle node, calledtheroot, whichhasnoparents, andallothernodeshaveoneparent.
If weconvertadirectedtreeintoanundirectedgraph, weseethatthemoralizationstep willnotaddanylinksasallnodeshaveatmostoneparent, andasaconsequencethe corresponding moralized graph will be an undirected tree.
Examples of undirected anddirectedtreesareshownin Figure8.39(a)and8.39(b).
Notethatadistribution represented as a directed tree can easily be converted into one represented by an Exercise 8.18 undirectedtree, andviceversa.
Iftherearenodesinadirectedgraphthathavemorethanoneparent, butthereis stillonlyonepath(ignoringthedirectionofthearrows)betweenanytwonodes, then the graph is a called a polytree, as illustrated in Figure 8.39(c).
Such a graph will have more than one node with the property of having no parents, and furthermore, thecorrespondingmoralizedundirectedgraphwillhaveloops.
8.4.3 Factor graphs The sum-product algorithm that we derive in the next section is applicable to undirectedanddirectedtreesandtopolytrees.
Itcanbecastinaparticularlysimple and general form if we first introduce a new graphical construction called a factor graph(Frey,1998; Kschischnangetal.,2001).
Both directed and undirected graphs allow a global function of several vari- ablestobeexpressedasaproductoffactorsoversubsetsofthosevariables.
Factor graphsmakethisdecompositionexplicitbyintroducingadditionalnodesforthefac- torsthemselvesinadditiontothenodesrepresentingthevariables.
Theyalsoallow ustobemoreexplicitaboutthedetailsofthefactorization, asweshallsee.
Letuswritethejointdistributionoverasetofvariablesintheformofaproduct offactors p(x)= fs(xs) (8.59) s where xs denotes a subset of the variables.
For convenience, we shall denote the 400 8.
GRAPHICALMODELS Figure8.40 Exampleofafactorgraph, whichcorresponds x1 x2 x3 tothefactorization(8.60).
fa fb fc fd individual variables by xi, however, as in earlier discussions, these can comprise groups of variables (such as vectors or matrices).
Each factor fs is a function of a correspondingsetofvariablesxs.
Directedgraphs, whosefactorizationisdefinedby(8.5), representspecialcases of (8.59) in which the factors fs(xs) are local conditional distributions.
Similarly, undirected graphs, given by (8.39), are a special case in which the factors are po- tential functions over the maximal cliques (the normalizing coefficient 1/Z can be viewedasafactordefinedovertheemptysetofvariables).
Inafactorgraph, thereisanode(depictedasusualbyacircle)foreveryvariable inthedistribution, aswasthecasefordirectedandundirectedgraphs.
Therearealso additionalnodes(depictedbysmallsquares)foreachfactorfs(xs)inthejointdis- tribution.
Finally, thereareundirectedlinksconnectingeachfactornodetoallofthe variablesnodesonwhichthatfactordepends.
Consider, forexample, adistribution thatisexpressedintermsofthefactorization p(x)=fa(x 1 , x 2 )fb(x 1 , x 2 )fc(x 2 , x 3 )fd(x 3 ).
(8.60) Thiscanbeexpressedbythefactorgraphshownin Figure8.40.
Notethatthereare twofactorsfa(x 1 , x 2 )andfb(x 1 , x 2 )thataredefinedoverthesamesetofvariables.
In an undirected graph, the product of two such factors would simply be lumped together into the same clique potential.
Similarly, fc(x 2 , x 3 ) and fd(x 3 ) could be combined into a single potential over x 2 and x 3.
The factor graph, however, keeps such factors explicit and so is able to convey more detailed information about the underlyingfactorization.
x1 x2 x1 x2 x1 x2 f fa fb x3 x3 x3 (a) (b) (c) Figure8.41 (a)Anundirectedgraphwithasinglecliquepotentialψ(x 1 , x 2 , x 3 ).
(b)Afactorgraphwithfactor f(x 1 , x 2 , x 3 ) = ψ(x 1 , x 2 , x 3 ) representing the same distribution as the undirected graph.
(c) A different factor graphrepresentingthesamedistribution, whosefactorssatisfyf a (x 1 , x 2 , x 3 )f b (x 1 , x 2 )=ψ(x 1 , x 2 , x 3 ).
8.4.
Inferencein Graphical Models 401 x1 x2 x1 x2 x1 x2 f fc fa fb x3 x3 x3 (a) (b) (c) Figure8.42 (a)Adirectedgraphwiththefactorizationp(x 1 )p(x 2 )p(x 3 |x 1 , x 2 ).
(b)Afactorgraphrepresenting the same distribution as the directed graph, whose factor satisfies f(x 1 , x 2 , x 3 ) = p(x 1 )p(x 2 )p(x 3 |x 1 , x 2 ).
(c) A different factor graph representing the same distribution with factors f a (x 1 ) = p(x 1 ), f b (x 2 ) = p(x 2 ) and f c (x 1 , x 2 , x 3 )=p(x 3 |x 1 , x 2 ).
Factorgraphsaresaidtobebipartitebecausetheyconsistoftwodistinctkinds ofnodes, andalllinksgobetweennodesofoppositetype.
Ingeneral, factorgraphs can therefore always be drawn as two rows of nodes (variable nodes at the top and factornodesatthebottom)withlinksbetweentherows, asshownintheexamplein Figure 8.40.
In some situations, however, other ways of laying out the graph may be more intuitive, for example when the factor graph is derived from a directed or undirectedgraph, asweshallsee.
Ifwearegivenadistributionthatisexpressedintermsofanundirectedgraph, thenwecanreadilyconvertittoafactorgraph.
Todothis, wecreatevariablenodes corresponding to the nodes in the original undirected graph, and then create addi- tionalfactornodescorrespondingtothemaximalcliquesxs.
Thefactorsfs(xs)are thensetequaltothecliquepotentials.
Notethattheremaybeseveraldifferentfactor graphsthatcorrespondtothesameundirectedgraph.
Theseconceptsareillustrated in Figure8.41.
Similarly, toconvertadirectedgraphtoafactorgraph, wesimplycreatevariable nodesinthefactorgraphcorrespondingtothenodesofthedirectedgraph, andthen create factor nodes corresponding to the conditional distributions, and then finally add the appropriate links.
Again, there can be multiple factor graphs all of which correspond to the same directed graph.
The conversion of a directed graph to a factorgraphisillustratedin Figure8.42.
Wehavealreadynotedtheimportanceoftree-structuredgraphsforperforming efficientinference.
Ifwetakeadirectedorundirectedtreeandconvertitintoafactor graph, thentheresultwillagainbeatree(inotherwords, thefactorgraphwillhave no loops, and there will be one and only one path connecting any two nodes).
In the case of a directed polytree, conversion to an undirected graph results in loops duetothemoralizationstep, whereasconversiontoafactorgraphagainresultsina tree, as illustrated in Figure 8.43.
In fact, local cycles in a directed graph due to links connecting parents of a node can be removed on conversion to a factor graph bydefiningtheappropriatefactorfunction, asshownin Figure8.44.
We have seen that multiple different factor graphs can represent the same di- rected or undirected graph.
This allows factor graphs to be more specific about the 402 8.
GRAPHICALMODELS (a) (b) (c) Figure8.43 (a)Adirectedpolytree.
(b)Theresultofconvertingthepolytreeintoanundirectedgraphshowing thecreationofloops.(c)Theresultofconvertingthepolytreeintoafactorgraph, whichretainsthetreestructure.
preciseformofthefactorization.
Figure8.45showsanexampleofafullyconnected undirected graph along with two different factor graphs.
In (b), the joint distri- bution is given by a general form p(x) = f(x 1 , x 2 , x 3 ), whereas in (c), it is given bythemorespecificfactorizationp(x) = fa(x 1 , x 2 )fb(x 1 , x 3 )fc(x 2 , x 3 ).
Itshould be emphasized that the factorization in (c) does not correspond to any conditional independenceproperties.
8.4.4 The sum-product algorithm Weshallnowmakeuseofthefactorgraphframeworktoderiveapowerfulclass ofefficient, exactinferencealgorithmsthatareapplicabletotree-structuredgraphs.
Here we shall focus on the problem of evaluating local marginals over nodes or subsets of nodes, which will lead us to the sum-product algorithm.
Later we shall modifythetechniquetoallowthemostprobablestatetobefound, givingrisetothe max-sumalgorithm.
Also we shall suppose that all of the variables in the model are discrete, and so marginalization corresponds to performing sums.
The framework, however, is equallyapplicabletolinear-Gaussianmodelsinwhichcasemarginalizationinvolves integration, andweshallconsideranexampleofthisindetailwhenwediscusslinear Section13.3 dynamicalsystems.
Figure8.44 (a) A fragment of a di- x1 x2 x1 x2 rected graph having a lo- cal cycle.
(b) Conversion to a fragment of a factor graph having a tree struc- f(x1, x2, x3) ture, inwhichf(x 1 , x 2 , x 3 )= p(x 1 )p(x 2 |x 1 )p(x 3 |x 1 , x 2 ).
x3 x3 (a) (b) 8.4.
Inferencein Graphical Models 403 x1 x2 x1 x2 x1 fa x2 f(x1, x2, x3) fb fc x3 x3 x3 (a) (b) (c) Figure8.45 (a)Afullyconnectedundirectedgraph.
(b)and(c)Twofactorgraphseachofwhichcorresponds totheundirectedgraphin(a).
Thereisanalgorithmforexactinferenceondirectedgraphswithoutloopsknown asbeliefpropagation(Pearl,1988; Lauritzenand Spiegelhalter,1988), andisequiv- alenttoaspecialcaseofthesum-productalgorithm.
Hereweshallconsideronlythe sum-productalgorithmbecauseitissimplertoderiveandtoapply, aswellasbeing moregeneral.
Weshallassumethattheoriginalgraphisanundirectedtreeoradirectedtreeor polytree, sothatthecorrespondingfactorgraphhasatreestructure.
Wefirstconvert the original graph into a factor graph so that we can deal with both directed and undirectedmodelsusingthesameframework.
Ourgoalistoexploitthestructureof thegraphtoachievetwothings: (i)toobtainanefficient, exactinferencealgorithm forfindingmarginals;(ii)insituationswhereseveralmarginalsarerequiredtoallow computationstobesharedefficiently.
We begin by considering the problem of finding the marginal p(x) for partic- ular variable node x.
For the moment, we shall suppose that all of the variables arehidden.
Laterweshallseehowtomodifythealgorithmtoincorporateevidence correspondingtoobservedvariables.
Bydefinition, themarginalisobtainedbysum- mingthejointdistributionoverallvariablesexceptxsothat p(x)= p(x) (8.61) x\x where x\x denotes the set of variables in x with variable x omitted.
The idea is tosubstituteforp(x)usingthefactorgraphexpression(8.59)andtheninterchange summations and products in order to obtain an efficient algorithm.
Consider the fragment of graph shown in Figure 8.46 in which we see that the tree structure of thegraphallowsustopartitionthefactorsinthejointdistributionintogroups, with onegroupassociatedwitheachofthefactornodesthatisaneighbourofthevariable nodex.
Weseethatthejointdistributioncanbewrittenasaproductoftheform p(x)= Fs(x, Xs) (8.62) s∈ne(x) ne(x) denotes the set of factor nodes that are neighbours of x, and Xs denotes the setofallvariablesinthesubtreeconnectedtothevariablenodexviathefactornode 404 8.
GRAPHICALMODELS Figure8.46 Afragmentofafactorgraphillustratingthe evaluationofthemarginalp(x).
µfs→x(x) fs x )s X, x(s F fs, and Fs(x, Xs) represents the product of all the factors in the group associated withfactorfs.
Substituting(8.62)into(8.61)andinterchangingthesumsandproducts, weob- tain p(x) = Fs(x, Xs) s∈ ne(x) X s = µf →x(x).
(8.63) s s∈ne(x) Herewehaveintroducedasetoffunctionsµf →x(x), definedby s µf →x(x)≡ Fs(x, Xs) (8.64) s X s which can be viewed as messages from the factor nodes fs to the variable node x.
We see that the required marginal p(x) is given by the product of all the incoming messagesarrivingatnodex.
Inordertoevaluatethesemessages, weagainturnto Figure8.46andnotethat each factor Fs(x, Xs) is described by a factor (sub-)graph and so can itself be fac- torized.
Inparticular, wecanwrite where, forconvenience, wehavedenotedthevariablesassociatedwithfactorfx, in that the set of variables {x, x 1 ,..., x M } is the set of variables on which the factor fs depends, andsoitcanalsobedenotedxs, usingthenotationof(8.59).
Substituting(8.65)into(8.64)weobtain µf s →x(x) = ...
fs(x, x 1 ,..., x M) Gm(xm, Xsm) x 1 x M m∈n e(f s)\x X xm x 1 x M m∈ne(f s)\x 8.4.
Inferencein Graphical Models 405 Figure8.47 Illustrationofthefactorizationofthesubgraphas- x M sociatedwithfactornodef s.
µx M→fs (x M) fs x µfs→x(x) xm Gm(xm, Xsm) wherene(fs)denotesthesetofvariablenodesthatareneighboursofthefactornode fs, and ne(fs)\x denotes the same set but with node x removed.
Here we have definedthefollowingmessagesfromvariablenodestofactornodes µx →f (xm)≡ Gm(xm, Xsm).
(8.67) m s X sm Wehavethereforeintroducedtwodistinctkindsofmessage, thosethatgofromfactor nodestovariablenodesdenotedµf→x(x), andthosethatgofromvariablenodesto factor nodes denoted µx→f(x).
In each case, we see that messages passed along a linkarealwaysafunctionofthevariableassociatedwiththevariablenodethatlink connectsto.
Theresult(8.66)saysthattoevaluatethemessagesentbyafactornodetoavari- ablenodealongthelinkconnectingthem, taketheproductoftheincomingmessages along all other links coming into the factor node, multiply by the factor associated with that node, and then marginalize over all of the variables associated with the incoming messages.
This is illustrated in Figure 8.47.
It is important to note that a factor node can send a message to a variable node once it has received incoming messagesfromallotherneighbouringvariablenodes.
Finally, wederiveanexpressionforevaluatingthemessagesfromvariablenodes to factor nodes, again by making use of the (sub-)graph factorization.
From Fig- ure 8.48, we see that term Gm(xm, Xsm) associated with node xm is given by a productofterms Fl(xm, Xml)eachassociatedwithoneofthefactornodesflthatis linkedtonodexm (excludingnodefs), sothat Gm(xm, Xsm)= Fl(xm, Xml) (8.68) l∈ne(x m)\f s wheretheproductistakenoverallneighboursofnodexm exceptfornodefs.
Note that each of the factors Fl(xm, Xml) represents a subtree of the original graph of precisely the same kind as introduced in (8.62).
Substituting (8.68) into (8.67), we 406 8.
GRAPHICALMODELS Figure8.48 Illustration of the evaluation of the message sent by a f L variablenodetoanadjacentfactornode.
xm fs fl Fl(xm, Xml) thenobtain µx →f (xm) = Fl(xm, Xml) m s l∈ne( x m)\f s X ml = µf →x (xm) (8.69) l m l∈ne(x m)\f s wherewehaveusedthedefinition(8.64)ofthemessagespassedfromfactornodesto variablenodes.
Thustoevaluatethemessagesentbyavariablenodetoanadjacent factor node along the connecting link, we simply take the product of the incoming messages along all of the other links.
Note that any variable node that has only two neighbours performs no computation but simply passes messages through un- changed.
Also, we note that a variable node can send a message to a factor node onceithasreceivedincomingmessagesfromallotherneighbouringfactornodes.
Recallthatourgoalistocalculatethemarginalforvariablenodex, andthatthis marginalisgivenbytheproductofincomingmessagesalongallofthelinksarriving atthatnode.
Eachofthesemessagescanbecomputedrecursivelyintermsofother messages.
Inordertostartthisrecursion, wecanviewthenodexastherootofthe treeandbeginattheleafnodes.
Fromthedefinition(8.69), weseethatifaleafnode isavariablenode, thenthemessagethatitsendsalongitsoneandonlylinkisgiven by µx→f(x)=1 (8.70) as illustrated in Figure 8.49(a).
Similarly, if the leaf node is a factor node, we see from(8.66)thatthemessagesentshouldtaketheform µf→x(x)=f(x) (8.71) Figure8.49 The sum-product algorithm µx→f(x)=1 µf→x(x)=f(x) begins with messages sent bytheleafnodes, whichde- pend on whether the leaf x f f x node is (a) a variable node, (a) (b) or(b)afactornode.
8.4.
Inferencein Graphical Models 407 asillustratedin Figure8.49(b).
Atthispoint, itisworthpausingtosummarizetheparticularversionofthesum- product algorithm obtained so far for evaluating the marginal p(x).
We start by viewing the variable node x as the root of the factor graph and initiating messages attheleavesofthegraphusing(8.70)and(8.71).
Themessagepassingsteps(8.66) and (8.69) are then applied recursively until messages have been propagated along everylink, andtherootnodehasreceivedmessagesfromallofitsneighbours.
Each node can send a message towards the root once it has received messages from all of its other neighbours.
Once the root node has received messages from all of its neighbours, therequiredmarginalcanbeevaluatedusing(8.63).
Weshallillustrate thisprocessshortly.
Toseethateachnodewillalwaysreceiveenoughmessagestobeabletosendout amessage, wecanuseasimpleinductiveargumentasfollows.
Clearly, foragraph comprising a variable root node connected directly to several factor leaf nodes, the algorithm trivially involves sending messages of the form (8.71) directly from the leavestotheroot.
Nowimaginebuildingupageneralgraphbyaddingnodesoneat atime, andsupposethatforsomeparticulargraphwehaveavalidalgorithm.
When one more (variable or factor) node is added, it can be connected only by a single linkbecausetheoverallgraphmustremainatree, andsothenewnodewillbealeaf node.
It therefore sends a message to the node to which it is linked, which in turn will therefore receive all the messages it requires in order to send its own message towards the root, and so again we have a valid algorithm, thereby completing the proof.
Nowsupposewewishtofindthemarginalsforeveryvariablenodeinthegraph.
Thiscouldbedonebysimplyrunningtheabovealgorithmafreshforeachsuchnode.
However, thiswouldbeverywastefulasmanyoftherequiredcomputationswould be repeated.
We can obtain a much more efficient procedure by ‘overlaying’ these multiple message passing algorithms to obtain the general sum-product algorithm as follows.
Arbitrarily pick any (variable or factor) node and designate it as the root.
Propagate messages from the leaves to the root as before.
At this point, the root node will have received messages from all of its neighbours.
It can therefore send out messages to all of its neighbours.
These in turn will then have received messagesfromalloftheirneighboursandsocansendoutmessagesalongthelinks going away from the root, and so on.
In this way, messages are passed outwards from the root all the way to the leaves.
By now, a message will have passed in both directions across every link in the graph, and every node will have received a message from all of its neighbours.
Again a simple inductive argument can be Exercise 8.20 usedtoverifythevalidityofthismessagepassingprotocol.
Becauseeveryvariable nodewillhavereceivedmessagesfromallofitsneighbours, wecanreadilycalculate the marginal distribution for every variable in the graph.
The number of messages that have to be computed is given by twice the number of links in the graph and so involves only twice the computation involved in finding a single marginal.
By comparison, ifwehadrunthesum-product algorithm separatelyfor eachnode, the amount of computation would grow quadratically with the size of the graph.
Note thatthisalgorithmisinfactindependentofwhichnodewasdesignatedastheroot, 408 8.
GRAPHICALMODELS Figure8.50 The sum-product algorithm can be viewed purely in terms of messages sent out by factor nodes to other factor nodes.
In this example, theoutgoingmessageshownbythebluearrow is obtained by taking the product of all the in- x1 comingmessagesshownbygreenarrows, mul- tiplyingbythefactorf s, andmarginalizingover x3 thevariablesx 1andx 2.
x2 fs and indeed the notion of one node having a special status was introduced only as a convenientwaytoexplainthemessagepassingprotocol.
Next suppose we wish to find the marginal distributions p(xs) associated with thesetsofvariablesbelongingtoeachofthefactors.
Byasimilarargumenttothat Exercise 8.21 usedabove, itiseasytoseethatthemarginalassociatedwithafactorisgivenbythe productofmessagesarrivingatthefactornodeandthelocalfactoratthatnode p(xs)=fs(xs) µx →f (xi) (8.72) i s i∈ne(f s) in complete analogy with the marginals at the variable nodes.
If the factors are parameterized functions and we wish to learn the values of the parameters using the EMalgorithm, thenthesemarginalsarepreciselythequantitieswewillneedto calculateinthe Estep, asweshallseeindetailwhenwediscussthehidden Markov modelin Chapter13.
Themessagesentbyavariablenodetoafactornode, aswehaveseen, issimply the product of the incoming messages on other links.
We can if we wish view the sum-product algorithm in a slightly different form by eliminating messages from variablenodestofactornodesandsimplyconsideringmessagesthataresentoutby factornodes.
Thisismosteasilyseenbyconsideringtheexamplein Figure8.50.
So far, we have rather neglected the issue of normalization.
If the factor graph wasderivedfromadirectedgraph, thenthejointdistributionisalreadycorrectlynor- malized, andsothemarginalsobtainedbythesum-productalgorithmwillsimilarly be normalized correctly.
However, if we started from an undirected graph, then in generaltherewillbeanunknownnormalizationcoefficient1/Z.
Aswiththesimple chain example of Figure 8.38, this is easily handled by working with an unnormal- ized version p(x) of the joint distribution, where p(x) = p(x)/Z.
We first run the sum-productalgorithmtofindthecorrespondingunnormalizedmarginals p(xi).
The coefficient 1/Z is then easily obtained by normalizing any one of these marginals, andthisiscomputationallyefficientbecausethenormalizationisdoneoverasingle variableratherthanovertheentiresetofvariablesaswouldberequiredtonormalize p(x)directly.
At this point, it may be helpful to consider a simple example to illustrate the operation of the sum-product algorithm.
Figure 8.51 shows a simple 4-node factor 8.4.
Inferencein Graphical Models 409 Figure8.51 Asimplefactorgraphusedtoillustratethe x1 x2 x3 sum-productalgorithm.
fa fb fc x4 graphwhoseunnormalizedjointdistributionisgivenby p(x)=fa(x 1 , x 2 )fb(x 2 , x 3 )fc(x 2 , x 4 ).
(8.73) Inordertoapplythesum-productalgorithmtothisgraph, letusdesignatenodex 3 astheroot, inwhichcasetherearetwoleafnodesx 1 andx 4.
Startingwiththeleaf nodes, wethenhavethefollowingsequenceofsixmessages µx 1 →f a (x 1 ) = 1 (8.74) µf a →x 2 (x 2 ) = fa(x 1 , x 2 ) (8.75) x 1 µx 4 →f c (x 4 ) = 1 (8.76) µf c →x 2 (x 2 ) = fc(x 2 , x 4 ) (8.77) x 4 µx 2 →f b (x 2 ) = µf a →x 2 (x 2 )µf c →x 2 (x 2 ) (8.78) µf b →x 3 (x 3 ) = fb(x 2 , x 3 )µx 2 →f b .
(8.79) x 2 Thedirectionofflowofthesemessagesisillustratedin Figure8.52.
Oncethismes- sage propagation is complete, we can then propagate messages from the root node outtotheleafnodes, andthesearegivenby µx 3 →f b (x 3 ) = 1 (8.80) µf b →x 2 (x 2 ) = fb(x 2 , x 3 ) (8.81) x 3 µx 2 →f a (x 2 ) = µf b →x 2 (x 2 )µf c →x 2 (x 2 ) (8.82) µf a →x 1 (x 1 ) = fa(x 1 , x 2 )µx 2 →f a (x 2 ) (8.83) x 2 µx 2 →f c (x 2 ) = µf a →x 2 (x 2 )µf b →x 2 (x 2 ) (8.84) µf c →x 4 (x 4 ) = fc(x 2 , x 4 )µx 2 →f c (x 2 ).
(8.85) x 2 410 8.
GRAPHICALMODELS x1 x2 x3 x1 x2 x3 x4 x4 (a) (b) Figure8.52 Flowofmessagesforthesum-productalgorithmappliedtotheexamplegraphin Figure8.51.
(a) Fromtheleafnodesx 1andx 4towardstherootnodex 3.
(b)Fromtherootnodetowardstheleafnodes.
One message has now passed in each direction across each link, and we can now evaluate the marginals.
As a simple check, let us verify that the marginal p(x 2 ) is givenbythecorrectexpression.
Using(8.63)andsubstitutingforthemessagesusing theaboveresults, wehave p(x 2 ) = µf a →x 2 (x 2 )µf b → x 2 (x 2 )µf c →x 2 (x 2 ) = fa(x 1 , x 2 ) fb(x 2 , x 3 ) fc(x 2 , x 4 ) x x x 1 3 4 = fa(x 1 , x 2 )fb(x 2 , x 3 )fc(x 2 , x 4 ) x x x 1 2 4 = p(x) (8.86) x x x 1 3 4 asrequired.
Sofar, wehaveassumedthatallofthevariablesinthegrapharehidden.
Inmost practicalapplications, asubsetofthevariableswillbeobserved, andwewishtocal- culateposteriordistributionsconditionedontheseobservations.
Observednodesare easilyhandledwithinthesum-productalgorithmasfollows.
Supposewepartitionx into hidden variables h and observed variables v, and that the observe d value of v is denoted v .
Then we simply multiply the joint distribution p(x) by i I(vi, vi), where I(v, v) = 1 if v = v and I(v, v) = 0 otherwise.
This product corresponds to p(h, v = v ) and hence is an unnormalized version of p(h|v = v ).
By run- ningthesum-productalgorithm, wecanefficientlycalculatetheposteriormarginals p(hi |v = v )uptoanormalizationcoefficientwhosevaluecanbefoundefficiently usingalocalcomputation.
Anysummationsovervariablesinvthencollapseintoa singleterm.
Wehaveassumedthroughoutthissectionthatwearedealingwithdiscretevari- ables.
However, thereisnothingspecifictodiscretevariableseitherinthegraphical framework or in the probabilistic construction of the sum-product algorithm.
For 8.4.
Inferencein Graphical Models 411 Table8.1 Exampleofajointdistributionovertwobinaryvariablesfor x=0 x=1 which the maximum of the joint distribution occurs for dif- y =0 0.3 0.4 ferent variable values compared to the maxima of the two y =1 0.3 0.0 marginals.
continuous variables the summations are simply replaced by integrations.
We shall giveanexampleofthesum-productalgorithmappliedtoagraphoflinear-Gaussian Section13.3 variableswhenweconsiderlineardynamicalsystems.
8.4.5 The max-sum algorithm Thesum-productalgorithmallowsustotakeajointdistributionp(x)expressed as a factor graph and efficiently find marginals over the component variables.
Two other common tasks are to find a setting of the variables that has the largest prob- ability and to find the value of that probability.
These can be addressed through a closelyrelatedalgorithmcalledmax-sum, whichcanbeviewedasanapplicationof dynamicprogramminginthecontextofgraphicalmodels(Cormenetal.,2001).
A simple approach to finding latent variable values having high probability would be to run the sum-product algorithm to obtain the marginals p(xi) for ev- eryvariable, andthen, foreachmarginalinturn, tofindthevaluex thatmaximizes i that marginal.
However, this would give the set of values that are individually the most probable.
In practice, we typically wish to find the set of values that jointly havethelargestprobability, inotherwordsthevectorxmax thatmaximizesthejoint distribution, sothat xmax =argmaxp(x) (8.87) x forwhichthecorrespondingvalueofthejointprobabilitywillbegivenby p(xmax)=maxp(x).
(8.88) x Ingeneral, xmax isnotthesameasthesetofx values, aswecaneasilyshowusing i a simple example.
Consider the joint distribution p(x, y) over two binary variables x, y ∈{0,1}givenin Table8.1.
Thejointdistributionismaximizedbysettingx= 1andy =0, correspondingthevalue0.4.
However, themarginalforp(x), obtained bysummingoverbothvaluesofy, isgivenbyp(x=0)=0.6andp(x=1)=0.4, and similarly the marginal for y is given by p(y = 0) = 0.7 and p(y = 1) = 0.3, and so the marginals are maximized by x = 0 and y = 0, which corresponds to a valueof0.3forthejointdistribution.
Infact, itisnotdifficulttoconstructexamples forwhichthesetofindividuallymostprobablevalueshasprobabilityzerounderthe Exercise 8.27 jointdistribution.
We therefore seek an efficient algorithm for finding the value of x that maxi- mizes the joint distribution p(x) and that will allow us to obtain the value of the jointdistributionatitsmaximum.
Toaddressthesecondoftheseproblems, weshall simplywriteoutthemaxoperatorintermsofitscomponents maxp(x)=max...
maxp(x) (8.89) x x 1 x M 412 8.
GRAPHICALMODELS where M is the total number of variables, and then substitute for p(x) using its expansion in terms of a product of factors.
In deriving the sum-product algorithm, wemadeuseofthedistributivelaw(8.53)formultiplication.
Herewemakeuseof theanalogouslawforthemaxoperator max(ab, ac)=amax(b, c) (8.90) whichholdsifa 0(aswillalwaysbethecaseforthefactorsinagraphicalmodel).
Thisallowsustoexchangeproductswithmaximizations.
Considerfirstthesimpleexampleofachainofnodesdescribedby(8.49).
The evaluationoftheprobabilitymaximumcanbewrittenas 1 maxp(x)= max···max[ψ 1,2 (x 1 , x 2 )···ψN−1, N(x N−1 , x N)] x Z x 1 x N 1 = max ψ 1,2 (x 1 , x 2 ) ···maxψN−1, N(x N−1 , x N) .
Z x x 1 N As with the calculation of marginals, we see that exchanging the max and product operatorsresultsinamuchmoreefficientcomputation, andonethatiseasilyinter- pretedintermsofmessagespassedfromnodex N backwardsalongthechaintonode x 1.
We can readily generalize this result to arbitrary tree-structured factor graphs by substituting the expression (8.59) for the factor graph expansion into (8.89) and again exchanging maximizations with products.
The structure of this calculation is identicaltothatofthesum-productalgorithm, andsowecansimplytranslatethose resultsintothepresentcontext.
Inparticular, supposethatwedesignateaparticular variablenodeasthe‘root’ofthegraph.
Thenwestartasetofmessagespropagating inwards from the leaves of the tree towards the root, with each node sending its messagetowardstherootonceithasreceivedallincomingmessagesfromitsother neighbours.
The final maximization is performed over the product of all messages arrivingattherootnode, andgivesthemaximumvalueforp(x).
Thiscouldbecalled themax-productalgorithmandisidenticaltothesum-productalgorithmexceptthat summations are replaced by maximizations.
Note that at this stage, messages have beensentfromleavestotheroot, butnotintheotherdirection.
In practice, products of many small probabilities can lead to numerical under- flowproblems, andsoitisconvenienttoworkwiththelogarithmofthejointdistri- bution.
Thelogarithmisamonotonicfunction, sothatifa>bthenlna>lnb, and hencethemaxoperatorandthelogarithmfunctioncanbeinterchanged, sothat ln maxp(x) =maxlnp(x).
(8.91) x x Thedistributivepropertyispreservedbecause max(a+b, a+c)=a+max(b, c).
(8.92) Thus taking the logarithm simply has the effect of replacing the products in the max-product algorithm with sums, and so we obtain themax-sum algorithm.
From 8.4.
Inferencein Graphical Models 413 the results (8.66) and (8.69) derived earlier for the sum-product algorithm, we can readily write down the max-sum algorithm in terms of message passing simply by replacing‘sum’with‘max’andreplacingproductswithsumsoflogarithmstogive ⎡ ⎤ ⎣ ⎦ 1 M m∈ne(f s)\x µx→f(x) = µf →x(x).
(8.94) l l∈ne(x)\f Theinitialmessagessentbytheleafnodesareobtainedbyanalogywith(8.70)and (8.71)andaregivenby µx→f(x) = 0 (8.95) µf→x(x) = lnf(x) (8.96) while at the root node the maximum probability can then be computed, by analogy with(8.63), using ⎡ ⎤ pmax =max ⎣ µf →x(x) ⎦ .
(8.97) x s s∈ne(x) Sofar, wehaveseenhowtofindthemaximumofthejointdistributionbyprop- agatingmessagesfromtheleavestoanarbitrarilychosenrootnode.
Theresultwill be the same irrespective of which node is chosen as the root.
Now we turn to the secondproblemoffindingtheconfigurationofthevariablesforwhichthejointdis- tributionattainsthismaximumvalue.
Sofar, wehavesentmessagesfromtheleaves to the root.
The process of evaluating (8.97) will also give the value xmax for the mostprobablevalueoftherootnodevariable, definedby ⎡ ⎤ xmax =argmax ⎣ µf →x(x) ⎦ .
(8.98) s x s∈ne(x) Atthispoint, wemightbetemptedsimplytocontinuewiththemessagepassingal- gorithm and send messages from the root back out to the leaves, using (8.93) and (8.94), then apply (8.98) to all of the remaining variable nodes.
However, because we are now maximizing rather than summing, it is possible that there may be mul- tiple configurations of x all of which give rise to the maximum value for p(x).
In such cases, this strategy can fail because it is possible for the individual variable values obtained by maximizing the product of messages at each node to belong to different maximizing configurations, giving an overall configuration that no longer correspondstoamaximum.
The problem can be resolved by adopting a rather different kind of message passing from the root node to the leaves.
To see how this works, let us return once againtothesimplechainexampleof N variablesx 1 ,..., x N eachhaving K states, 414 8.
GRAPHICALMODELS Figure8.53 A lattice, or trellis, diagram show- ing explicitly the K possible states (one per row ofthediagram)foreachofthevariablesx n inthe k =1 chain model.
In this illustration K = 3.
The ar- rowshowsthedirectionofmessagepassinginthe max-productalgorithm.
Foreverystatek ofeach variablex n(correspondingtocolumnnofthedia- gram)thefunctionφ(x n )definesauniquestateat thepreviousvariable, indicatedbytheblacklines.
k =2 The two paths through the lattice correspond to configurationsthatgivetheglobalmaximumofthe joint probability distribution, and either of these canbefoundbytracingbackalongtheblacklines intheoppositedirectiontothearrow.
k =3 n−2 n−1 n n+1 corresponding to the graph shown in Figure 8.38.
Suppose we take node x N to be therootnode.
Theninthefirstphase, wepropagatemessagesfromtheleafnodex 1 totherootnodeusing µx n →f n, n+1 (xn) = µf n−1 , n →x n (xn) µf n−1, n →x n (xn) = m x ax lnfn−1, n(xn−1 , xn)+µx n−1 →fn−1, n(xn) n−1 which follow from applying (8.94) and (8.93) to this particular graph.
The initial messagesentfromtheleafnodeissimply µx 1 →f 1,2 (x 1 )=0.
(8.99) Themostprobablevalueforx N isthengivenby xm N ax =argmax µf N−1, N →x N (x N) .
(8.100) x N Nowweneedtodeterminethestatesofthepreviousvariablesthatcorrespondtothe samemaximizingconfiguration.
Thiscanbedonebykeepingtrackofwhichvalues of the variables gave rise to the maximum state of each variable, in other words by storingquantitiesgivenby φ(xn)=argmax lnfn−1, n(xn−1 , xn)+µx n−1 →fn−1, n(xn) .
(8.101) x n−1 To understand better what is happening, it is helpful to represent the chain of vari- ablesintermsofalatticeortrellisdiagramasshownin Figure8.53.
Notethatthis is not a probabilistic graphical model because the nodes represent individual states of variables, while each variable corresponds to a column of such states in the di- agram.
For each state of a given variable, there is a unique state of the previous variable that maximizes the probability (ties are broken either systematically or at random), correspondingtothefunctionφ(xn)givenby(8.101), andthisisindicated 8.4.
Inferencein Graphical Models 415 bythelinesconnectingthenodes.
Onceweknowthemostprobablevalueofthefi- nalnodex N, wecanthensimplyfollowthelinkbacktofindthemostprobablestate ofnodex N−1andsoonbacktotheinitialnodex 1.
Thiscorrespondstopropagating amessagebackdownthechainusing xmax =φ(xmax) (8.102) n−1 n and is known as back-tracking.
Note that there could be several values of xn−1 all ofwhichgivethemaximumvaluein(8.101).
Providedwechoseoneofthesevalues when we do the back-tracking, we are assured of a globally consistent maximizing configuration.
In Figure 8.53, we have indicated two paths, each of which we shall suppose corresponds to a global maximum of the joint probability distribution.
If k = 2 and k = 3 each represent possible values of xmax, then starting from either state N and tracing back along the black lines, which corresponds to iterating (8.102), we obtain a valid global maximum configuration.
Note that if we had run a forward pass of max-sum message passing followed by a backward pass and then applied (8.98)ateachnodeseparately, wecouldendupselectingsomestatesfromonepath and some from the other path, giving an overall configuration that is not a global maximizer.
Weseethatitisnecessaryinsteadtokeeptrackofthemaximizingstates duringtheforwardpassusingthefunctionsφ(xn)andthenuseback-trackingtofind aconsistentsolution.
The extension to a general tree-structured factor graph should now be clear.
If a message is sent from a factor node f to a variable node x, a maximization is performedoverallothervariablenodesx 1 ,..., x M thatareneighboursofthatfac- tor node, using (8.93).
When we perform this maximization, we keep a record of which values of the variables x 1 ,..., x M gave rise to the maximum.
Then in the back-tracking step, having found xmax, we can then use these stored values to as- sign consistent maximizing states xmax,..., xmax.
The max-sum algorithm, with 1 M back-tracking, gives an exact maximizing configuration for the variables provided the factor graph is a tree.
An important application of this technique is for finding the most probable sequence of hidden states in a hidden Markov model, in which Section13.2 caseitisknownasthe Viterbialgorithm.
As with the sum-product algorithm, the inclusion of evidence in the form of observed variables is straightforward.
The observed variables are clamped to their observedvalues, andthemaximizationisperformedovertheremaininghiddenvari- ables.
This canbeshown formally byincluding identity functions for theobserved variablesintothefactorfunctions, aswedidforthesum-productalgorithm.
Itisinterestingtocomparemax-sumwiththeiteratedconditionalmodes(ICM) algorithmdescribedonpage389.
Eachstepin ICMiscomputationallysimplerbe- cause the ‘messages’ that are passed from one node to the next comprise a single value consisting of the new state of the node for which the conditional distribution is maximized.
The max-sum algorithm is more complex because the messages are functions of node variables x and hence comprise a set of K values for each pos- sible state of x.
Unlike max-sum, however, ICM is not guaranteed to find a global maximumevenfortree-structuredgraphs.
416 8.
GRAPHICALMODELS 8.4.6 Exact inference in general graphs Thesum-productandmax-sumalgorithmsprovideefficientandexactsolutions to inference problems in tree-structured graphs.
For many practical applications, however, wehavetodealwithgraphshavingloops.
The message passing framework can be generalized to arbitrary graph topolo- gies, givinganexactinferenceprocedureknownasthejunctiontreealgorithm(Lau- ritzen and Spiegelhalter, 1988; Jordan, 2007).
Here we give a brief outline of the key steps involved.
This is not intended to convey a detailed understanding of the algorithm, butrather togiveaflavourof thevariousstages involved.
If thestarting point is a directed graph, it is first converted to an undirected graph by moraliza- tion, whereasifstartingfromanundirectedgraphthisstepisnotrequired.
Nextthe graph is triangulated, which involves finding chord-less cycles containing four or morenodesandaddingextralinkstoeliminatesuchchord-lesscycles.
Forinstance, in the graph in Figure 8.36, the cycle A–C–B–D–A is chord-less a link could be addedbetween Aand B oralternativelybetween C and D.
Notethatthejointdis- tributionfortheresultingtriangulatedgraphisstilldefinedbyaproductofthesame potentialfunctions, butthesearenowconsideredtobefunctionsoverexpandedsets of variables.
Next the triangulated graph is used to construct a new tree-structured undirectedgraphcalledajointree, whosenodescorrespondtothemaximalcliques of the triangulated graph, and whose links connect pairs of cliques that have vari- ables in common.
The selection of which pairs of cliques to connect in this way is importantandisdonesoastogiveamaximalspanningtreedefinedasfollows.
Of allpossibletreesthatlinkupthecliques, theonethatischosenisoneforwhichthe weightofthetreeislargest, wheretheweightforalinkisthenumberofnodesshared bythetwocliquesitconnects, andtheweightforthetreeisthesumoftheweights for the links.
If the tree is condensed, so that any clique that is a subset of another cliqueisabsorbedintothelargerclique, thisgivesajunctiontree.
Asaconsequence ofthetriangulationstep, theresultingtreesatisfiestherunningintersectionproperty, whichmeansthatifavariableiscontainedintwocliques, thenitmustalsobecon- tained in every clique on the path that connects them.
This ensures that inference about variables will be consistent across the graph.
Finally, a two-stage message passingalgorithm, essentiallyequivalenttothesum-productalgorithm, cannowbe applied to this junction tree in order to find marginals and conditionals.
Although the junction tree algorithm sounds complicated, at its heart is the simple idea that wehaveusedalreadyofexploitingthefactorizationpropertiesofthedistributionto allow sums and products to be interchanged so that partial summations can be per- formed, thereby avoiding having to work directly with the joint distribution.
The role of the junction tree is to provide a precise and efficient way to organize these computations.
It is worth emphasizing that this is achieved using purely graphical operations! The junction tree is exact for arbitrary graphs and is efficient in the sense that foragivengraphtheredoesnotingeneralexistacomputationallycheaperapproach.
Unfortunately, thealgorithmmustworkwiththejointdistributionswithineachnode (eachofwhichcorrespondstoacliqueofthetriangulatedgraph)andsothecompu- tationalcostofthealgorithmisdeterminedbythenumberofvariablesinthelargest 8.4.
Inferencein Graphical Models 417 cliqueandwillgrowexponentiallywiththisnumberinthecaseofdiscretevariables.
An important concept is the treewidth of a graph (Bodlaender, 1993), which is de- finedintermsofthenumberofvariablesinthelargestclique.
Infact, itisdefinedto beasonelessthanthesizeofthelargestclique, toensurethatatreehasatreewidth of1.
Becausethereingeneraltherecanbemultipledifferentjunctiontreesthatcan be constructed from a given starting graph, the treewidth is defined by the junction tree for which the largest clique has the fewest variables.
If the treewidth of the originalgraphishigh, thejunctiontreealgorithmbecomesimpractical.
8.4.7 Loopy belief propagation For many problems of practical interest, it will not be feasible to use exact in- ference, and so we need to exploit effective approximation methods.
An important classofsuchapproximations, thatcanbroadlybecalledvariationalmethods, willbe discussedindetailin Chapter10.
Complementingthesedeterministicapproachesis awiderangeofsamplingmethods, alsocalled Monte Carlomethods, thatarebased on stochastic numerical sampling from distributions and that will be discussed at lengthin Chapter11.
Hereweconsideronesimpleapproachtoapproximateinferenceingraphswith loops, which builds directly on the previous discussion of exact inference in trees.
Theideaissimplytoapplythesum-productalgorithmeventhoughthereisnoguar- antee that it will yield good results.
This approach is known asloopy belief propa- gation(Freyand Mac Kay,1998)andispossiblebecausethemessagepassingrules (8.66)and(8.69)forthesum-productalgorithmarepurelylocal.
However, because the graph now has cycles, information can flow many times around the graph.
For somemodels, thealgorithmwillconverge, whereasforothersitwillnot.
Inordertoapplythisapproach, weneedtodefineamessagepassingschedule.
Let us assume that one message is passed at a time on any given link and in any givendirection.
Eachmessagesentfromanodereplacesanypreviousmessagesent in the same direction across the same link and will itself be a function only of the mostrecentmessagesreceivedbythatnodeatpreviousstepsofthealgorithm.
We have seen that a message can only be sent across a link from a node when all other messages have been received by that node across its other links.
Because there are loops in the graph, this raises the problem of how to initiate the message passingalgorithm.
Toresolvethis, wesuppose thataninitialmessagegivenbythe unitfunctionhasbeenpassedacrosseverylinkineachdirection.
Everynodeisthen inapositiontosendamessage.
There are now many possible ways to organize the message passing schedule.
For example, the flooding schedule simultaneously passes a message across every linkinbothdirectionsateachtimestep, whereasschedulesthatpassonemessageat atimearecalledserialschedules.
Following Kschischnang et al.
(2001), we will say that a (variable or factor) node a has a message pending on its link to a node b if node a has received any message on any of its other links since the last time it send a message to b.
Thus, when a node receives a message on one of its links, this creates pending messages on all of its other links.
Only pending messages need to be transmitted because 418 8.
GRAPHICALMODELS othermessageswouldsimplyduplicatethepreviousmessageonthesamelink.
For graphs that have a tree structure, any schedule that sends only pending messages will eventually terminate once a message has passed in each direction across every Exercise 8.29 link.
At this point, there are no pending messages, and the product of the received messagesateveryvariablegivetheexactmarginal.
Ingraphshavingloops, however, thealgorithmmayneverterminatebecausetheremightalwaysbependingmessages, although in practice it is generally found to converge within a reasonable time for most applications.
Once the algorithm has converged, or once it has been stopped ifconvergenceisnotobserved, the(approximate)localmarginalscanbecomputed usingtheproductofthemostrecentlyreceivedincomingmessagestoeachvariable nodeorfactornodeoneverylink.
In some applications, the loopy belief propagation algorithm can give poor re- sults, whereasinotherapplicationsithasproventobeveryeffective.
Inparticular, state-of-the-art algorithms for decoding certain kinds of error-correcting codes are equivalenttoloopybeliefpropagation(Gallager,1963; Berrouetal.,1993; Mc Eliece etal.,1998; Mac Kayand Neal,1999; Frey,1998).
8.4.8 Learning the graph structure In our discussion of inference in graphical models, we have assumed that the structure of the graph is known and fixed.
However, there is also interest in go- ing beyond the inference problem and learning the graph structure itself from data (Friedmanand Koller,2003).
Thisrequiresthatwedefineaspaceofpossiblestruc- turesaswellasameasurethatcanbeusedtoscoreeachstructure.
From a Bayesian viewpoint, we would ideally like to compute a posterior dis- tribution over graph structures and to make predictions by averaging with respect to this distribution.
If we have a prior p(m) over graphs indexed by m, then the posteriordistributionisgivenby p(m|D)∝p(m)p(D|m) (8.103) where D is the observed data set.
The model evidence p(D|m) then provides the scoreforeachmodel.
However, evaluationoftheevidenceinvolvesmarginalization overthelatentvariablesandpresentsachallengingcomputationalproblemformany models.
Exploringthespaceofstructurescanalsobeproblematic.
Becausethenumber of different graph structures grows exponentially with the number of nodes, it is oftennecessarytoresorttoheuristicstofindgoodcandidates.
Exercises 8.1 ( ) www Bymarginalizingoutthevariablesinorder, showthattherepresentation (8.5) for the joint distribution of a directed graph is correctly normalized, provided eachoftheconditionaldistributionsisnormalized.
8.2 ( ) www Show that the property of there being no directed cycles in a directed graphfollowsfromthestatementthatthereexistsanorderednumberingofthenodes suchthatforeachnodetherearenolinksgoingtoalower-numberednode.
Exercises 419 Table8.2 Thejointdistributionoverthreebinaryvariables.
a b c p(a, b, c) 0 0 0 0.192 0 0 1 0.144 0 1 0 0.048 0 1 1 0.216 1 0 0 0.192 1 0 1 0.064 1 1 0 0.048 1 1 1 0.096 8.3 ( ) Consider three binary variables a, b, c ∈ {0,1} having the joint distribution givenin Table8.2.
Showbydirectevaluationthatthisdistributionhastheproperty that a and b are marginally dependent, so that p(a, b) = p(a)p(b), but that they become independent when conditioned on c, so that p(a, b|c) = p(a|c)p(b|c) for bothc=0andc=1.
8.4 ( ) Evaluate the distributions p(a), p(b|c), and p(c|a) corresponding to the joint distribution given in Table 8.2.
Hence show by direct evaluation that p(a, b, c) = p(a)p(c|a)p(b|c).
Drawthecorrespondingdirectedgraph.
8.5 ( ) www Draw a directed probabilistic graphical model corresponding to the relevancevectormachinedescribedby(7.79)and(7.80).
8.6 ( ) Forthemodelshownin Figure8.13, wehaveseenthatthenumberofparameters requiredtospecifytheconditionaldistributionp(y|x 1 ,..., x M), wherexi ∈{0,1}, couldbereducedfrom2M to M+1bymakinguseofthelogisticsigmoidrepresen- tation(8.10).
Analternativerepresentation(Pearl,1988)isgivenby M p(y =1|x 1 ,..., x M)=1−(1−µ 0 ) (1−µi) x i (8.104) i=1 wheretheparametersµirepresenttheprobabilitiesp(xi =1), andµ 0isanadditional parameterssatisfying0 µ 0 1.
Theconditionaldistribution(8.104)isknownas thenoisy-OR.
Showthatthiscanbeinterpretedasa‘soft’(probabilistic)formofthe logical ORfunction(i.
e., thefunctionthatgivesy = 1wheneveratleastoneofthe xi =1).
Discusstheinterpretationofµ 0.
8.7 ( ) Usingtherecursionrelations(8.15)and(8.16), showthatthemeanandcovari- anceofthejointdistributionforthegraphshownin Figure8.14aregivenby(8.17) and(8.18), respectively.
8.8 ( ) www Showthata⊥⊥b, c|dimpliesa⊥⊥b|d.
8.9 ( ) www Usingthed-separationcriterion, showthattheconditionaldistribution for a node x in a directed graph, conditioned on all of the nodes in the Markov blanket, isindependentoftheremainingvariablesinthegraph.
420 8.
GRAPHICALMODELS Figure8.54 Exampleofagraphicalmodelusedtoexplorethecon- a b ditionalindependencepropertiesofthehead-to-head patha–c–bwhenadescendantofc, namelythenode d, isobserved.
c d 8.10 ( ) Considerthedirectedgraphshownin Figure8.54inwhichnoneofthevariables is observed.
Show that a ⊥⊥ b | ∅.
Suppose we now observe the variable d.
Show thatingenerala⊥ ⊥b|d.
8.11 ( ) Considertheexampleofthecarfuelsystemshownin Figure8.21, andsuppose thatinsteadofobservingthestateofthefuelgauge Gdirectly, thegaugeisseenby thedriver Dwhoreportstousthereadingonthegauge.
Thisreportiseitherthatthe gaugeshowsfull D =1orthatitshowsempty D =0.
Ourdriverisabitunreliable, asexpressedthroughthefollowingprobabilities p(D =1|G=1) = 0.9 (8.105) p(D =0|G=0) = 0.9.
(8.106) Suppose that the driver tells us that the fuel gauge shows empty, in other words that we observe D = 0.
Evaluate the probability that the tank is empty given only this observation.
Similarly, evaluate the corresponding probability given also the observation that the battery is flat, and note that this second probability is lower.
Discusstheintuitionbehindthisresult, andrelatetheresultto Figure8.54.
8.12 ( ) www Showthatthereare2M(M−1)/2 distinctundirectedgraphsoverasetof M distinctrandomvariables.
Drawthe8possibilitiesforthecaseof M =3.
8.13 ( ) Consider the use of iterated conditional modes (ICM) to minimize the energy functiongivenby(8.42).
Writedownanexpressionforthedifferenceinthevalues oftheenergyassociatedwiththetwostatesofaparticularvariablexj, withallother variablesheldfixed, andshowthatitdependsonlyonquantitiesthatarelocaltoxj inthegraph.
8.14 ( ) Consider a particular case of the energy function given by (8.42) in which the coefficients β = h = 0.
Show that the most probable configuration of the latent variablesisgivenbyxi =yi foralli.
8.15 ( ) www Show that the joint distribution p(xn−1 , xn) for two neighbouring nodesinthegraphshownin Figure8.38isgivenbyanexpressionoftheform(8.58).
Exercises 421 8.16 ( ) Considertheinferenceproblemofevaluatingp(xn |x N)forthegraphshown algorithmdiscussedin Section8.4.1canbeusedtosolvethisefficiently, anddiscuss whichmessagesaremodifiedandinwhatway.
8.17 ( ) Consider a graph of the form shown in Figure 8.38 having N = 5 nodes, in which nodes x 3 and x 5 are observed.
Use d-separation to show that x 2 ⊥⊥ x 5 | x 3.
Showthatifthemessagepassingalgorithmof Section8.4.1isappliedtotheevalu- ationofp(x 2 |x 3 , x 5 ), theresultwillbeindependentofthevalueofx 5.
8.18 ( ) www Show that a distribution represented by a directed tree can trivially bewrittenasanequivalentdistributionoverthecorrespondingundirectedtree.
Also showthatadistributionexpressedasanundirectedtreecan, bysuitablenormaliza- tion of the clique potentials, be written as a directed tree.
Calculate the number of distinctdirectedtreesthatcanbeconstructedfromagivenundirectedtree.
8.19 ( ) Apply the sum-product algorithm derived in Section 8.4.4 to the chain-of- nodesmodeldiscussedin Section8.4.1andshowthattheresults(8.54),(8.55), and (8.57)arerecoveredasaspecialcase.
8.20 ( ) www Considerthemessagepassingprotocolforthesum-productalgorithmon atree-structuredfactorgraphinwhichmessagesarefirstpropagatedfromtheleaves toanarbitrarilychosenrootnodeandthenfromtherootnodeouttotheleaves.
Use proof by induction to show that the messages can be passed in such an order that at every step, each node that must send a message has received all of the incoming messagesnecessarytoconstructitsoutgoingmessages.
8.21 ( ) www Show that the marginal distributions p(xs) over the sets of variables xs associatedwitheachofthefactorsfx(xs)inafactorgraphcanbefoundbyfirst runningthesum-productmessagepassingalgorithmandthenevaluatingtherequired marginalsusing(8.72).
8.22 ( ) Consideratree-structuredfactorgraph, inwhichagivensubsetofthevariable nodesformaconnectedsubgraph(i.
e., anyvariablenodeofthesubsetisconnected to at least one of the other variable nodes via a single factor node).
Show how the sum-product algorithm can be used to compute the marginal distribution over that subset.
8.23 ( ) www In Section8.4.4, weshowedthatthemarginaldistributionp(xi)fora variablenodexi inafactorgraphisgivenbytheproductofthemessagesarrivingat thisnodefromneighbouringfactornodesintheform(8.63).
Showthatthemarginal p(xi) can also be written as the product of the incoming message along any one of thelinkswiththeoutgoingmessagealongthesamelink.
8.24 ( ) Showthatthemarginaldistributionforthevariablesxs inafactorfs(xs)in atree-structuredfactorgraph, afterrunningthesum-productmessagepassingalgo- rithm, canbewrittenastheproductofthemessagearrivingatthefactornodealong allitslinks, timesthelocalfactorf(xs), intheform(8.72).
422 8.
GRAPHICALMODELS 8.25 ( ) In (8.86), we verified that the sum-product algorithm run on the graph in Figure8.51withnodex 3 designatedastherootnodegivesthecorrectmarginalfor x 2.
Showthatthecorrectmarginalsareobtainedalsoforx 1 andx 3.
Similarly, show thattheuseoftheresult(8.72)afterrunningthesum-productalgorithmonthisgraph givesthecorrectjointdistributionforx 1 , x 2.
8.26 ( ) Consideratree-structuredfactorgraphoverdiscretevariables, andsupposewe wishtoevaluatethejointdistributionp(xa, xb)associatedwithtwovariablesxaand xb that do not belong to a common factor.
Define a procedure for using the sum- productalgorithmtoevaluatethisjointdistributioninwhichoneofthevariablesis successivelyclampedtoeachofitsallowedvalues.
8.27 ( ) Considertwodiscretevariablesxandy eachhavingthreepossiblestates, for examplex, y ∈ {0,1,2}.
Constructajointdistributionp(x, y)overthesevariables having the property that the value x that maximizes the marginal p(x), along with the value y that maximizes the marginal p(y), together have probability zero under thejointdistribution, sothatp( x, y)=0.
8.28 ( ) www The concept of a pending message in the sum-product algorithm for afactorgraphwasdefinedin Section8.4.7.
Showthatifthegraphhasoneormore cycles, there will always be at least one pending message irrespective of how long thealgorithmruns.
8.29 ( ) www Showthatifthesum-productalgorithmisrunonafactorgraphwitha treestructure(noloops), thenafterafinitenumberofmessageshavebeensent, there willbenopendingmessages.
9 Mixture Models and EM If we define a joint distribution over observed and latent variables, the correspond- ingdistributionoftheobservedvariablesaloneisobtainedbymarginalization.
This allows relatively complex marginal distributions over observed variables to be ex- pressed in terms of more tractable joint distributions over the expanded space of observed and latent variables.
The introduction of latent variables thereby allows complicated distributions to be formed from simpler components.
In this chapter, we shall see that mixture distributions, such as the Gaussian mixture discussed in Section 2.3.9, can be interpreted in terms of discrete latent variables.
Continuous latentvariableswillformthesubjectof Chapter12.
As well as providing a framework for building more complex probability dis- tributions, mixture models can also be used to cluster data.
We therefore begin our discussion of mixture distributions by considering the problem of finding clusters in a set of data points, which we approach first using a nonprobabilistic technique Section9.1 calledthe K-meansalgorithm(Lloyd,1982).
Thenweintroducethelatentvariable 423 424 9.
MIXTUREMODELSANDEM viewofmixturedistributionsinwhichthediscretelatentvariablescanbeinterpreted Section9.2 asdefiningassignmentsofdatapointstospecificcomponentsofthemixture.
Agen- eral technique for finding maximum likelihood estimators in latent variable models is the expectation-maximization (EM) algorithm.
We first of all use the Gaussian mixturedistributiontomotivatethe EMalgorithminafairlyinformalway, andthen Section9.3 we give a more careful treatment based on the latent variable viewpoint.
We shall seethatthe K-meansalgorithmcorrespondstoaparticularnonprobabilisticlimitof Section9.4 EMappliedtomixturesof Gaussians.
Finally, wediscuss EMinsomegenerality.
Gaussian mixture models are widely used in data mining, pattern recognition, machinelearning, andstatisticalanalysis.
Inmanyapplications, theirparametersare determinedbymaximumlikelihood, typicallyusingthe EMalgorithm.
However, as we shall see there are some significant limitations to the maximum likelihood ap- proach, andin Chapter10weshallshowthatanelegant Bayesiantreatmentcanbe given using the framework of variational inference.
This requires little additional computation compared with EM, and it resolves the principal difficulties of maxi- mumlikelihoodwhilealsoallowingthenumberofcomponentsinthemixturetobe inferredautomaticallyfromthedata.
9.1.
K-means Clustering Webeginbyconsideringtheproblemofidentifyinggroups, orclusters, ofdatapoints in a multidimensional space.
Suppose we have a data set {x 1 ,..., x N } consisting of N observationsofarandom D-dimensional Euclideanvariablex.
Ourgoalisto partition the data set into some number K of clusters, where we shall suppose for themomentthatthevalueof K isgiven.
Intuitively, wemightthinkofaclusteras comprising a group of data points whose inter-point distances are small compared with the distances to points outside of the cluster.
We can formalize this notion by firstintroducingasetof D-dimensionalvectorsµ , wherek = 1,..., K, inwhich k µ is a prototype associated with the kth cluster.
As we shall see shortly, we can k think of the µ as representing the centres of the clusters.
Our goal is then to find k an assignment of data points to clusters, as well as a set of vectors{µ }, such that k thesumofthesquaresofthedistancesofeachdatapointtoitsclosestvectorµ , is k aminimum.
Itisconvenientatthispointtodefinesomenotationtodescribetheassignment ofdatapointstoclusters.
Foreachdatapointxn, weintroduceacorrespondingset ofbinaryindicatorvariablesrnk ∈{0,1}, wherek =1,..., K describingwhichof the K clustersthedatapointxn isassignedto, sothatifdatapointxn isassignedto clusterk thenrnk = 1, andrnj = 0forj = k.
Thisisknownasthe1-of-K coding scheme.
We can then define an objective function, sometimes called a distortion measure, givenby N K J = rnk xn −µ k 2 (9.1) n=1k=1 which represents the sum of the squares of the distances of each data point to its 9.1.
K-means Clustering 425 assigned vector µ k .
Our goal is to find values for the {rnk } and the {µ k } so as to minimize J.
We can do this through an iterative procedure in which each iteration involvestwosuccessivestepscorrespondingtosuccessiveoptimizationswithrespect tothernkandtheµ k .
Firstwechoosesomeinitialvaluesfortheµ k .
Theninthefirst phase we minimize J with respect to the rnk, keeping the µ k fixed.
In the second phase we minimize J with respect to the µ k , keeping rnk fixed.
This two-stage optimization is then repeated until convergence.
We shall see that these two stages ofupdatingrnk andupdatingµ k correspondrespectivelytothe E(expectation)and Section9.4 M(maximization)stepsofthe EMalgorithm, andtoemphasizethisweshallusethe terms Estepand Mstepinthecontextofthe K-meansalgorithm.
Considerfirstthedeterminationofthernk.
Because J in(9.1)isalinearfunc- tionofrnk, thisoptimizationcanbeperformedeasilytogiveaclosedformsolution.
The terms involving different n are independent and so we can optimize for each n separately by choosing rnk to be 1 for whichever value of k gives the minimum value of xn −µ k 2.
In other words, we simply assign the nth data point to the closestclustercentre.
Moreformally, thiscanbeexpressedas 1 ifk =argmin j xn −µ j 2 rnk = (9.2) 0 otherwise.
Nowconsidertheoptimizationoftheµ k withthernk heldfixed.
Theobjective function J is a quadratic function of µ , and it can be minimized by setting its k derivativewithrespecttoµ tozerogiving k N 2 rnk(xn −µ k )=0 (9.3) n=1 whichwecaneasilysolveforµ togive k µ = n rnkxn .
(9.4) k n rnk The denominator in this expression is equal to the number of points assigned to cluster k, and so this result has a simple interpretation, namely set µ equal to the k meanofallofthedatapointsxnassignedtoclusterk.
Forthisreason, theprocedure isknownasthe K-meansalgorithm.
Thetwophasesofre-assigningdatapointstoclustersandre-computingtheclus- termeansarerepeatedinturnuntilthereisnofurtherchangeintheassignments(or untilsomemaximumnumberofiterationsisexceeded).
Becauseeachphasereduces Exercise 9.1 thevalueoftheobjectivefunction J, convergenceofthealgorithmisassured.
How- ever, itmayconvergetoalocalratherthanglobalminimumof J.
Theconvergence propertiesofthe K-meansalgorithmwerestudiedby Mac Queen(1967).
Appendix A The K-means algorithm is illustrated using the Old Faithful data set in Fig- ure 9.1.
For the purposes of this example, we have made a linear re-scaling of the data, known as standardizing, such that each of the variables has zero mean and unit standard deviation.
For this example, we have chosen K = 2, and so in this 426 9.
MIXTUREMODELSANDEM 2 (a) 2 (b) 2 (c) 0 0 0 −2 −2 −2 −2 0 2 −2 0 2 −2 0 2 2 (d) 2 (e) 2 (f) 0 0 0 −2 −2 −2 −2 0 2 −2 0 2 −2 0 2 2 (g) 2 (h) 2 (i) 0 0 0 −2 −2 −2 −2 0 2 −2 0 2 −2 0 2 Figure 9.1 Illustration of the K-means algorithm using the re-scaled Old Faithful data set.
(a) Green points denotethedatasetinatwo-dimensional Euclideanspace.
Theinitialchoicesforcentresµ andµ areshown 1 2 bytheredandbluecrosses, respectively.
(b)Intheinitial Estep, eachdatapointisassignedeithertothered cluster or to the blue cluster, according to which cluster centre is nearer.
This is equivalent to classifying the pointsaccordingtowhich sideoftheperpendicularbisectorofthetwoclustercentres, shown bythemagenta line, theylieon.
(c)Inthesubsequent Mstep, eachclustercentreisre-computedtobethemeanofthepoints assignedtothecorrespondingcluster.
(d)–(i)showsuccessive Eand Mstepsthroughtofinalconvergenceof thealgorithm.
9.1.
K-means Clustering 427 Figure9.2 Plotofthecostfunction J givenby (9.1)aftereach Estep(bluepoints) 1000 and M step (red points) of the K- means algorithm for the example shown in Figure 9.1.
The algo- J rithm has converged after the third Mstep, andthefinal EMcyclepro- ducesnochangesineithertheas- 500 signmentsortheprototypevectors.
0 1 2 3 4 case, theassignmentofeachdatapointtothenearestclustercentreisequivalenttoa classificationofthedatapointsaccordingtowhichsidetheylieoftheperpendicular bisector of the two cluster centres.
A plot of the cost function J given by (9.1) for the Old Faithfulexampleisshownin Figure9.2.
Notethatwehavedeliberatelychosenpoorinitialvaluesfortheclustercentres so that the algorithm takes several steps before convergence.
In practice, a better initialization procedure would be to choose the cluster centres µ to be equal to a k randomsubsetof K datapoints.
Itisalsoworthnotingthatthe K-meansalgorithm itself is often used to initialize the parameters in a Gaussian mixture model before Section9.2.2 applyingthe EMalgorithm.
A direct implementation of the K-means algorithm as discussed here can be relativelyslow, becauseineach Estepitisnecessarytocomputethe Euclideandis- tance between every prototype vector and every data point.
Various schemes have beenproposedforspeedingupthe K-meansalgorithm, someofwhicharebasedon precomputingadatastructuresuchasatreesuchthatnearbypointsareinthesame subtree (Ramasubramanian and Paliwal, 1990; Moore, 2000).
Other approaches makeuseofthetriangleinequalityfordistances, therebyavoidingunnecessarydis- tancecalculations(Hodgson,1998; Elkan,2003).
Sofar, wehaveconsideredabatchversionof K-meansinwhichthewholedata set is used together to update the prototype vectors.
We can also derive an on-line Section2.3.5 stochastic algorithm (Mac Queen, 1967) by applying the Robbins-Monro procedure totheproblemoffindingtherootsoftheregressionfunctiongivenbythederivatives Exercise 9.2 of J in(9.1)withrespecttoµ .
Thisleadstoasequentialupdateinwhich, foreach k datapointxn inturn, weupdatethenearestprototypeµ k using µn k ew =µo k ld+ηn(xn −µo k ld) (9.5) whereηn isthelearningrateparameter, whichistypicallymadetodecreasemono- tonicallyasmoredatapointsareconsidered.
The K-meansalgorithmisbasedontheuseofsquared Euclideandistanceasthe measureofdissimilaritybetweenadatapointandaprototypevector.
Notonlydoes thislimitthetypeofdatavariablesthatcanbeconsidered(itwouldbeinappropriate forcaseswheresomeorallofthevariablesrepresentcategoricallabelsforinstance), 428 9.
MIXTUREMODELSANDEM Section2.3.7 butitcanalsomakethedeterminationoftheclustermeansnonrobusttooutliers.
We can generalize the K-means algorithm by introducing a more general dissimilarity measure V(x, x )betweentwovectorsxandx andthenminimizingthefollowing distortionmeasure N K J = rnk V(xn,µ k ) (9.6) n=1k=1 whichgivesthe K-medoidsalgorithm.
The Estepagaininvolves, forgivencluster prototypesµ , assigningeachdatapointtotheclusterforwhichthedissimilarityto k thecorrespondingprototypeissmallest.
Thecomputationalcostofthisis O(KN), asisthecaseforthestandard K-meansalgorithm.
Forageneralchoiceofdissimi- laritymeasure, the Mstepispotentiallymorecomplexthanfor K-means, andsoit iscommontorestricteachclusterprototypetobeequaltooneofthedatavectorsas- signedtothatcluster, asthisallowsthealgorithmtobeimplementedforanychoice of dissimilarity measure V(·,·) so long as it can be readily evaluated.
Thus the M stepinvolves, foreachclusterk, adiscretesearchoverthe Nk pointsassignedtothat cluster, whichrequires O(N2)evaluationsof V(·,·).
k One notable feature of the K-means algorithm is that at each iteration, every datapointisassigneduniquelytoone, andonlyone, oftheclusters.
Whereassome data points will be much closer to a particular centre µ than to any other centre, k there may be other data points that lie roughly midway between cluster centres.
In the latter case, it is not clear that the hard assignment to the nearest cluster is the most appropriate.
We shall see in the next section that by adopting a probabilistic approach, weobtain‘soft’assignmentsofdatapointstoclustersinawaythatreflects the level of uncertainty over the most appropriate assignment.
This probabilistic formulationbringswithitnumerousbenefits.
9.1.1 Image segmentation and compression As an illustration of the application of the K-means algorithm, we consider the related problems of image segmentation and image compression.
The goal of segmentation is to partition an image into regions each of which has a reasonably homogeneousvisualappearanceorwhichcorrespondstoobjectsorpartsofobjects (Forsythand Ponce,2003).
Eachpixelinanimageisapointina3-dimensionalspace comprisingtheintensitiesofthered, blue, andgreenchannels, andoursegmentation algorithm simply treats each pixel in the image as a separate data point.
Note that strictly this space is not Euclidean because the channel intensities are bounded by theinterval[0,1].
Nevertheless, wecanapplythe K-meansalgorithmwithoutdiffi- culty.
Weillustratetheresultofrunning K-meanstoconvergence, foranyparticular valueof K, byre-drawingtheimagereplacingeachpixelvectorwiththe{R, G, B} intensitytripletgivenbythecentreµ towhichthatpixelhasbeenassigned.
Results k forvariousvaluesof K areshownin Figure9.3.
Weseethatforagivenvalueof K, thealgorithmisrepresentingtheimageusingapaletteofonly K colours.
Itshould beemphasizedthatthisuseof K-meansisnotaparticularlysophisticatedapproach toimagesegmentation, notleastbecauseittakesnoaccountofthespatialproximity ofdifferentpixels.
Theimagesegmentationproblemisingeneralextremelydifficult 9.1.
K-means Clustering 429 K =2 K =3 K =10 Original image Figure9.3 Twoexamplesoftheapplicationofthe K-meansclusteringalgorithmtoimagesegmentationshow- ing the initial images together with their K-means segmentations obtained using various values of K.
This alsoillustratesoftheuseofvectorquantizationfordatacompression, inwhichsmallervaluesof K givehigher compressionattheexpenseofpoorerimagequality.
andremainsthesubjectofactiveresearchandisintroducedheresimplytoillustrate thebehaviourofthe K-meansalgorithm.
We can also use the result of a clustering algorithm to perform data compres- sion.
It is important to distinguish between lossless data compression, in which the goal is to be able to reconstruct the original data exactly from the compressed representation, and lossy data compression, in which we accept some errors in the reconstructioninreturnforhigherlevelsofcompressionthancanbeachievedinthe lossless case.
We can apply the K-means algorithm to the problem of lossy data compression as follows.
For each of the N data points, we store only the identity k of the cluster to which it is assigned.
We also store the values of the K clus- ter centres µ , which typically requires significantly less data, provided we choose k K N.
Eachdatapointisthenapproximatedbyitsnearestcentreµ .
Newdata k points can similarly be compressed by first finding the nearest µ and then storing k thelabelk insteadoftheoriginaldatavector.
Thisframeworkisoftencalledvector quantization, andthevectorsµ arecalledcode-bookvectors.
k 430 9.
MIXTUREMODELSANDEM The image segmentation problem discussed above also provides an illustration of the use of clustering for data compression.
Suppose the original image has N pixelscomprising{R, G, B}valueseachofwhichisstoredwith8bitsofprecision.
Then to transmit the whole image directly would cost 24N bits.
Now suppose we first run K-means on the image data, and then instead of transmitting the original pixel intensity vectors we transmit the identity of the nearest vector µ .
Because k thereare K suchvectors, thisrequireslog K bitsperpixel.
Wemustalsotransmit 2 the K code book vectors µ , which requires 24K bits, and so the total number of k bits required to transmit the image is 24K +Nlog K (rounding up to the nearest 2 integer).
The original image shown in Figure 9.3 has 240×180 = 43,200 pixels andsorequires24×43,200=1,036,800bitstotransmitdirectly.
Bycomparison, the compressed images require 43,248 bits (K = 2), 86,472 bits (K = 3), and 173,040bits(K =10), respectively, totransmit.
Theserepresentcompressionratios comparedtotheoriginalimageof4.2%,8.3%, and16.7%, respectively.
Weseethat thereisatrade-offbetweendegreeofcompressionandimagequality.
Notethatour aiminthisexampleistoillustratethe K-meansalgorithm.
Ifwehadbeenaimingto produceagoodimagecompressor, thenitwouldbemorefruitfultoconsidersmall blocksofadjacentpixels, forinstance5×5, andtherebyexploitthecorrelationsthat existinnaturalimagesbetweennearbypixels.
9.2.
Mixtures of Gaussians In Section2.3.9wemotivatedthe Gaussianmixturemodelasasimplelinearsuper- positionof Gaussiancomponents, aimedatprovidingaricherclassofdensitymod- elsthanthesingle Gaussian.
Wenowturntoaformulationof Gaussianmixturesin termsofdiscretelatentvariables.
Thiswillprovideuswithadeeperinsightintothis importantdistribution, andwillalsoservetomotivatetheexpectation-maximization algorithm.
Recall from (2.188) that the Gaussian mixture distribution can be written as a linearsuperpositionof Gaussiansintheform K p(x)= πk N(x|µ k ,Σk).
(9.7) k=1 Letusintroducea K-dimensionalbinaryrandomvariablezhavinga1-of-K repre- sentation in which a particular element zk is equal to 1 and al l other elements are equal to 0.
The values of zk therefore satisfy zk ∈ {0,1} and k zk = 1, and we see that there are K possible states for the vector z according to which element is nonzero.
We shall define the joint distribution p(x, z) in terms of a marginal dis- tribution p(z) and a conditional distribution p(x|z), corresponding to the graphical model in Figure 9.4.
The marginal distribution over z is specified in terms of the mixingcoefficientsπk, suchthat p(zk =1)=πk 9.2.
Mixturesof Gaussians 431 Figure9.4 Graphical representation of a mixture model, in which z the joint distribution is expressed in the form p(x, z) = p(z)p(x|z).
x wheretheparameters{πk }mustsatisfy 0 πk 1 (9.8) togetherwith K πk =1 (9.9) k=1 in order to be valid probabilities.
Because z uses a 1-of-K representation, we can alsowritethisdistributionintheform K z p(z)= π k.
(9.10) k k=1 Similarly, theconditionaldistributionofxgivenaparticularvalueforzisa Gaussian p(x|zk =1)=N(x|µ k ,Σk) whichcanalsobewrittenintheform K p(x|z)= N(x|µ k ,Σk) z k.
(9.11) k=1 The joint distribution is given by p(z)p(x|z), and the marginal distribution of x is Exercise 9.3 thenobtainedbysummingthejointdistributionoverallpossiblestatesofztogive K p(x)= p(z)p(x|z)= πk N(x|µ k ,Σk) (9.12) z k=1 wherewehavemadeuseof(9.10)and(9.11).
Thusthemarginaldistributionofxis t hen, because we have represented the marginal distribution in the form p(x) = z p(x, z), itfollowsthatforeveryobserveddatapointxnthereisacorresponding latentvariablezn.
Wehavethereforefoundanequivalentformulationofthe Gaussianmixturein- volving an explicit latent variable.
It might seem that we have not gained much by doing so.
However, we are now able to work with the joint distribution p(x, z) 432 9.
MIXTUREMODELSANDEM insteadofthemarginaldistributionp(x), andthiswillleadtosignificantsimplifica- tions, mostnotablythroughtheintroductionoftheexpectation-maximization(EM) algorithm.
Another quantity that will play an important role is the conditional probability ofzgivenx.
Weshalluseγ(zk)todenotep(zk = 1|x), whosevaluecanbefound using Bayes’theorem γ(zk)≡p(zk =1|x) = p(zk =1)p(x|zk =1) K p(zj =1)p(x|zj =1) j=1 πk N(x|µ k ,Σk) = .
(9.13) K πj N(x|µ j ,Σj) j=1 We shall view πk as the prior probability of zk = 1, and the quantity γ(zk) as the correspondingposteriorprobabilityoncewehaveobservedx.
Asweshallseelater, γ(zk)canalsobeviewedastheresponsibilitythatcomponentk takesfor‘explain- ing’theobservationx.
Section8.1.2 We can use the technique of ancestral sampling to generate random samples distributedaccordingtothe Gaussianmixturemodel.
Todothis, wefirstgeneratea valueforz, whichwedenote z, fromthemarginaldistributionp(z)andthengenerate avalueforxfromtheconditionaldistributionp(x| z).
Techniquesforsamplingfrom standarddistributionsarediscussedin Chapter11.
Wecandepictsamplesfromthe joint distribution p(x, z) by plotting points at the corresponding values of x and thencolouringthemaccordingtothevalueofz, inotherwordsaccordingtowhich Gaussiancomponentwasresponsibleforgeneratingthem, asshownin Figure9.5(a).
Similarly samples from the marginal distribution p(x) are obtained by taking the samplesfromthejointdistributionandignoringthevaluesofz.
Theseareillustrated in Figure9.5(b)byplottingthexvalueswithoutanycolouredlabels.
Wecanalsousethissyntheticdatasettoillustratethe‘responsibilities’byeval- uating, for every data point, the posterior probability for each component in the mixture distribution from which this data set was generated.
In particular, we can represent the value of the responsibilities γ(znk) associated with data point xn by plottingthecorrespondingpointusingproportionsofred, blue, andgreeninkgiven byγ(znk)fork = 1,2,3, respectively, asshownin Figure9.5(c).
So, forinstance, a data point for which γ(zn1 ) = 1 will be coloured red, whereas one for which γ(zn2 ) = γ(zn3 ) = 0.5 will be coloured with equal proportions of blue and green ink and so will appear cyan.
This should be compared with Figure 9.5(a) in which the data points were labelled using the true identity of the component from which theyweregenerated.
9.2.1 Maximum likelihood Supposewehaveadatasetofobservations{x 1 ,..., x N }, andwewishtomodel thisdatausingamixtureof Gaussians.
Wecanrepresentthisdatasetasan N ×D 9.2.
Mixturesof Gaussians 433 1 1 1 (a) (b) (c) 0.5 0.5 0.5 0 0 0 0 0.5 1 0 0.5 1 0 0.5 1 Figure9.5 Exampleof500pointsdrawnfromthemixtureof3Gaussiansshownin Figure2.23.
(a)Samples fromthejointdistributionp(z)p(x|z)inwhichthethreestatesofz, correspondingtothethreecomponentsofthe mixture, aredepictedinred, green, andblue, and(b)thecorrespondingsamplesfromthemarginaldistribution p(x), whichisobtainedbysimplyignoringthevaluesofzandjustplottingthexvalues.
Thedatasetin(a)is saidtobecomplete, whereasthatin(b)isincomplete.
(c)Thesamesamplesinwhichthecoloursrepresentthe valueoftheresponsibilitiesγ(z nk )associatedwithdatapointx n, obtainedbyplottingthecorrespondingpoint usingproportionsofred, blue, andgreeninkgivenbyγ(z nk )fork=1,2,3, respectively matrix X in which the nth row is given by x T.
Similarly, the corresponding latent n variables will be denoted by an N ×K matrix Z with rows z T.
If we assume that n the data points are drawn independently from the distribution, then we can express the Gaussianmixturemodelforthisi.
i.
d.
datasetusingthegraphicalrepresentation shownin Figure9.6.
From(9.7)thelogofthelikelihoodfunctionisgivenby N K lnp(X|π,µ,Σ)= ln πk N(xn |µ k ,Σk) .
(9.14) n=1 k=1 Before discussing how to maximize this function, it is worth emphasizing that there is a significant problem associated with the maximum likelihood framework applied to Gaussian mixture models, due to the presence of singularities.
For sim- plicity, consider a Gaussian mixture whose components have covariance matrices given by Σk = σ k 2I, where I is the unit matrix, although the conclusions will hold forgeneralcovariancematrices.
Supposethatoneofthecomponentsofthemixture model, letussaythejthcomponent, hasitsmeanµ exactlyequaltooneofthedata j Figure9.6 Graphical representation of a Gaussian mixture model zn forasetof N i.
i.
d.
datapoints{x n }, withcorresponding π latentpoints{z n }, wheren=1,..., N.
xn µ Σ N 434 9.
MIXTUREMODELSANDEM Figure9.7 Illustration of how singularities in the likelihood function arise with mixtures of Gaussians.
This should be com- p(x) paredwiththecaseofasingle Gaus- sianshownin Figure1.14forwhichno singularitiesarise.
x points so that µ j = xn for some value of n.
This data point will then contribute a terminthelikelihoodfunctionoftheform 1 1 N(xn |xn,σ j 2I)= (2π)1/2σj .
(9.15) If we consider the limit σj → 0, then we see that this term goes to infinity and so the log likelihood function will also go to infinity.
Thus the maximization of the log likelihood function is not a well posed problem because such singularities will always be present and will occur whenever one of the Gaussian components ‘collapses’ onto a specific data point.
Recall that this problem did not arise in the case of a single Gaussian distribution.
To understand the difference, note that if a single Gaussian collapses onto a data point it will contribute multiplicative factors tothelikelihoodfunctionarisingfromtheotherdatapointsandthesefactorswillgo to zero exponentially fast, giving an overall likelihood that goes to zero rather than infinity.
However, once we have (at least) two components in the mixture, one of the components can have a finite variance and therefore assign finite probability to all of the data points while the other component can shrink onto one specific data pointandtherebycontributeaneverincreasingadditivevaluetotheloglikelihood.
Thisisillustratedin Figure9.7.
Thesesingularitiesprovideanotherexampleofthe severe over-fitting that can occur in a maximum likelihood approach.
We shall see Section10.1 thatthisdifficultydoesnotoccurifweadopta Bayesianapproach.
Forthemoment, however, wesimplynotethatinapplyingmaximumlikelihoodto Gaussianmixture models wemust takesteps toavoid finding suchpathological solutions andinstead seeklocalmaximaofthelikelihoodfunctionthatarewellbehaved.
Wecanhopeto avoidthesingularitiesbyusingsuitableheuristics, forinstancebydetectingwhena Gaussiancomponentiscollapsingandresettingitsmeantoarandomlychosenvalue whilealsoresettingitscovariancetosomelargevalue, andthencontinuingwiththe optimization.
A further issue in finding maximum likelihood solutions arises from the fact thatforanygivenmaximumlikelihoodsolution, a K-componentmixturewillhave a total of K! equivalent solutions corresponding to the K! ways of assigning K setsofparametersto K components.
Inotherwords, foranygiven(nondegenerate) pointinthespaceofparametervaluestherewillbeafurther K!−1additionalpoints all of which give rise to exactly the same distribution.
This problem is known as 9.2.
Mixturesof Gaussians 435 identifiability(Casellaand Berger,2002)andisanimportantissuewhenwewishto interpret the parameter values discovered by a model.
Identifiability will also arise whenwediscussmodelshavingcontinuouslatentvariablesin Chapter12.
However, forthepurposesoffindingagooddensitymodel, itisirrelevantbecauseanyofthe equivalentsolutionsisasgoodasanyother.
Maximizing the log likelihood function (9.14) for a Gaussian mixture model turnsouttobeamorecomplexproblemthanforthecaseofasingle Gaussian.
The difficulty arises from the presence of the summation over k that appears inside the logarithm in (9.14), so that the logarithm function no longer acts directly on the Gaussian.
If we set the derivatives of the log likelihood to zero, we will no longer obtainaclosedformsolution, asweshallseeshortly.
Oneapproachistoapplygradient-basedoptimizationtechniques(Fletcher,1987; Nocedal and Wright, 1999; Bishop and Nabney, 2008).
Although gradient-based techniques are feasible, and indeed will play an important role when we discuss mixture density networks in Chapter 5, we now consider an alternative approach known as the EM algorithm which has broad applicability and which will lay the foundationsforadiscussionofvariationalinferencetechniquesin Chapter10.
9.2.2 EM for Gaussian mixtures Anelegantandpowerfulmethodforfindingmaximumlikelihoodsolutionsfor modelswithlatentvariablesiscalledtheexpectation-maximizationalgorithm, or EM algorithm (Dempster et al., 1977; Mc Lachlan and Krishnan, 1997).
Later we shall giveageneraltreatmentof EM, andweshallalsoshowhow EMcanbegeneralized Section10.1 to obtain the variational inference framework.
Initially, we shall motivate the EM algorithm by giving a relatively informal treatment in the context of the Gaussian mixturemodel.
Weemphasize, however, that EMhasbroadapplicability, andindeed itwillbeencounteredinthecontextofavarietyofdifferentmodelsinthisbook.
Letusbeginbywritingdowntheconditionsthatmustbesatisfiedatamaximum ofthelikelihoodfunction.
Settingthederivativesoflnp(X|π,µ,Σ)in(9.14)with respecttothemeansµ ofthe Gaussiancomponentstozero, weobtain k N 0=− n=1( π j k π N j N (x ( n ) x | * µ n | k µ , j Σ ,Σ k) j) + Σk(xn −µ k ) (9.16) γ(znk) wherewehavemadeuseoftheform(2.43)forthe Gaussiandistribution.
Notethat the posterior probabilities, or responsibilities, given by (9.13) appear naturally on theright-handside.
MultiplyingbyΣ −1 (whichweassumetobenonsingular)and k rearrangingweobtain N 1 µ k = γ(znk)xn (9.17) Nk n=1 wherewehavedefined N Nk = γ(znk).
(9.18) n=1 436 9.
MIXTUREMODELSANDEM We can interpret Nk as the effective number of points assigned to cluster k.
Note carefully the form of this solution.
We see that the mean µ for the kth Gaussian k componentisobtainedbytakingaweightedmeanofallofthepointsinthedataset, inwhichtheweightingfactorfordatapointxn isgivenbytheposteriorprobability γ(znk)thatcomponentkwasresponsibleforgeneratingxn.
Ifwesetthederivativeoflnp(X|π,µ,Σ)withrespecttoΣktozero, andfollow a similar line of reasoning, making use of the result for the maximum likelihood Section2.3.4 solutionforthecovariancematrixofasingle Gaussian, weobtain N 1 Σk = γ(znk)(xn −µ k )(xn −µ k )T (9.19) Nk n=1 which has the same form as the corresponding result for a single Gaussian fitted to the data set, but again with each data point weighted by the corresponding poste- rior probability and with the denominator given by the effective number of points associatedwiththecorrespondingcomponent.
Finally, we maximize lnp(X|π,µ,Σ) with respect to the mixing coefficients πk.
Here we must take account of the constraint (9.9), which requires the mixing Appendix E coefficients to sum to one.
This can be achieved using a Lagrange multiplier and maximizingthefollowingquantity K lnp(X|π,µ,Σ)+λ πk −1 (9.20) k=1 whichgives N N(xn |µ k ,Σk) 0= +λ (9.21) n=1 j πj N(xn |µ j ,Σj) whereagainweseetheappearanceoftheresponsibilities.
Ifwenowmultiplyboth sides by πk and sum over k making use of the constraint (9.9), we find λ = −N.
Usingthistoeliminateλandrearrangingweobtain Nk πk = (9.22) N sothatthemixingcoefficientforthekth componentisgivenbytheaveragerespon- sibilitywhichthatcomponenttakesforexplainingthedatapoints.
It is worth emphasizing that the results (9.17), (9.19), and (9.22) do not con- stitute a closed-form solution for the parameters of the mixture model because the responsibilitiesγ(znk)dependonthoseparametersinacomplexwaythrough(9.13).
However, theseresultsdosuggestasimpleiterativeschemeforfindingasolutionto themaximumlikelihoodproblem, whichasweshallseeturnsouttobeaninstance of the EM algorithm for the particular case of the Gaussian mixture model.
We firstchoosesomeinitialvaluesforthemeans, covariances, andmixingcoefficients.
Then we alternate between the following two updates that we shall call the E step 9.2.
Mixturesof Gaussians 437 2 2 2 L=1 0 0 0 −2 −2 −2 −2 0 (a) 2 −2 0 (b) 2 −2 0 (c) 2 2 2 2 L=2 L=5 L=20 0 0 0 −2 −2 −2 −2 0 (d) 2 −2 0 (e) 2 −2 0 (f) 2 Figure9.8 Illustrationofthe EMalgorithmusingthe Old Faithfulsetasusedfortheillustrationofthe K-means algorithmin Figure9.1.
Seethetextfordetails.
and the M step, for reasons that will become apparent shortly.
In the expectation step, or Estep, weusethecurrentvaluesfortheparameterstoevaluatetheposterior probabilities, orresponsibilities, givenby(9.13).
Wethenusetheseprobabilitiesin the maximization step, or M step, to re-estimate the means, covariances, and mix- ing coefficients using the results (9.17), (9.19), and (9.22).
Note that in so doing we first evaluate the new means using (9.17) and then use these new values to find the covariances using (9.19), in keeping with the corresponding result for a single Gaussian distribution.
We shall show that each update to the parameters resulting from an E step followed by an M step is guaranteed to increase the log likelihood Section9.4 function.
In practice, the algorithm is deemed to have converged when the change in the log likelihood function, or alternatively in the parameters, falls below some threshold.
Weillustratethe EMalgorithmforamixtureoftwo Gaussiansappliedto therescaled Old Faithfuldatasetin Figure9.8.
Hereamixtureoftwo Gaussians isused, withcentresinitializedusingthesamevaluesasforthe K-meansalgorithm in Figure 9.1, and with precision matrices initialized to be proportional to the unit matrix.
Plot (a) shows the data points in green, together with the initial configura- tionofthemixturemodelinwhichtheonestandard-deviationcontoursforthetwo 438 9.
MIXTUREMODELSANDEM Gaussian components are shown as blue and red circles.
Plot (b) shows the result oftheinitial Estep, inwhicheachdatapointisdepictedusingaproportionofblue ink equal to the posterior probability of having been generated from the blue com- ponent, andacorrespondingproportionofredinkgivenbytheposteriorprobability ofhavingbeengeneratedbytheredcomponent.
Thus, pointsthathaveasignificant probabilityforbelongingtoeitherclusterappearpurple.
Thesituationafterthefirst M step is shown in plot (c), in which the mean of the blue Gaussian has moved to themeanofthedataset, weightedbytheprobabilitiesofeachdatapointbelonging tothebluecluster, inotherwordsithasmovedtothecentreofmassoftheblueink.
Similarly, the covariance of the blue Gaussian is set equal to the covariance of the blueink.
Analogousresultsholdfortheredcomponent.
Plots(d),(e), and(f)show the results after 2, 5, and 20 complete cycles of EM, respectively.
In plot (f) the algorithmisclosetoconvergence.
Notethatthe EMalgorithmtakesmanymoreiterationstoreach(approximate) convergence compared with the K-means algorithm, and that each cycle requires significantly more computation.
It is therefore common to run the K-means algo- rithm in order to find a suitable initialization for a Gaussian mixture model that is subsequently adapted using EM.
The covariance matrices can conveniently be ini- tialized to the sample covariances of the clusters found by the K-means algorithm, andthemixingcoefficientscanbesettothefractionsofdatapointsassignedtothe respectiveclusters.
Aswithgradient-basedapproachesformaximizingtheloglike- lihood, techniquesmustbeemployedtoavoidsingularitiesofthelikelihoodfunction in which a Gaussian component collapses onto a particular data point.
Itshould be emphasizedthattherewillgenerallybemultiplelocalmaximaoftheloglikelihood function, andthat EMisnotguaranteedtofindthelargestofthesemaxima.
Because the EMalgorithmfor Gaussianmixturesplayssuchanimportantrole, wesummarize itbelow.
EMfor Gaussian Mixtures Givena Gaussianmixturemodel, thegoalistomaximizethelikelihoodfunction with respect to the parameters (comprising the means and covariances of the componentsandthemixingcoefficients).
1.
Initialize the means µ k , covariances Σk and mixing coefficients πk, and evaluatetheinitialvalueoftheloglikelihood.
2.
Estep.
Evaluatetheresponsibilitiesusingthecurrentparametervalues πk N(xn |µ k ,Σk) γ(znk)= .
(9.23) K πj N(xn |µ j ,Σj) j=1 9.3.
An Alternative Viewof EM 439 3.
Mstep.
Re-estimatetheparametersusingthecurrentresponsibilities N 1 µn k ew = γ(znk)xn (9.24) Nk n=1 N 1 Σn k ew = γ(znk)(xn −µn k ew)(xn −µn k ew) T (9.25) Nk n=1 πnew = Nk (9.26) k N where N Nk = γ(znk).
(9.27) n=1 4.
Evaluatetheloglikelihood N K lnp(X|µ,Σ,π)= ln πk N(xn |µ k ,Σk) (9.28) n=1 k=1 andcheckforconvergenceofeithertheparametersortheloglikelihood.
If theconvergencecriterionisnotsatisfiedreturntostep2.
9.3.
An Alternative View of EM In this section, we present a complementary view of the EM algorithm that recog- nizes the key role played by latent variables.
We discuss this approach first of all in an abstract setting, and then for illustration we consider once again the case of Gaussianmixtures.
Thegoalofthe EMalgorithmistofindmaximumlikelihoodsolutionsformod- elshavinglatentvariables.
Wedenotethesetofallobserveddataby X, inwhichthe nth row represents x T, and similarly we denote the set of all latent variables by Z, n withacorrespondingrowz T.
Thesetofallmodelparametersisdenotedbyθ, and n sotheloglikelihoodfunctionisgivenby lnp(X|θ)=ln p(X, Z|θ) .
(9.29) Z Notethatourdiscussionwillapplyequallywelltocontinuouslatentvariablessimply byreplacingthesumover Zwithanintegral.
Akeyobservationisthatthesummationoverthelatentvariablesappearsinside the logarithm.
Even if the joint distribution p(X, Z|θ) belongs to the exponential 440 9.
MIXTUREMODELSANDEM family, the marginal distribution p(X|θ) typically does not as a result of this sum- mation.
Thepresenceofthesumpreventsthelogarithmfromactingdirectlyonthe jointdistribution, resultingincomplicatedexpressionsforthemaximumlikelihood solution.
Now suppose that, for each observation in X, we were told the corresponding value of the latent variable Z.
We shall call {X, Z} the complete data set, and we shall refer to the actual observed data X as incomplete, as illustrated in Figure 9.5.
Thelikelihoodfunctionforthecompletedatasetsimplytakestheformlnp(X, Z|θ), andweshallsupposethatmaximizationofthiscomplete-dataloglikelihoodfunction isstraightforward.
In practice, however, we are not given the complete data set {X, Z}, but only theincompletedata X.
Ourstateofknowledgeofthevaluesofthelatentvariables in Z is given only by the posterior distribution p(Z|X,θ).
Because we cannot use the complete-data log likelihood, we consider instead its expected value under the posteriordistributionofthelatentvariable, whichcorresponds(asweshallsee)tothe Estepofthe EMalgorithm.
Inthesubsequent Mstep, wemaximizethisexpectation.
If the current estimate for the parameters is denoted θold , then a pair of successive Eand Mstepsgivesrisetoarevisedestimateθnew .
Thealgorithmisinitializedby choosingsomestartingvaluefortheparametersθ 0.
Theuseoftheexpectationmay seemsomewhatarbitrary.
However, weshallseethemotivationforthischoicewhen wegiveadeepertreatmentof EMin Section9.4.
In the E step, we use the current parameter values θold to find the posterior distributionofthelatentvariablesgivenbyp(Z|X,θold ).
Wethenusethisposterior distributiontofindtheexpectationofthecomplete-dataloglikelihoodevaluatedfor somegeneralparametervalueθ.
Thisexpectation, denoted Q(θ,θold ), isgivenby Q(θ,θold )= p(Z|X,θold )lnp(X, Z|θ).
(9.30) Z Inthe Mstep, wedeterminetherevisedparameterestimateθnew bymaximizingthis function θnew =argmax Q(θ,θold ).
(9.31) θ Note that in the definition of Q(θ,θold ), the logarithm acts directly on the joint distributionp(X, Z|θ), andsothecorresponding M-stepmaximizationwill, bysup- position, betractable.
Thegeneral EMalgorithmissummarizedbelow.
Ithastheproperty, asweshall show later, that each cycle of EM will increase the incomplete-data log likelihood Section9.4 (unlessitisalreadyatalocalmaximum).
The General EMAlgorithm Givenajointdistributionp(X, Z|θ)overobservedvariables Xandlatentvari- ables Z, governedbyparametersθ, thegoalistomaximizethelikelihoodfunc- tionp(X|θ)withrespecttoθ.
1.
Chooseaninitialsettingfortheparametersθold .
9.3.
An Alternative Viewof EM 441 2.
Estep Evaluatep(Z|X,θold ).
3.
Mstep Evaluateθnew givenby θnew =argmax Q(θ,θold ) (9.32) θ where Q(θ,θold )= p(Z|X,θold )lnp(X, Z|θ).
(9.33) Z 4.
Checkforconvergenceofeithertheloglikelihoodortheparametervalues.
Iftheconvergencecriterionisnotsatisfied, thenlet θold ←θnew (9.34) andreturntostep2.
The EMalgorithmcanalsobeusedtofind MAP(maximumposterior)solutions Exercise 9.4 for models in which a prior p(θ) is defined over the parameters.
In this case the E stepremainsthesameasinthemaximumlikelihoodcase, whereasinthe Mstepthe quantitytobemaximizedisgivenby Q(θ,θold )+lnp(θ).
Suitablechoicesforthe priorwillremovethesingularitiesofthekindillustratedin Figure9.7.
Herewehaveconsideredtheuseofthe EMalgorithmtomaximizealikelihood function when there are discrete latent variables.
However, it can also be applied when the unobserved variables correspond to missing values in the data set.
The distributionoftheobservedvaluesisobtainedbytakingthejointdistributionofall the variables and then marginalizing over the missing ones.
EM can then be used to maximize the corresponding likelihood function.
We shall show an example of the application of this technique in the context of principal component analysis in Figure12.11.
Thiswillbeavalidprocedureifthedatavaluesaremissingatrandom, meaning that the mechanism causing values to be missing does not depend on the unobserved values.
In many situations this will not be the case, for instance if a sensor fails to return a value whenever the quantity it is measuring exceeds some threshold.
9.3.1 Gaussian mixtures revisited We now consider the application of this latent variable view of EM to the spe- cificcaseofa Gaussianmixturemodel.
Recallthatourgoalistomaximizethelog likelihoodfunction(9.14), whichiscomputedusingtheobserveddataset X, andwe saw that this was more difficult than for the case of a single Gaussian distribution duetothepresenceofthesummationoverk thatoccursinsidethelogarithm.
Sup- posethenthatinadditiontotheobserveddataset X, wewerealsogiventhevalues of the corresponding discrete variables Z.
Recall that Figure 9.5(a) shows a ‘com- plete’ data set (i.
e., one that includes labels showing which component generated eachdatapoint)while Figure9.5(b)showsthecorresponding‘incomplete’dataset.
Thegraphicalmodelforthecompletedataisshownin Figure9.9.
442 9.
MIXTUREMODELSANDEM Figure9.9 Thisshowsthesamegraphasin Figure9.6exceptthat zn we now suppose that the discrete variables z n are ob- π served, aswellasthedatavariablesx n.
xn µ Σ N Now consider the problem of maximizing the likelihood for the complete data set{X, Z}.
From(9.10)and(9.11), thislikelihoodfunctiontakestheform N K p(X, Z|µ,Σ,π)= π k z nk N(xn |µ k ,Σk) z nk (9.35) n=1k=1 whereznk denotesthekth componentofzn.
Takingthelogarithm, weobtain N K lnp(X, Z|µ,Σ,π)= znk {lnπk+ln N(xn |µ k ,Σk)}.
(9.36) n=1k=1 Comparison with the log likelihood function (9.14) for the incomplete data shows that the summation over k and the logarithm have been interchanged.
The loga- rithm now acts directly on the Gaussian distribution, which itself is a member of the exponential family.
Not surprisingly, this leads to a much simpler solution to themaximumlikelihoodproblem, aswenowshow.
Considerfirstthemaximization with respect to the means and covariances.
Because zn is a K-dimensional vec- tor with all elements equal to 0 except for a single element having the value 1, the complete-data log likelihood function is simply a sum of K independent contribu- tions, one for each mixture component.
Thus the maximization with respect to a meanoracovarianceisexactlyasforasingle Gaussian, exceptthatitinvolvesonly thesubsetofdatapointsthatare‘assigned’tothatcomponent.
Forthemaximization with respect to the mixing coefficients, we note that these are coupled for different valuesofk byvirtueofthesummationconstraint(9.9).
Again, thiscanbeenforced usinga Lagrangemultiplierasbefore, andleadstotheresult N 1 πk = znk (9.37) N n=1 so that the mixing coefficients are equal to the fractions of data points assigned to thecorrespondingcomponents.
Thus we see that the complete-data log likelihood function can be maximized trivially in closed form.
In practice, however, we do not have values for the latent variables so, as discussed earlier, we consider the expectation, with respect to the posterior distribution of the latent variables, of the complete-data log likelihood.
9.3.
An Alternative Viewof EM 443 Using (9.10) and (9.11) together with Bayes’ theorem, we see that this posterior distributiontakestheform N K p(Z|X,µ,Σ,π)∝ [πk N(xn |µ k ,Σk)] z nk.
(9.38) n=1k=1 and hence factorizes over n so that under the posterior distribution the {zn } are Exercise 9.5 independent.
Thisiseasilyverifiedbyinspectionofthedirectedgraphin Figure9.6 Section8.2 and making use of the d-separation criterion.
The expected value of the indicator variableznk underthisposteriordistributionisthengivenby znk[πk N(xn |µ k ,Σk)] z nk E[znk] = z n k z πj N(xn |µ j ,Σj) nj z nj πk N(xn |µ k ,Σk) = =γ(znk) (9.39) K πj N(xn |µ j ,Σj) j=1 whichisjusttheresponsibilityofcomponentkfordatapointxn.
Theexpectedvalue ofthecomplete-dataloglikelihoodfunctionisthereforegivenby N K E Z [lnp(X, Z|µ,Σ,π)]= γ(znk){lnπk+ln N(xn |µ k ,Σk)}.
(9.40) n=1k=1 Wecannowproceedasfollows.
Firstwechoosesomeinitialvaluesfortheparam- etersµold,Σold andπold, andusethesetoevaluatetheresponsibilities(the Estep).
Wethenkeeptheresponsibilitiesfixedandmaximize(9.40)withrespecttoµ k ,Σk and πk (the M step).
This leads to closed form solutions forµnew, Σnew and πnew Gaussianmixturesasderivedearlier.
Weshallgainmoreinsightintotheroleofthe expectedcomplete-dataloglikelihoodfunctionwhenwegiveaproofofconvergence ofthe EMalgorithmin Section9.4.
9.3.2 Relation to K-means Comparison of the K-means algorithm with the EM algorithm for Gaussian mixtures shows that there is a close similarity.
Whereas the K-means algorithm performs a hard assignment of data points to clusters, in which each data point is associated uniquely with one cluster, the EM algorithm makes a soft assignment based on the posterior probabilities.
In fact, we can derive the K-means algorithm asaparticularlimitof EMfor Gaussianmixturesasfollows.
Consider a Gaussian mixture model in which the covariance matrices of the mixture components are given by I, where is a variance parameter that is shared 444 9.
MIXTUREMODELSANDEM byallofthecomponents, and Iistheidentitymatrix, sothat 1 1 p(x|µ k ,Σk)= (2π )1/2 exp − 2 x−µ k 2 .
(9.41) We now consider the EM algorithm for a mixture of K Gaussians of this form in whichwetreat asafixedconstant, insteadofaparametertobere-estimated.
From (9.13) the posterior probabilities, or responsibilities, for a particular data point xn, aregivenby γ(znk)= πkexp{− xn −µ k 2/2 } .
(9.42) j πjexp − xn −µ j 2/2 If we consider the limit → 0, we see that in the denominator the term for which xn −µ j 2 is smallest will go to zero most slowly, and hence the responsibilities γ(znk)forthedatapointxn allgotozeroexceptfortermj, forwhichtheresponsi- bilityγ(znj)willgotounity.
Notethatthisholdsindependentlyofthevaluesofthe πk solongasnoneoftheπk iszero.
Thus, inthislimit, weobtainahardassignment of data points to clusters, just as in the K-means algorithm, so that γ(znk) → rnk where rnk is defined by (9.2).
Each data point is thereby assigned to the cluster havingtheclosestmean.
The EMre-estimationequationfortheµ , givenby(9.17), thenreducestothe k K-meansresult(9.4).
Notethatthere-estimationformulaforthemixingcoefficients (9.22)simplyre-setsthevalueofπktobeequaltothefractionofdatapointsassigned toclusterk, althoughtheseparametersnolongerplayanactiveroleinthealgorithm.
Finally, inthelimit → 0theexpectedcomplete-dataloglikelihood, givenby Exercise 9.11 (9.40), becomes N K 1 E Z [lnp(X, Z|µ,Σ,π)]→− rnk xn −µ k 2+const.
(9.43) 2 n=1k=1 Thusweseethatinthislimit, maximizingtheexpectedcomplete-dataloglikelihood is equivalent to minimizing the distortion measure J for the K-means algorithm givenby(9.1).
Notethatthe K-meansalgorithmdoesnotestimatethecovariancesoftheclus- tersbutonlytheclustermeans.
Ahard-assignmentversionofthe Gaussianmixture modelwithgeneralcovariancematrices, knownastheelliptical K-meansalgorithm, hasbeenconsideredby Sungand Poggio(1994).
9.3.3 Mixtures of Bernoulli distributions So far in this chapter, we have focussed on distributions over continuous vari- ables described by mixtures of Gaussians.
As a further example of mixture mod- elling, andtoillustratethe EMalgorithminadifferentcontext, wenowdiscussmix- tures of discrete binary variables described by Bernoulli distributions.
This model isalsoknownaslatentclassanalysis(Lazarsfeldand Henry,1968; Mc Lachlanand Peel, 2000).
As well as being of practical importance in its own right, our discus- sionof Bernoullimixtureswillalsolaythefoundationforaconsiderationofhidden Section13.2 Markovmodelsoverdiscretevariables.
9.3.
An Alternative Viewof EM 445 Consider a set of D binary variables xi, where i = 1,..., D, each of which is governedbya Bernoullidistributionwithparameterµi, sothat D p(x|µ)= µ x i i(1−µi)(1−x i) (9.44) i=1 variablesxi areindependent, givenµ.
Themeanandcovarianceofthisdistribution areeasilyseentobe E[x] = µ (9.45) cov[x] = diag{µi(1−µi)}.
(9.46) Nowletusconsiderafinitemixtureofthesedistributionsgivenby K p(x|µ,π)= πkp(x|µ k ) (9.47) k=1 whereµ={µ 1 ,...,µ K },π ={π 1 ,...,πK }, and D p(x|µ k )= µ x ki i(1−µki)(1−x i).
(9.48) i=1 Exercise 9.12 Themeanandcovarianceofthismixturedistributionaregivenby K E[x] = πk µ k (9.49) k=1 K cov[x] = πk Σk+µ k µT k −E[x]E[x]T (9.50) k=1 where Σk = diag{µki(1−µki)}.
Because the covariance matrix cov[x] is no longer diagonal, the mixture distribution can capture correlations between the vari- ables, unlikeasingle Bernoullidistribution.
If we are given a data set X = {x 1 ,..., x N } then the log likelihood function forthismodelisgivenby N K lnp(X|µ,π)= ln πkp(xn |µ k ) .
(9.51) n=1 k=1 Again we see the appearance of the summation inside the logarithm, so that the maximumlikelihoodsolutionnolongerhasclosedform.
We now derive the EM algorithm for maximizing the likelihood function for themixtureof Bernoullidistributions.
Todothis, wefirstintroduceanexplicitlatent 446 9.
MIXTUREMODELSANDEM variablezassociatedwitheachinstanceofx.
Asinthecaseofthe Gaussianmixture, z = (z 1 ,..., z K)T is a binary K-dimensional variable having a single component equalto1, withallothercomponentsequalto0.
Wecanthenwritetheconditional distributionofx, giventhelatentvariable, as K p(x|z,µ)= p(x|µ ) z k (9.52) k k=1 whilethepriordistributionforthelatentvariablesisthesameasforthemixtureof Gaussiansmodel, sothat K p(z|π)= π z k.
(9.53) k k=1 Ifweformtheproductofp(x|z,µ)andp(z|π)andthenmarginalizeoverz, thenwe Exercise 9.14 recover(9.47).
Inordertoderivethe EMalgorithm, wefirstwritedownthecomplete-datalog likelihoodfunction, whichisgivenby N K lnp(X, Z|µ,π)= znk lnπk n=1k=1 D + [xnilnµki+(1−xni)ln(1−µki)] (9.54) i=1 where X={xn }and Z={zn }.
Nextwetaketheexpectationofthecomplete-data loglikelihoodwithrespecttotheposteriordistributionofthelatentvariablestogive N K E Z [lnp(X, Z|µ,π)]= γ(znk) lnπk n=1k=1 D + [xnilnµki+(1−xni)ln(1−µki)] (9.55) i=1 whereγ(znk) = E[znk]istheposteriorprobability, orresponsibility, ofcomponent kgivendatapointxn.
Inthe Estep, theseresponsibilitiesareevaluatedusing Bayes’ theorem, whichtakestheform znk[πkp(xn |µ k )] z nk γ(znk)=E[znk] = z n k z πjp(xn |µ j ) nj z nj πkp(xn |µ k ) = .
(9.56) K πjp(xn |µ j ) j=1 9.3.
An Alternative Viewof EM 447 If we consider the sum over n in (9.55), we see that the responsibilities enter onlythroughtwoterms, whichcanbewrittenas N Nk = γ(znk) (9.57) n=1 N 1 xk = γ(znk)xn (9.58) Nk n=1 where Nk istheeffectivenumberofdatapointsassociatedwithcomponentk.
Inthe Mstep, wemaximizetheexpectedcomplete-dataloglikelihoodwithrespecttothe parametersµ andπ.
If wesetthederivativeof (9.55)withrespecttoµ equalto k k Exercise 9.15 zeroandrearrangetheterms, weobtain µ k =xk.
(9.59) We see that this sets the mean of component k equal to a weighted mean of the data, withweightingcoefficientsgivenbytheresponsibilitiesthatcomponentktakes for data points.
For the maximization with res pect to πk, we need to introduce a Lagrange multiplier to enforce the constraint k πk = 1.
Following analogous Exercise 9.16 stepstothoseusedforthemixtureof Gaussians, wethenobtain Nk πk = (9.60) N whichrepresentstheintuitivelyreasonableresultthatthemixingcoefficientforcom- ponentk isgivenbytheeffectivefractionofpointsinthedatasetexplainedbythat component.
Note that in contrast to the mixture of Gaussians, there are no singularities in which the likelihood function goes to infinity.
This can be seen by noting that the Exercise 9.17 likelihood function is bounded above because 0 p(xn |µ k ) 1.
There exist singularities at which the likelihood function goes to zero, but these will not be found by EM provided it is not initialized to a pathological starting point, because the EMalgorithmalwaysincreasesthevalueofthelikelihoodfunction, untilalocal Section9.4 maximum is found.
We illustrate the Bernoulli mixture model in Figure 9.10 by using it to model handwritten digits.
Here the digit images have been turned into binary vectors by setting all elements whose values exceed 0.5 to 1 and setting the remainingelementsto0.
Wenowfitadatasetof N = 600suchdigits, comprising the digits ‘2’, ‘3’, and ‘4’, with a mixture of K = 3 Bernoulli distributions by running10iterationsofthe EMalgorithm.
Themixingcoefficientswereinitialized toπk =1/K, andtheparametersµkj weresettorandomvalueschosen uniformlyin therange(0.25,0.75)andthennormalizedtosatisfytheconstraintthat j µkj =1.
Weseethatamixtureof3Bernoullidistributionsisabletofindthethreeclustersin thedatasetcorrespondingtothedifferentdigits.
The conjugate prior for the parameters of a Bernoulli distribution is given by thebetadistribution, andwehaveseenthatabetapriorisequivalenttointroducing 448 9.
MIXTUREMODELSANDEM Figure9.10 Illustrationofthe Bernoullimixturemodelinwhichthetoprowshowsexamplesfromthedigitsdata setafterconvertingthepixelvaluesfromgreyscaletobinaryusingathresholdof0.5.
Onthebottomrowthefirst threeimagesshowtheparametersµ kiforeachofthethreecomponentsinthemixturemodel.
Asacomparison, wealsofitthesamedatasetusingasinglemultivariate Bernoullidistribution, againusingmaximumlikelihood.
Thisamountstosimplyaveragingthecountsineachpixelandisshownbytheright-mostimageonthebottom row.
Section2.1.1 additional effective observations of x.
We can similarly introduce priors into the Bernoulli mixture model, and use EM to maximize the posterior probability distri- Exercise 9.18 butions.
It is straightforward to extend the analysis of Bernoulli mixtures to the case of Exercise 9.19 multinomialbinaryvariableshaving M >2statesbymakinguseofthediscretedis- tribution(2.26).
Again, wecanintroduce Dirichletpriorsoverthemodelparameters ifdesired.
9.3.4 EM for Bayesian linear regression As a third example of the application of EM, we return to the evidence ap- proximation for Bayesian linear regression.
In Section 3.5.2, we obtained the re- estimationequationsforthehyperparametersαandβ byevaluationoftheevidence and then setting the derivatives of the resulting expression to zero.
We now turn to analternativeapproachforfindingαandβ basedonthe EMalgorithm.
Recallthat ourgoalistomaximizetheevidencefunctionp(t|α,β)givenby(3.77)withrespect toαandβ.
Becausetheparametervectorwismarginalizedout, wecanregarditas alatentvariable, andhencewecanoptimizethismarginallikelihoodfunctionusing EM.
Inthe Estep, wecomputetheposteriordistributionofwgiventhecurrentset- tingoftheparametersαandβ andthenusethistofindtheexpectedcomplete-data loglikelihood.
Inthe Mstep, wemaximizethisquantitywithrespecttoαandβ.
We havealreadyderivedtheposteriordistributionofw becausethisisgivenby(3.49).
Thecomplete-dataloglikelihoodfunctionisthengivenby lnp(t, w|α,β)=lnp(t|w,β)+lnp(w|α) (9.61) 9.3.
An Alternative Viewof EM 449 wherethelikelihoodp(t|w,β)andthepriorp(w|α)aregivenby(3.10)and(3.52), respectively, and y(x, w) is given by (3.3).
Taking the expectation with respect to theposteriordistributionofwthengives M α α N β E[lnp(t, w|α,β)] = ln − E w Tw + ln 2 2π 2 2 2π N β − E (tn −w Tφ n )2 .
(9.62) 2 n=1 Settingthederivativeswithrespecttoαtozero, weobtainthe Mstepre-estimation Exercise 9.20 equation M M α= = .
(9.63) E[w Tw] m T N m N +Tr(SN) Exercise 9.21 Ananalogousresultholdsforβ.
Note that this re-estimation equation takes a slightly different form from the corresponding result (3.92) derived by direct evaluation of the evidence function.
However, theyeachinvolvecomputationandinversion(oreigendecomposition)of an M ×M matrixandhencewillhavecomparablecomputationalcostperiteration.
These two approaches to determiningα should of course converge to the same result(assumingtheyfindthesamelocalmaximumoftheevidencefunction).
This canbeverifiedbyfirstnotingthatthequantityγ isdefinedby M 1 γ =M −α =M −αTr(SN).
(9.64) λi+α i=1 Atastationarypointoftheevidencefunction, there-estimationequation(3.92)will beself-consistentlysatisfied, andhencewecansubstituteforγ togive αm T N m N =γ =M −αTr(SN) (9.65) andsolvingforαweobtain(9.63), whichispreciselythe EMre-estimationequation.
As a final example, we consider a closely related model, namely the relevance vectormachineforregressiondiscussedin Section7.2.1.
Thereweuseddirectmax- imizationofthemarginallikelihoodtoderivere-estimationequationsforthehyper- parametersαandβ.
Hereweconsideranalternativeapproachinwhichweviewthe weightvectorwasalatentvariableandapplythe EMalgorithm.
The Estepinvolves findingtheposteriordistributionovertheweights, andthisisgivenby(7.81).
Inthe Mstepwemaximizetheexpectedcomplete-dataloglikelihood, whichisdefinedby E w [lnp(t|X, w,β)p(w|α)] (9.66) where the expectation is taken with respect to the posterior distribution computed usingthe‘old’parametervalues.
Tocomputethenewparametervalueswemaximize Exercise 9.22 withrespecttoαandβ togive 450 9.
MIXTUREMODELSANDEM 1 αnew = (9.67) i m2 i +Σii (βnew) −1 = t−Φm N 2+β−1 i γi (9.68) N These re-estimation equations are formally equivalent to those obtained by direct Exercise 9.23 maxmization.
9.4.
The EM Algorithm in General Theexpectationmaximizationalgorithm, or EMalgorithm, isageneraltechniquefor finding maximum likelihood solutions for probabilistic models having latent vari- ables(Dempsteretal.,1977; Mc Lachlanand Krishnan,1997).
Herewegiveavery general treatment of the EM algorithm and in the process provide a proof that the EM algorithm derived heuristically in Sections 9.2 and 9.3 for Gaussian mixtures does indeed maximize the likelihood function (Csisza`r and Tusna`dy, 1984; Hath- away,1986; Nealand Hinton,1999).
Ourdiscussionwillalsoformthebasisforthe Section10.1 derivationofthevariationalinferenceframework.
Consider a probabilistic model in which we collectively denote all of the ob- served variables by X and all of the hidden variables by Z.
The joint distribution p(X, Z|θ) is governed by a set of parameters denoted θ.
Our goal is to maximize thelikelihoodfunctionthatisgivenby p(X|θ)= p(X, Z|θ).
(9.69) Z Here we are assuming Z is discrete, although the discussion is identical if Z com- prises continuous variables or a combination of discrete and continuous variables, withsummationreplacedbyintegrationasappropriate.
We shall suppose that direct optimization of p(X|θ) is difficult, but that opti- mization of the complete-data likelihood function p(X, Z|θ) is significantly easier.
Next we introduce a distribution q(Z) defined over the latent variables, and we ob- servethat, foranychoiceofq(Z), thefollowingdecompositionholds lnp(X|θ)=L(q,θ)+KL(q p) (9.70) wherewehavedefined p(X, Z|θ) L(q,θ) = q(Z)ln (9.71) q(Z) Z p(Z|X,θ) KL(q p) = − q(Z)ln .
(9.72) q(Z) Z Note that L(q,θ) is a functional (see Appendix D for a discussion of functionals) of the distribution q(Z), and a function of the parameters θ.
It is worth studying 9.4.
The EMAlgorithmin General 451 Figure9.11 Illustrationofthedecompositiongiven by (9.70), which holds for any choice of distribution q(Z).
Because the KL(q||p) Kullback-Leibler divergence satisfies KL(q p) 0, we see that the quan- tity L(q,θ)isalowerboundonthelog likelihoodfunctionlnp(X|θ).
L(q,θ) lnp(X|θ) carefullytheformsoftheexpressions(9.71)and(9.72), andinparticularnotingthat they differ in sign and also that L(q,θ) contains the joint distribution of X and Z while KL(q p) contains the conditional distribution of Z given X.
To verify the Exercise 9.24 decomposition(9.70), wefirstmakeuseoftheproductruleofprobabilitytogive lnp(X, Z|θ)=lnp(Z|X,θ)+lnp(X|θ) (9.73) whichwethensubstituteintotheexpressionfor L(q,θ).
Thisgivesrisetotwoterms, one of which cancels KL(q p) while the other gives the required log likelihood lnp(X|θ)afternotingthatq(Z)isanormalizeddistributionthatsumsto1.
From (9.72), we see that KL(q p) is the Kullback-Leibler divergence between q(Z) and the posterior distribution p(Z|X,θ).
Recall that the Kullback-Leibler di- Section1.6.1 vergencesatisfies KL(q p) 0, withequalityif, andonlyif, q(Z) = p(Z|X,θ).
It thereforefollowsfrom(9.70)that L(q,θ) lnp(X|θ), inotherwordsthat L(q,θ) is a lower bound on lnp(X|θ).
The decomposition (9.70) is illustrated in Fig- ure9.11.
The EM algorithm is a two-stage iterative optimization technique for finding maximum likelihood solutions.
We can use the decomposition (9.70) to define the EM algorithm and to demonstrate that it does indeed maximize the log likelihood.
Suppose that the current value of the parameter vector is θold .
In the E step, the lowerbound L(q,θold )ismaximizedwithrespecttoq(Z)whileholdingθold fixed.
The solution to this maximization problem is easily seen by noting that the value oflnp(X|θold )doesnotdependonq(Z)andsothelargestvalueof L(q,θold )will occurwhenthe Kullback-Leiblerdivergencevanishes, inotherwordswhenq(Z)is equal to the posterior distribution p(Z|X,θold ).
In this case, the lower bound will equaltheloglikelihood, asillustratedin Figure9.12.
Inthesubsequent Mstep, thedistributionq(Z)isheldfixedandthelowerbound L(q,θ) is maximized with respect to θ to give some new value θnew .
This will causethelowerbound Ltoincrease(unlessitisalreadyatamaximum), whichwill necessarilycausethecorrespondingloglikelihoodfunctiontoincrease.
Becausethe distributionqisdeterminedusingtheoldparametervaluesratherthanthenewvalues and is held fixed during the M step, it will not equal the new posterior distribution p(Z|X,θnew ), andhencetherewillbeanonzero KLdivergence.
Theincreaseinthe log likelihood function is therefore greater than the increase in the lower bound, as 452 9.
MIXTUREMODELSANDEM Figure9.12 Illustration of the E step of KL(q||p)=0 the EM algorithm.
The q distribution is set equal to the posterior distribution for the current parameter val- ues θold, causing the lower bound to move up to the same value as the log like- lihood function, with the KL L(q,θold ) lnp(X|θold ) divergencevanishing.
shownin Figure9.13.
Ifwesubstituteq(Z)=p(Z|X,θold )into(9.71), weseethat, afterthe Estep, thelowerboundtakestheform L(q,θ) = p(Z|X,θold )lnp(X, Z|θ)− p(Z|X,θold )lnp(Z|X,θold ) Z Z = Q(θ,θold )+const (9.74) wheretheconstantissimplythenegativeentropyof theq distributionandisthere- foreindependentofθ.
Thusinthe Mstep, thequantitythatisbeingmaximizedisthe expectationofthecomplete-dataloglikelihood, aswesawearlierinthecaseofmix- tures of Gaussians.
Note that the variable θ over which we are optimizing appears onlyinsidethelogarithm.
Ifthejointdistributionp(Z, X|θ)comprisesamemberof theexponentialfamily, oraproductofsuchmembers, thenweseethatthelogarithm willcanceltheexponentialandleadtoan Mstepthatwillbetypicallymuchsimpler thanthemaximizationofthecorrespondingincomplete-dataloglikelihoodfunction p(X|θ).
Theoperationofthe EMalgorithmcanalsobeviewedinthespaceofparame- ters, as illustrated schematically in Figure 9.14.
Here the red curve depicts the (in- Figure9.13 Illustration of the M step of the EM algorithm.
The distribution q(Z) KL(q||p) is held fixed and the lower bound L(q,θ) is maximized with respect to the parameter vector θ to give a revised value θnew.
Because the KL divergence is nonnegative, this causes the log likelihood lnp(X|θ) to increase by at least as much as thelowerbounddoes.
L(q,θnew ) lnp(X|θnew ) 9.4.
The EMAlgorithmin General 453 Figure9.14 The EM algorithm involves alter- nately computing a lower bound on the log likelihood for the cur- rent parameter values and then lnp(X|θ) maximizing this bound to obtain the new parameter values.
See thetextforafulldiscussion.
L(q,θ) θold θnew completedata)loglikelihoodfunctionwhosevaluewewishtomaximize.
Westart withsomeinitialparametervalueθold , andinthefirst Estepweevaluatetheposte- riordistributionoverlatentvariables, whichgivesrisetoalowerbound L(θ,θ(old) ) whosevalueequalstheloglikelihoodatθ(old) , asshownbythebluecurve.
Notethat the bound makes a tangential contact with the log likelihood at θ(old) , so that both Exercise 9.25 curves have the same gradient.
This bound is a convex function having a unique maximum(formixturecomponentsfromtheexponentialfamily).
Inthe Mstep, the boundismaximizedgivingthevalueθ(new) , whichgivesalargervalueofloglikeli- hoodthanθ(old) .
Thesubsequent Estepthenconstructsaboundthatistangentialat θ(new) asshownbythegreencurve.
For the particular case of an independent, identically distributed data set, X will comprise N data points {xn } while Z will comprise N corresponding latent variables{zn }, wheren = 1,..., N.
Fromtheindependenceassumption, wehave p(X, Z) = n p(xn, zn) and, by marginalizing over the {zn } we have p(X) = n p(xn).
Using the sum and product rules, we see that the posterior probability thatisevaluatedinthe Esteptakestheform N p(xn, zn |θ) N p(X, Z|θ) p(Z|X,θ)= = n=1 = p(zn |xn,θ) (9.75) p(X, Z|θ) N p(xn, zn |θ) n=1 Z Z n=1 and so the posterior distribution also factorizes with respect to n.
In the case of the Gaussianmixturemodelthissimplysaysthattheresponsibilitythateachofthe mixture components takes for a particular data point xn depends only on the value ofxn andon theparameters θ of themixture components, not onthevaluesof the otherdatapoints.
Wehaveseenthatboththe Eandthe Mstepsofthe EMalgorithmareincreas- ing the value of a well-defined bound on the log likelihood function and that the 454 9.
MIXTUREMODELSANDEM complete EM cycle will change the model parameters in such a way as to cause the log likelihood to increase (unless it is already at a maximum, in which case the parametersremainunchanged).
Wecanalsousethe EMalgorithmtomaximizetheposteriordistributionp(θ|X) formodelsinwhichwehaveintroducedapriorp(θ)overtheparameters.
Toseethis, wenotethatasafunctionofθ, wehavep(θ|X)=p(θ, X)/p(X)andso lnp(θ|X)=lnp(θ, X)−lnp(X).
(9.76) Makinguseofthedecomposition(9.70), wehave lnp(θ|X) = L(q,θ)+KL(q p)+lnp(θ)−lnp(X) L(q,θ)+lnp(θ)−lnp(X).
(9.77) where lnp(X) is a constant.
We can again optimize the right-hand side alternately withrespecttoq andθ.
Theoptimizationwithrespecttoq givesrisetothesame E- stepequationsasforthestandard EMalgorithm, becauseq onlyappearsin L(q,θ).
The M-stepequationsaremodifiedthroughtheintroductionofthepriortermlnp(θ), whichtypicallyrequiresonlyasmallmodificationtothestandardmaximumlikeli- hood M-stepequations.
The EMalgorithmbreaksdownthepotentiallydifficultproblemofmaximizing thelikelihoodfunctionintotwostages, the Estepandthe Mstep, eachofwhichwill often prove simpler to implement.
Nevertheless, for complex models it may be the case that either the E step or the M step, or indeed both, remain intractable.
This leadstotwopossibleextensionsofthe EMalgorithm, asfollows.
Thegeneralized EM, or GEM, algorithmaddressestheproblemofanintractable M step.
Instead of aiming to maximize L(q,θ) with respect to θ, it seeks instead to change the parameters in such a way as to increase its value.
Again, because L(q,θ)isalowerboundontheloglikelihoodfunction, eachcomplete EMcycleof the GEMalgorithmisguaranteedtoincreasethevalueoftheloglikelihood(unless the parameters already correspond to a local maximum).
One way to exploit the GEM approach would be to use one of the nonlinear optimization strategies, such as the conjugate gradients algorithm, during the M step.
Another form of GEM algorithm, knownastheexpectationconditionalmaximization, or ECM, algorithm, involves making several constrained optimizations within each M step (Meng and Rubin,1993).
Forinstance, theparametersmightbepartitionedintogroups, andthe Mstepisbrokendownintomultiplestepseachofwhichinvolvesoptimizingoneof thesubsetwiththeremainderheldfixed.
We can similarly generalize the E step of the EM algorithm by performing a partial, ratherthancomplete, optimizationof L(q,θ)withrespecttoq(Z)(Nealand Hinton,1999).
Aswehaveseen, foranygivenvalueofθthereisauniquemaximum of L(q,θ)withrespecttoq(Z)thatcorrespondstotheposteriordistributionqθ(Z)= p(Z|X,θ) and that for this choice of q(Z) the bound L(q,θ) is equal to the log likelihood function lnp(X|θ).
It follows that any algorithm that converges to the global maximum of L(q,θ) will find a value of θ that is also a global maximum of the log likelihood lnp(X|θ).
Provided p(X, Z|θ) is a continuous function of θ Exercises 455 then, bycontinuity, anylocalmaximumof L(q,θ)willalsobealocalmaximumof lnp(X|θ).
Considerthecaseof N independentdatapointsx 1 ,..., x N withcorresponding latentvariablesz 1 ,..., z N.
Thejointdistributionp(X, Z|θ)factorizesoverthedata points, and this structure can be exploited in an incremental form of EM in which at each EM cycle only one data point is processed at a time.
In the E step, instead ofrecomputingtheresponsibilitiesforallofthedatapoints, wejustre-evaluatethe responsibilitiesforonedatapoint.
Itmightappearthatthesubsequent Mstepwould require computation involving the responsibilities for all of the data points.
How- ever, if the mixture components are members of the exponential family, then the responsibilities enter only through simple sufficient statistics, and these can be up- datedefficiently.
Consider, forinstance, thecaseofa Gaussianmixture, andsuppose we perform an update for data point m in which the corresponding old and new values of theresponsibilities aredenotedγold(zmk)and γnew(zmk).
Inthe Mstep, the required sufficient statistics can be updated incrementally.
For instance, for the Exercise 9.26 meansthesufficientstatisticsaredefinedby(9.17)and(9.18)fromwhichweobtain µn k ew =µo k ld+ γnew(zmk N )− new γold(zmk) xm −µo k ld (9.78) k togetherwith N k new =N k old+γnew(zmk)−γold(zmk).
(9.79) Thecorrespondingresultsforthecovariancesandthemixingcoefficientsareanalo- gous.
Thus both the E step and the M step take fixed time that is independent of the totalnumberofdatapoints.
Becausetheparametersarerevisedaftereachdatapoint, rather than waiting until after the whole data set is processed, this incremental ver- sioncanconvergefasterthanthebatchversion.
Each Eor Mstepinthisincremental algorithm is increasing the value of L(q,θ) and, as we have shown above, if the algorithmconvergestoalocal(orglobal)maximumof L(q,θ), thiswillcorrespond toalocal(orglobal)maximumoftheloglikelihoodfunctionlnp(X|θ).
Exercises 9.1 ( ) www Considerthe K-meansalgorithmdiscussedin Section9.1.
Showthatas a consequence of there being a finite number of possible assignments for the set of discreteindicatorvariablesrnk, andthatforeachsuchassignmentthereisaunique optimumforthe{µ }, the K-meansalgorithmmustconvergeafterafinitenumber k ofiterations.
9.2 ( ) Apply the Robbins-Monro sequential estimation procedure described in Sec- tion 2.3.5 to the problem of finding the roots of the regression function given by thederivativesof J in(9.1)withrespecttoµ .
Showthatthisleadstoastochastic k K-means algorithm in which, for each data point xn, the nearest prototype µ k is updatedusing(9.5).
456 9.
MIXTUREMODELSANDEM 9.3 ( ) www Considera Gaussianmixturemodelinwhichthemarginaldistribution p(z)forthelatentvariableisgivenby(9.10), andtheconditionaldistributionp(x|z) for the observed variable is given by (9.11).
Show that the marginal distribution p(x), obtained by summing p(z)p(x|z) over all possible values of z, is a Gaussian mixtureoftheform(9.7).
9.4 ( ) Suppose we wish to use the EM algorithm to maximize the posterior distri- bution over parameters p(θ|X) for a model containing latent variables, where X is the observed data set.
Show that the E step remains the same as in the maximum likelihood case, whereas in the M step the quantity to be maximized is given by Q(θ,θold )+lnp(θ)where Q(θ,θold )isdefinedby(9.30).
9.5 ( ) Considerthedirectedgraphfora Gaussianmixturemodelshownin Figure9.6.
By making use of the d-separation criterion discussed in Section 8.2, show that the posterior distribution of the latent variables factorizes with respect to the different datapointssothat N p(Z|X,µ,Σ,π)= p(zn |xn,µ,Σ,π).
(9.80) n=1 9.6 ( ) Consider a special case of a Gaussian mixture model in which the covari- ance matrices Σk of the components are all constrained to have a common value Σ.
Derive the EM equations for maximizing the likelihood function under such a model.
9.7 ( ) www Verifythatmaximizationofthecomplete-dataloglikelihood(9.36)for a Gaussianmixturemodelleadstotheresultthatthemeansandcovariancesofeach component are fitted independently to the corresponding group of data points, and themixingcoefficientsaregivenbythefractionsofpointsineachgroup.
9.8 ( ) www Showthatifwemaximize(9.40)withrespecttoµ whilekeepingthe k responsibilitiesγ(znk)fixed, weobtaintheclosedformsolutiongivenby(9.17).
9.9 ( ) Showthatifwemaximize(9.40)withrespecttoΣk andπk whilekeepingthe responsibilities γ(znk) fixed, we obtain the closed form solutions given by (9.19) and(9.22).
9.10 ( ) Consideradensitymodelgivenbyamixturedistribution K p(x)= πkp(x|k) (9.81) k=1 and suppose that we partition the vector x into two parts so that x = (xa, xb).
Show that the conditional density p(xb |xa) is itself a mixture distribution and find expressionsforthemixingcoefficientsandforthecomponentdensities.
Exercises 457 9.11 ( ) In Section 9.3.2, we obtained a relationship between K means and EM for Gaussian mixtures by considering a mixture model in which all components have covariance I.
Show that in the limit → 0, maximizing the expected complete- data log likelihood for this model, given by (9.40), is equivalent to minimizing the distortionmeasure J forthe K-meansalgorithmgivenby(9.1).
9.12 ( ) www Consideramixturedistributionoftheform K p(x)= πkp(x|k) (9.82) k=1 wheretheelementsofxcouldbediscreteorcontinuousoracombinationofthese.
Denote the mean and covariance of p(x|k) by µ k and Σk, respectively.
Show that themeanandcovarianceofthemixturedistributionaregivenby(9.49)and(9.50).
9.13 ( ) Using the re-estimation equations for the EM algorithm, show that a mix- ture of Bernoulli distributions, with its parameters set to values corresponding to a maximumofthelikelihoodfunction, hasthepropertythat N 1 E[x]= xn ≡x.
(9.83) N n=1 Henceshowthatiftheparametersofthismodelareinitializedsuchthatallcompo- nents have the same mean µ = µ for k = 1,..., K, then the EM algorithm will k convergeafteroneiteration, foranychoiceoftheinitialmixingcoefficients, andthat thissolutionhasthepropertyµ =x.
Notethatthisrepresentsadegeneratecaseof k the mixture model in which all of the components are identical, and in practice we trytoavoidsuchsolutionsbyusinganappropriateinitialization.
9.14 ( ) Considerthejointdistributionoflatentandobservedvariablesforthe Bernoulli distributionobtainedbyformingtheproductofp(x|z,µ)givenby(9.52)andp(z|π) givenby(9.53).
Showthatifwemarginalizethisjointdistributionwithrespecttoz, thenweobtain(9.47).
9.15 ( ) www Show that if we maximize the expected complete-data log likelihood function(9.55)foramixtureof Bernoullidistributionswithrespecttoµ , weobtain k the Mstepequation(9.59).
9.16 ( ) Show that if we maximize the expected complete-data log likelihood function (9.55)foramixtureof Bernoullidistributionswithrespecttothemixingcoefficients πk, usinga Lagrangemultipliertoenforcethesummationconstraint, weobtainthe Mstepequation(9.60).
9.17 ( ) www Show that as a consequence of the constraint 0 p(xn |µ k ) 1 for the discrete variable xn, the incomplete-data log likelihood function for a mixture of Bernoullidistributionsisboundedabove, andhencethattherearenosingularities forwhichthelikelihoodgoestoinfinity.
458 9.
MIXTUREMODELSANDEM 9.18 ( ) Consider a Bernoulli mixture model as discussed in Section 9.3.3, together with a prior distribution p(µ k |ak, bk) over each of the parameter vectors µ k given bythebetadistribution(2.13), anda Dirichletpriorp(π|α)givenby(2.38).
Derive the EMalgorithmformaximizingtheposteriorprobabilityp(µ,π|X).
9.19 ( ) Consider a D-dimensional variablexeachofwhosecomponentsiisitselfa multinomialvariableofdegree M sothatxisabinaryvectorwithc omponentsxij j xij =1for alli.
Supposethatthedistributionofthesevariablesisdescribedbyamixtureofthe discretemultinomialdistributionsconsideredin Section2.2sothat K p(x)= πkp(x|µ k ) (9.84) k=1 where D M p(x|µ )= µ x ij.
(9.85) k kij i=1j=1 The parameters µkij represent the probabil ities p(xij = 1|µ k ) and must satisfy 0 µkij 1 together with the constraint j µkij = 1 for all values of k and i.
Given an observed data set {xn }, where n = 1,..., N, derive the E and M step equations of the EM algorithm for optimizing the mixing coefficients πk and the componentparametersµkij ofthisdistributionbymaximumlikelihood.
9.20 ( ) www Show that maximization of the expected complete-data log likelihood function (9.62) for the Bayesian linear regression model leads to the M step re- estimationresult(9.63)forα.
9.21 ( ) Usingtheevidenceframeworkof Section3.5, derivethe M-stepre-estimation equationsfortheparameterβ inthe Bayesianlinearregressionmodel, analogousto theresult(9.63)forα.
9.22 ( ) By maximization of the expected complete-data log likelihood defined by (9.66), derivethe Mstepequations(9.67)and(9.68)forre-estimatingthehyperpa- rametersoftherelevancevectormachineforregression.
9.23 ( ) www In Section 7.2.1 we used direct maximization of the marginal like- lihood to derive the re-estimation equations (7.87) and (7.88) for finding values of the hyperparameters α and β for the regression RVM.
Similarly, in Section 9.3.4 we used the EM algorithm to maximize the same marginal likelihood, giving the re-estimationequations(9.67)and(9.68).
Showthatthesetwosetsofre-estimation equationsareformallyequivalent.
9.24 ( ) Verifytherelation (9.70) inwhich L(q,θ)and KL(q p)aredefined by(9.71) and(9.72), respectively.
Exercises 459 9.25 ( ) www Show that the lower bound L(q,θ) given by (9.71), with q(Z) = p(Z|X,θ(old) ), hasthesamegradientwithrespecttoθastheloglikelihoodfunction lnp(X|θ)atthepointθ =θ(old) .
9.26 ( ) www Consider the incremental form of the EM algorithm for a mixture of Gaussians, inwhichtheresponsibilitiesarerecomputedonlyforaspecificdatapoint xm.
Starting from the M-step formulae (9.17) and (9.18), derive the results (9.78) and(9.79)forupdatingthecomponentmeans.
9.27 ( ) Derive M-step formulae for updating the covariance matrices and mixing coefficients in a Gaussian mixture model when the responsibilities are updated in- crementally, analogoustotheresult(9.78)forupdatingthemeans.
10 Approximate Inference Acentraltaskintheapplicationofprobabilisticmodelsistheevaluationofthepos- teriordistributionp(Z|X)ofthelatentvariables Zgiventheobserved(visible)data variables X, and the evaluation of expectations computed with respect to this dis- tribution.
The model might also contain some deterministic parameters, which we willleaveimplicitforthemoment, oritmaybeafully Bayesianmodelinwhichany unknown parameters are given prior distributions and are absorbed into the set of latentvariablesdenotedbythevector Z.
Forinstance, inthe EMalgorithmweneed to evaluate the expectation of the complete-data log likelihood with respect to the posteriordistributionofthelatentvariables.
Formanymodelsofpracticalinterest, it willbeinfeasibletoevaluatetheposteriordistributionorindeedtocomputeexpec- tationswithrespecttothisdistribution.
Thiscouldbebecausethedimensionalityof thelatentspaceistoohightoworkwithdirectlyorbecausetheposteriordistribution hasahighlycomplexformfor whichexpectationsarenotanalytically tractable.
In thecaseofcontinuousvariables, therequiredintegrationsmaynothaveclosed-form 461 462 10.
APPROXIMATEINFERENCE analyticalsolutions, whilethedimensionalityofthespaceandthecomplexityofthe integrand may prohibit numerical integration.
For discrete variables, the marginal- izations involve summing over all possible configurations of the hidden variables, and though this is always possible in principle, we often find in practice that there may be exponentially many hidden states so that exact calculation is prohibitively expensive.
In such situations, we need to resort to approximation schemes, and these fall broadly into two classes, according to whether they rely on stochastic or determin- isticapproximations.
Stochastictechniquessuchas Markovchain Monte Carlo, de- scribedin Chapter11, haveenabledthewidespreaduseof Bayesianmethodsacross many domains.
They generally have the property that given infinite computational resource, theycangenerateexactresults, andtheapproximationarisesfromtheuse ofafiniteamountofprocessortime.
Inpractice, samplingmethodscanbecompu- tationally demanding, often limiting their use to small-scale problems.
Also, it can be difficult to know whether a sampling scheme is generating independent samples fromtherequireddistribution.
In this chapter, we introduce a range of deterministic approximation schemes, some of which scale well to large applications.
These are based on analytical ap- proximationstotheposteriordistribution, forexamplebyassumingthatitfactorizes in a particular way or that it has a specific parametric form such as a Gaussian.
As such, they can never generate exact results, and so their strengths and weaknesses arecomplementarytothoseofsamplingmethods.
In Section 4.4, we discussed the Laplace approximation, which is based on a local Gaussianapproximationtoamode(i.
e., amaximum)ofthedistribution.
Here weturntoafamilyofapproximationtechniquescalledvariationalinferenceorvari- ational Bayes, whichusemoreglobalcriteriaandwhichhavebeenwidelyapplied.
Weconcludewithabriefintroductiontoanalternativevariationalframeworkknown asexpectationpropagation.
10.1.
Variational Inference Variational methods have their origins in the 18th century with the work of Euler, Lagrange, and others on the calculus of variations.
Standard calculus is concerned with finding derivatives of functions.
We can think of a function as a mapping that takesthevalueofavariableastheinputandreturnsthevalueofthefunctionasthe output.
The derivative of the function then describes how the output value varies as we make infinitesimal changes to the input value.
Similarly, we can define a functionalasamappingthattakesafunctionastheinputandthatreturnsthevalue ofthefunctionalastheoutput.
Anexamplewouldbetheentropy H[p], whichtakes aprobabilitydistributionp(x)astheinputandreturnsthequantity H[p]= p(x)lnp(x)dx (10.1) 10.1.
Variational Inference 463 astheoutput.
Wecantheintroducetheconceptofafunctionalderivative, whichex- presseshowthevalueofthefunctionalchangesinresponsetoinfinitesimalchanges totheinputfunction(Feynmanetal.,1964).
Therulesforthecalculusofvariations mirrorthoseofstandardcalculusandarediscussedin Appendix D.
Manyproblems can be expressed in terms of an optimization problem in which the quantity being optimized is a functional.
The solution is obtained by exploring all possible input functions to find the one that maximizes, or minimizes, the functional.
Variational methods have broad applicability and include such areas as finite element methods (Kapur,1989)andmaximumentropy(Schwarz,1988).
Although there is nothing intrinsically approximate about variational methods, they do naturally lend themselves to finding approximate solutions.
This is done by restricting the range of functions over which the optimization is performed, for instance by considering only quadratic functions or by considering functions com- posedofalinearcombinationoffixedbasisfunctionsinwhichonlythecoefficients of the linear combination can vary.
In the case of applications to probabilistic in- ference, the restriction may for example take the form of factorization assumptions (Jordanetal.,1999; Jaakkola,2001).
Nowletusconsiderinmoredetailhowtheconceptofvariationaloptimization canbeappliedtotheinferenceproblem.
Supposewehaveafully Bayesianmodelin which all parameters are given prior distributions.
The model may also have latent variables as well as parameters, and we shall denote the set of all latent variables and parameters by Z.
Similarly, we denote the set of all observed variables by X.
For example, we might have a set of N independent, identically distributed data, specifiesthejointdistributionp(X, Z), andourgoalistofindanapproximationfor theposteriordistributionp(Z|X)aswellasforthemodelevidencep(X).
Asinour discussionof EM, wecandecomposethelogmarginalprobabilityusing lnp(X)=L(q)+KL(q p) (10.2) wherewehavedefined p(X, Z) L(q) = q(Z)ln d Z (10.3) q(Z) p(Z|X) KL(q p) = − q(Z)ln d Z.
(10.4) q(Z) Thisdiffersfromourdiscussionof EMonlyinthattheparametervectorθnolonger appears, because the parameters are now stochastic variables and are absorbed into Z.
Sinceinthischapterwewillmainlybeinterestedincontinuousvariableswehave used integrations rather than summations in formulating this decomposition.
How- ever, theanalysisgoesthroughunchangedifsomeorallofthevariablesarediscrete simply by replacing the integrations with summations as required.
As before, we canmaximizethelowerbound L(q)byoptimizationwithrespecttothedistribution q(Z), whichisequivalenttominimizingthe KLdivergence.
Ifweallowanypossible choice for q(Z), then the maximum of the lower bound occurs when the KL diver- gence vanishes, which occurs when q(Z) equals the posterior distribution p(Z|X).
464 10.
APPROXIMATEINFERENCE 1 40 0.8 30 0.6 20 0.4 10 0.2 0 0 −2 −1 0 1 2 3 4 −2 −1 0 1 2 3 4 Figure10.1 Illustrationofthevariationalapproximationfortheexampleconsideredearlierin Figure4.14.
The left-handplotshowstheoriginaldistribution(yellow)alongwiththe Laplace(red)andvariational(green)approx- imations, andtheright-handplotshowsthenegativelogarithmsofthecorrespondingcurves.
However, we shall suppose the model is such that working with the true posterior distributionisintractable.
Wethereforeconsiderinsteadarestrictedfamilyofdistributionsq(Z)andthen seekthememberofthisfamilyforwhichthe KLdivergenceisminimized.
Ourgoal is to restrict the family sufficiently that they comprise only tractable distributions, whileatthesametimeallowingthefamilytobesufficientlyrichandflexiblethatit canprovideagoodapproximationtothetrueposteriordistribution.
Itisimportantto emphasizethattherestrictionisimposedpurelytoachievetractability, andthatsub- jecttothisrequirementweshoulduseasrichafamilyofapproximatingdistributions aspossible.
Inparticular, thereisno‘over-fitting’associatedwithhighlyflexibledis- tributions.
Usingmoreflexibleapproximationssimplyallowsustoapproachthetrue posteriordistributionmoreclosely.
Onewaytorestrictthefamilyofapproximatingdistributionsistouseaparamet- ric distribution q(Z|ω) governed by a set of parameters ω.
The lower bound L(q) then becomes a function of ω, and we can exploit standard nonlinear optimization techniques to determine the optimal values for the parameters.
An example of this approach, inwhichthevariationaldistributionisa Gaussianandwehaveoptimized withrespecttoitsmeanandvariance, isshownin Figure10.1.
10.1.1 Factorized distributions Here we consider an alternative way in which to restrict the family of distri- butions q(Z).
Suppose we partition the elements of Z into disjoint groups that we denoteby Zi wherei=1,..., M.
Wethenassumethattheqdistributionfactorizes withrespecttothesegroups, sothat M q(Z)= qi(Zi).
(10.5) i=1 10.1.
Variational Inference 465 Itshouldbeemphasizedthatwearemakingnofurtherassumptionsaboutthedistri- bution.
Inparticular, weplacenorestrictiononthefunctionalformsoftheindividual factors qi(Zi).
This factorized form of variational inference corresponds to an ap- proximationframeworkdevelopedinphysicscalledmeanfieldtheory(Parisi,1988).
Amongstalldistributionsq(Z)havingtheform(10.5), wenowseekthatdistri- butionforwhichthelowerbound L(q)islargest.
Wethereforewishtomakeafree form(variational)optimizationof L(q)withrespecttoallofthedistributionsqi(Zi), which we do by optimizing with respect to each of the factors in turn.
To achieve this, wefirstsubstitute(10.5)into(10.3)andthendissectoutthedependenceonone ofthefactorsqj(Zj).
Denotingqj(Zj)bysimplyqj tokeepthenotationuncluttered, wethenobtain L(q) = qi lnp(X, Z)− lnqi d Z i i = qj lnp(X, Z) qid Zi d Zj − qjlnqjd Zj +const i =j = qjln p(X, Zj)d Zj − qjlnqjd Zj +const (10.6) wherewehavedefinedanewdistribution p(X, Zj)bytherelation ln p(X, Zj)=E i =j[lnp(X, Z)]+const.
(10.7) Herethenotation E i =j[···]denotesanexpectationwithrespecttotheqdistributions overallvariableszi fori =j, sothat E i =j[lnp(X, Z)]= lnp(X, Z) qid Zi.
(10.8) i =j Now suppose we keep the {qi =j } fixed and maximize L(q) in (10.6) with re- spect to all possible forms for the distribution qj(Zj).
This is easily done by rec- ognizing that (10.6) is a negative Kullback-Leibler divergence between qj(Zj) and p(X, Zj).
Thusmaximizing(10.6)isequivalenttominimizingthe Kullback-Leibler Leonhard Euler contributions, heformulatedthemoderntheoryofthe 1707–1783 function, he developed (together with Lagrange) the calculus of variations, and he discovered the formula Euler was a Swiss mathematician eiπ = −1, which relates four of the most important and physicist who worked in St.
numbersinmathematics.
Duringthelast17yearsof Petersburg and Berlin and who is his life, he was almost totally blind, and yet he pro- widely considered to be one of the ducednearlyhalfofhisresultsduringthisperiod.
greatestmathematiciansofalltime.
Heiscertainlythemostprolific, and hiscollectedworksfill75volumes.
Amongsthismany 466 10.
APPROXIMATEINFERENCE divergence, and the minimum occurs when qj(Zj) = p(X, Zj).
Thus we obtain a generalexpressionfortheoptimalsolutionq j (Zj)givenby lnq j (Zj)=E i =j[lnp(X, Z)]+const.
(10.9) Itisworthtakingafewmomentstostudytheformofthissolutionasitprovidesthe basis for applications of variational methods.
It says that the log of the optimal so- lutionforfactorqj isobtainedsimplybyconsideringthelogofthejointdistribution overallhiddenandvisiblevariablesandthentakingtheexpectationwithrespectto alloftheotherfactors{qi }fori =j.
The additive constant in (10.9) is set by normalizing the distribution q j (Zj).
Thusifwetaketheexponentialofbothsidesandnormalize, wehave exp(E i =j[lnp(X, Z)]) q j (Zj)= .
exp(E i =j[lnp(X, Z)]) d Zj Inpractice, weshallfinditmoreconvenienttoworkwiththeform(10.9)andthenre- instatethenormalizationconstant(whererequired)byinspection.
Thiswillbecome clearfromsubsequentexamples.
The set of equations given by (10.9) for j = 1,..., M represent a set of con- sistencyconditionsforthemaximumofthelowerboundsubjecttothefactorization constraint.
However, they do not represent an explicit solution because the expres- sionontheright-handsideof(10.9)fortheoptimumq j (Zj)dependsonexpectations computedwithrespecttotheotherfactorsqi(Zi)fori = j.
Wewillthereforeseek a consistent solution by first initializing all of the factors qi(Zi) appropriately and then cycling through the factors and replacing each in turn with a revised estimate givenbytheright-handsideof(10.9)evaluatedusingthecurrentestimatesforallof the other factors.
Convergence is guaranteed because bound is convexwith respect toeachofthefactorsqi(Zi)(Boydand Vandenberghe,2004).
10.1.2 Properties of factorized approximations Ourapproachtovariationalinferenceisbasedonafactorizedapproximationto thetrueposteriordistribution.
Letusconsiderforamomenttheproblemofapprox- imatingageneraldistributionbyafactorizeddistribution.
Tobeginwith, wediscuss the problem of approximating a Gaussian distribution using a factorized Gaussian, which will provide useful insight into the types of inaccuracy introduced in using factorized approximations.
Consider a Gaussian distribution p(z) = N(z|µ,Λ −1) over two correlated variables z = (z 1 , z 2 ) in which the mean and precision have elements µ Λ Λ µ= 1 , Λ= 11 12 (10.10) µ Λ Λ 2 21 22 and Λ 21 = Λ 12 due to the symmetry of the precision matrix.
Now suppose we wishtoapproximatethisdistributionusingafactorized Gaussianoftheformq(z)= q 1 (z 1 )q 2 (z 2 ).
We first apply the general result (10.9) to find an expression for the 10.1.
Variational Inference 467 optimalfactorq 1 (z 1 ).
Indoingsoitisusefultonotethatontheright-handsidewe onlyneedtoretainthosetermsthathavesomefunctionaldependenceonz 1 because allothertermscanbeabsorbedintothenormalizationconstant.
Thuswehave lnq 1 (z 1 ) = E z 2 [ lnp(z)]+const 1 = E z − (z 1 −µ 1 )2Λ 11 −(z 1 −µ 1 )Λ 12 (z 2 −µ 2 ) +const 2 2 1 = − 2 z 1 2Λ 11 +z 1 µ 1 Λ 11 −z 1 Λ 12 (E[z 2 ]−µ 2 )+const.
(10.11) Nextweobservethattheright-handsideofthisexpressionisaquadraticfunctionof z 1, andsowecanidentifyq (z 1 )asa Gaussiandistribution.
Itisworthemphasizing that we did not assume that q(zi) is Gaussian, but rather we derived this result by variational optimization of the KL divergence over all possible distributions q(zi).
Note also that we do not need to consider the additive constant in (10.9) explicitly because it represents the normalization constant that can be found at the end by Section2.3.1 inspectionifrequired.
Usingthetechniqueofcompletingthesquare, wecanidentify themeanandprecisionofthis Gaussian, giving q (z 1 )=N(z 1 |m 1 ,Λ − 11 1) (10.12) where m 1 =µ 1 −Λ − 11 1Λ 12 (E[z 2 ]−µ 2 ).
(10.13) Bysymmetry, q 2 (z 2 )isalso Gaussianandcanbewrittenas q 2 (z 2 )=N(z 2 |m 2 ,Λ − 22 1) (10.14) inwhich m 2 =µ 2 −Λ − 22 1Λ 21 (E[z 1 ]−µ 1 ).
(10.15) Note that these solutions are coupled, so thatq (z 1 ) depends on expectations com- puted with respect to q (z 2 ) and vice versa.
In general, we address this by treating thevariationalsolutionsasre-estimationequationsandcyclingthroughthevariables in turn updating them until some convergence criterion is satisfied.
We shall see an example of this shortly.
Here, however, we note that the problem is sufficiently simplethataclosedformsolutioncanbefound.
Inparticular, because E[z 1 ] = m 1 and E[z 2 ] = m 2, we see that the two equations are satisfied if we take E[z 1 ] = µ 1 and E[z 2 ]=µ 2, anditiseasilyshownthatthisistheonlysolutionprovidedthedis- Exercise 10.2 tribution is nonsingular.
This result is illustrated in Figure 10.2(a).
We see that the meaniscorrectlycapturedbutthatthevarianceofq(z)iscontrolledbythedirection ofsmallestvarianceofp(z), andthatthevariancealongtheorthogonaldirectionis significantly under-estimated.
It is a general result that a factorized variational ap- proximation tends to give approximations to the posterior distribution that are too compact.
Bywayofcomparison, supposeinsteadthatwehadbeenminimizingthereverse Kullback-Leiblerdivergence KL(p q).
Asweshallsee, thisformof KLdivergence 468 10.
APPROXIMATEINFERENCE Figure10.2 Comparison of 1 1 the two alternative forms for the Kullback-Leibler divergence.
The green contours corresponding to z2 z2 1, 2, and 3 standard deviations for a correlated Gaussian distribution p(z) over two variables z 1 and z 2, 0.5 0.5 and the red contours represent the corresponding levels for an approximating distribution q(z) over the same variables given by 0 0 the product of two independent 0 0.5 z1 1 0 0.5 z1 1 univariate Gaussian distributions (a) (b) whose parameters are obtained by minimization of (a) the Kullback- Leibler divergence KL(q p), and (b) the reverse Kullback-Leibler divergence KL(p q).
is used in an alternative approximate inference framework called expectation prop- Section10.7 agation.
We therefore consider the general problem of minimizing KL(p q) when q(Z)isafactorizedapproximationoftheform(10.5).
The KLdivergencecanthen bewrittenintheform M KL(p q)=− p(Z) lnqi(Zi) d Z+const (10.16) i=1 where the constant term is simply the entropy of p(Z) and so does not depend on q(Z).
We can now optimize with respect to each of the factors qj(Zj), which is Exercise 10.3 easilydoneusinga Lagrangemultipliertogive q j (Zj)= p(Z) d Zi =p(Zj).
(10.17) i =j In this case, we find that the optimal solution for qj(Zj) is just given by the corre- spondingmarginaldistributionofp(Z).
Notethatthisisaclosed-formsolutionand sodoesnotrequireiteration.
To apply this result to the illustrative example of a Gaussian distribution p(z) over a vector z we can use (2.98), which gives the result shown in Figure 10.2(b).
We see that once again the mean of the approximation is correct, but that it places significantprobabilitymassinregionsofvariablespacethathaveverylowprobabil- ity.
Thedifferencebetweenthesetworesultscanbeunderstoodbynotingthatthere isalargepositivecontributiontothe Kullback-Leiblerdivergence p(Z) KL(q p)=− q(Z)ln d Z (10.18) q(Z) 10.1.
Variational Inference 469 (a) (b) (c) Figure10.3 Anothercomparisonofthetwoalternativeformsforthe Kullback-Leiblerdivergence.
(a)Theblue contoursshowabimodaldistributionp(Z)givenbyamixtureoftwo Gaussians, andtheredcontourscorrespond to the single Gaussian distribution q(Z) that best approximates p(Z) in the sense of minimizing the Kullback- Leiblerdivergence KL(p q).
(b)Asin(a)butnowtheredcontourscorrespondtoa Gaussiandistributionq(Z) foundbynumericalminimizationofthe Kullback-Leiblerdivergence KL(q p).
(c)Asin(b)butshowingadifferent localminimumofthe Kullback-Leiblerdivergence.
from regions of Z space in which p(Z) is near zero unless q(Z) is also close to zero.
Thus minimizing this form of KL divergence leads to distributions q(Z) that avoid regions in which p(Z) is small.
Conversely, the Kullback-Leibler divergence KL(p q)isminimizedbydistributionsq(Z)thatarenonzeroinregionswherep(Z) isnonzero.
We can gain further insight into the different behaviour of the two KL diver- gences if we consider approximating a multimodal distribution by a unimodal one, as illustrated in Figure 10.3.
In practical applications, the true posterior distri- bution will often be multimodal, with most of the posterior mass concentrated in somenumberofrelativelysmallregionsofparameterspace.
Thesemultiplemodes may arise through nonidentifiability in the latent space or through complex nonlin- eardependenceontheparameters.
Bothtypesofmultimodalitywereencounteredin Chapter9inthecontextof Gaussianmixtures, wheretheymanifestedthemselvesas multiplemaximainthelikelihoodfunction, andavariationaltreatmentbasedonthe minimization of KL(q p) will tend to find one of these modes.
By contrast, if we were to minimize KL(p q), the resulting approximations would average across all ofthemodesand, inthecontextofthemixturemodel, wouldleadtopoorpredictive distributions (because the average of two good parameter values is typically itself notagoodparametervalue).
Itispossibletomakeuseof KL(p q)todefineauseful inferenceprocedure, butthisrequiresaratherdifferentapproachtotheonediscussed Section10.7 here, andwillbeconsideredindetailwhenwediscussexpectationpropagation.
Thetwoformsof Kullback-Leiblerdivergencearemembersofthealphafamily 470 10.
APPROXIMATEINFERENCE ofdivergences(Aliand Silvey,1966; Amari,1985; Minka,2005)definedby 4 Dα(p q)= 1−α2 1− p(x)(1+α)/2q(x)(1−α)/2dx (10.19) where−∞ < α < ∞isacontinuousparameter.
The Kullback-Leiblerdivergence KL(p q)correspondstothelimitα→1, whereas KL(q p)correspondstothelimit Exercise 10.6 α → −1.
For all values of α we have Dα(p q) 0, with equality if, and only if, p(x) = q(x).
Supposep(x)isafixeddistribution, andweminimize Dα(p q)with respect to some set of distributions q(x).
Then for α −1 the divergence is zero forcing, sothatanyvaluesofxforwhichp(x)=0willhaveq(x)=0, andtypically q(x)willunder-estimatethesupportofp(x)andwilltendtoseekthemodewiththe largest mass.
Conversely for α 1 the divergence is zero-avoiding, so that values ofxforwhichp(x)>0willhaveq(x)>0, andtypicallyq(x)willstretchtocover all of p(x), and will over-estimate the support of p(x).
When α = 0 we obtain a symmetricdivergencethatislinearlyrelatedtothe Hellingerdistancegivenby D H (p q)= p(x)1/2−q(x)1/2 dx.
(10.20) Thesquarerootofthe Hellingerdistanceisavaliddistancemetric.
10.1.3 Example: The univariate Gaussian Wenowillustratethefactorizedvariationalapproximationusinga Gaussiandis- tributionoverasinglevariablex(Mac Kay,2003).
Ourgoalistoinfertheposterior distribution for the mean µ and precision τ, given a data set D = {x 1 ,..., x N } of observedvaluesofxwhichareassumedtobedrawnindependentlyfromthe Gaus- sian.
Thelikelihoodfunctionisgivenby N τ N/2 τ p(D|µ,τ)= exp − (xn −µ)2 .
(10.21) 2π 2 n=1 Wenowintroduceconjugatepriordistributionsforµandτ givenby p(µ|τ) = N µ|µ 0 ,(λ 0 τ) −1 (10.22) p(τ) = Gam(τ|a 0 , b 0 ) (10.23) where Gam(τ|a 0 , b 0 ) is the gamma distribution defined by (2.146).
Together these Section2.3.6 distributionsconstitutea Gaussian-Gammaconjugatepriordistribution.
Forthissimpleproblemtheposteriordistributioncanbefoundexactly, andagain Exercise 2.44 takes the form of a Gaussian-gamma distribution.
However, for tutorial purposes wewillconsiderafactorizedvariationalapproximationtotheposteriordistribution givenby q(µ,τ)=qµ(µ)qτ(τ).
(10.24) 10.1.
Variational Inference 471 Notethatthetrueposteriordistributiondoesnotfactorizeinthisway.
Theoptimum factors qµ(µ) and qτ(τ) can be obtained from the general result (10.9) as follows.
Forqµ(µ)wehave lnq µ (µ) = E τ [lnp (D|µ,τ)+lnp(µ|τ)]+const N E[τ] = − λ 0 (µ−µ 0 )2+ (xn −µ) 2 +const.
(10.25) 2 n=1 Completingthesquareoverµweseethatqµ(µ)isa Gaussian N µ|µN,λ − N 1 with Exercise 10.7 meanandprecisiongivenby λ µ +Nx 0 0 µN = (10.26) λ +N 0 λN = (λ 0 +N)E[τ].
(10.27) Note that for N → ∞ this gives the maximum likelihood result in which µN = x andtheprecisionisinfinite.
Similarly, theoptimalsolutionforthefactorqτ(τ)isgivenby lnq τ (τ) = E µ[lnp(D|µ,τ)+lnp(µ|τ)]+lnp(τ)+const N = (a −1)lnτ −b τ + lnτ 0 0 2 N τ − E µ (xn −µ)2+λ 0 (µ−µ 0 )2 +const (10.28) 2 n=1 andhenceqτ(τ)isagammadistribution Gam(τ|a N, b N)withparameters N a N = a 0 + (10.29) 2 N 1 b N = b 0 + E µ (xn −µ)2+λ 0 (µ−µ 0 )2 .
(10.30) 2 n=1 Exercise 10.8 Againthisexhibitstheexpectedbehaviourwhen N →∞.
Itshouldbeemphasizedthatwedidnotassumethesespecificfunctionalforms fortheoptimaldistributionsqµ(µ)andqτ(τ).
Theyarosenaturallyfromthestructure Section10.4.1 ofthelikelihoodfunctionandthecorrespondingconjugatepriors.
Thuswehaveexpressionsfortheoptimaldistributionsqµ(µ)andqτ(τ)eachof whichdependsonmomentsevaluatedwithrespecttotheotherdistribution.
Oneap- proachtofindingasolutionisthereforetomakeaninitialguessfor, say, themoment E[τ] and use this to re-compute the distribution qµ(µ).
Given this revised distri- bution we can then extract the required moments E[µ] and E[µ2], and use these to recomputethedistributionqτ(τ), andsoon.
Sincethespaceofhiddenvariablesfor this example is only two dimensional, we can illustrate the variational approxima- tion to the posterior distribution by plotting contours of both the true posterior and thefactorizedapproximation, asillustratedin Figure10.4.
472 10.
APPROXIMATEINFERENCE 2 2 (a) (b) τ τ 1 1 0 0 µ µ −1 0 1 −1 0 1 2 2 (c) (d) τ τ 1 1 0 0 µ µ −1 0 1 −1 0 1 Figure10.4 Illustrationofvariationalinferenceforthemeanµandprecisionτ ofaunivariate Gaussiandistribu- tion.
Contoursofthetrueposteriordistributionp(µ,τ|D)areshowningreen.(a)Contoursoftheinitialfactorized approximationq µ (µ)q τ (τ)areshowninblue.
(b)Afterre-estimatingthefactorq µ (µ).
(c)Afterre-estimatingthe factorq τ (τ).
(d)Contoursoftheoptimalfactorizedapproximation, towhichtheiterativeschemeconverges, are showninred.
In general, we will need to use an iterative approach such as this in order to solvefortheoptimalfactorizedposteriordistribution.
Fortheverysimpleexample we are considering here, however, we can find an explicit solution by solving the simultaneousequationsfortheoptimalfactorsqµ(µ)andqτ(τ).
Beforedoingthis, we can simplify these expressions by considering broad, noninformative priors in which µ 0 = a 0 = b 0 = λ 0 = 0.
Although these parameter settings correspond to improperpriors, weseethattheposteriordistributionisstillwelldefined.
Usingthe Appendix B standardresult E[τ] = a N/b N forthemeanofagammadistribution, togetherwith (10.29)and(10.30), wehave N 1 1 E[τ] =E N (xn −µ)2 =x2−2x E[µ]+E[µ2].
(10.31) n=1 Then, using (10.26) and (10.27), we obtain the first and second order moments of 10.1.
Variational Inference 473 qµ(µ)intheform 1 E[µ]=x, E[µ2]=x2+ .
(10.32) NE[τ] Exercise 10.9 Wecannowsubstitutethesemomentsinto(10.31)andthensolvefor E[τ]togive 1 1 = (x2−x2) E[τ] N −1 N 1 = N −1 (xn −x)2.
(10.33) n=1 Werecognizetheright-handsideasthefamiliarunbiasedestimatorforthevariance of a univariate Gaussian distribution, and so we see that the use of a Bayesian ap- Section1.2.4 proachhasavoidedthebiasofthemaximumlikelihoodsolution.
10.1.4 Model comparison As well as performing inference over the hidden variables Z, we may also wish to compare a set of candidate models, labelled by the index m, and having priorprobabilitiesp(m).
Ourgoalisthentoapproximatetheposteriorprobabilities p(m|X), where X is the observed data.
This is a slightly more complex situation than that considered so far because different models may have different structure and indeed different dimensionality for the hidden variables Z.
We cannot there- foresimplyconsiderafactorizedapproximationq(Z)q(m), butmustinsteadrecog- nize that the posterior over Z must be conditioned on m, and so we must consider q(Z, m) = q(Z|m)q(m).
Wecanreadilyverifythefollowingdecompositionbased Exercise 10.10 onthisvariationaldistribution p(Z, m|X) lnp(X)=L m − q(Z|m)q(m)ln q(Z|m)q(m) (10.34) m Z wherethe L m isalowerboundonlnp(X)andisgivenby p(Z, X, m) L m = q(Z|m)q(m)ln q(Z|m)q(m) .
(10.35) m Z Hereweareassumingdiscrete Z, butthesameanalysisappliestocontinuouslatent variablesprovidedthesummationsarereplacedwithintegrations.
Wecanmaximize Exercise 10.11 L mwithrespecttothedistributionq(m)usinga Lagrangemultiplier, withtheresult q(m)∝p(m)exp{L m }.
(10.36) However, ifwemaximize L m withrespecttotheq(Z|m), wefindthatthesolutions for different m are coupled, as we expect because they are conditioned on m.
We proceedinsteadbyfirstoptimizingeachoftheq(Z|m)individuallybyoptimization 474 10.
APPROXIMATEINFERENCE of (10.35), and then subsequently determining the q(m) using (10.36).
After nor- malization the resulting values for q(m) can be used for model selection or model averagingintheusualway.
10.2.
Illustration: Variational Mixture of Gaussians Wenowreturntoourdiscussionofthe Gaussianmixturemodelandapplythevari- ational inference machinery developed in the previous section.
This will provide a goodillustrationoftheapplicationofvariationalmethodsandwillalsodemonstrate howa Bayesiantreatmentelegantlyresolvesmanyofthedifficultiesassociatedwith themaximumlikelihoodapproach(Attias,1999b).
Thereaderisencouragedtowork through this example in detail as it provides many insights into the practical appli- cationofvariationalmethods.
Many Bayesianmodels, correspondingtomuchmore sophisticateddistributions, canbesolvedbystraightforwardextensionsandgeneral- izationsofthisanalysis.
Ourstartingpointisthelikelihoodfunctionforthe Gaussianmixturemodel, il- lustrated by the graphical model in Figure 9.6.
For each observation xn we have a corresponding latent variable zn comprising a 1-of-K binary vector with ele- ments znk for k = 1,..., K.
As before we denote the observed data set by X = From (9.10) we can write down the conditional distribution of Z, given the mixing coefficientsπ, intheform N K p(Z|π)= π z nk.
(10.37) k n=1k=1 Similarly, from (9.11), we can write down the conditional distribution of the ob- serveddatavectors, giventhelatentvariablesandthecomponentparameters N K p(X|Z,µ,Λ)= N xn |µ k ,Λ − k 1 z nk (10.38) n=1k=1 where µ = {µ k } and Λ = {Λk }.
Note that we are working in terms of precision matricesratherthancovariancematricesasthissomewhatsimplifiesthemathemat- ics.
Nextweintroducepriorsovertheparametersµ,Λandπ.
Theanalysisiscon- Section10.4.1 siderably simplified if we use conjugate prior distributions.
We therefore choose a Dirichletdistributionoverthemixingcoefficientsπ K p(π)=Dir(π|α 0 )=C(α 0 ) π k α 0 −1 (10.39) k=1 wherebysymmetrywehavechosenthesameparameterα 0 foreachofthecompo- nents, and C(α 0 )isthenormalizationconstantforthe Dirichletdistributiondefined 10.2.
Illustration: Variational Mixtureof Gaussians 475 Figure10.5 Directedacyclicgraphrepresentingthe Bayesianmix- π zn Λ ture of Gaussians model, in which the box (plate) de- notes a set of N i.
i.
d.
observations.
Here µ denotes {µ k }andΛdenotes{Λ k }.
xn µ N Section2.2.1 by (B.23).
As we have seen, the parameter α 0 can be interpreted as the effective priornumberofobservationsassociatedwitheachcomponentofthemixture.
Ifthe value of α 0 is small, then the posterior distribution will be influenced primarily by thedataratherthanbytheprior.
Similarly, we introduce an independent Gaussian-Wishart prior governing the meanandprecisionofeach Gaussiancomponent, givenby p(µ,Λ) = p(µ|Λ)p(Λ) K = N µ k |m 0 ,(β 0 Λk) −1 W(Λk |W 0 ,ν 0 ) (10.40) k=1 becausethisrepresentstheconjugatepriordistributionwhenboththemeanandpre- Section2.3.6 cisionareunknown.
Typicallywewouldchoosem 0 =0bysymmetry.
The resulting model can be represented as a directed graph as shown in Fig- ure10.5.
NotethatthereisalinkfromΛtoµsincethevarianceofthedistribution overµin(10.40)isafunctionofΛ.
Thisexampleprovidesaniceillustrationofthedistinctionbetweenlatentvari- ablesandparameters.
Variablessuchaszn thatappearinsidetheplateareregarded as latent variables because the number of such variables grows with the size of the data set.
By contrast, variables such as µ that are outside the plate are fixed in numberindependentlyofthesizeofthedataset, andsoareregardedasparameters.
From the perspective of graphical models, however, there is really no fundamental differencebetweenthem.
10.2.1 Variational distribution Inordertoformulateavariationaltreatmentofthismodel, wenextwritedown thejointdistributionofalloftherandomvariables, whichisgivenby p(X, Z,π,µ,Λ)=p(X|Z,µ,Λ)p(Z|π)p(π)p(µ|Λ)p(Λ) (10.41) inwhichthevariousfactorsaredefinedabove.
Thereadershouldtakeamomentto verifythatthisdecompositiondoesindeedcorrespondtotheprobabilisticgraphical observed.
476 10.
APPROXIMATEINFERENCE We now consider a variational distribution which factorizes between the latent variablesandtheparameterssothat q(Z,π,µ,Λ)=q(Z)q(π,µ,Λ).
(10.42) It is remarkable that this is the only assumption that we need to make in order to obtainatractablepracticalsolutiontoour Bayesianmixturemodel.
Inparticular, the functionalformofthefactorsq(Z)andq(π,µ,Λ)willbedeterminedautomatically by optimization of the variational distribution.
Note that we are omitting the sub- scriptsontheq distributions, muchaswedowiththepdistributionsin(10.41), and arerelyingontheargumentstodistinguishthedifferentdistributions.
The corresponding sequential update equations for these factors can be easily derivedbymakinguseofthegeneralresult(10.9).
Letusconsiderthederivationof theupdateequationforthefactorq(Z).
Thelogoftheoptimizedfactorisgivenby lnq (Z)=E π,µ,Λ [lnp(X, Z,π,µ,Λ)]+const.
(10.43) Wenowmakeuseofthedecomposition(10.41).
Notethatweareonlyinterestedin thefunctionaldependenceoftheright-handsideonthevariable Z.
Thusanyterms that do not depend on Z can be absorbed into the additive normalization constant, giving lnq (Z)=E π[lnp(Z|π)]+E µ,Λ [lnp(X|Z,µ,Λ)]+const.
(10.44) Substituting for the two conditional distributions on the right-hand side, and again absorbinganytermsthatareindependentof Zintotheadditiveconstant, wehave N K lnq (Z)= znklnρnk +const (10.45) n=1k=1 wherewehavedefined 1 D lnρnk = E[lnπk]+ E[ln|Λk |]− ln(2π) 2 2 1 − 2 E µ k ,Λk (xn −µ k )TΛk(xn −µ k ) (10.46) where Disthedimensionalityofthedatavariablex.
Takingtheexponentialofboth sidesof(10.45)weobtain N K q (Z)∝ ρ z nk.
(10.47) nk n=1k=1 Requiring that this distribution be normalized, and noting that for each value of n Exercise 10.12 thequantitiesznk arebinaryandsumto1overallvaluesofk, weobtain N K q (Z)= r z nk (10.48) nk n=1k=1 10.2.
Illustration: Variational Mixtureof Gaussians 477 where ρnk rnk = .
(10.49) K ρnj j=1 Weseethattheoptimalsolutionforthefactorq(Z)takesthesamefunctionalform as the prior p(Z|π).
Note that because ρnk is given by the exponential of a real quantity, thequantitiesrnk willbenonnegativeandwillsumtoone, asrequired.
Forthediscretedistributionq (Z)wehavethestandardresult E[znk]=rnk (10.50) from which we see that the quantities rnk are playing the role of responsibilities.
Notethattheoptimalsolutionforq (Z)dependsonmomentsevaluatedwithrespect tothedistributionsofothervariables, andsoagainthevariationalupdateequations arecoupledandmustbesolvediteratively.
Atthispoint, weshallfinditconvenienttodefinethreestatisticsoftheobserved datasetevaluatedwithrespecttotheresponsibilities, givenby N Nk = rnk (10.51) n=1 N 1 xk = rnkxn (10.52) Nk n=1 N 1 Sk = rnk(xn −xk)(xn −xk)T.
(10.53) Nk n=1 Notethattheseareanalogoustoquantitiesevaluatedinthemaximumlikelihood EM algorithmforthe Gaussianmixturemodel.
Now let us consider the factor q(π,µ,Λ) in the variational posterior distribu- tion.
Againusingthegeneralresult(10.9)wehave K lnq (π,µ,Λ)=lnp(π)+ lnp(µ k ,Λk)+E Z [lnp(Z|π)] k=1 K N + E[znk]ln N xn |µ k ,Λ − k 1 +const.
(10.54) k=1n=1 We observe that the right-hand side of this expression decomposes into a sum of termsinvolvingonlyπ togetherwithtermsonlyinvolvingµandΛ, whichimplies that the variational posterior q(π,µ,Λ) factorizes to give q(π)q(µ,Λ).
Further- more, the terms involving µ and Λ themselves comprise a sum over k of terms involvingµ k andΛk leadingtothefurtherfactorization K q(π,µ,Λ)=q(π) q(µ k ,Λk).
(10.55) k=1 478 10.
APPROXIMATEINFERENCE Identifyingthetermsontheright-handsideof(10.54)thatdependonπ, wehave K K N lnq (π)=(α 0 −1) lnπk+ rnklnπk+const (10.56) k=1 k=1n=1 where we have used (10.50).
Taking the exponential of both sides, we recognize q (π)asa Dirichletdistribution q (π)=Dir(π|α) (10.57) whereαhascomponentsαk givenby αk =α 0 +Nk.
(10.58) Finally, thevariationalposteriordistributionq (µ k ,Λk)doesnotfactorizeinto theproductofthemarginals, butwecanalwaysusetheproductruletowriteitinthe formq (µ k ,Λk)=q (µ k |Λk)q (Λk).
Thetwofactorscanbefoundbyinspecting (10.54)andreadingoffthosetermsthatinvolveµ k andΛk.
Theresult, asexpected, Exercise 10.13 isa Gaussian-Wishartdistributionandisgivenby q (µ k ,Λk)=N µ k |mk,(βkΛk) −1 W(Λk |Wk,νk) (10.59) wherewehavedefined βk = β 0 +Nk (10.60) 1 mk = (β 0 m 0 +Nkxk) (10.61) βk W k −1 = W 0 −1+Nk Sk+ β 0 β 0 + N N k k (xk −m 0 )(xk −m 0 )T (10.62) νk = ν 0 +Nk.
(10.63) Theseupdateequationsareanalogoustothe M-stepequationsofthe EMalgorithm for the maximum likelihood solution of the mixture of Gaussians.
We see that the computations that must be performed in order to update the variational posterior distributionoverthemodelparametersinvolveevaluationofthesamesumsoverthe dataset, asaroseinthemaximumlikelihoodtreatment.
Inordertoperformthisvariational Mstep, weneedtheexpectations E[znk] = rnk representingtheresponsibilities.
Theseareobtainedbynormalizingtheρnk that aregivenby(10.46).
Weseethatthisexpressioninvolvesexpectationswithrespect to the variational distributions of the parameters, and these are easily evaluated to Exercise 10.14 give E µ k ,Λk (xn −µ k )TΛk(xn −µ k ) = Dβ k −1+νk(xn −mk)TWk(xn −mk) (10.64) D lnΛ k ≡E[ln|Λk |] = ψ νk+1−i +Dln2+ln|Wk | (10.65) 2 i=1 lnπ k ≡E[lnπk] = ψ(αk)−ψ(α ) (10.66) 10.2.
Illustration: Variational Mixtureof Gaussians 479 wherewehaveintroduceddefin itionsofΛ kandπ k, andψ(·)isthedigammafunction defined by (B.25), with α = k αk.
The results (10.65) and (10.66) follow from Appendix B thestandardpropertiesofthe Wishartand Dirichletdistributions.
If we substitute (10.64), (10.65), and (10.66) into (10.46) and make use of (10.49), weobtainthefollowingresultfortheresponsibilities rnk ∝π kΛ 1 k /2 exp − 2 D βk − ν 2 k (xn −mk)TWk(xn −mk) .
(10.67) Noticethesimilaritytothecorrespondingresultfortheresponsibilitiesinmaximum likelihood EM, whichfrom(9.13)canbewrittenintheform 1 rnk ∝πk |Λk |1/2exp − (xn −µ k )TΛk(xn −µ k ) (10.68) 2 wherewehaveusedtheprecisioninplaceofthecovariancetohighlightthesimilarity to(10.67).
Thus the optimization of the variational posterior distribution involves cycling betweentwostagesanalogoustothe Eand Mstepsofthemaximumlikelihood EM algorithm.
Inthevariationalequivalentofthe Estep, weusethecurrentdistributions overthemodelparameterstoevaluatethemomentsin(10.64),(10.65), and(10.66) and hence evaluate E[znk] = rnk.
Then in the subsequent variational equivalent of the M step, we keep these responsibilities fixed and use them to re-compute the variationaldistributionovertheparametersusing(10.57)and(10.59).
Ineachcase, weseethatthevariationalposteriordistributionhasthesamefunctionalformasthe correspondingfactorinthejointdistribution(10.41).
Thisisageneralresultandis Section10.4.1 aconsequenceofthechoiceofconjugatedistributions.
Figure10.6showstheresultsofapplyingthisapproachtotherescaled Old Faith- fuldatasetfora Gaussianmixturemodelhaving K = 6components.
Weseethat after convergence, there are only two components for which the expected values of the mixing coefficients are numerically distinguishable from their prior values.
This effect can be understood qualitatively in terms of the automatic trade-off in a Section3.4 Bayesianmodelbetweenfittingthedataandthecomplexityofthemodel, inwhich thecomplexitypenaltyarisesfromcomponentswhoseparametersarepushedaway from their prior values.
Components that take essentially no responsibility for ex- plaining the data points have rnk 0 and hence Nk 0.
From (10.58), we see that αk α 0 and from (10.60)–(10.63) we see that the other parameters revert to theirpriorvalues.
Inprinciplesuchcomponentsarefittedslightlytothedatapoints, but for broad priors this effect is too small to be seen numerically.
For the varia- tional Gaussianmixturemodeltheexpectedvaluesofthemixingcoefficientsinthe Exercise 10.15 posteriordistributionaregivenby E[πk]= αk+Nk .
(10.69) Kα +N 0 Consideracomponentforwhich Nk 0andαk α 0.
Ifthepriorisbroadsothat α 0 → 0, then E[πk] → 0andthecomponentplaysnoroleinthemodel, whereasif 480 10.
APPROXIMATEINFERENCE Figure10.6 Variational Bayesian mixture of K = 6 Gaussians ap- 0 15 plied to the Old Faithful data set, in which the ellipses denote the one standard-deviation density contours foreachofthecomponents, andthe densityofredinkinsideeachellipse corresponds to the mean value of themixingcoefficientforeachcom- ponent.
The number in the top left of each diagram shows the num- ber of iterations of variational infer- ence.
Componentswhoseexpected mixingcoefficientarenumericallyin- distinguishable from zero are not plotted.
60 120 the prior tightly constrains the mixing coefficients so that α 0 → ∞, then E[πk] → 1/K.
In Figure 10.6, the prior over the mixing coefficients is a Dirichlet of the form (10.39).
Recallfrom Figure2.5thatforα 0 < 1thepriorfavourssolutionsinwhich someofthemixingcoefficientsarezero.
Figure10.6wasobtainedusingα 0 =10−3, and resulted in two components having nonzero mixing coefficients.
If instead we choose α 0 = 1 we obtain three components with nonzero mixing coefficients, and forα=10allsixcomponentshavenonzeromixingcoefficients.
As we haveseen there is aclose similarity between the variational solution for the Bayesianmixtureof Gaussiansandthe EMalgorithmformaximumlikelihood.
Infactifweconsiderthelimit N →∞thenthe Bayesiantreatmentconvergestothe maximum likelihood EM algorithm.
For anything other than very small data sets, thedominantcomputationalcostofthevariationalalgorithmfor Gaussianmixtures arises from the evaluation of the responsibilities, together with the evaluation and inversionoftheweighteddatacovariancematrices.
Thesecomputationsmirrorpre- ciselythosethatariseinthemaximumlikelihood EMalgorithm, andsothereislittle computational overhead in using this Bayesian approach as compared to the tradi- tional maximum likelihood one.
There are, however, some substantial advantages.
Firstofall, thesingularitiesthatariseinmaximumlikelihoodwhena Gaussiancom- ponent ‘collapses’ onto a specific data point are absent in the Bayesian treatment.
10.2.
Illustration: Variational Mixtureof Gaussians 481 Indeed, thesesingularitiesareremovedifwesimplyintroduceapriorandthenusea MAPestimateinsteadofmaximumlikelihood.
Furthermore, thereisnoover-fitting if we choose a large number K of components in the mixture, as we saw in Fig- ure 10.6.
Finally, the variational treatment opens up the possibility of determining the optimal number of components in the mixture without resorting to techniques Section10.2.4 suchascrossvalidation.
10.2.2 Variational lower bound We can also straightforwardly evaluate the lower bound (10.3) for this model.
In practice, it is useful to be able to monitor the bound during the re-estimation in ordertotestforconvergence.
Itcanalsoprovideavaluablecheckonboththemath- ematicalexpressionsforthesolutionsandtheirsoftwareimplementation, becauseat eachstepoftheiterativere-estimationprocedurethevalueofthisboundshouldnot decrease.
Wecantakethisastagefurthertoprovideadeepertestofthecorrectness ofboththemathematicalderivationoftheupdateequationsandoftheirsoftwareim- plementationbyusingfinitedifferencestocheckthateachupdatedoesindeedgive a(constrained)maximumofthebound(Svense´nand Bishop,2004).
Forthevariationalmixtureof Gaussians, thelowerbound(10.3)isgivenby p(X, Z,π,µ,Λ) L = q(Z,π,µ,Λ)ln dπdµdΛ q(Z,π,µ,Λ) Z = E[lnp(X, Z,π,µ,Λ)]−E[lnq(Z,π,µ,Λ)] = E[lnp(X|Z,µ,Λ)]+E[lnp(Z|π)]+E[lnp(π)]+E[lnp(µ,Λ)] −E[lnq(Z)]−E[lnq(π)]−E[lnq(µ,Λ)] (10.70) where, to keep the notation uncluttered, we have omitted the superscript on the q distributions, alongwiththesubscriptsontheexpectationoperatorsbecauseeach expectationistakenwithrespecttoalloftherandomvariablesinitsargument.
The Exercise 10.16 varioustermsintheboundareeasilyevaluatedtogivethefollowingresults K E[lnp(X|Z,µ,Λ)]= 2 1 Nk lnΛ k −Dβ k −1−νk Tr(Sk Wk) k=1 −νk(xk −mk)TWk(xk −mk)−Dln(2π) (10.71) N K E[lnp(Z|π)] = rnklnπ k (10.72) n=1k=1 K E[lnp(π)] = ln C(α 0 )+(α 0 −1) lnπ k (10.73) k=1 482 10.
APPROXIMATEINFERENCE K E[lnp(µ,Λ)]= 1 Dln(β 0 /2π)+lnΛ k − Dβ 0 2 βk k=1 −β 0 νk(mk −m 0 )TWk(mk −m 0 ) +Kln B(W 0 ,ν 0 ) K K + (ν 0 − 2 D−1) lnΛ k − 2 1 νk Tr(W 0 −1Wk) (10.74) k=1 k=1 N K E[lnq(Z)] = rnklnrnk (10.75) n=1k=1 K E[lnq(π)] = (αk −1)lnπ k+ln C(α) (10.76) k=1 K E[lnq(µ,Λ)] = 1 lnΛ k + D ln βk − D −H[q(Λk)] (10.77) 2 2 2π 2 k=1 where Disthedimensionalityofx, H[q(Λk)]istheentropyofthe Wishartdistribu- tiongivenby(B.82), andthecoefficients C(α)and B(W,ν)aredefinedby(B.23) and(B.79), respectively.
Notethatthetermsinvolvingexpectationsofthelogsofthe q distributions simply represent the negative entropies of those distributions.
Some simplificationsandcombinationoftermscanbeperformedwhentheseexpressions aresummedtogivethelowerbound.
However, wehavekepttheexpressionssepa- rateforeaseofunderstanding.
Finally, itisworthnotingthatthelowerboundprovidesanalternativeapproach forderivingthevariationalre-estimationequationsobtainedin Section10.2.1.
Todo thisweusethefactthat, sincethemodelhasconjugatepriors, thefunctionalformof thefactorsinthevariationalposterior distributionisknown, namelydiscretefor Z, Dirichlet for π, and Gaussian-Wishart for (µ k ,Λk).
By taking general parametric formsforthesedistributionswecanderivetheformofthelowerboundasafunction of the parameters of the distributions.
Maximizing the bound with respect to these Exercise 10.18 parametersthengivestherequiredre-estimationequations.
10.2.3 Predictive density In applications of the Bayesian mixture of Gaussians model we will often be interested in the predictive density for a new value x of the observed variable.
As- sociatedwiththisobservationwillbeacorrespondinglatentvariable z, andthepre- dictivedensityisthengivenby p(x |X)= p(x | z,µ,Λ)p( z|π)p(π,µ,Λ|X)dπdµdΛ (10.78) bz 10.2.
Illustration: Variational Mixtureof Gaussians 483 where p(π,µ,Λ|X) is the (unknown) true posterior distribution of the parameters.
Using(10.37)and(10.38)wecanfirstperformthesummationover ztogive K p(x |X)= πk N x |µ k ,Λ − k 1 p(π,µ,Λ|X)dπdµdΛ.
(10.79) k=1 Because the remaining integrations are intractable, we approximate the predictive densitybyreplacingthetrueposteriordistributionp(π,µ,Λ|X)withitsvariational approximationq(π)q(µ,Λ)togive K p(x |X)= πk N x |µ k ,Λ − k 1 q(π)q(µ k ,Λk)dπdµ k dΛk (10.80) k=1 wherewehavemadeuseofthefactorization(10.55) andineachtermwehaveim- plicitly integrated out all variables {µ j ,Λj } for j = k The remaining integrations Exercise 10.19 cannowbeevaluatedanalyticallygivingamixtureof Student’st-distributions K 1 p(x |X)= α αk St(x |mk, Lk,νk+1−D) (10.81) k=1 inwhichthekth componenthasmeanmk, andtheprecisionisgivenby (νk +1−D)βk Lk = Wk (10.82) (1+βk) inwhichνkisgivenby(10.63).
Whenthesize N ofthedatasetislargethepredictive Exercise 10.20 distribution(10.81)reducestoamixtureof Gaussians.
10.2.4 Determining the number of components We have seen that the variational lower bound can be used to determine a pos- Section10.1.4 terior distribution over the number K of components in the mixture model.
There is, however, one subtlety that needs to be addressed.
For any given setting of the parameters in a Gaussian mixture model (except for specific degenerate settings), therewillexistotherparametersettingsforwhichthedensityovertheobservedvari- ableswillbeidentical.
Theseparametervaluesdifferonlythroughare-labellingof thecomponents.
Forinstance, consideramixtureoftwo Gaussiansandasingleob- servedvariablex, inwhichtheparametershavethevaluesπ 1 = a,π 2 = b,µ 1 = c, µ 2 = d, σ 1 = e, σ 2 = f.
Then the parameter values π 1 = b, π 2 = a, µ 1 = d, µ 2 = c, σ 1 = f, σ 2 = e, in which the two components have been exchanged, will bysymmetrygiverisetothesamevalueofp(x).
Ifwehaveamixturemodelcom- prising K components, theneachparametersettingwillbeamemberofafamilyof Exercise 10.21 K! equivalentsettings.
Inthecontextofmaximumlikelihood, thisredundancyisirrelevantbecausethe parameter optimization algorithm (for example EM) will, depending on the initial- ization of the parameters, find one specific solution, and the other equivalent solu- tionsplaynorole.
Ina Bayesiansetting, however, wemarginalizeoverallpossible 484 10.
APPROXIMATEINFERENCE Figure10.7 Plotofthevariationallowerbound L versus the number K of com- ponents in the Gaussian mixture model, for the Old Faithful data, showing a distinct peak at K = 2 components.
For each value of K, the model is trained from 100 different random starts, and theresultsshownas‘+’symbols p(D|K) plotted with small random hori- zontal perturbations so that they can be distinguished.
Note that some solutions find suboptimal local maxima, but that this hap- pensinfrequently.
1 2 3 4 5 6 K parametervalues.
Wehaveseenin Figure10.2thatifthetrueposteriordistribution ismultimodal, variationalinferencebasedontheminimizationof KL(q p)willtend toapproximatethedistributionintheneighbourhoodofoneofthemodesandignore the others.
Again, because equivalent modes have equivalent predictive densities, thisisofnoconcernprovidedweareconsideringamodelhavingaspecificnumber K ofcomponents.
If, however, wewishtocomparedifferentvaluesof K, thenwe needtotakeaccountofthismultimodality.
Asimpleapproximatesolutionistoadd Exercise 10.22 atermln K! ontothelowerboundwhenusedformodelcomparisonandaveraging.
Figure 10.7 shows a plot of the lower bound, including the multimodality fac- tor, versus the number K of components for the Old Faithful data set.
It is worth emphasizingonceagainthatmaximumlikelihoodwouldleadtovaluesofthelikeli- hoodfunctionthatincreasemonotonicallywith K (assumingthesingularsolutions have been avoided, and discounting the effects of local maxima) and so cannot be usedtodetermineanappropriatemodelcomplexity.
Bycontrast, Bayesianinference Section3.4 automaticallymakesthetrade-offbetweenmodelcomplexityandfittingthedata.
Thisapproachtothedeterminationof K requiresthatarangeofmodelshaving different K valuesbetrainedandcompared.
Analternativeapproachtodetermining a suitable value for K is to treat the mixing coefficients π as parameters and make point estimates of their values by maximizing the lower bound (Corduneanu and Bishop, 2001) with respect to π instead of maintaining a probability distribution Exercise 10.23 overthemasinthefully Bayesianapproach.
Thisleadstothere-estimationequation N 1 πk = rnk (10.83) N n=1 andthismaximizationisinterleavedwiththevariationalupdatesfortheqdistribution over the remaining parameters.
Components that provide insufficient contribution 10.2.
Illustration: Variational Mixtureof Gaussians 485 to explaining the data will have their mixing coefficients driven to zero during the optimization, andsotheyareeffectivelyremovedfromthemodelthroughautomatic relevance determination.
This allows us to make a single training run in which we start with a relatively large initial value of K, and allow surplus components to be prunedoutofthemodel.
Theoriginsofthesparsitywhenoptimizingwithrespectto Section7.2.2 hyperparametersisdiscussedindetailinthecontextoftherelevancevectormachine.
10.2.5 Induced factorizations In deriving these variational update equations for the Gaussian mixture model, we assumed a particular factorization of the variational posterior distribution given by(10.42).
However, theoptimalsolutionsforthevariousfactorsexhibitadditional factorizations.
Inparticular, thesolutionforq (µ,Λ)isgivenbytheproductofan independentdistributionq (µ k ,Λk)overeachofthecomponentsk ofthemixture, whereas the variational posterior distribution q (Z) over the latent variables, given by(10.48), factorizesintoanindependentdistributionq (zn)foreachobservationn (notethatitdoesnotfurtherfactorizewithrespecttokbecause, foreachvalueofn, theznk areconstrainedtosumtooneoverk).
Theseadditionalfactorizationsarea consequenceoftheinteractionbetweentheassumedfactorizationandtheconditional independence properties of the true distribution, as characterized by the directed graphin Figure10.5.
We shall refer to these additional factorizations as induced factorizations be- causetheyarisefromaninteractionbetweenthefactorizationassumedinthevaria- tionalposteriordistribution andtheconditional independencepropertiesofthetrue joint distribution.
In a numerical implementation of the variational approach it is important to take account of such additional factorizations.
For instance, it would be very inefficient to maintain a full precision matrix for the Gaussian distribution over a set of variables if the optimal form for that distribution always had a diago- nalprecisionmatrix(correspondingtoafactorizationwithrespecttotheindividual variablesdescribedbythat Gaussian).
Suchinducedfactorizationscaneasilybedetectedusingasimplegraphicaltest basedond-separationasfollows.
Wepartitionthelatentvariablesintothreedisjoint groups A, B, Candthenletussupposethatweareassumingafactorizationbetween Candtheremaininglatentvariables, sothat q(A, B, C)=q(A, B)q(C).
(10.84) Using the general result (10.9), together with the product rule for probabilities, we seethattheoptimalsolutionforq(A, B)isgivenby lnq (A, B) = E [lnp(X, A, B, C)]+const C = E C [lnp(A, B|X, C)]+const.
(10.85) We now ask whether this resulting solution will factorize between A and B, in other words whether q (A, B) = q (A)q (B).
This will happen if, and only if, lnp(A, B|X, C) = lnp(A|X, C)+lnp(B|X, C), that is, if the conditional inde- pendencerelation A⊥⊥B|X, C (10.86) 486 10.
APPROXIMATEINFERENCE issatisfied.
Wecantesttoseeifthisrelationdoeshold, foranychoiceof Aand B bymakinguseofthed-separationcriterion.
Toillustratethis, consideragainthe Bayesianmixtureof Gaussiansrepresented by the directed graph in Figure 10.5, in which we are assuming a variational fac- torization given by (10.42).
We can see immediately that the variational posterior distributionovertheparametersmustfactorizebetweenπandtheremainingparam- eters µ and Λ because all paths connecting π to either µ or Λ must pass through oneofthenodeszn allofwhichareintheconditioningsetforourconditionalinde- pendencetestandallofwhicharehead-to-tailwithrespecttosuchpaths.
10.3.
Variational Linear Regression As a second illustration of variational inference, we return to the Bayesian linear regression model of Section 3.3.
In the evidence framework, we approximated the integrationoverαandβ bymakingpointestimatesobtainedbymaximizingthelog marginal likelihood.
A fully Bayesian approach would integrate over the hyperpa- rameters as well as over the parameters.
Although exact integration is intractable, we can use variational methods to find a tractable approximation.
In order to sim- plifythediscussion, weshallsupposethatthenoiseprecisionparameterβisknown, and is fixed to its true value, although the framework is easily extended to include Exercise 10.26 the distribution over β.
For the linear regression model, the variational treatment willturnouttobeequivalenttotheevidenceframework.
Nevertheless, itprovidesa goodexerciseintheuseofvariationalmethodsandwillalsolaythefoundationfor variationaltreatmentof Bayesianlogisticregressionin Section10.6.
Recallthatthelikelihoodfunctionforw, andtheprioroverw, aregivenby N p(t|w) = N(tn |w Tφ n ,β −1) (10.87) n=1 p(w|α) = N(w|0,α −1I) (10.88) where φ n = φ(xn).
We now introduce a prior distribution over α.
From our dis- cussion in Section 2.3.6, we know that the conjugate prior for the precision of a Gaussianisgivenbyagammadistribution, andsowechoose p(α)=Gam(α|a 0 , b 0 ) (10.89) where Gam(·|·,·)isdefinedby(B.26).
Thusthejointdistributionofallthevariables isgivenby p(t, w,α)=p(t|w)p(w|α)p(α).
(10.90) Thiscanberepresentedasadirectedgraphicalmodelasshownin Figure10.8.
10.3.1 Variational distribution Ourfirstgoalistofindanapproximationtotheposteriordistributionp(w,α|t).
Todothis, weemploythevariationalframeworkof Section10.1, withavariational 10.3.
Variational Linear Regression 487 α Figure10.8 Probabilisticgraphicalmodelrepresentingthejointdis- tribution (10.90) for the Bayesian linear regression φn model.
w β tn N posteriordistributiongivenbythefactorizedexpression q(w,α)=q(w)q(α).
(10.91) Wecanfindre-estimationequationsforthefactorsinthisdistributionbymakinguse of the general result (10.9).
Recall that for each factor, we take the log of the joint distributionoverallvariablesandthenaveragewithrespecttothosevariablesnotin that factor.
Consider first the distribution over α.
Keeping only terms that have a functionaldependenceonα, wehave lnq (α)=lnp(α)+E [lnp(w|α)]+const w M α = (a 0 −1)lnα−b 0 α+ lnα− E[w Tw]+const.
(10.92) 2 2 Werecognizethisasthelogofagammadistribution, andsoidentifyingthecoeffi- cientsofαandlnαweobtain q (α)=Gam(α|a N, b N) (10.93) where M a N = a 0 + (10.94) 2 1 b N = b 0 + E[w Tw].
(10.95) 2 Similarly, we can find the variational re-estimation equation for the posterior distribution over w.
Again, using the general result (10.9), and keeping only those termsthathaveafunctionaldependenceonw, wehave lnq (w) = lnp(t|w)+E α[lnp(w|α)]+const (10.96) N β 1 = − {w Tφ n −tn }2− E[α]w Tw+const (10.97) 2 2 n=1 1 = − w T E[α]I+βΦTΦ w+βw TΦT t+const.
(10.98) 2 Becausethisisaquadraticform, thedistributionq (w)is Gaussian, andsowecan completethesquareintheusualwaytoidentifythemeanandcovariance, giving q (w)=N(w|m N, SN) (10.99) 488 10.
APPROXIMATEINFERENCE where m N = βSNΦT t (10.100) SN = E[α]I+βΦTΦ −1 .
(10.101) Note the close similarity to the posterior distribution (3.52) obtained when α was treatedasafixedparameter.
Thedifferenceisthathereαisreplacedbyitsexpecta- tion E[α]underthevariationaldistribution.
Indeed, wehavechosentousethesame notationforthecovariancematrix SN inbothcases.
Usingthestandardresults(B.27),(B.38), and(B.39), wecanobtaintherequired momentsasfollows E[α] = a N/b N (10.102) E[ww T] = m Nm T N +SN.
(10.103) Theevaluationofthevariationalposteriordistributionbeginsbyinitializingthepa- rameters of one of the distributions q(w) or q(α), and then alternately re-estimates thesefactorsinturnuntilasuitableconvergencecriterionissatisfied(usuallyspeci- fiedintermsofthelowerboundtobediscussedshortly).
Itisinstructivetorelatethevariationalsolutiontothatfoundusingtheevidence frameworkin Section3.5.
Todothisconsiderthecasea 0 = b 0 = 0, corresponding tothelimitofaninfinitelybroadprioroverα.
Themeanofthevariationalposterior distributionq(α)isthengivenby E[α]= a N = M/2 = M .
(10.104) b N E[w Tw]/2 m T N m N +Tr(SN) Comparison with (9.63) shows that in the case of this particularly simple model, the variational approach gives precisely the same expression as that obtained by maximizing the evidence function using EM except that the point estimate for α is replaced by its expected value.
Because the distribution q(w) depends on q(α) onlythroughtheexpectation E[α], weseethatthetwoapproacheswillgiveidentical resultsforthecaseofaninfinitelybroadprior.
10.3.2 Predictive distribution The predictive distribution over t, given a new input x, is easily evaluated for thismodelusingthe Gaussianvariationalposteriorfortheparameters p(t|x, t) = p(t|x, w)p(w|t)dw p(t|x, w)q(w)dw = N(t|w Tφ(x),β −1)N(w|m N, SN)dw = N(t|m Tφ(x),σ2(x)) (10.105) N 10.3.
Variational Linear Regression 489 where we have evaluated the integral by making use of the result (2.115) for the linear-Gaussianmodel.
Heretheinput-dependentvarianceisgivenby 1 σ2(x)= +φ(x)TSN φ(x).
(10.106) β Notethatthistakesthesameformastheresult(3.59)obtainedwithfixedα except thatnowtheexpectedvalue E[α]appearsinthedefinitionof SN.
10.3.3 Lower bound Anotherquantityofimportanceisthelowerbound Ldefinedby L(q) = E[lnp(w,α, t)]−E[lnq(w,α)] = E w [lnp(t|w)]+E w,α[lnp(w|α)]+E α[lnp(α)] −E α[lnq(w)] w −E[lnq(α)].
(10.107) Exercise 10.27 Evaluationofthevarioustermsisstraightforward, makinguseofresultsobtainedin previouschapters, andgives N β β E[lnp(t|w)] w = ln − t T t+βm T N ΦT t 2 2π 2 β − Tr ΦTΦ(m Nm T N +SN) (10.108) 2 M M E[lnp(w|α)] w,α = − ln(2π)+ (ψ(a N)−lnb N) 2 2 − a N m T N m N +Tr(SN) (10.109) 2b N E[lnp(α)]α = a 0 lnb 0 +(a 0 −1)[ψ(a N)−lnb N] −b 0 a N −lnΓ(a N) (10.110) b N 1 M −E[lnq(w)] w = ln|SN |+ [1+ln(2π)] (10.111) 2 2 −E[lnq(α)]α = lnΓ(a N)−(a N −1)ψ(a N)−lnb N +a N.
(10.112) Figure10.9showsaplotofthelowerbound L(q)versusthedegreeofapolynomial model for a synthetic data set generated from a degree three polynomial.
Here the priorparametershavebeensettoa 0 =b 0 =0, correspondingtothenoninformative prior p(α) ∝ 1/α, which is uniform over lnα as discussed in Section 2.3.6.
As we saw in Section 10.1, the quantity L represents lower bound on the log marginal likelihoodp(t|M)forthemodel.
Ifweassignequalpriorprobabilitiesp(M)tothe different values of M, then we can interpret L as an approximation to the poste- rior model probability p(M|t).
Thus the variational framework assigns the highest probabilitytothemodelwith M = 3.
Thisshouldbecontrastedwiththemaximum likelihood result, which assigns ever smaller residual error to models of increasing complexityuntiltheresidualerrorisdriventozero, causingmaximumlikelihoodto favourseverelyover-fittedmodels.
490 10.
APPROXIMATEINFERENCE Figure10.9 Plot of the lower bound L ver- sus the order M of the polyno- mial, for a polynomial model, in which a set of 10 data points is generatedfromapolynomialwith M = 3 sampled over the inter- val(−5,5)withadditive Gaussian noiseofvariance0.09.
Thevalue of the bound gives the log prob- ability of the model, and we see thatthevalueoftheboundpeaks at M = 3, corresponding to the true model from which the data setwasgenerated.
1 3 5 7 9 10.4.
Exponential Family Distributions In Chapter 2, we discussed the important role played by the exponential family of distributions and their conjugate priors.
For many of the models discussed in this book, thecomplete-datalikelihoodisdrawnfromtheexponentialfamily.
However, in general this will not be the case for the marginal likelihood function for the ob- serveddata.
Forexample, inamixtureof Gaussians, thejointdistributionofobser- vations xn and corresponding hidden variables zn is a member of the exponential family, whereasthemarginaldistributionofxn isamixtureof Gaussiansandhence isnot.
Up to now we have grouped the variables in the model into observed variables and hidden variables.
We now make a further distinction between latent variables, denoted Z, andparameters, denotedθ, whereparametersareintensive(fixedinnum- ber independent of the size of the data set), whereas latent variables are extensive (scaleinnumber withthesizeofthedataset).
Forexample, ina Gaussianmixture model, theindicatorvariableszkn (whichspecifywhichcomponentkisresponsible for generating data point xn) represent the latent variables, whereas the means µ k , precisionsΛk andmixingproportionsπk representtheparameters.
Consider the case of independent identically distributed data.
We denote the datavaluesby X = {xn }, wheren = 1,...
N, withcorrespondinglatentvariables Z = {zn }.
Nowsupposethatthejointdistributionofobservedandlatentvariables isamemberoftheexponentialfamily, parameterizedbynaturalparametersηsothat N p(X, Z|η)= h(xn, zn)g(η)exp ηTu(xn, zn) .
(10.113) n=1 Weshallalsouseaconjugatepriorforη, whichcanbewrittenas p(η|ν 0 , v 0 )=f(ν 0 ,χ 0 )g(η) ν 0exp νo ηTχ 0 .
(10.114) Recall that the conjugate prior distribution can be interpreted as a prior number ν 0 ofobservationsallhavingthevalueχ fortheuvector.
Nowconsideravariational 0 10.4.
Exponential Family Distributions 491 distribution that factorizes between the latent variables and the parameters, so that q(Z,η) = q(Z)q(η).
Using the general result (10.9), we can solve for the two factorsasfollows lnq (Z) = E η[lnp(X, Z|η)]+const N = lnh(xn, zn)+E[ηT]u(xn, zn) +const.
(10.115) n=1 Thus we see that this decomposes into a sum of independent terms, one for each v alue of n, and hence the solution for q (Z) will factorize over n so that q (Z) = Section10.2.5 n q (zn).
Thisisanexampleofaninducedfactorization.
Takingtheexponential ofbothsides, wehave q (zn)=h(xn, zn)g(E[η])exp E[ηT]u(xn, zn) (10.116) where the normalization coefficient has been re-instated by comparison with the standardformfortheexponentialfamily.
Similarly, forthevariationaldistributionovertheparameters, wehave lnq (η)=lnp(η|ν 0 ,χ 0 )+E Z [lnp(X, Z|η)]+const (10.117) N = ν 0 lng(η)+ηTχ 0 + lng(η)+ηTE zn [u(xn, zn)] +const.
(10.118) n=1 Again, takingtheexponentialofbothsides, andre-instatingthenormalizationcoef- ficientbyinspection, wehave q (η)=f(νN,χ N )g(η) ν N exp ηTχ N (10.119) wherewehavedefined νN = ν 0 +N (10.120) N χ N = χ 0 + E zn [u(xn, zn)].
(10.121) n=1 Notethatthesolutionsforq (zn)andq (η)arecoupled, andsowesolvethemiter- ativelyinatwo-stageprocedure.
Inthevariational Estep, weevaluatetheexpected sufficient statistics E[u(xn, zn)] using the current posterior distribution q(zn) over thelatentvariablesandusethistocomputearevisedposteriordistributionq(η)over the parameters.
Then in the subsequent variational M step, we use this revised pa- rameter posterior distribution to find the expected natural parameters E[ηT], which givesrisetoarevisedvariationaldistributionoverthelatentvariables.
10.4.1 Variational message passing Wehaveillustratedtheapplicationofvariationalmethodsbyconsideringaspe- cific model, the Bayesian mixture of Gaussians, in some detail.
This model can be 492 10.
APPROXIMATEINFERENCE describedbythedirectedgraphshownin Figure10.5.
Hereweconsidermoregen- erally the use of variational methods for models described by directed graphs and deriveanumberofwidelyapplicableresults.
Thejointdistributioncorrespondingtoadirectedgraphcanbewrittenusingthe decomposition p(x)= p(xi |pa i ) (10.122) i wherexi denotesthevariable(s)associatedwithnodei, andpa i denotestheparent setcorrespondingtonodei.
Notethatxi maybealatentvariableoritmaybelong tothesetofobservedvariables.
Nowconsideravariationalapproximationinwhich thedistributionq(x)isassumedtofactorizewithrespecttothexi sothat q(x)= qi(xi).
(10.123) i Notethatforobservednodes, thereisnofactorq(xi)inthevariationaldistribution.
Wenowsubstitute(10.122)intoourgeneralresult(10.9)togive lnq j (xj)=E i =j lnp(xi |pa i ) +const.
(10.124) i Any terms on the right-hand side that do not depend on xj can be absorbed into the additive constant.
In fact, the only terms that do depend on xj are the con- ditional distribution for xj given by p(xj |pa j ) together with any other conditional distributions that have xj in the conditioning set.
By definition, these conditional distributionscorrespondtothechildrenofnodej, andtheythereforealsodependon the co-parents of the child nodes, i.
e., the other parents of the child nodes besides nodexj itself.
Weseethatthesetofallnodesonwhichq (xj)dependscorresponds to the Markov blanket of node xj, as illustrated in Figure 8.26.
Thus the update of the factors in the variational posterior distribution represents a local calculation onthegraph.
Thismakespossibletheconstructionofgeneralpurposesoftwarefor variationalinferenceinwhichtheformofthemodeldoesnotneedtobespecifiedin advance(Bishopetal.,2003).
If we nowspecialize to thecase of amodel in which allof the conditional dis- tributionshaveaconjugate-exponentialstructure, thenthevariationalupdateproce- dure can be cast in terms of a local message passing algorithm (Winn and Bishop, 2005).
Inparticular, thedistributionassociatedwithaparticularnodecanbeupdated once that node has received messages from all of its parents and all of its children.
Thisinturnrequiresthatthechildrenhavealreadyreceivedmessagesfromtheirco- parents.
Theevaluationofthelowerboundcanalsobesimplifiedbecausemanyof therequiredquantitiesarealreadyevaluatedaspartofthemessagepassingscheme.
Thisdistributedmessagepassingformulationhasgoodscalingpropertiesandiswell suitedtolargenetworks.
10.5.
Local Variational Methods 493 10.5.
Local Variational Methods Thevariationalframeworkdiscussedin Sections10.1and10.2canbeconsidereda ‘global’methodinthesensethatitdirectlyseeksanapproximationtothefullposte- riordistributionoverallrandomvariables.
Analternative‘local’approachinvolves finding bounds on functions over individual variables or groups of variables within amodel.
Forinstance, wemightseekaboundonaconditionaldistributionp(y|x), which is itself just one factor in a much larger probabilistic model specified by a directed graph.
The purpose of introducing the bound of course is to simplify the resultingdistribution.
Thislocalapproximationcanbeappliedtomultiplevariables in turn until a tractable approximation is obtained, and in Section 10.6.1 we shall giveapracticalexampleofthisapproachinthecontextoflogisticregression.
Here wefocusondevelopingtheboundsthemselves.
Wehavealreadyseeninourdiscussionofthe Kullback-Leiblerdivergencethat the convexity of the logarithm function played a key role in developing the lower boundintheglobalvariationalapproach.
Wehavedefineda(strictly)convexfunc- Section1.6.1 tion as one for which every chord lies above the function.
Convexity also plays a central role in the local variational framework.
Note that our discussion will ap- plyequallytoconcavefunctionswith‘min’and‘max’interchangedandwithlower boundsreplacedbyupperbounds.
Let us begin by considering a simple example, namely the function f(x) = exp(−x), whichisaconvexfunctionofx, andwhichisshownintheleft-handplot of Figure10.10.
Ourgoalistoapproximatef(x)byasimplerfunction, inparticular alinearfunctionofx.
From Figure10.10, weseethatthislinearfunctionwillbea lower bound on f(x) if it corresponds to a tangent.
We can obtain the tangent line y(x)ataspecificvalueofx, sayx=ξ, bymakingafirstorder Taylorexpansion y(x)=f(ξ)+f (ξ)(x−ξ) (10.125) so that y(x) f(x) with equality when x = ξ.
For our example function f(x) = Figure10.10 In the left-hand fig- 1 0.4 uretheredcurveshowsthefunction λξ−g(λ) exp(−x), and the blue line shows the tangent at x = ξ defined by (10.125) with ξ = 1.
This line has slopeλ = f (ξ) = −exp(−ξ).
Note that any other tangent line, for ex- 0.5 0.2 ampletheonesshowningreen, will have a smaller value of y at x = ξ.
The right-hand figure shows the corresponding plot of the function λξ − g(λ), where g(λ) is given by 0 0 (10.131), versus λ for ξ = 1, in 0 ξ 1.5 x 3 −1 −0.5 λ 0 which themaximum correspondsto λ=−exp(−ξ)=−1/e.
494 10.
APPROXIMATEINFERENCE y y f(x) f(x) −g(λ) x x λx λx−g(λ) Figure10.11 Intheleft-handplottheredcurveshowsaconvexfunctionf(x), andthebluelinerepresentsthe linearfunctionλx, whichisalowerboundonf(x)becausef(x)>λxforallx.
Forthegivenvalueofslopeλthe contactpointofthetangentlinehavingthesameslopeisfoundbyminimizingwithrespecttoxthediscrepancy (shownbythegreendashedlines)givenbyf(x)−λx.
Thisdefinesthedualfunctiong(λ), whichcorresponds tothe(negativeofthe)interceptofthetangentlinehavingslopeλ.
exp(−x), wethereforeobtainthetangentlineintheform y(x)=exp(−ξ)−exp(−ξ)(x−ξ) (10.126) which is a linear function parameterized by ξ.
For consistency with subsequent discussion, letusdefineλ=−exp(−ξ)sothat y(x,λ)=λx−λ+λln(−λ).
(10.127) Differentvaluesofλcorrespondtodifferenttangentlines, andbecauseallsuchlines are lower bounds on the function, we have f(x) y(x,λ).
Thus we can write the functionintheform f(x)=max{λx−λ+λln(−λ)}.
(10.128) λ Wehavesucceededinapproximatingtheconvexfunctionf(x)byasimpler, lin- earfunctiony(x,λ).
Thepricewehavepaidisthatwehaveintroducedavariational parameterλ, andtoobtainthetightestboundwemustoptimizewithrespecttoλ.
Wecanformulatethisapproachmoregenerallyusingtheframeworkofconvex duality(Rockafellar,1972; Jordanetal.,1999).
Considertheillustrationofaconvex function f(x) shown in the left-hand plot in Figure 10.11.
In this example, the function λx is a lower bound on f(x) but it is not the best lower bound that can beachievedbyalinearfunctionhavingslopeλ, becausethetightestboundisgiven bythetangent line.
Letuswritetheequation ofthetangentline, havingslopeλ as λx−g(λ)wherethe(negative)interceptg(λ)clearlydependsontheslopeλofthe tangent.
Todeterminetheintercept, wenotethatthelinemustbemovedverticallyby anamountequaltothesmallestverticaldistancebetweenthelineandthefunction, asshownin Figure10.11.
Thus g(λ) = −min{f(x)−λx} x = max{λx−f(x)}.
(10.129) x 10.5.
Local Variational Methods 495 Now, instead of fixing λ and varying x, we can consider a particular x and then adjust λ until the tangent plane is tangent at that particular x.
Because the y value ofthetangentlineataparticularxismaximizedwhenthatvaluecoincideswithits contactpoint, wehave f(x)=max{λx−g(λ)}.
(10.130) λ We see that the functions f(x) and g(λ) play a dual role, and are related through (10.129)and(10.130).
Let us apply these duality relations to our simple example f(x) = exp(−x).
From(10.129)weseethatthemaximizingvalueofxisgivenbyξ =−ln(−λ), and back-substitutingweobtaintheconjugatefunctiong(λ)intheform g(λ)=λ−λln(−λ) (10.131) asobtainedpreviously.
Thefunctionλξ−g(λ)isshown, forξ =1intheright-hand plot in Figure 10.10.
As a check, we can substitute (10.131) into (10.130), which givesthemaximizingvalueofλ = −exp(−x), andback-substitutingthenrecovers theoriginalfunctionf(x)=exp(−x).
Forconcavefunctions, wecanfollowasimilarargumenttoobtainupperbounds, inwhichmax’isreplacedwith‘min’, sothat f(x) = min{λx−g(λ)} (10.132) λ g(λ) = min{λx−f(x)}.
(10.133) x If the function of interest is not convex (or concave), then we cannot directly apply the method above to obtain a bound.
However, we can first seek invertible transformationseitherofthefunctionorofitsargumentwhichchangeitintoacon- vex form.
We then calculate the conjugate function and then transform back to the originalvariables.
An important example, which arises frequently in pattern recognition, is the logisticsigmoidfunctiondefinedby 1 σ(x)= .
(10.134) 1+e−x As it stands this function is neither convex nor concave.
However, if we take the logarithmweobtainafunctionwhichisconcave, asiseasilyverifiedbyfindingthe Exercise 10.30 second derivative.
From (10.133) the corresponding conjugate function then takes theform g(λ)=min{λx−f(x)}=−λlnλ−(1−λ)ln(1−λ) (10.135) x whichwerecognizeasthebinaryentropyfunctionforavariablewhoseprobability Appendix B ofhavingthevalue1isλ.
Using(10.132), wethenobtainanupperboundonthelog sigmoid lnσ(x) λx−g(λ) (10.136) 496 10.
APPROXIMATEINFERENCE 1 1 ξ=2.5 λ=0.2 0.5 0.5 λ=0.7 0 0 −6 0 6 −6 −ξ 0 ξ 6 Figure10.12 Theleft-handplotshowsthelogisticsigmoidfunctionσ(x)definedby(10.134)inred, together withtwoexamplesoftheexponentialupperbound(10.137)showninblue.
Theright-handplotshowsthelogistic sigmoid again in red together with the Gaussian lower bound (10.144) shown in blue.
Here the parameter ξ=2.5, andtheboundisexactatx=ξandx=−ξ, denotedbythedashedgreenlines.
andtakingtheexponential, weobtainanupperboundonthelogisticsigmoiditself oftheform σ(x) exp(λx−g(λ)) (10.137) whichisplottedfortwovaluesofλontheleft-handplotin Figure10.12.
Wecanalsoobtainalowerboundonthesigmoidhavingthefunctionalformof a Gaussian.
Todothis, wefollow Jaakkolaand Jordan(2000)andmaketransforma- tionsbothoftheinputvariableandofthefunctionitself.
Firstwetakethelogofthe logisticfunctionandthendecomposeitsothat lnσ(x) = −ln(1+e −x )=−ln e −x/2(e x/2+e −x/2) = x/2−ln(e x/2+e −x/2).
(10.138) We now note that the function f(x) = −ln(ex/2 +e−x/2) is a convex function of Exercise 10.31 thevariablex2, ascanagainbeverifiedbyfindingthesecondderivative.
Thisleads toalowerboundonf(x), whichisalinearfunctionofx2 whoseconjugatefunction isgivenby √ g(λ)=max λx2−f x2 .
(10.139) x2 Thestationarityconditionleadsto dx d 1 x 0=λ− f(x)=λ+ tanh .
(10.140) dx2dx 4x 2 If we denote this value of x, corresponding to the contact point of the tangent line forthisparticularvalueofλ, byξ, thenwehave 1 ξ 1 1 λ(ξ)=− tanh =− σ(ξ)− .
(10.141) 4ξ 2 2ξ 2 10.5.
Local Variational Methods 497 Instead of thinking of λ as the variational parameter, we can let ξ play this role as thisleadstosimplerexpressionsfortheconjugatefunction, whichisthengivenby g(λ)=λ(ξ)ξ2−f(ξ)=λ(ξ)ξ2+ln(e ξ/2+e −ξ/2).
(10.142) Hencetheboundonf(x)canbewrittenas f(x) λx2−g(λ)=λx2−λξ2−ln(e ξ/2+e −ξ/2).
(10.143) Theboundonthesigmoidthenbecomes σ(x) σ(ξ)exp (x−ξ)/2−λ(ξ)(x2−ξ2) (10.144) whereλ(ξ)isdefinedby(10.141).
Thisboundisillustratedintheright-handplotof Figure 10.12.
Wesee thatthe bound hasthe formof theexponential of aquadratic function of x, which will prove useful when we seek Gaussian representations of Section4.5 posteriordistributionsdefinedthroughlogisticsigmoidfunctions.
The logistic sigmoid arises frequently in probabilistic models over binary vari- ablesbecauseitisthefunctionthattransformsalogoddsratiointoaposteriorprob- ability.
The corresponding transformation for a multiclass distribution is given by Section4.3 the softmax function.
Unfortunately, the lower bound derived here for the logistic sigmoid does not directly extend to the softmax.
Gibbs (1997) proposes a method forconstructinga Gaussiandistributionthatisconjecturedtobeabound(although norigorousproofisgiven), whichmaybeusedtoapplylocalvariationalmethodsto multiclassproblems.
Weshallseeanexampleoftheuseoflocalvariationalboundsin Sections10.6.1.
For the moment, however, it is instructive to consider in general terms how these boundscanbeused.
Supposewewishtoevaluateanintegraloftheform I = σ(a)p(a)da (10.145) whereσ(a)isthelogisticsigmoid, andp(a)isa Gaussianprobabilitydensity.
Such integrals arise in Bayesian models when, for instance, we wish to evaluate the pre- dictivedistribution, inwhichcasep(a)representsaposteriorparameterdistribution.
Becausetheintegralisintractable, weemploythevariationalbound(10.144), which we write in the form σ(a) f(a,ξ) where ξ is a variational parameter.
The inte- gralnowbecomestheproductoftwoexponential-quadraticfunctionsandsocanbe integratedanalyticallytogiveaboundon I I f(a,ξ)p(a)da=F(ξ).
(10.146) We now have the freedom to choose the variational parameter ξ, which we do by finding the value ξ that maximizes the function F(ξ).
The resulting value F(ξ ) represents the tightest bound within this family of bounds and can be used as an approximation to I.
This optimized bound, however, will in general not be exact.
498 10.
APPROXIMATEINFERENCE Althoughtheboundσ(a) f(a,ξ)onthelogisticsigmoidcanbeoptimizedexactly, therequiredchoiceforξdependsonthevalueofa, sothattheboundisexactforone valueofaonly.
Becausethequantity F(ξ)isobtainedbyintegratingoverallvalues ofa, thevalueofξ representsacompromise, weightedbythedistributionp(a).
10.6.
Variational Logistic Regression We now illustrate the use of local variational methods by returning to the Bayesian logistic regression model studied in Section 4.5.
There we focussed on the use of the Laplaceapproximation, whilehereweconsideravariationaltreatmentbasedon the approach of Jaakkola and Jordan (2000).
Like the Laplace method, this also leadstoa Gaussianapproximationtotheposteriordistribution.
However, thegreater flexibility of the variational approximation leads to improved accuracy compared to the Laplace method.
Furthermore (unlike the Laplace method), the variational approachisoptimizingawelldefinedobjectivefunctiongivenbyarigourousbound on the model evidence.
Logistic regression has also been treated by Dybowski and Roberts(2005)froma Bayesianperspectiveusing Monte Carlosamplingtechniques.
10.6.1 Variational posterior distribution Hereweshallmakeuseofavariationalapproximationbasedonthelocalbounds introduced in Section 10.5.
This allows the likelihood function for logistic regres- sion, which is governed by the logistic sigmoid, to be approximated by the expo- nential of a quadratic form.
It is therefore again convenient to choose a conjugate Gaussianprioroftheform(4.140).
Forthemoment, weshalltreatthehyperparam- etersm 0 and S 0 asfixedconstants.
In Section10.6.3, weshalldemonstratehowthe variational formalism can be extended to the case where there are unknown hyper- parameterswhosevaluesaretobeinferredfromthedata.
Inthevariationalframework, weseektomaximizealowerboundonthemarginal likelihood.
Forthe Bayesianlogisticregressionmodel, themarginallikelihoodtakes theform N p(t)= p(t|w)p(w)dw = p(tn |w) p(w)dw.
(10.147) n=1 Wefirstnotethattheconditionaldistributionfortcanbewrittenas p(t|w) = σ(a) t{1−σ(a)}1−t t 1−t 1 1 = 1− 1+e−a 1+e−a e−a = e at =e at σ(−a) (10.148) 1+e−a where a = w Tφ.
In order to obtain a lower bound on p(t), we make use of the variational lower bound on the logistic sigmoid function given by (10.144), which 10.6.
Variational Logistic Regression 499 wereproducehereforconvenience σ(z) σ(ξ)exp (z−ξ)/2−λ(ξ)(z2−ξ2) (10.149) where 1 1 λ(ξ)= σ(ξ)− .
(10.150) 2ξ 2 Wecanthereforewrite p(t|w)=e at σ(−a) e at σ(ξ)exp −(a+ξ)/2−λ(ξ)(a2−ξ2) .
(10.151) Notethatbecausethisboundisappliedtoeachofthetermsinthelikelihoodfunction separately, there is a variational parameter ξn corresponding to each training set observation(φ n , tn).
Usinga=w Tφ, andmultiplyingbythepriordistribution, we obtainthefollowingboundonthejointdistributionoftandw p(t, w)=p(t|w)p(w) h(w,ξ)p(w) (10.152) whereξdenotestheset{ξn }ofvariationalparameters, and N h(w,ξ) = σ(ξn)exp w Tφ n tn −(w Tφ n +ξn)/2 n=1 −λ(ξn)([w Tφ n ]2−ξ n 2) .
(10.153) Evaluationoftheexactposteriordistributionwouldrequirenormalizationoftheleft- hand side of this inequality.
Because this is intractable, we work instead with the right-hand side.
Note that the function on the right-hand side cannot be interpreted asaprobabilitydensitybecauseitisnotnormalized.
Onceitisnormalizedtogivea variationalposteriordistributionq(w), however, itnolongerrepresentsabound.
Becausethelogarithmfunctionismonotonicallyincreasing, theinequality A B impliesln A ln B.
Thisgivesalowerboundonthelogofthejointdistribution oftandwoftheform N ln{p(t|w)p(w)} lnp(w)+ lnσ(ξn)+w Tφ n tn n=1 −(w Tφ n +ξn)/2−λ(ξn)([w Tφ n ]2−ξ n 2) .
(10.154) Substitutingforthepriorp(w), theright-handsideofthisinequalitybecomes, asa functionofw 1 − (w−m )TS −1(w−m ) 2 0 0 0 N + w Tφ n (tn −1/2)−λ(ξn)w T(φ n φT n )w +const.
(10.155) n=1 500 10.
APPROXIMATEINFERENCE Thisisaquadraticfunctionofw, andsowecanobtainthecorrespondingvariational approximation to the posterior distribution by identifying the linear and quadratic termsinw, givinga Gaussianvariationalposterioroftheform q(w)=N(w|m N, SN) (10.156) where N m N = SN S − 0 1m 0 + (tn −1/2)φ n (10.157) n=1 N S − N 1 = S − 0 1+2 λ(ξn)φ n φT n .
(10.158) n=1 Aswiththe Laplaceframework, wehaveagainobtaineda Gaussianapproximation totheposteriordistribution.
However, theadditionalflexibilityprovidedbythevari- ationalparameters{ξn }leadstoimprovedaccuracyintheapproximation(Jaakkola and Jordan,2000).
Here we have considered a batch learning context in which all of the training data is available at once.
However, Bayesian methods are intrinsically well suited tosequentiallearninginwhichthedatapointsareprocessedoneatatimeandthen discarded.
The formulation of this variational approach for the sequential case is Exercise 10.32 straightforward.
Notethattheboundgivenby(10.149)appliesonlytothetwo-classproblemand sothisapproachdoesnotdirectlygeneralizetoclassificationproblemswith K > 2 classes.
An alternative bound for the multiclass case has been explored by Gibbs (1997).
10.6.2 Optimizing the variational parameters Wenowhaveanormalized Gaussianapproximationtotheposteriordistribution, whichweshalluseshortlytoevaluatethepredictivedistributionfornewdatapoints.
First, however, weneedtodeterminethevariationalparameters{ξn }bymaximizing thelowerboundonthemarginallikelihood.
To do this, we substitute the inequality (10.152) back into the marginal likeli- hoodtogive lnp(t)=ln p(t|w)p(w)dw ln h(w,ξ)p(w)dw =L(ξ).
(10.159) Aswiththeoptimization ofthehyperparameterα inthelinearregressionmodelof Section3.5, therearetwoapproachestodeterminingtheξn.
Inthefirstapproach, we recognizethatthefunction L(ξ)isdefinedbyanintegrationoverw andsowecan view w as a latent variable and invoke the EM algorithm.
In the second approach, weintegrateoverwanalyticallyandthenperformadirectmaximizationoverξ.
Let usbeginbyconsideringthe EMapproach.
The EM algorithm starts by choosing some initial values for the parameters {ξn }, which we denote collectively by ξold .
In the E step of the EM algorithm, 10.6.
Variational Logistic Regression 501 we then use these parameter values to find the posterior distribution overw, which is given by (10.156).
In the M step, we then maximize the expected complete-data loglikelihoodwhichisgivenby Q(ξ,ξold )=E[lnh(w,ξ)p(w)] (10.160) wheretheexpectationistakenwithrespecttotheposteriordistributionq(w)evalu- atedusingξold .
Notingthatp(w)doesnotdependonξ, andsubstitutingforh(w,ξ) weobtain N Q(ξ,ξold )= lnσ(ξn)−ξn/2−λ(ξn)(φT n E[ww T]φ n −ξ n 2) +const n=1 (10.161) where‘const’denotestermsthatareindependentofξ.
Wenowsetthederivativewith respecttoξn equaltozero.
Afewlinesofalgebra, makinguseofthedefinitionsof σ(ξ)andλ(ξ), thengives 0=λ (ξn)(φT n E[ww T]φ n −ξ n 2).
(10.162) We now note that λ (ξ) is a monotonic function of ξ for ξ 0, and that we can restrict attention to nonnegative values of ξ without loss of generality due to the symmetry of the bound around ξ = 0.
Thus λ (ξ) = 0, and hence we obtain the Exercise 10.33 followingre-estimationequations (ξ n new)2 =φT n E[ww T]φ n =φT n SN +m Nm T N φ n (10.163) wherewehaveused(10.156).
Let us summarize the EM algorithm for finding the variational posterior distri- bution.
Wefirstinitializethevariationalparametersξold .
Inthe Estep, weevaluate the posterior distribution over w given by (10.156), in which the mean and covari- ancearedefinedby(10.157)and(10.158).
Inthe Mstep, wethenusethisvariational posterior to compute a new value for ξ given by (10.163).
The E and M steps are repeateduntilasuitableconvergencecriterionissatisfied, whichinpracticetypically requiresonlyafewiterations.
An alternative approach to obtaining re-estimation equations for ξ is to note that in the integral over w in the definition (10.159) of the lower bound L(ξ), the integrandhasa Gaussian-likeformandsotheintegralcanbeevaluatedanalytically.
Having evaluated the integral, we can then differentiate with respect to ξn.
It turns out that this gives rise to exactly the same re-estimation equations as does the EM Exercise 10.34 approachgivenby(10.163).
As we have emphasized already, in the application of variational methods it is usefultobeabletoevaluatethelowerbound L(ξ)givenby(10.159).
Theintegration overw canbeperformedanalyticallybynotingthatp(w)is Gaussianandh(w,ξ) is the exponential of a quadratic function of w.
Thus, by completing the square andmakinguseofthestandardresultforthenormalizationcoefficientofa Gaussian Exercise 10.35 distribution, wecanobtainaclosedformsolutionwhichtakestheform 502 10.
APPROXIMATEINFERENCE 6 6 4 4 2 2 0 0 −2 0.
0 1 0.
2 5 0.
7 50 .9 9 −2 −4 −4 −6 −6 −4 −2 0 2 4 −4 −2 0 2 4 Figure 10.13 Illustration of the Bayesian approach to logistic regression for a simple linearly separable data set.
The plot on the left shows the predictive distribution obtained using variational inference.
We see that the decision boundary lies roughly mid way between the clusters of data points, and that the contours of the predictivedistributionsplayoutawayfromthedatareflectingthegreateruncertaintyintheclassificationofsuch regions.
The plot on the right shows the decision boundaries corresponding to five samples of the parameter vectorwdrawnfromtheposteriordistributionp(w|t).
L(ξ) = 1 2 ln | | S S N | | − 2 1 m T N S − N 1m N + 2 1 m T 0 S − 0 1m 0 0 N 1 + lnσ(ξn)− ξn −λ(ξn)ξ n 2 .
(10.164) 2 n=1 This variational framework can also be applied to situations in which the data is arriving sequentially (Jaakkola and Jordan, 2000).
In this case we maintain a Gaussianposteriordistributionoverw, whichisinitializedusingthepriorp(w).
As eachdatapointarrives, theposteriorisupdatedbymakinguseofthebound(10.151) andthennormalizedtogiveanupdatedposteriordistribution.
The predictive distribution is obtained by marginalizing over the posterior dis- tribution, and takes the same form as for the Laplace approximation discussed in Section4.5.2.
Figure10.13showsthevariationalpredictivedistributionsforasyn- theticdataset.
Thisexampleprovidesinterestinginsightsintotheconceptof‘large margin’, whichwasdiscussedin Section7.1andwhichhasqualitativelysimilarbe- haviourtothe Bayesiansolution.
10.6.3 Inference of hyperparameters Sofar, wehavetreatedthehyperparameterαinthepriordistributionasaknown constant.
Wenowextendthe Bayesianlogisticregressionmodeltoallowthevalueof thisparameter tobeinferredfromthedataset.
Thiscanbeachievedbycombining the global and local variational approximations into a single framework, so as to maintainalowerboundonthemarginallikelihoodateachstage.
Suchacombined approach was adopted by Bishop and Svense´n (2003) in the context of a Bayesian treatmentofthehierarchicalmixtureofexpertsmodel.
10.6.
Variational Logistic Regression 503 Specifically, weconsideronceagainasimpleisotropic Gaussianpriordistribu- tionoftheform p(w|α)=N(w|0,α −1I).
(10.165) Ouranalysisisreadilyextendedtomoregeneral Gaussianpriors, forinstanceifwe wish to associate a different hyperparameter with different subsets of the parame- ters wj.
As usual, we consider a conjugate hyperprior over α given by a gamma distribution p(α)=Gam(α|a 0 , b 0 ) (10.166) governedbytheconstantsa 0 andb 0.
Themarginallikelihoodforthismodelnowtakestheform p(t)= p(w,α, t)dwdα (10.167) wherethejointdistributionisgivenby p(w,α, t)=p(t|w)p(w|α)p(α).
(10.168) We are now faced with an analytically intractable integration over w and α, which weshalltacklebyusingboththelocalandglobalvariationalapproachesinthesame model To begin with, we introduce a variational distribution q(w,α), and then apply thedecomposition(10.2), whichinthisinstancetakestheform lnp(t)=L(q)+KL(q p) (10.169) where the lower bound L(q)and the Kullback-Leibler divergence KL(q p)are de- finedby p(w,α, t) L(q) = q(w,α)ln dwdα (10.170) q(w,α) p(w,α|t)) KL(q p) = − q(w,α)ln dwdα.
(10.171) q(w,α) At this point, the lower bound L(q) is still intractable due to the form of the likelihood factor p(t|w).
We therefore apply the local variational bound to each of thelogisticsigmoidfactorsasbefore.
Thisallowsustousetheinequality(10.152) andplacealowerboundon L(q), whichwillthereforealsobealowerboundonthe logmarginallikelihood lnp(t) L(q) L (q,ξ) h(w,ξ)p(w|α)p(α) = q(w,α)ln dwdα.
(10.172) q(w,α) Next we assume that the variational distribution factorizes between parameters and hyperparameterssothat q(w,α)=q(w)q(α).
(10.173) 504 10.
APPROXIMATEINFERENCE Withthisfactorizationwecanappealtothegeneralresult(10.9)tofindexpressions for the optimal factors.
Consider first the distribution q(w).
Discarding terms that areindependentofw, wehave lnq(w) = E α[ln{h(w,ξ)p(w|α)p(α)}]+const = lnh(w,ξ)+E α[lnp(w|α)]+const.
Wenowsubstituteforlnh(w,ξ)using(10.153), andforlnp(w|α)using(10.165), giving E[α] N lnq(w)=− w Tw+ (tn −1/2)w Tφ n −λ(ξn)w Tφ n φT n w +const.
2 n=1 We see that this is a quadratic function of w and so the solution for q(w) will be Gaussian.
Completingthesquareintheusualway, weobtain q(w)=N(w|µ N ,ΣN) (10.174) wherewehavedefined N Σ − N 1µ N = (tn −1/2)φ n (10.175) n=1 N Σ − N 1 = E[α]I+2 λ(ξn)φ n φT n .
(10.176) n=1 Similarly, theoptimalsolutionforthefactorq(α)isobtainedfrom lnq(α)=E [lnp(w|α)]+lnp(α)+const.
w Substitutingforlnp(w|α)using(10.165), andforlnp(α)using(10.166), weobtain M α lnq(α)= lnα− E w Tw +(a −1)lnα−b α+const.
0 0 2 2 Werecognizethisasthelogofagammadistribution, andsoweobtain 1 q(α)=Gam(α|a N, b N)= Γ(a ) a b 0 0α a 0 −1e −b 0 α (10.177) 0 where M a N = a 0 + (10.178) 2 1 b N = b 0 + E w w Tw .
(10.179) 2 10.7.
Expectation Propagation 505 Wealsoneedtooptimizethevariationalparametersξn, andthisisalsodoneby maximizingthelowerbound L (q,ξ).
Omittingtermsthatareindependentofξ, and integratingoverα, wehave L (q,ξ)= q(w)lnh(w,ξ)dw+const.
(10.180) Note that this has precisely the same form as (10.159), and so we can again appeal to our earlier result (10.163), which can be obtained by direct optimization of the marginallikelihoodfunction, leadingtore-estimationequationsoftheform (ξ n new)2 =φT n ΣN +µ N µT N φ n .
(10.181) We have obtained re-estimation equations for the three quantities q(w), q(α), andξ, andsoaftermakingsuitableinitializations, wecancyclethroughthesequan- Appendix B tities, updatingeachinturn.
Therequiredmomentsaregivenby E[α] = a N (10.182) b N E w Tw = ΣN +µT N µ N .
(10.183) 10.7.
Expectation Propagation Weconcludethischapterbydiscussinganalternativeformofdeterministicapprox- imate inference, known as expectation propagation or EP (Minka, 2001a; Minka, 2001b).
As with the variational Bayes methods discussed so far, this too is based on the minimization of a Kullback-Leibler divergence but now of the reverse form, whichgivestheapproximationratherdifferentproperties.
Considerforamomenttheproblemofminimizing KL(p q)withrespecttoq(z) whenp(z)isafixeddistributionandq(z)isamemberoftheexponentialfamilyand so, from(2.194), canbewrittenintheform q(z)=h(z)g(η)exp ηTu(z) .
(10.184) Asafunctionofη, the Kullback-Leiblerdivergencethenbecomes KL(p q)=−lng(η)−ηTE p(z) [u(z)]+const (10.185) wheretheconstanttermsareindependentofthenaturalparametersη.
Wecanmini- mize KL(p q)withinthisfamilyofdistributionsbysettingthegradientwithrespect toη tozero, giving −∇lng(η)=E p(z) [u(z)].
(10.186) However, we have already seen in (2.226) that the negative gradient of lng(η) is given by the expectation of u(z) under the distribution q(z).
Equating these two results, weobtain E q(z) [u(z)]=E p(z) [u(z)].
(10.187) 506 10.
APPROXIMATEINFERENCE Weseethattheoptimumsolutionsimplycorrespondstomatchingtheexpectedsuf- ficientstatistics.
So, forinstance, ifq(z)isa Gaussian N(z|µ,Σ)thenweminimize the Kullback-Leiblerdivergencebysettingthemeanµofq(z)equaltothemeanof the distribution p(z) and the covariance Σ equal to the covariance of p(z).
This is sometimescalledmomentmatching.
Anexampleofthiswasseenin Figure10.3(a).
Now let us exploit this result to obtain a practical algorithm for approximate inference.
Formanyprobabilisticmodels, thejointdistributionofdata Dandhidden variables(includingparameters)θcomprisesaproductoffactorsintheform p(D,θ)= fi(θ).
(10.188) i This would arise, for example, in a model for independent, identically distributed data in which there is one factor fn(θ) = p(xn |θ) for each data point xn, along withafactorf 0 (θ)=p(θ)correspondingtotheprior.
Moregenerally, itwouldalso applytoanymodeldefinedbyadirectedprobabilisticgraphinwhicheachfactorisa conditionaldistributioncorrespondingtooneofthenodes, oranundirectedgraphin whicheachfactorisacliquepotential.
Weareinterestedinevaluatingtheposterior distribution p(θ|D) for the purpose of making predictions, as well as the model evidencep(D)forthepurposeofmodelcomparison.
From(10.188)theposterioris givenby 1 p(θ|D)= p(D) fi(θ) (10.189) i andthemodelevidenceisgivenby p(D)= fi(θ)dθ.
(10.190) i Here we are considering continuous variables, but the following discussion applies equally to discrete variables with integrals replaced by summations.
We shall sup- posethatthemarginalizationoverθ, alongwiththemarginalizationswithrespectto the posterior distribution required to make predictions, are intractable so that some formofapproximationisrequired.
Expectationpropagationisbasedonanapproximationtotheposteriordistribu- tionwhichisalsogivenbyaproductoffactors 1 q(θ)= fi(θ) (10.191) Z i in which each factor fi(θ) in the approximation corresponds to one of the factors fi(θ) in the true posterior (10.189), and the factor 1/Z is the normalizing constant needed to ensure that the left-hand side of (10.191) integrates to unity.
In order to obtain a practical algorithm, we need to constrain the factors fi(θ) in some way, and in particular we shall assume that they come from the exponential family.
The productofthefactorswillthereforealsobefromtheexponentialfamilyandsocan 10.7.
Expectation Propagation 507 bedescribedbyafinitesetofsufficientstatistics.
Forexample, ifeachofthefi(θ) isa Gaussian, thentheoverallapproximationq(θ)willalsobe Gaussian.
Ideallywewouldliketodeterminethefi(θ)byminimizingthe Kullback-Leibler divergencebetweenthetrueposteriorandtheapproximationgivenby ’ ’ 1 ’ 1 KL(p q)=KL p(D) fi(θ)’ ’Z fi(θ) .
(10.192) i i Notethatthisisthereverseformof KLdivergencecomparedwiththatusedinvaria- tionalinference.
Ingeneral, thisminimizationwillbeintractablebecausethe KLdi- vergenceinvolvesaveragingwithrespecttothetruedistribution.
Asaroughapprox- imation, wecouldinsteadminimizethe KLdivergencesbetweenthecorresponding pairs fi(θ) and fi(θ) of factors.
This represents a much simpler problem to solve, andhastheadvantagethatthealgorithmisnoniterative.
However, becauseeachfac- tor is individually approximated, the product of the factors could well give a poor approximation.
Expectationpropagationmakesamuchbetterapproximationbyoptimizingeach factor in turn in the context of all of the remaining factors.
It starts by initializing the factors fi(θ), and then cycles through the factors refining them one at a time.
This is similar in spirit to the update of factors in the variational Bayes framework considered earlier.
Suppose we wish to refine factor fj(θ).
We first remove this factorfromtheproducttogive i =j fi(θ).
Conceptually, wewillnowdeterminea revisedformofthefactorfj(θ)byensuringthattheproduct qnew(θ)∝f j(θ) f i(θ) (10.193) i =j isascloseaspossibleto fj(θ) fi(θ) (10.194) i =j in which we keep fixed all of the factors fi(θ) for i = j.
This ensures that the approximationismostaccurateintheregionsofhighposteriorprobabilityasdefined bytheremainingfactors.
Weshallseeanexampleofthiseffectwhenweapply EP Section10.7.1 to the ‘clutter problem’.
To achieve this, we first remove the factor fj(θ) from the currentapproximationtotheposteriorbydefiningtheunnormalizeddistribution q(θ) q \j (θ)= .
(10.195) fj(θ) Note that we could instead find q\j(θ) from the product of factors i = j, although inpracticedivisionisusuallyeasier.
Thisisnowcombinedwiththefactorfj(θ)to giveadistribution 1 fj(θ)q \j (θ) (10.196) Zj 508 10.
APPROXIMATEINFERENCE 1 40 0.8 30 0.6 20 0.4 10 0.2 0 0 −2 −1 0 1 2 3 4 −2 −1 0 1 2 3 4 Figure 10.14 Illustration of the expectation propagation approximation using a Gaussian distribution for the exampleconsideredearlierin Figures4.14and10.1.
Theleft-handplotshowstheoriginaldistribution(yellow) along with the Laplace (red), global variational (green), and EP (blue) approximations, and the right-hand plot showsthecorrespondingnegativelogarithmsofthedistributions.
Notethatthe EPdistributionisbroaderthan thatvariationalinference, asaconsequenceofthedifferentformof KLdivergence.
where Zj isthenormalizationconstantgivenby Zj = fj(θ)q \j (θ)dθ.
(10.197) Wenowdeterminearevisedfactorfj(θ)byminimizingthe Kullback-Leiblerdiver- gence ’ KL fj(θ)q\j(θ) ’ ’ ’qnew(θ) .
(10.198) Zj Thisiseasilysolvedbecausetheapproximatingdistributionqnew(θ)isfromtheex- ponentialfamily, andsowecanappealtotheresult(10.187), whichtellsusthatthe parameters of qnew(θ) are obtained by matching its expected sufficient statistics to thecorrespondingmomentsof(10.196).
Weshallassumethatthisisatractableoper- ation.
Forexample, ifwechooseq(θ)tobea Gaussiandistribution N(θ|µ,Σ), then µissetequaltothemeanofthe(unnormalized)distributionfj(θ)q\j(θ), andΣis settoitscovariance.
Moregenerally, itisstraightforwardtoobtaintherequiredex- pectationsforanymemberoftheexponentialfamily, provideditcanbenormalized, becausetheexpectedstatisticscanberelatedtothederivativesofthenormalization coefficient, asgivenby(2.226).
The EPapproximationisillustratedin Figure10.14.
From (10.193), we see that the revised factor fj(θ) can be found by taking qnew(θ)anddividingouttheremainingfactorssothat qnew(θ) fj(θ)=K q\j(θ) (10.199) wherewehaveused(10.195).
Thecoefficient K isdeterminedbymultiplyingboth 10.7.
Expectation Propagation 509 sidesof(10.199)byq\i(θ)andintegratingtogive K = f j(θ)q \j (θ)dθ (10.200) wherewehaveusedthefactthatqnew(θ)isnormalized.
Thevalueof Kcantherefore befoundbymatchingzeroth-ordermoments f j(θ)q \j (θ)dθ = fj(θ)q \j (θ)dθ.
(10.201) Combining this with (10.197), we then see that K = Zj and so can be found by evaluatingtheintegralin(10.197).
In practice, several passes are made through the set of factors, revising each factorinturn.
Theposteriordistributionp(θ|D)isthenapproximatedusing(10.191), andthemodelevidencep(D)canbeapproximatedbyusing(10.190)withthefactors fi(θ)replacedbytheirapproximationsfi(θ).
Expectation Propagation We are given a joint distribution over observed data D and stochastic variables θintheformofaproductoffactors p(D,θ)= fi(θ) (10.202) i and we wish to approximate the posterior distributionp(θ|D) by a distribution oftheform 1 q(θ)= fi(θ).
(10.203) Z i Wealsowishtoapproximatethemodelevidencep(D).
1.
Initializealloftheapproximatingfactorsfi(θ).
2.
Initializetheposteriorapproximationbysetting q(θ)∝ fi(θ).
(10.204) i 3.
Untilconvergence: (a) Chooseafactorfj(θ)torefine.
(b) Removefj(θ)fromtheposteriorbydivision q(θ) q \j (θ)= .
(10.205) fj(θ) 510 10.
APPROXIMATEINFERENCE (c) Evaluatethenewposteriorbysettingthesufficientstatistics(moments) ofqnew(θ)equaltothoseofq\j(θ)fj(θ), includingevaluationofthe normalizationconstant Zj = q \j (θ)fj(θ)dθ.
(10.206) (d) Evaluateandstorethenewfactor qnew(θ) fj(θ)=Zj q\j(θ) .
(10.207) 4.
Evaluatetheapproximationtothemodelevidence p(D) fi(θ)dθ.
(10.208) i A special case of EP, known as assumed density filtering (ADF) or moment matching (Maybeck, 1982; Lauritzen, 1992; Boyen and Koller, 1998; Opper and Winther, 1999), is obtained by initializing all of the approximating factors except thefirsttounityandthenmakingonepassthroughthefactorsupdatingeachofthem once.
Assumeddensityfilteringcanbeappropriateforon-linelearninginwhichdata pointsarearrivinginasequenceandweneedtolearnfromeachdatapointandthen discarditbeforeconsideringthenextpoint.
However, inabatchsettingwehavethe opportunity to re-use the data points many times in order to achieve improved ac- curacy, anditisthisideathatisexploitedinexpectationpropagation.
Furthermore, if we apply ADF to batch data, the results will have an undesirable dependence on the (arbitrary) order in which the data points are considered, which again EP can overcome.
One disadvantage of expectation propagation is that there is no guarantee that the iterations will converge.
However, for approximations q(θ) in the exponential family, iftheiterationsdoconverge, theresultingsolutionwillbeastationarypoint of a particular energy function (Minka, 2001a), although each iteration of EP does not necessarily decrease the value of this energy function.
This is in contrast to variational Bayes, which iteratively maximizes a lower bound on the log marginal likelihood, in which each iteration is guaranteed not to decrease the bound.
It is possible to optimize the EP cost function directly, in which case it is guaranteed to converge, although the resulting algorithms can be slower and more complex to implement.
Another difference between variational Bayes and EP arises from the form of KL divergence that is minimized by the two algorithms, because the former mini- mizes KL(q p) whereas the latter minimizes KL(p q).
As we saw in Figure 10.3, fordistributionsp(θ)whicharemultimodal, minimizing KL(p q)canleadtopoor approximations.
In particular, if EP is applied to mixtures the results are not sen- sible because the approximation tries to capture all of the modes of the posterior distribution.
Conversely, in logistic-type models, EP often out-performs both local variationalmethodsandthe Laplaceapproximation(Kussand Rasmussen,2006).
10.7.
Expectation Propagation 511 Figure10.15 Illustration of the clutter problem foradataspacedimensionalityof D = 1.
Training data points, de- noted by the crosses, are drawn from a mixture of two Gaussians with components shown in red andgreen.
Thegoalistoinferthe meanofthegreen Gaussianfrom theobserveddata.
−5 0 θ 5 x 10 10.7.1 Example: The clutter problem Following Minka(2001b), weillustratethe EPalgorithmusingasimpleexam- ple in which the goal is to infer the mean θ of a multivariate Gaussian distribution overavariablexgivenasetofobservationsdrawnfromthatdistribution.
Tomake theproblemmoreinteresting, theobservationsareembeddedinbackgroundclutter, whichitselfisalso Gaussiandistributed, asillustratedin Figure10.15.
Thedistribu- tion of observed values x is therefore a mixture of Gaussians, which we take to be oftheform p(x|θ)=(1−w)N(x|θ, I)+w N(x|0, a I) (10.209) where w is the proportion of background clutter and is assumed to be known.
The prioroverθistakentobe Gaussian p(θ)=N(θ|0, b I) (10.210) and Minka (2001a) chooses the parameter values a = 10, b = 100 and w = 0.5.
Thejointdistributionof N observations D ={x 1 ,..., x N }andθisgivenby N p(D,θ)=p(θ) p(xn |θ) (10.211) n=1 and so the posterior distribution comprises a mixture of 2N Gaussians.
Thus the computational cost of solving this problem exactly would grow exponentially with the size of the data set, and so an exact solution is intractable for moderately large N.
To apply EP to the clutter problem, we first identify the factors f 0 (θ) = p(θ) andfn(θ) = p(xn |θ).
Nextweselectanapproximatingdistributionfromtheexpo- nentialfamily, andforthisexampleitisconvenienttochooseaspherical Gaussian q(θ)=N(θ|m, v I).
(10.212) 512 10.
APPROXIMATEINFERENCE The factor approximations will therefore take the form of exponential-quadratic functionsoftheform fn(θ)=sn N(θ|mn, vn I) (10.213) wheren = 1,..., N, andwesetf 0 (θ)equaltothepriorp(θ).
Notethattheuseof N(θ|·,·)doesnotimplythattheright-handsideisawell-defined Gaussiandensity (infact, asweshall see, thevarianceparametervn canbenegative) butissimply a convenient shorthand notation.
The approximations fn(θ), for n = 1,..., N, can be initialized to unity, corresponding to sn = (2πvn)D/2, vn → ∞ and mn = 0, where D is the dimensionality of x and hence of θ.
The initial q(θ), defined by (10.191), isthereforeequaltotheprior.
We then iteratively refine the factors by taking one factor fn(θ) at a time and applying (10.205), (10.206), and (10.207).
Note that we do not need to revise the Exercise 10.37 termf 0 (θ)becausean EPupdatewillleavethistermunchanged.
Herewestatethe resultsandleavethereadertofillinthedetails.
Firstweremovethecurrentestimatefn(θ)fromq(θ)bydivisionusing(10.205) Exercise 10.38 togiveq\n(θ), whichhasmeanandinversevariancegivenby m \n = m+v \n v n −1(m−mn) (10.214) (v \n ) −1 = v −1−v −1.
(10.215) n Nextweevaluatethenormalizationconstant Zn using(10.206)togive Zn =(1−w)N(xn |m \n ,(v \n +1)I)+w N(xn |0, a I).
(10.216) Similarly, we compute the mean and variance of qnew(θ) by finding the mean and Exercise 10.39 varianceofq\n(θ)fn(θ)togive v\n m = m \n +ρn v\n+1 (xn −m \n ) (10.217) v = v \n−ρn v ( \ v n \n + )2 1 +ρn(1−ρn) (v\n D )2 ( v x \n n + − 1 m )2 \n 2 (10.218) wherethequantity w ρn =1− N(xn |0, a I) (10.219) Zn hasasimpleinterpretationastheprobabilityofthepointxn notbeingclutter.
Then weuse(10.207)tocomputetherefinedfactorfn(θ)whoseparametersaregivenby v −1 = (vnew) −1−(v \n ) −1 (10.220) n mn = m \n +(vn+v \n )(v \n ) −1(mnew−m \n ) (10.221) Zn sn = (2πvn)D/2N(mn |m\n,(vn+v\n)I) .
(10.222) Thisrefinementprocessisrepeateduntilasuitableterminationcriterionissatisfied, forinstancethatthemaximumchangeinparametervaluesresultingfromacomplete 10.7.
Expectation Propagation 513 −5 0 5 θ 10 −5 0 5 θ 10 Figure 10.16 Examples of the approximation of specific factors for a one-dimensional version of the clutter problem, showingf n (θ)inblue, fe n (θ)inred, andq\n(θ)ingreen.
Noticethatthecurrentformforq\n(θ)controls therangeofθoverwhichfe n (θ)willbeagoodapproximationtof n (θ).
pass through all factors is less than some threshold.
Finally, we use (10.208) to evaluatetheapproximationtothemodelevidence, givenby N p(D) (2πvnew) D/2exp(B/2) sn(2πvn) −D/2 (10.223) n=1 where N B = (mnew)Tmnew − m T n mn .
(10.224) v vn n=1 Examplesfactorapproximationsfortheclutterproblemwithaone-dimensionalpa- rameterspaceθ areshownin Figure10.16.
Notethatthefactorapproximationscan have infinite or even negative values for the ‘variance’ parameter vn.
This simply correspondstoapproximationsthatcurveupwardsinsteadofdownwardsandarenot necessarily problematic provided the overall approximate posterior q(θ) has posi- tivevariance.
Figure10.17comparestheperformanceof EPwithvariational Bayes (meanfieldtheory)andthe Laplaceapproximationontheclutterproblem.
10.7.2 Expectation propagation on graphs Sofarinourgeneraldiscussionof EP, wehaveallowedthefactorsfi(θ)inthe distributionp(θ)tobefunctionsofallofthecomponentsofθ, andsimilarlyforthe approximatingfactorsf(θ)intheapproximatingdistributionq(θ).
Wenowconsider situationsinwhichthefactorsdependonlyonsubsetsofthevariables.
Suchrestric- tions can be conveniently expressed using the framework of probabilistic graphical models, asdiscussedin Chapter8.
Hereweuseafactorgraphrepresentationbecause thisencompassesbothdirectedandundirectedgraphs.
514 10.
APPROXIMATEINFERENCE Posterior mean laplace vb ep FLOPS rorr E Evidence 100 vb 10−5 laplace ep 104 106 FLOPS rorr E 10−200 10−202 10−204 104 106 Figure10.17 Comparisonofexpectationpropagation, variationalinference, andthe Laplaceapproximationon the clutter problem.
The left-hand plot shows the error in the predicted posterior mean versus the number of floatingpointoperations, andtheright-handplotshowsthecorrespondingresultsforthemodelevidence.
Weshallfocusonthecaseinwhichtheapproximatingdistributionisfullyfac- torized, andweshallshowthatinthiscaseexpectationpropagationreducestoloopy belief propagation (Minka, 2001a).
To start with, we show this in the context of a simpleexample, andthenweshallexplorethegeneralcase.
Firstofall, recallfrom(10.17)thatifweminimizethe Kullback-Leiblerdiver- gence KL(p q)withrespecttoafactorizeddistributionq, thentheoptimalsolution foreachfactorissimplythecorrespondingmarginalofp.
Now consider the factor graph shown on the left in Figure 10.18, which was Section8.4.4 introducedearlierinthecontextofthesum-productalgorithm.
Thejointdistribution isgivenby p(x)=fa(x 1 , x 2 )fb(x 2 , x 3 )fc(x 2 , x 4 ).
(10.225) Weseekanapproximationq(x)thathasthesamefactorization, sothat q(x)∝fa(x 1 , x 2 )fb(x 2 , x 3 )fc(x 2 , x 4 ).
(10.226) Notethatnormalizationconstantshavebeenomitted, andthesecanbere-instatedat theendbylocalnormalization, asisgenerallydoneinbeliefpropagation.
Nowsup- posewerestrictattentiontoapproximationsinwhichthefactorsthemselvesfactorize withrespecttotheindividualvariablessothat q(x)∝fa1 (x 1 )fa2 (x 2 )fb2 (x 2 )fb3 (x 3 )fc2 (x 2 )fc4 (x 4 ) (10.227) whichcorrespondstothefactorgraphshownontherightin Figure10.18.
Because the individual factors are factorized, the overall distribution q(x) is itself fully fac- torized.
Nowweapplythe EPalgorithmusingthefullyfactorizedapproximation.
Sup- pose that we have initialized all of the factors and that we choose to refine factor 10.7.
Expectation Propagation 515 x1 x2 x3 x1 x2 x3 fa fb f˜ a1 f˜ a2 f˜ b2 f˜ b3 f˜ c2 fc f˜ c4 x4 x4 Figure10.18 Ontheleftisasimplefactorgraphfrom Figure8.51andreproducedhereforconvenience.
On therightisthecorrespondingfactorizedapproximation.
fb(x 2 , x 3 ) = fb2 (x 2 )fb3 (x 3 ).
We first remove this factor from the approximating distributiontogive \b q (x)=fa1 (x 1 )fa2 (x 2 )fc2 (x 2 )fc4 (x 4 ) (10.228) andwethenmultiplythisbytheexactfactorfb(x 2 , x 3 )togive p(x)=q \b (x)fb(x 2 , x 3 )=f a1 (x 1 )f a2 (x 2 )f c2 (x 2 )f c4 (x 4 )fb(x 2 , x 3 ).
(10.229) Wenowfindqnew(x)byminimizingthe Kullback-Leiblerdivergence KL( p qnew).
Theresult, asnotedabove, isthatqnew(z)comprisestheproductoffactors, onefor each variable xi, in which each factor is given by the corresponding marginal of p(x).
Thesefourmarginalsaregivenby p(x 1 ) ∝ f a1 (x 1 ) (10.230) p(x 2 ) ∝ f a2 (x 2 )f c2 (x 2 ) fb(x 2 , x 3 ) (10.231) x 3 p(x 3 ) ∝ fb(x 2 , x 3 )f a2 (x 2 )f c2 (x 2 ) (10.232) x 2 p(x 4 ) ∝ f c4 (x 4 ) (10.233) and qnew(x) is obtained by multiplying these marginals together.
We see that the only factors in q(x) that change when we update fb(x 2 , x 3 ) are those that involve the variables in fb namely x 2 and x 3.
To obtain the refined factor fb(x 2 , x 3 ) = f b2 (x 2 )f b3 (x 3 )wesimplydivideqnew(x)byq\b(x), whichgives fb2 (x 2 ) ∝ fb(x 2 , x 3 ) (10.234) x 3 fb3 (x 3 ) ∝ fb(x 2 , x 3 )fa2 (x 2 )fc2 (x 2 ) .
(10.235) x 2 516 10.
APPROXIMATEINFERENCE Section8.4.4 These are precisely the messages obtained using belief propagation in which mes- sagesfromvariablenodestofactornodeshavebeenfoldedintothemessagesfrom factor nodes to variable nodes.
In particular, fb2 (x 2 ) corresponds to the message µf b →x 2 (x 2 )sentbyfactornodefb tovariablenodex 2 andisgivenby(8.81).
Simi- larly, ifwesubstitute(8.78)into(8.79), weobtain(10.235)inwhichfa2 (x 2 )corre- spondstoµf a →x 2 (x 2 )andfc2 (x 2 )correspondstoµf c →x 2 (x 2 ), givingthemessage fb3 (x 3 )whichcorrespondstoµf b →x 3 (x 3 ).
Thisresultdiffersslightlyfromstandardbeliefpropagationinthatmessagesare passedinbothdirectionsatthesametime.
Wecaneasilymodifythe EPprocedure to give the standard form of the sum-product algorithm by updating just one of the factors at a time, for instance if we refine only fb3 (x 3 ), then fb2 (x 2 ) is unchanged by definition, while the refined version of fb3 (x 3 ) is again given by (10.235).
If we are refining only one term at a time, then we can choose the order in which the refinements are done as we wish.
In particular, for a tree-structured graph we can follow a two-pass update scheme, corresponding to the standard belief propagation schedule, which will result in exact inference of the variable and factor marginals.
Theinitializationoftheapproximationfactorsinthiscaseisunimportant.
Nowletusconsiderageneralfactorgraphcorrespondingtothedistribution p(θ)= fi(θ i) (10.236) i whereθ irepresentsthesubsetofvariablesassociatedwithfactorfi.
Weapproximate thisusingafullyfactorizeddistributionoftheform q(θ)∝ fik(θk) (10.237) i k whereθk correspondstoanindividualvariablenode.
Supposethatwewishtorefine the particular term fjl(θl) keeping all other terms fixed.
We first remove the term fj(θ j)fromq(θ)togive q \j (θ)∝ f ik(θk) (10.238) i =j k andthenmultiplybytheexactfactorfj(θ j).
Todeterminetherefinedtermfjl(θl), we need only consider the functional dependence on θl, and so we simply find the correspondingmarginalof q \j (θ)fj(θ j).
(10.239) Uptoamultiplicativeconstant, thisinvolvestakingthemarginaloffj(θ j)multiplied byanytermsfromq\j(θ)thatarefunctionsofanyofthevariablesinθ j.
Termsthat correspond to other factors fi(θ i) for i = j will cancel between numerator and denominatorwhenwesubsequentlydividebyq\j(θ).
Wethereforeobtain fjl(θl)∝ fj(θ j) fkm(θm).
(10.240) θ m= l ∈θ j k m= l Exercises 517 Werecognizethisasthesum-productruleintheforminwhichmessagesfromvari- ablenodestofactornodeshavebeeneliminated, asillustratedbytheexampleshown in Figure 8.50.
The quantity fjm(θm) corresponds to the message µf →θ (θm), j m which factor node j sends to variable node m, and the product over k in (10.240) is over all factors that depend on the variables θm that have variables (other than variableθl)incommonwithfactorfj(θ j).
Inotherwords, tocomputetheoutgoing messagefromafactornode, wetaketheproductofalltheincomingmessagesfrom otherfactornodes, multiplybythelocalfactor, andthenmarginalize.
Thus, the sum-product algorithm arises as a special case of expectation propa- gationifweuseanapproximatingdistributionthatisfullyfactorized.
Thissuggests that more flexible approximating distributions, corresponding to partially discon- nected graphs, could be used to achieve higher accuracy.
Another generalization is togroupfactorsfi(θ i)togetherintosetsandtorefineallthefactorsinasettogether at each iteration.
Both of these approaches can lead to improvements in accuracy (Minka,2001b).
Ingeneral, theproblemofchoosingthebestcombinationofgroup- inganddisconnectionisanopenresearchissue.
Wehaveseenthatvariationalmessagepassingandexpectationpropagationop- timize two different forms of the Kullback-Leibler divergence.
Minka (2005) has shownthatabroadrangeofmessagepassingalgorithmscanbederivedfromacom- mon framework involving minimization of members of the alpha family of diver- gences, given by (10.19).
These include variational message passing, loopy belief propagation, and expectation propagation, as well as a range of other algorithms, whichwedonothavespacetodiscusshere, suchastree-reweightedmessagepass- ing(Wainwrightetal.,2005), fractionalbeliefpropagation(Wiegerinckand Heskes, 2003), andpower EP(Minka,2004).
Exercises 10.1 ( ) www Verify that the log marginal distribution of the observed data lnp(X) canbedecomposedintotwotermsintheform(10.2)where L(q)isgivenby(10.3) and KL(q p)isgivenby(10.4).
10.2 ( ) Usetheproperties E[z 1 ]=m 1and E[z 2 ]=m 2tosolvethesimultaneousequa- tions (10.13) and (10.15), and hence show that, provided the original distribution p(z)isnonsingular, theuniquesolutionforthemeansofthefactorsintheapproxi- mationdistributionisgivenby E[z 1 ]=µ 1 and E[z 2 ]=µ 2.
10.3 ( ) www Considerafactorizedvariationaldistributionq(Z)oftheform(10.5).
By using the technique of Lagrange multipliers, verify that minimization of the Kullback-Leibler divergence KL(p q) with respect to one of the factors qi(Zi), keepingallotherfactorsfixed, leadstothesolution(10.17).
10.4 ( ) Supposethatp(x)issomefixeddistributionandthatwewishtoapproximate it using a Gaussian distribution q(x) = N(x|µ,Σ).
By writing down the form of the KLdivergence KL(p q)fora Gaussianq(x)andthendifferentiating, showthat 518 10.
APPROXIMATEINFERENCE minimizationof KL(p q)withrespecttoµandΣleadstotheresultthatµisgiven bytheexpectationofxunderp(x)andthatΣisgivenbythecovariance.
10.5 ( ) www Consideramodelinwhichthesetofallhiddenstochasticvariables, de- notedcollectivelyby Z, comprisessomelatentvariablesztogetherwithsomemodel parameters θ.
Suppose we use a variational distribution that factorizes between la- tentvariablesandparameterssothatq(z,θ)=q z (z)qθ(θ), inwhichthedistribution qθ(θ)isapproximatedbyapointestimateoftheformqθ(θ) = δ(θ−θ 0 )whereθ 0 is a vector of free parameters.
Show that variational optimization of this factorized distribution is equivalent to an EM algorithm, in which the E step optimizes q z (z), andthe Mstepmaximizestheexpectedcomplete-datalogposteriordistributionofθ withrespecttoθ 0.
10.6 ( ) Thealphafamilyofdivergencesisdefinedby(10.19).
Showthatthe Kullback- Leibler divergence KL(p q) corresponds to α → 1.
This can be done by writing p = exp( lnp) = 1+ lnp+O( 2)andthentaking → 0.
Similarlyshowthat KL(q p)correspondstoα→−1.
10.7 ( ) Considertheproblemofinferringthemeanandprecisionofaunivariate Gaus- sian using a factorized variational approximation, as considered in Section 10.1.3.
Showthatthefactorqµ(µ)isa Gaussianoftheform N(µ|µN,λ − N 1)withmeanand precision given by (10.26) and (10.27), respectively.
Similarly show that the factor qτ(τ)isagammadistributionoftheform Gam(τ|a N, b N)withparametersgivenby (10.29)and(10.30).
10.8 ( ) Consider the variational posterior distribution for the precision of a univariate Gaussianwhoseparametersaregivenby(10.29)and(10.30).
Byusingthestandard results for the mean and variance of the gamma distribution given by (B.27) and (B.28), show that if we let N → ∞, this variational posterior distribution has a meangivenbytheinverseofthemaximumlikelihoodestimatorforthevarianceof thedata, andavariancethatgoestozero.
10.9 ( ) Bymakinguseofthestandardresult E[τ]=a N/b N forthemeanofagamma distribution, together with (10.26), (10.27), (10.29), and (10.30), derive the result (10.33)forthereciprocaloftheexpectedprecisioninthefactorizedvariationaltreat- mentofaunivariate Gaussian.
10.10 ( ) www Derivethedecompositiongivenby(10.34)thatisusedtofindapproxi- mateposteriordistributionsovermodelsusingvariationalinference.
10.11 ( ) www Byusinga Lagrangemultipliertoenforcethenormalizationconstraint onthedistributionq(m), showthatthemaximumofthelowerbound(10.35)isgiven by(10.36).
10.12 ( ) Starting from the joint distribution (10.41), and applying the general result (10.9), showthattheoptimalvariationaldistributionq (Z)overthelatentvariables for the Bayesian mixture of Gaussians is given by (10.48) by verifying the steps giveninthetext.
Exercises 519 10.13 ( ) www Startingfrom(10.54), derivetheresult(10.59)fortheoptimumvari- ationalposteriordistributionoverµ k andΛk inthe Bayesianmixtureof Gaussians, and hence verify the expressions for the parameters of this distribution given by (10.60)–(10.63).
10.14 ( ) Usingthedistribution(10.59), verifytheresult(10.64).
10.15 ( ) Usingtheresult(B.17), showthattheexpectedvalueofthemixingcoefficients inthevariationalmixtureof Gaussiansisgivenby(10.69).
10.16 ( ) www Verify the results (10.71) and (10.72) for the first two terms in the lowerboundforthevariational Gaussianmixturemodelgivenby(10.70).
10.17 ( ) Verifytheresults(10.73)–(10.77)fortheremainingtermsinthelowerbound forthevariational Gaussianmixturemodelgivenby(10.70).
10.18 ( ) In this exercise, we shall derive the variational re-estimation equations for the Gaussianmixturemodelbydirectdifferentiationofthelowerbound.
Todothis we assume that the variational distribution has the factorization defined by (10.42) (10.70)andhenceobtainthelowerboundasafunctionoftheparametersofthevaria- tionaldistribution.
Then, bymaximizingtheboundwithrespecttotheseparameters, derivethere-estimationequationsforthefactorsinthevariationaldistribution, and showthatthesearethesameasthoseobtainedin Section10.2.1.
10.19 ( ) Derivetheresult(10.81)forthepredictivedistributioninthevariationaltreat- mentofthe Bayesianmixtureof Gaussiansmodel.
10.20 ( ) www Thisexerciseexploresthevariational Bayessolutionforthemixtureof Gaussiansmodelwhenthesize N ofthedatasetislargeandshowsthatitreduces(as wewouldexpect)tothemaximumlikelihoodsolutionbasedon EMderivedin Chap- ter9.
Notethatresultsfrom Appendix Bmaybeusedtohelpanswerthisexercise.
First show that the posterior distribution q (Λk) of the precisions becomes sharply peakedaroundthemaximumlikelihoodsolution.
Dothesamefortheposteriordis- tribution of the means q (µ k |Λk).
Next consider the posterior distribution q (π) for the mixing coefficients and show that this too becomes sharply peaked around the maximum likelihood solution.
Similarly, show that the responsibilities become equaltothecorrespondingmaximumlikelihoodvaluesforlarge N, bymakinguse ofthefollowingasymptoticresultforthedigammafunctionforlargex ψ(x)=lnx+O(1/x).
(10.241) Finally, bymakinguseof(10.80), showthatforlarge N, thepredictivedistribution becomesamixtureof Gaussians.
10.21 ( ) Showthatthenumberofequivalentparametersettingsduetointerchangesym- metriesinamixturemodelwith K componentsis K!.
520 10.
APPROXIMATEINFERENCE 10.22 ( ) Wehaveseenthateachmodeoftheposteriordistributionina Gaussianmix- turemodelisamemberofafamilyof K! equivalentmodes.
Supposethattheresult of running the variational inference algorithm is an approximate posterior distribu- tion q that is localized in the neighbourhood of one of the modes.
We can then approximate the full posterior distribution as a mixture of K! such q distributions, once centred on each mode and having equal mixing coefficients.
Show that if we assume negligible overlap between the components of the q mixture, the resulting lowerbounddiffersfromthatforasinglecomponentq distributionthroughthead- ditionofanextratermln K!.
10.23 ( ) www Consider a variational Gaussian mixture model in which there is no priordistributionovermixingcoefficients{πk }.
Instead, themixingcoefficientsare treated as parameters, whose values are to be found by maximizing the variational lowerboundonthelogmarginallikelihood.
Showthatmaximizingthislowerbound with respect to the mixing coefficients, using a Lagrange multiplier to enforce the constraint that the mixing coefficients sum to one, leads to the re-estimation result (10.83).
Notethatthereisnoneedtoconsiderallofthetermsinthelowerboundbut onlythedependenceoftheboundonthe{πk }.
10.24 ( ) www Wehaveseenin Section10.2thatthesingularitiesarisinginthemax- imum likelihood treatment of Gaussian mixture models do not arise in a Bayesian treatment.
Discuss whether such singularities would arise if the Bayesian model weresolvedusingmaximumposterior(MAP)estimation.
10.25 ( ) Thevariationaltreatmentofthe Bayesianmixtureof Gaussians, discussedin Section10.2, madeuseofafactorizedapproximation(10.5)totheposteriordistribu- tion.
Aswesawin Figure10.2, thefactorizedassumptioncausesthevarianceofthe posteriordistributiontobeunder-estimatedforcertaindirectionsinparameterspace.
Discussqualitativelytheeffectthiswillhaveonthevariationalapproximationtothe model evidence, and how this effect will vary with the number of components in the mixture.
Hence explain whether the variational Gaussian mixture will tend to under-estimateorover-estimatetheoptimalnumberofcomponents.
10.26 ( ) Extend the variational treatment of Bayesian linear regression to include a gamma hyperprior Gam(β|c 0 , d 0 ) over β and solve variationally, by assuming a factorizedvariationaldistributionoftheformq(w)q(α)q(β).
Derivethevariational update equations for the three factors in the variational distribution and also obtain anexpressionforthelowerboundandforthepredictivedistribution.
10.27 ( ) Bymakinguseoftheformulaegivenin Appendix Bshowthatthevariational lowerboundforthelinearbasisfunctionregressionmodel, definedby(10.107), can bewrittenintheform(10.107)withthevarioustermsdefinedby(10.108)–(10.112).
10.28 ( ) Rewrite the model for the Bayesian mixture of Gaussians, introduced in Section 10.2, as a conjugate model from the exponential family, as discussed in Section 10.4.
Hence use the general results (10.115) and (10.119) to derive the specificresults(10.48),(10.57), and(10.59).
Exercises 521 10.29 ( ) www Show that the function f(x) = ln(x) is concave for 0 < x < ∞ by computing its second derivative.
Determine the form of the dual function g(λ) defined by (10.133), and verify that minimization of λx − g(λ) with respect to λ accordingto(10.132)indeedrecoversthefunctionln(x).
10.30 ( ) Byevaluatingthesecondderivative, showthattheloglogisticfunctionf(x) = −ln(1+e−x) is concave.
Derive the variational upper bound (10.137) directly by makingasecondorder Taylorexpansionoftheloglogisticfunctionaroundapoint x=ξ.
10.31 ( ) By finding the second derivative with respect to x, show that the function f(x) = −ln(ex/2 +e−x/2) is a concave function of x.
Now consider the second derivativeswithrespecttothevariablex2andhenceshowthatitisaconvexfunction ofx2.
Plotgraphsoff(x)againstxandagainstx2.
Derivethelowerbound(10.144) onthelogisticsigmoidfunctiondirectlybymakingafirstorder Taylorseriesexpan- sionofthefunctionf(x)inthevariablex2 centredonthevalueξ2.
10.32 ( ) www Considerthevariationaltreatmentoflogisticregressionwithsequen- tial learning in which data points are arriving one at a time and each must be pro- cessed and discarded before the next data point arrives.
Show that a Gaussian ap- proximation to the posterior distribution can be maintained through the use of the lowerbound(10.151), inwhichthedistributionisinitializedusingtheprior, andas eachdatapointisabsorbeditscorrespondingvariationalparameterξn isoptimized.
10.33 ( ) By differentiating the quantity Q(ξ,ξold ) defined by (10.161) with respect to the variational parameter ξn show that the update equation for ξn for the Bayesian logisticregressionmodelisgivenby(10.163).
10.34 ( ) Inthisexercisewederivere-estimationequationsforthevariationalparame- tersξinthe Bayesianlogisticregressionmodelof Section4.5bydirectmaximization ofthelowerboundgivenby(10.164).
Todothissetthederivativeof L(ξ)withre- specttoξnequaltozero, makinguseoftheresult(3.117)forthederivativeofthelog of adeterminant, together with theexpressions(10.157) and (10.158) whichdefine themeanandcovarianceofthevariationalposteriordistributionq(w).
10.35 ( ) Derivetheresult(10.164)forthelowerbound L(ξ)inthevariationallogistic regression model.
This is most easily done by substituting the expressions for the Gaussian prior q(w) = N(w|m 0 , S 0 ), together with the lower bound h(w,ξ) on the likelihood function, into the integral (10.159) which defines L(ξ).
Next gather together the terms which depend on w in the exponential and complete the square to give a Gaussian integral, which can then be evaluated by invoking the standard result for the normalization coefficient of a multivariate Gaussian.
Finally take the logarithmtoobtain(10.164).
10.36 ( ) Considerthe ADFapproximationschemediscussedin Section10.7, andshow that inclusion of the factor fj(θ) leads to an update of the model evidence of the form pj(D) pj−1 (D)Zj (10.242) 522 10.
APPROXIMATEINFERENCE where Zj isthenormalizationconstantdefinedby(10.197).
Byapplyingthisresult recursively, andinitializingwithp 0 (D)=1, derivetheresult p(D) Zj.
(10.243) j 10.37 ( ) www Considertheexpectationpropagationalgorithmfrom Section10.7, and suppose that one of the factors f 0 (θ) in the definition (10.188) has the same expo- nential family functional formas theapproximating distributionq(θ).
Showthat if the factor f 0 (θ) is initialized to be f 0 (θ), then an EP update to refine f 0 (θ) leaves f 0 (θ)unchanged.
Thissituationtypicallyariseswhenoneofthefactorsistheprior p(θ), andsoweseethatthepriorfactorcanbeincorporatedonceexactlyanddoes notneedtoberefined.
10.38 ( ) In this exercise and the next, we shall verify the results (10.214)–(10.224) for the expectation propagation algorithm applied to the clutter problem.
Begin by usingthedivisionformula(10.205)toderivetheexpressions(10.214)and(10.215) by completing the square inside the exponential to identify the mean and variance.
Also, showthatthenormalizationconstant Zn, definedby(10.206), isgivenforthe clutter problem by (10.216).
This can be done by making use of the general result (2.115).
10.39 ( ) Show that the mean and variance of qnew(θ) for EP applied to the clutter problem are given by (10.217) and (10.218).
To do this, first prove the following resultsfortheexpectationsofθandθθT underqnew(θ) E[θ] = m \n +v \n∇ m\n ln Zn (10.244) E[θTθ] = 2(v \n )2∇ v\n ln Zn+2E[θ]Tm \n− m \n 2 (10.245) and then make use of the result (10.216) for Zn.
Next, prove the results (10.220)– (10.222) by using (10.207) and completing the square in the exponential.
Finally, use(10.208)toderivetheresult(10.223).
11 Sampling Methods Formostprobabilisticmodelsofpracticalinterest, exactinferenceisintractable, and so we have to resort to some form of approximation.
In Chapter 10, we discussed inferencealgorithmsbasedondeterministicapproximations, whichincludemethods such as variational Bayes and expectation propagation.
Here we consider approxi- mate inference methods based on numerical sampling, also known as Monte Carlo techniques.
Althoughforsomeapplicationstheposteriordistributionoverunobservedvari- ableswillbeofdirectinterestinitself, formostsituationstheposteriordistribution isrequiredprimarilyforthepurposeofevaluatingexpectations, forexampleinorder tomakepredictions.
Thefundamentalproblemthatwethereforewishtoaddressin thischapterinvolvesfindingtheexpectationofsomefunctionf(z)withrespecttoa probabilitydistributionp(z).
Here, thecomponentsofzmightcomprisediscreteor continuousvariablesorsomecombinationofthetwo.
Thusinthecaseofcontinuous 523 524 11.
SAMPLINGMETHODS Figure11.1 Schematic illustration of a function f(z) f(z) whose expectation is to be evaluated with p(z) respecttoadistributionp(z).
z variables, wewishtoevaluatetheexpectation E[f]= f(z)p(z)dz (11.1) where the integral is replaced by summation in the case of discrete variables.
This isillustratedschematicallyforasinglecontinuousvariablein Figure11.1.
Weshall supposethatsuchexpectationsaretoocomplextobeevaluatedexactlyusinganalyt- icaltechniques.
The general idea behind sampling methods is to obtain a set of samples z(l) (where l = 1,..., L) drawn independently from the distribution p(z).
This allows theexpectation(11.1)tobeapproximatedbyafinitesum L f = 1 f(z(l)).
(11.2) L l=1 Aslongasthesamplesz(l) aredrawnfromthedistributionp(z), then E[f ] = E[f] andsotheestimatorf hasthecorrectmean.
Thevarianceoftheestimatorisgiven Exercise 11.1 by var[f ]= 1 E (f −E[f])2 (11.3) L isthevarianceofthefunctionf(z)underthedistributionp(z).
Itisworthemphasiz- ing that the accuracy of the estimator therefore does not depend on the dimension- ality of z, and that, in principle, high accuracy may be achievable with a relatively small number of samples z(l).
In practice, ten or twenty independent samples may sufficetoestimateanexpectationtosufficientaccuracy.
Theproblem, however, isthatthesamples{z(l)}mightnotbeindependent, and so the effective sample size might be much smaller than the apparent sample size.
Also, referring back to Figure 11.1, we note that if f(z) is small in regions where p(z) is large, and vice versa, then the expectation may be dominated by regions of small probability, implying that relatively large sample sizes will be required to achievesufficientaccuracy.
For many models, the joint distribution p(z) is conveniently specified in terms ofagraphicalmodel.
Inthecaseofadirectedgraphwithnoobservedvariables, itis 11.
SAMPLINGMETHODS 525 straightforwardtosamplefromthejointdistribution(assumingthatitispossibleto sample from the conditional distributions at each node) using the following ances- tral sampling approach, discussed briefly in Section 8.1.2.
The joint distribution is specifiedby M p(z)= p(zi |pa i ) (11.4) i=1 where zi are the set of variables associated with node i, and pa i denotes the set of variables associated with the parents of node i.
To obtain a sample from the joint distribution, wemakeonepassthroughthesetofvariablesintheorderz 1 ,..., z M sampling from the conditional distributions p(zi |pa i ).
This is always possible be- causeateachstepalloftheparentvalueswillhavebeeninstantiated.
Afteronepass throughthegraph, wewillhaveobtainedasamplefromthejointdistribution.
Now consider the case of a directed graph in which some of the nodes are in- stantiatedwithobservedvalues.
Wecaninprincipleextendtheaboveprocedure, at leastinthecaseofnodesrepresentingdiscretevariables, togivethefollowinglogic sampling approach (Henrion, 1988), which can be seen as a special case of impor- tance sampling discussed in Section 11.1.4.
At each step, when a sampled value is obtained for a variable zi whose value is observed, the sampled value is compared totheobservedvalue, andiftheyagreethenthesamplevalueisretainedandtheal- gorithmproceedstothenextvariableinturn.
However, ifthesampledvalueandthe observedvaluedisagree, thenthewholesamplesofarisdiscardedandthealgorithm starts again with the first node in the graph.
This algorithm samples correctly from theposteriordistributionbecauseitcorrespondssimplytodrawingsamplesfromthe joint distribution of hidden variables and data variables and then discarding those samplesthatdisagreewiththeobserveddata(withtheslightsavingofnotcontinu- ingwiththesamplingfromthejointdistributionassoonasonecontradictoryvalueis observed).
However, theoverallprobabilityofacceptingasamplefromtheposterior decreases rapidly as the number of observed variables increases and as the number of states that those variables can take increases, and so this approach is rarely used inpractice.
In the case of probability distributions defined by an undirected graph, there is noone-passsamplingstrategythatwillsampleevenfromthepriordistributionwith noobservedvariables.
Instead, computationallymoreexpensivetechniquesmustbe employed, suchas Gibbssampling, whichisdiscussedin Section11.3.
Aswellassamplingfromconditionaldistributions, wemayalsorequiresamples fromamarginaldistribution.
Ifwealreadyhaveastrategyforsamplingfromajoint distribution p(u, v), then it is straightforward to obtain samples from the marginal distributionp(u)simplybyignoringthevaluesforvineachsample.
There are numerous texts dealing with Monte Carlo methods.
Those of partic- ular interest from the statistical inference perspective include Chen et al.
(2001), Gamerman (1997), Gilks et al.
(1996), Liu (2001), Neal (1996), and Robert and Casella(1999).
Alsotherearereviewarticlesby Besagetal.
(1995), Brooks(1998), Diaconisand Saloff-Coste(1998), Jerrumand Sinclair(1996), Neal(1993), Tierney (1994), and Andrieu et al.
(2003) that provide additional information on sampling 526 11.
SAMPLINGMETHODS methodsforstatisticalinference.
Diagnostic tests for convergence of Markov chain Monte Carlo algorithms are summarizedin Robertand Casella(1999), andsomepracticalguidanceontheuseof samplingmethodsinthecontextofmachinelearningisgivenin Bishopand Nabney (2008).
11.1.
Basic Sampling Algorithms In this section, we consider some simple strategies for generating random samples from a given distribution.
Because the samples will be generated by a computer algorithm they will in fact be pseudo-random numbers, that is, they will be deter- ministicallycalculated, butmustneverthelesspassappropriatetestsforrandomness.
Generatingsuchnumbersraisesseveralsubtleties(Pressetal.,1992)thatlieoutside the scope of this book.
Here we shall assume that an algorithm has been provided thatgeneratespseudo-randomnumbersdistributeduniformlyover(0,1), andindeed mostsoftwareenvironmentshavesuchafacilitybuiltin.
11.1.1 Standard distributions Wefirstconsiderhowtogeneraterandomnumbersfromsimplenonuniformdis- tributions, assumingthatwealreadyhaveavailableasourceofuniformlydistributed random numbers.
Suppose that z is uniformly distributed over the interval (0,1), and that we transform the values of z using some function f(·) so that y = f(z).
Thedistributionofy willbegovernedby dz p(y)=p(z) (11.5) dy where, inthiscase, p(z) = 1.
Ourgoalistochoosethefunctionf(z)suchthatthe resultingvaluesofy havesomespecificdesireddistributionp(y).
Integrating(11.5) weobtain y z =h(y)≡ p( y)d y (11.6) −∞ Exercise 11.2 which is the indefinite integral of p(y).
Thus, y = h−1(z), and so we have to transform the uniformly distributed random numbers using a function which is the inverse of the indefinite integral of the desired distribution.
This is illustrated in Figure11.2.
Considerforexampletheexponentialdistribution p(y)=λexp(−λy) (11.7) where0 y < ∞.
Inthiscasethelowerlimitoftheintegralin(11.6)is0, andso h(y) = 1−exp(−λy).
Thus, if we transform our uniformly distributed variablez usingy =−λ−1ln(1−z), theny willhaveanexponentialdistribution.
11.1.
Basic Sampling Algorithms 527 Figure11.2 Geometrical interpretation of the trans- formation method for generating nonuni- 1 formly distributed random numbers.
h(y) istheindefiniteintegralofthedesireddis- h(y) tribution p(y).
If a uniformly distributed random variable z is transformed using y = h−1(z), theny will bedistributedac- cordingtop(y).
p(y) 0 y Another example of a distribution to which the transformation method can be appliedisgivenbythe Cauchydistribution 1 1 p(y)= .
(11.8) π1+y2 In this case, the inverse of the indefinite integral can be expressed in terms of the Exercise 11.3 ‘tan’function.
The generalization to multiple variables is straightforward and involves the Ja- cobianofthechangeofvariables, sothat ∂(z 1 ,..., z M) ∂(y 1 ,..., y M) As a final example of the transformation method we consider the Box-Muller methodforgeneratingsamplesfroma Gaussiandistribution.
First, supposewegen- eratepairsofuniformlydistributedrandomnumbersz 1 , z 2 ∈(−1,1), whichwecan do by transforming a variable distributed uniformly over (0,1) using z → 2z −1.
Next we discard each pair unless it satisfies z2 +z2 1.
This leads to a uniform 1 2 distribution of points inside the unit circle with p(z 1 , z 2 ) = 1/π, as illustrated in Figure11.3.
Then, foreachpairz 1 , z 2 weevaluatethequantities Figure11.3 The Box-Muller method for generating Gaussian dis- 1 tributed random numbers starts by generating samples fromauniformdistributioninsidetheunitcircle.
z2 −1 −1 z1 1 528 11.
SAMPLINGMETHODS −2lnz 1/2 1 y 1 = z 1 r2 (11.10) −2lnz 1/2 2 y 2 = z 2 r2 (11.11) Exercise 11.4 wherer2 =z 1 2+z 2 2.
Thenthejointdistributionofy 1 andy 2 isgivenby ∂(z 1 , z 2 ) p(y 1 , y 2 ) = p(z 1 , z 2 ) ∂(y , y ) 1 2 1 1 = √ exp(−y2/2) √ exp(−y2/2) (11.12) 1 2 2π 2π and so y 1 and y 2 are independent and each has a Gaussian distribution with zero meanandunitvariance.
Ify hasa Gaussiandistributionwithzeromeanandunitvariance, thenσy+µ willhavea Gaussiandistributionwithmeanµandvarianceσ2.
Togeneratevector- valued variables having a multivariate Gaussian distribution with mean µ and co- varianceΣ, wecanmakeuseofthe Choleskydecomposition, whichtakestheform Σ = LLT (Pressetal.,1992).
Then, ifzisavectorvaluedrandomvariablewhose componentsareindependentand Gaussiandistributedwithzeromeanandunitvari- Exercise 11.5 ance, theny =µ+LzwillhavemeanµandcovarianceΣ.
Obviously, the transformation technique depends for its success on the ability tocalculateandtheninverttheindefiniteintegraloftherequireddistribution.
Such operationswillonlybefeasibleforalimitednumberofsimpledistributions, andso we must turn to alternative approaches in search of a more general strategy.
Here weconsidertwotechniquescalledrejectionsamplingandimportancesampling.
Al- thoughmainlylimitedtounivariatedistributionsandthusnotdirectlyapplicableto complexproblemsinmanydimensions, theydoformimportantcomponentsinmore generalstrategies.
11.1.2 Rejection sampling Therejectionsamplingframeworkallowsustosamplefromrelativelycomplex distributions, subjecttocertainconstraints.
Webeginbyconsideringunivariatedis- tributionsanddiscusstheextensiontomultipledimensionssubsequently.
Supposewewishtosamplefromadistributionp(z)thatisnotoneofthesimple, standarddistributionsconsideredsofar, andthatsamplingdirectlyfromp(z)isdif- ficult.
Furthermoresuppose, asisoftenthecase, thatweareeasilyabletoevaluate p(z)foranygivenvalueofz, uptosomenormalizingconstant Z, sothat 1 p(z)= p(z) (11.13) Zp where p(z)canreadilybeevaluated, but Zp isunknown.
In order to apply rejection sampling, we need some simpler distribution q(z), sometimescalledaproposaldistribution, fromwhichwecanreadilydrawsamples.
11.1.
Basic Sampling Algorithms 529 Figure11.4 Intherejectionsamplingmethod, kq(z) samples are drawn from a sim- kq(z0) ple distribution q(z) and rejected if they fall in the grey area be- tween the unnormalized distribu- tionep(z) and the scaled distribu- tionkq(z).
Theresultingsamples p˜(z) are distributed according to p(z), u0 whichisthenormalizedversionof ep(z).
z0 z We next introduce a constant k whose value is chosen such that kq(z) p(z) for all values of z.
The function kq(z) is called the comparison function and is illus- tratedforaunivariatedistributionin Figure11.4.
Eachstepoftherejectionsampler involvesgenerating two random numbers.
First, we generate a numberz 0 from the distributionq(z).
Next, wegenerateanumberu 0 fromtheuniformdistributionover [0, kq(z 0 )].
This pair of random numbers has uniform distribution under the curve ofthefunctionkq(z).
Finally, ifu 0 > p(z 0 )thenthesampleisrejected, otherwise u 0 is retained.
Thus the pair is rejected if it lies in the grey shaded region in Fig- ure11.4.
Theremainingpairsthenhaveuniformdistributionunderthecurveof p(z), Exercise 11.6 andhencethecorrespondingz valuesaredistributedaccordingtop(z), asdesired.
Theoriginalvaluesofzaregeneratedfromthedistributionq(z), andthesesam- ples are then accepted with probability p(z)/kq(z), and so the probability that a samplewillbeacceptedisgivenby p(accept) = { p(z)/kq(z)}q(z)dz 1 = p(z)dz.
(11.14) k Thus the fraction of points that are rejected by this method depends on the ratio of theareaundertheunnormalizeddistribution p(z)totheareaunderthecurvekq(z).
We therefore see that the constant k should be as small as possible subject to the limitationthatkq(z)mustbenowherelessthan p(z).
Asanillustrationoftheuseofrejectionsampling, considerthetaskofsampling fromthegammadistribution baza−1exp(−bz) Gam(z|a, b)= (11.15) Γ(a) which, for a > 1, has a bell-shaped form, as shown in Figure 11.5.
A suitable proposal distribution is therefore the Cauchy (11.8) because this too is bell-shaped andbecausewecanusethetransformationmethod, discussedearlier, tosamplefrom it.
Weneedtogeneralizethe Cauchyslightlytoensurethatitnowherehasasmaller valuethanthegammadistribution.
Thiscanbeachievedbytransformingauniform random variable y using z = btany +c, which gives random numbers distributed Exercise 11.7 accordingto.
530 11.
SAMPLINGMETHODS Figure11.5 Plot showing the gamma distribu- 0.15 tiongivenby(11.15)asthegreen curve, with a scaled Cauchy pro- posaldistributionshownbythered curve.
Samples from the gamma 0.1 distribution can be obtained by sampling from the Cauchy and p(z) then applying the rejection sam- plingcriterion.
0.05 0 0 10 20 30 z k q(z)= .
(11.16) 1+(z−c)2/b2 Theminimumrejectrateisobtainedbysettingc = a−1, b2 = 2a−1andchoos- ing the constant k to be as small as possible while still satisfying the requirement kq(z) p(z).
Theresultingcomparisonfunctionisalsoillustratedin Figure11.5.
11.1.3 Adaptive rejection sampling In many instances where we might wish to apply rejection sampling, it proves difficulttodetermineasuitableanalyticformfortheenvelopedistributionq(z).
An alternative approach is to construct the envelope function on the fly based on mea- sured values of the distribution p(z) (Gilks and Wild, 1992).
Construction of an envelopefunctionisparticularlystraightforwardforcasesinwhichp(z)islogcon- cave, in other words when lnp(z) has derivatives that are nonincreasing functions of z.
The construction of a suitable envelope function is illustrated graphically in Figure11.6.
The function lnp(z) and its gradient are evaluated at some initial set of grid points, and the intersections of the resulting tangent lines are used to construct the envelope function.
Next a sample value is drawn from the envelope distribution.
Exercise 11.9 This is straightforward because the log of the envelope distribution is a succession Figure11.6 Inthecaseofdistributionsthatare lnp(z) log concave, an envelope function foruseinrejectionsamplingcanbe constructedusingthetangentlines computedatasetofgridpoints.
Ifa samplepointisrejected, itisadded tothesetofgridpointsandusedto refinetheenvelopedistribution.
z1 z2 z3 z 11.1.
Basic Sampling Algorithms 531 Figure11.7 Illustrative example of rejection 0.5 sampling involving sampling from a Gaussiandistributionp(z)shownby the green curve, by using rejection p(z) sampling from a proposal distri- bution q(z) that is also Gaussian and whose scaled version kq(z) is 0.25 shownbytheredcurve.
0 −5 0 z 5 oflinearfunctions, andhencetheenvelopedistributionitselfcomprisesapiecewise exponentialdistributionoftheform q(z)=kiλiexp{−λi(z−zi−1 )} zi−1 <z zi.
(11.17) Once a sample has been drawn, the usual rejection criterion can be applied.
If the sampleisaccepted, thenitwillbeadrawfromthedesireddistribution.
If, however, thesampleisrejected, thenitisincorporatedintothesetofgridpoints, anewtangent line is computed, and the envelope function is thereby refined.
As the number of grid points increases, so the envelope function becomes a better approximation of thedesireddistributionp(z)andtheprobabilityofrejectiondecreases.
Avariantofthealgorithmexiststhatavoidstheevaluationofderivatives(Gilks, 1992).
The adaptive rejection sampling framework can also be extended to distri- butions that are not log concave, simply by following each rejection sampling step with a Metropolis-Hastings step (to be discussed in Section 11.2.2), giving rise to adaptiverejection Metropolissampling(Gilksetal.,1995).
Clearlyforrejectionsamplingtobeofpracticalvalue, werequirethatthecom- parison function be close to the required distribution so that the rate of rejection is kepttoaminimum.
Nowletusexaminewhathappenswhenwetrytouserejection sampling in spaces of high dimensionality.
Consider, for the sake of illustration, a somewhat artificial problem in which we wish to sample from a zero-mean mul- tivariate Gaussian distribution with covariance σ2I, where I is the unit matrix, by p rejection sampling from a proposal distribution that is itself a zero-mean Gaussian distributionhavingcovarianceσ2I.
Obviously, wemusthaveσ2 σ2 inorderthat q q p there exists a k such that kq(z) p(z).
In D-dimensions the optimum value of k isgivenbyk = (σq/σp)D, asillustratedfor D = 1in Figure11.7.
Theacceptance ratewillbetheratioofvolumesunderp(z)andkq(z), which, becausebothdistribu- tionsarenormalized, isjust1/k.
Thustheacceptanceratediminishesexponentially withdimensionality.
Evenifσq exceedsσp byjustonepercent, for D = 1,000the acceptance ratio will be approximately 1/20,000.
In this illustrative example the comparison function is close to the required distribution.
For more practical exam- ples, where the desired distribution may be multimodal and sharply peaked, it will beextremelydifficulttofindagoodproposaldistributionandcomparisonfunction.
532 11.
SAMPLINGMETHODS Figure11.8 Importance sampling addresses the prob- lemofevaluatingtheexpectationofafunc- p(z) q(z) f(z) tionf(z)withrespecttoadistributionp(z) fromwhichitisdifficulttodrawsamplesdi- rectly.
Instead, samples {z(l)} are drawn from a simpler distribution q(z), and the correspondingtermsinthesummationare weightedbytheratiosp(z(l))/q(z(l)).
z Furthermore, the exponential decrease of acceptance rate with dimensionality is a genericfeatureofrejectionsampling.
Althoughrejectioncanbeausefultechnique in one or two dimensions it is unsuited to problems of high dimensionality.
It can, however, play a role as a subroutine in more sophisticated algorithms for sampling inhighdimensionalspaces.
11.1.4 Importance sampling Oneoftheprincipalreasonsforwishingtosamplefromcomplicatedprobability distributionsistobeabletoevaluateexpectationsoftheform(11.1).
Thetechnique of importance sampling provides a framework for approximating expectations di- rectlybutdoesnotitselfprovideamechanismfordrawingsamplesfromdistribution p(z).
The finite sum approximation to the expectation, given by (11.2), depends on being able to draw samples from the distributionp(z).
Suppose, however, that it is impracticaltosampledirectlyfromp(z)butthatwecanevaluatep(z)easilyforany given value of z.
One simplistic strategy for evaluating expectations would be to discretize z-space into a uniform grid and to evaluate the integrand as a sum of the form L E[f] p(z(l))f(z(l)).
(11.18) l=1 Anobviousproblemwiththisapproachisthatthenumberoftermsinthesummation growsexponentiallywiththedimensionalityofz.
Furthermore, aswehavealready noted, thekindsofprobabilitydistributionsofinterestwilloftenhavemuchoftheir massconfinedtorelativelysmallregionsofzspaceandsouniformsamplingwillbe veryinefficientbecauseinhigh-dimensionalproblems, onlyaverysmallproportion ofthesampleswillmakeasignificantcontributiontothesum.
Wewouldreallylike to choose the sample points to fall in regions where p(z) is large, or ideally where theproductp(z)f(z)islarge.
As in the case of rejection sampling, importance sampling is based on the use of aproposal distributionq(z)fromwhichitiseasytodrawsamples, asillustrated in Figure11.8.
Wecanthenexpresstheexpectationintheformofafinitesumover 11.1.
Basic Sampling Algorithms 533 samples{z(l)}drawnfromq(z) E[f] = f(z)p(z)dz p(z) = f(z) q(z)dz q(z) 1 L p(z(l)) f(z(l)).
(11.19) L q(z(l)) l=1 Thequantitiesrl = p(z(l))/q(z(l))areknownasimportanceweights, andtheycor- rectthebiasintroducedbysamplingfromthewrongdistribution.
Notethat, unlike rejectionsampling, allofthesamplesgeneratedareretained.
Itwilloftenbethecasethatthedistributionp(z)canonlybeevaluateduptoa normalizationconstant, sothatp(z) = p(z)/Zp where p(z)canbeevaluatedeasily, whereas Zp is unknown.
Similarly, we may wish to use an importance sampling distributionq(z)= q(z)/Zq, whichhasthesameproperty.
Wethenhave E[f] = f(z)p(z)dz Zq p(z) = f(z) q(z)dz Zp q(z) L Zq 1 rlf(z(l)).
(11.20) Zp L l=1 where rl = p(z(l))/ q(z(l)).
We can use the same sample set to evaluate the ratio Zp/Zq withtheresult Zp = 1 p(z)dz= p(z) q(z)dz Zq Zq q(z) L 1 rl (11.21) L l=1 andhence L E[f] wlf(z(l)) (11.22) l=1 wherewehavedefined rl p(z(l))/q(z(l)) wl = m rm = m p(z(m))/q(z(m)) .
(11.23) As with rejection sampling, the success of the importance sampling approach depends crucially on how well the sampling distribution q(z) matches the desired 534 11.
SAMPLINGMETHODS distributionp(z).
If, asisoftenthecase, p(z)f(z)isstronglyvaryingandhasasig- nificantproportionofitsmassconcentratedoverrelativelysmallregionsofzspace, then the set of importance weights {rl } may be dominated by a few weights hav- inglargevalues, withtheremainingweightsbeingrelativelyinsignificant.
Thusthe effectivesamplesizecanbemuchsmallerthantheapparentsamplesize L.
Theprob- lem is even more severe if none of the samples falls in the regions where p(z)f(z) is large.
In that case, the apparent variances of rl and rlf(z(l)) may be small even thoughtheestimateoftheexpectationmaybeseverelywrong.
Henceamajordraw- backof theimportance sampling method is thepotential to produceresults that are arbitrarily in error and with no diagnostic indication.
This also highlights a key re- quirement for the sampling distribution q(z), namely that it should not be small or zeroinregionswherep(z)maybesignificant.
Fordistributionsdefinedintermsofagraphicalmodel, wecanapplytheimpor- tancesamplingtechniqueinvariousways.
Fordiscretevariables, asimpleapproach is called uniform sampling.
The joint distribution for a directed graph is defined by (11.4).
Each sample from the joint distribution is obtained by first setting those variables zi that are in the evidence set equal to their observed values.
Each of the remainingvariablesisthensampledindependentlyfromauniformdistributionover thespaceofpossibleinstantiations.
Todeterminethecorrespondingweightassoci- atedwithasamplez(l), wenotethatthesamplingdistribution q(z)isuniformover the possible choices for z, and that p(z|x) = p(z), where x denotes the subset of variablesthatareobserved, andtheequalityfollowsfromthefactthateverysample z that is generated is necessarily consistent with the evidence.
Thus the weights rl aresimplyproportionaltop(z).
Notethatthevariablescanbesampledinanyorder.
Thisapproachcanyieldpoorresultsiftheposteriordistributionisfarfromuniform, asisoftenthecaseinpractice.
Animprovementonthisapproachiscalledlikelihoodweightedsampling(Fung and Chang, 1990; Shachter and Peot, 1990) and is based on ancestral sampling of thevariables.
Foreachvariableinturn, ifthatvariableisintheevidenceset, thenit isjustsettoitsinstantiatedvalue.
Ifitisnotintheevidenceset, thenitissampled from the conditional distribution p(zi |pa i ) in which the conditioning variables are set to their currently sampled values.
The weighting associated with the resulting samplezisthengivenby r(z)= zi ∈e p p ( ( z z i i | | p p a a i i ) ) zi ∈e p(zi 1 |pa i ) = zi ∈e p(zi |pa i ).
(11.24) Thismethodcanbefurtherextendedusingself-importancesampling(Shachterand Peot,1990)inwhichtheimportancesamplingdistributioniscontinuallyupdatedto reflectthecurrentestimatedposteriordistribution.
11.1.5 Sampling-importance-resampling The rejection sampling method discussed in Section 11.1.2 depends in part for its success on the determination of a suitable value for the constant k.
For many pairs of distributions p(z) and q(z), it will be impractical to determine a suitable 11.1.
Basic Sampling Algorithms 535 value for k in that any value that is sufficiently large to guarantee a bound on the desireddistributionwillleadtoimpracticallysmallacceptancerates.
Asinthecaseofrejectionsampling, thesampling-importance-resampling(SIR) approach also makes use of a sampling distribution q(z) but avoids having to de- termine the constant k.
There are two stages to the scheme.
In the first stage, L samples z(1),..., z(L) are drawn from q(z).
Then in the second stage, weights drawnfromthediscretedistribution(z(1),..., z(L))withprobabilitiesgivenbythe weights(w 1 ,..., w L).
The resulting L samples are only approximately distributed according to p(z), but the distribution becomes correct in the limit L → ∞.
To see this, consider the univariatecase, andnotethatthecumulativedistributionoftheresampledvaluesis givenby p(z a) = wl l: z(l) a I(z(l) a) p(z(l))/q(z(l)) = l (11.25) p(z(l))/q(z(l)) l where I(.) is the indicator function (which equals 1 if its argument is true and 0 otherwise).
Taking the limit L → ∞, and assuming suitable regularity of the dis- tributions, we can replace the sums by integrals weighted according to the original samplingdistributionq(z) I(z a){ p(z)/q(z)}q(z)dz p(z a) = { p(z)/q(z)}q(z)dz I(z a) p(z)dz = p(z)dz = I(z a)p(z)dz (11.26) whichisthecumulativedistributionfunctionofp(z).
Again, weseethatthenormal- izationofp(z)isnotrequired.
Forafinitevalueof L, andagiveninitialsampleset, theresampledvalueswill only approximately be drawn from the desired distribution.
As with rejection sam- pling, the approximation improves as the sampling distribution q(z) gets closer to thedesireddistributionp(z).
Whenq(z)=p(z), theinitialsamples(z(1),..., z(L)) havethedesireddistribution, andtheweightswn =1/Lsothattheresampledvalues alsohavethedesireddistribution.
If moments with respect to the distribution p(z) are required, then they can be 536 11.
SAMPLINGMETHODS evaluateddirectlyusingtheoriginalsamplestogetherwiththeweights, because E[f(z)] = f(z)p(z)dz f(z)[ p(z)/q(z)]q(z)dz = [ p(z)/q(z)]q(z)dz L wlf(zl).
(11.27) l=1 11.1.6 Sampling and the EM algorithm Inadditiontoprovidingamechanismfordirectimplementationofthe Bayesian framework, Monte Carlo methods can also play a role in the frequentist paradigm, forexampletofindmaximumlikelihoodsolutions.
Inparticular, samplingmethods canbeusedtoapproximatethe Estepofthe EMalgorithmformodelsinwhichthe E step cannot be performed analytically.
Consider a model with hidden variables Z, visible(observed)variables X, andparametersθ.
Thefunctionthatisoptimized with respect to θ in the M step is the expected complete-data log likelihood, given by Q(θ,θold )= p(Z|X,θold )lnp(Z, X|θ)d Z.
(11.28) Wecanusesamplingmethodstoapproximatethisintegralbyafinitesumoversam- ples{Z(l)}, whicharedrawnfromthecurrentestimatefortheposteriordistribution p(Z|X,θold ), sothat L 1 Q(θ,θold ) lnp(Z(l), X|θ).
(11.29) L l=1 The Qfunctionisthenoptimizedintheusualwayinthe Mstep.
Thisprocedureis calledthe Monte Carlo EMalgorithm.
It is straightforward to extend this to the problem of finding the mode of the posteriordistributionoverθ (the MAPestimate)whenapriordistributionp(θ)has beendefined, simplybyaddinglnp(θ)tothefunction Q(θ,θold )beforeperforming the Mstep.
A particular instance of the Monte Carlo EM algorithm, called stochastic EM, arisesifweconsiderafinitemixturemodel, anddrawjustonesampleateach Estep.
Herethelatentvariable Zcharacterizeswhichofthe K componentsofthemixture is responsible for generating each data point.
In the E step, a sample of Z is taken fromtheposteriordistributionp(Z|X,θold )where Xisthedataset.
Thiseffectively makesahardassignmentofeachdatapointtooneofthecomponentsinthemixture.
In the M step, this sampled approximation to the posterior distribution is used to updatethemodelparametersintheusualway.
11.2.
Markov Chain Monte Carlo 537 Nowsupposewemovefromamaximumlikelihoodapproachtoafull Bayesian treatmentinwhichwewishtosamplefromtheposteriordistributionovertheparam- eter vector θ.
In principle, we would like to draw samples from the joint posterior p(θ, Z|X), butweshallsupposethatthisiscomputationallydifficult.
Supposefur- therthatitisrelativelystraightforwardtosamplefromthecomplete-dataparameter posterior p(θ|Z, X).
This inspires the data augmentation algorithm, which alter- nates between two steps known as the I-step (imputation step, analogous to an E step)andthe P-step(posteriorstep, analogoustoan Mstep).
IPAlgorithm I-step.
We wish to sample from p(Z|X) but we cannot do this directly.
We thereforenotetherelation p(Z|X)= p(Z|θ, X)p(θ|X)dθ (11.30) andhenceforl =1,..., Lwefirstdrawasampleθ(l) fromthecurrentesti- mateforp(θ|X), andthenusethistodrawasample Z(l)fromp(Z|θ(l) , X).
P-step.
Giventherelation p(θ|X)= p(θ|Z, X)p(Z|X)d Z (11.31) we use the samples {Z(l)} obtained from the I-step to compute a revised estimateoftheposteriordistributionoverθgivenby L 1 p(θ|X) p(θ|Z(l), X).
(11.32) L l=1 Byassumption, itwillbefeasibletosamplefromthisapproximationinthe I-step.
Note that we are making a (somewhat artificial) distinction between parameters θ andhiddenvariables Z.
Fromnowon, weblurthisdistinctionandfocussimplyon theproblemofdrawingsamplesfromagivenposteriordistribution.
11.2.
Markov Chain Monte Carlo In the previous section, we discussed the rejection sampling and importance sam- plingstrategiesforevaluatingexpectationsoffunctions, andwesawthattheysuffer from severe limitations particularly in spaces of high dimensionality.
We therefore turn in this section to a very general and powerful framework called Markov chain Monte Carlo (MCMC), which allows sampling from a large class of distributions, 538 11.
SAMPLINGMETHODS and which scales well with the dimensionality of the sample space.
Markov chain Monte Carlo methods have their origins in physics (Metropolis and Ulam, 1949), and it was only towards the end of the 1980s that they started to have a significant impactinthefieldofstatistics.
As with rejection and importance sampling, we again sample from a proposal distribution.
Thistime, however, wemaintainarecordofthecurrentstatez(τ), and theproposaldistributionq(z|z(τ))dependsonthiscurrentstate, andsothesequence wewillassumethat p(z)canreadilybeevaluatedforanygivenvalueofz, although the value of Zp may be unknown.
The proposal distribution itself is chosen to be sufficiently simple that it is straightforward to draw samples from it directly.
At each cycle of the algorithm, we generate a candidate sample z from the proposal distributionandthenacceptthesampleaccordingtoanappropriatecriterion.
Inthebasic Metropolisalgorithm(Metropolisetal.,1953), weassumethatthe proposal distribution is symmetric, that is q(z A |z B) = q(z B |z A) for all values of z A andz B.
Thecandidatesampleisthenacceptedwithprobability p(z ) A(z , z(τ))=min 1, .
(11.33) p(z(τ)) Thiscanbeachievedbychoosingarandomnumberuwithuniformdistributionover the unit interval (0,1) and then accepting the sample if A(z , z(τ)) > u.
Note that ifthestepfromz(τ) toz causesanincreaseinthevalueofp(z), thenthecandidate pointiscertaintobekept.
If the candidate sample is accepted, then z(τ+1) = z , otherwise the candidate point z is discarded, z(τ+1) is set to z(τ) and another candidate sample is drawn fromthedistributionq(z|z(τ+1)).
Thisisincontrasttorejectionsampling, wherere- jectedsamplesaresimplydiscarded.
Inthe Metropolisalgorithmwhenacandidate pointisrejected, theprevioussampleisincludedinsteadinthefinallistofsamples, leading to multiple copies of samples.
Of course, in a practical implementation, only a single copy of each retained sample would be kept, along with an integer weighting factor recording how many times that state appears.
As we shall see, as long as q(z A |z B) is positive for any values of z A and z B (this is a sufficient but notnecessarycondition), thedistributionofz(τ) tendstop(z)asτ → ∞.
Itshould beemphasized, however, thatthesequencez(1), z(2),...
isnotasetofindependent samplesfromp(z)becausesuccessivesamplesarehighlycorrelated.
Ifwewishto obtain independent samples, then we can discard most of the sequence and just re- tain every Mth sample.
For M sufficiently large, the retained samples will for all practical purposes be independent.
Figure 11.9 shows a simple illustrative exam- pleofsamplingfromatwo-dimensional Gaussiandistributionusingthe Metropolis algorithminwhichtheproposaldistributionisanisotropic Gaussian.
Furtherinsightintothenatureof Markovchain Monte Carloalgorithmscanbe gleanedbylookingatthepropertiesofaspecificexample, namelyasimplerandom 11.2.
Markov Chain Monte Carlo 539 Figure11.9 A simple illustration using Metropo- 3 lis algorithm to sample from a Gaussian distribution whose one standard-deviationcontourisshown 2.5 bytheellipse.
Theproposaldistribu- tion is an isotropic Gaussian distri- bution whose standard deviation is 2 0.2.
Steps that are accepted are shown as green lines, and rejected steps are shown in red.
A total of 1.5 150 candidate samples are gener- ated, ofwhich43arerejected.
1 0.5 0 0 0.5 1 1.5 2 2.5 3 walk.
Considerastatespacez consistingoftheintegers, withprobabilities p(z(τ+1) =z(τ)) = 0.5 (11.34) p(z(τ+1) =z(τ)+1) = 0.25 (11.35) p(z(τ+1) =z(τ)−1) = 0.25 (11.36) where z(τ) denotes the state at step τ.
If the initial state is z(1) = 0, then by sym- metry the expected state at time τ will also be zero E[z(τ)] = 0, and similarly it is Exercise 11.10 easilyseenthat E[(z(τ))2]=τ/2.
Thusafterτ steps, therandomwalkhasonlytrav- elled a distance that on average is proportional to the square root ofτ.
This square rootdependenceistypicalofrandomwalkbehaviourandshowsthatrandomwalks are very inefficient in exploring the state space.
As we shall see, a central goal in designing Markovchain Monte Carlomethodsistoavoidrandomwalkbehaviour.
11.2.1 Markov chains Beforediscussing Markovchain Monte Carlomethodsinmoredetail, itisuse- fultostudysomegeneralpropertiesof Markovchainsinmoredetail.
Inparticular, we ask under what circumstances will a Markov chain converge to the desired dis- tribution.
A first-order Markov chain is defined to be a series of random variables z(1),..., z(M) such that the following conditional independence property holds for m∈{1,..., M −1} p(z(m+1)|z(1),..., z(m))=p(z(m+1)|z(m)).
(11.37) Thisofcoursecanberepresentedasadirectedgraphintheformofachain, anex- ampleofwhichisshownin Figure8.38.
Wecanthenspecifythe Markovchainby giving the probability distribution for the initial variable p(z(0)) together with the 540 11.
SAMPLINGMETHODS conditionalprobabilitiesforsubsequentvariablesintheformoftransitionprobabil- ities Tm(z(m), z(m+1)) ≡ p(z(m+1)|z(m)).
AMarkovchainiscalledhomogeneous ifthetransitionprobabilitiesarethesameforallm.
The marginal probability for a particular variable can be expressed in terms of themarginalprobabilityforthepreviousvariableinthechainintheform p(z(m+1))= p(z(m+1)|z(m))p(z(m)).
(11.38) z(m) A distribution is said to be invariant, or stationary, with respect to a Markov chain ifeachstepinthechainleavesthatdistributioninvariant.
Thus, forahomogeneous Markovchainwithtransitionprobabilities T(z , z), thedistributionp (z)isinvariant if p (z)= T(z, z)p (z).
(11.39) z Notethatagiven Markovchainmayhavemorethanoneinvariantdistribution.
For instance, ifthetransitionprobabilitiesaregivenbytheidentitytransformation, then anydistributionwillbeinvariant.
Asufficient(butnotnecessary)conditionforensuringthattherequireddistribu- tion p(z) is invariant is to choose the transition probabilities to satisfy the property ofdetailedbalance, definedby p (z)T(z, z)=p (z)T(z, z) (11.40) for the particular distribution p (z).
It is easily seen that a transition probability thatsatisfiesdetailedbalancewithrespecttoaparticulardistributionwillleavethat distributioninvariant, because p (z )T(z , z)= p (z)T(z, z )=p (z) p(z |z)=p (z).
(11.41) z z z AMarkovchainthatrespectsdetailedbalanceissaidtobereversible.
Our goal is to use Markov chains to sample from a given distribution.
We can achievethisifwesetupa Markovchainsuchthatthedesireddistributionisinvariant.
However, wemustalsorequirethatform → ∞, thedistributionp(z(m))converges to the required invariant distribution p (z), irrespective of the choice of initial dis- tribution p(z(0)).
This property is called ergodicity, and the invariant distribution is then called the equilibrium distribution.
Clearly, an ergodic Markov chain can haveonlyoneequilibriumdistribution.
Itcanbeshownthatahomogeneous Markov chainwillbeergodic, subjectonlytoweakrestrictionsontheinvariantdistribution andthetransitionprobabilities(Neal,1993).
In practice we often construct the transition probabilities from a set of ‘base’ transitions B 1 ,..., BK.
Thiscanbeachievedthroughamixturedistributionofthe form K T(z, z)= αk Bk(z, z) (11.42) k=1 11.2.
Markov Chain Monte Carlo 541 forsomesetofmixingcoefficientsα 1 ,...,αK satisfyingαk 0and k αk = 1.
Alternatively, thebasetransitionsmaybecombinedthroughsuccessiveapplication, sothat z 1 zn−1 If a distribution is invariant with respect to each of the base transitions, then obvi- ouslyitwillalsobeinvariantwithrespecttoeitherofthe T(z , z)givenby(11.42) or (11.43).
For the case of the mixture (11.42), if each of the base transitions sat- isfies detailed balance, then the mixture transition T will also satisfy detailed bal- ance.
Thisdoesnotholdforthetransitionprobabilityconstructedusing(11.43), al- thoughbysymmetrizingtheorderofapplicationofthebasetransitions, intheform ample of the use of composite transition probabilities is where each base transition changesonlyasubsetofthevariables.
11.2.2 The Metropolis-Hastings algorithm Earlier we introduced thebasic Metropolis algorithm, without actually demon- strating that it samples from the required distribution.
Before giving a proof, we first discuss a generalization, known as the Metropolis-Hastings algorithm (Hast- ings, 1970), to the case where the proposal distribution is no longer a symmetric functionofitsarguments.
Inparticularatstepτ ofthealgorithm, inwhichthecur- rent state is z(τ), we draw a sample z from the distribution qk(z|z(τ)) and then acceptitwithprobability Ak(z , zτ)where Ak(z , z(τ))=min 1, p p ( ( z z (τ ) ) ) q q k k ( ( z z (τ ) |z |z (τ ) ) ) .
(11.44) Hereklabelsthemembersofthesetofpossibletransitionsbeingconsidered.
Again, theevaluationoftheacceptancecriteriondoesnotrequireknowledgeofthenormal- izing constant Zp in the probability distribution p(z) = p(z)/Zp.
For a symmetric proposal distribution the Metropolis-Hastings criterion (11.44) reduces to the stan- dard Metropoliscriteriongivenby(11.33).
We can show that p(z) is an invariant distribution of the Markov chain defined by the Metropolis-Hastings algorithm by showing that detailed balance, defined by (11.40), issatisfied.
Using(11.44)wehave p(z)qk(z|z )Ak(z , z) = min(p(z)qk(z|z ), p(z )qk(z |z)) = min(p(z )qk(z |z), p(z)qk(z|z )) = p(z )qk(z |z)Ak(z, z ) (11.45) asrequired.
The specific choice of proposal distribution can have a marked effect on the performance of the algorithm.
For continuous state spaces, a common choice is a Gaussiancentredonthecurrentstate, leadingtoanimportanttrade-offindetermin- ing the variance parameter of this distribution.
If the variance is small, then the 542 11.
SAMPLINGMETHODS Figure11.10 Schematic illustration of the use of an isotropic Gaussian proposal distribution (blue circle) to sample from a correlated multivariate Gaussian distribution(redellipse)havingverydifferentstan- σmax dard deviations in different directions, using the Metropolis-Hastings algorithm.
In order to keep therejectionratelow, thescaleρoftheproposal ρ distributionshouldbeontheorderofthesmallest standard deviation σ min, which leads to random walkbehaviourinwhichthenumberofstepssep- σmin aratingstatesthatareapproximatelyindependent isoforder(σ max /σ min )2whereσ maxisthelargest standarddeviation.
proportionofacceptedtransitionswillbehigh, butprogressthroughthestatespace takes the form of a slow random walk leading to long correlation times.
However, ifthevarianceparameterislarge, thentherejectionratewillbehighbecause, inthe kind of complex problems we are considering, many of the proposed steps will be to states for which the probability p(z) is low.
Consider a multivariate distribution p(z) having strong correlations between the components of z, as illustrated in Fig- ure 11.10.
The scale ρ of the proposal distribution should be as large as possible without incurring high rejection rates.
This suggests that ρ should be of the same order as the smallest length scale σ min.
The system then explores the distribution along the more extended direction by means of a random walk, and so the number of steps to arrive at a state that is more or less independent of the original state is oforder(σ max /σ min )2.
Infactintwodimensions, theincreaseinrejectionrateasρ increasesisoffsetbythelargerstepssizesofthosetransitionsthatareaccepted, and more generally for a multivariate Gaussian the number of steps required to obtain independent samples scales like (σ max /σ 2 )2 where σ 2 is the second-smallest stan- darddeviation(Neal,1993).
Thesedetailsaside, itremainsthecasethatifthelength scalesoverwhichthedistributionsvaryareverydifferentindifferentdirections, then the Metropolis Hastingsalgorithmcanhaveveryslowconvergence.
11.3.
Gibbs Sampling Gibbssampling(Gemanand Geman,1984)isasimpleandwidelyapplicable Markov chain Monte Carlo algorithm and can be seen as a special case of the Metropolis- Hastingsalgorithm.
Considerthedistributionp(z)=p(z 1 ,..., z M)fromwhichwewishtosample, andsupposethatwehavechosensomeinitialstateforthe Markovchain.
Eachstep ofthe Gibbssamplingprocedureinvolvesreplacingthevalueofoneofthevariables byavaluedrawnfromthedistributionofthatvariableconditionedonthevaluesof theremainingvariables.
Thuswereplacezi byavaluedrawnfromthedistribution p(zi |z \i ), wherezi denotestheith componentofz, andz \i denotesz 1 ,..., z M but with zi omitted.
This procedure is repeated either by cycling through the variables 11.3.
Gibbs Sampling 543 in some particular order or by choosing the variable to be updated at each step at randomfromsomedistribution.
For example, suppose we have a distribution p(z 1 , z 2 , z 3 ) over three variables, (τ) (τ) (τ) and at step τ of the algorithm we have selected values z , z and z .
We first 1 2 3 (τ) (τ+1) replacez byanewvaluez obtainedbysamplingfromtheconditionaldistri- 1 1 bution p(z 1 |z 2 (τ) , z 3 (τ) ).
(11.46) (τ) (τ+1) Next we replace z by a value z obtained by sampling from the conditional 2 2 distribution p(z 2 |z 1 (τ+1) , z 3 (τ) ) (11.47) sothatthenewvalueforz 1isusedstraightawayinsubsequentsamplingsteps.
Then (τ+1) weupdatez 3 withasamplez 3 drawnfrom p(z 3 |z 1 (τ+1) , z 2 (τ+1) ) (11.48) andsoon, cyclingthroughthethreevariablesinturn.
Gibbs Sampling 1.
Initialize{zi : i=1,..., M} 2.
Forτ =1,..., T: – Samplez 1 (τ+1) ∼p(z 1 |z 2 (τ) , z 3 (τ) ,..., z M (τ) ).
– Samplez 2 (τ+1) ∼p(z 2 |z 1 (τ+1) , z 3 (τ) ,..., z M (τ) ).
.
.
.
– Samplez j (τ+1) ∼p(zj |z 1 (τ+1) ,..., z j (τ − + 1 1) , z j (τ + ) 1 ,..., z M (τ) ).
.
.
.
– Samplez M (τ+1) ∼p(z M |z 1 (τ+1) , z 2 (τ+1) ,..., z M (τ+ − 1 1 ) ).
Josiah Willard Gibbs Statesat Yale, apostforwhichhereceivednosalary 1839–1903 because at the time he had no publications.
He de- veloped the field of vector analysis and made contri- Gibbsspentalmosthisentirelifeliv- butions to crystallography and planetary orbits.
His ing in a house built by his father in mostfamouswork, entitled Onthe Equilibriumof Het- New Haven, Connecticut.
In 1863, erogeneous Substances, laid the foundations for the Gibbs was granted the first Ph D in scienceofphysicalchemistry.
engineering in the United States, and in 1871 he was appointed to the first chair of mathematical physics in the United 544 11.
SAMPLINGMETHODS To show that this procedure samples from the required distribution, we first of allnotethatthedistributionp(z)isaninvariantofeachofthe Gibbssamplingsteps individually and hence of the whole Markov chain.
This follows from the fact that whenwesamplefromp(zi |{z \i ), themarginaldistributionp(z \i )isclearlyinvariant becausethevalueofz \iisunchanged.
Also, eachstepbydefinitionsamplesfromthe correct conditional distribution p(zi |z \i ).
Because these conditional and marginal distributionstogetherspecifythejointdistribution, weseethatthejointdistribution isitselfinvariant.
The second requirement to be satisfied in order that the Gibbs sampling proce- duresamplesfromthecorrectdistributionisthatitbeergodic.
Asufficientcondition forergodicityisthatnoneoftheconditionaldistributionsbeanywherezero.
Ifthis isthecase, thenanypointinz spacecanbereachedfromanyotherpointinafinite number of steps involving one update of each of the component variables.
If this requirementisnotsatisfied, sothatsomeoftheconditionaldistributionshavezeros, thenergodicity, ifitapplies, mustbeprovenexplicitly.
Thedistributionofinitialstatesmustalsobespecifiedinordertocompletethe algorithm, although samples drawn after many iterations will effectively become independent of this distribution.
Of course, successive samples from the Markov chainwillbehighlycorrelated, andsotoobtainsamplesthatarenearlyindependent itwillbenecessarytosubsamplethesequence.
We can obtain the Gibbs sampling procedure as a particular instance of the Metropolis-Hastingsalgorithmasfollows.
Considera Metropolis-Hastingssampling stepinvolvingthevariablezkinwhichtheremainingvariablesz \kremainfixed, and forwhichthetransitionprobabilityfromztoz isgivenbyqk(z |z) = p(z k |z \k ).
We note that z \k = z \k because these components are unchanged by the sampling step.
Also, p(z) = p(zk |z \k )p(z \k ).
Thusthefactorthatdeterminestheacceptance probabilityinthe Metropolis-Hastings(11.44)isgivenby p(z )qk(z|z ) p(z k |z \k )p(z \k )p(zk |z \k ) A(z , z)= = =1 (11.49) p(z)qk(z |z) p(zk |z \k )p(z \k )p(z k |z \k ) where we have used z \k = z \k.
Thus the Metropolis-Hastings steps are always accepted.
Aswiththe Metropolisalgorithm, wecangainsomeinsightintothebehaviourof Gibbssamplingbyinvestigatingitsapplicationtoa Gaussiandistribution.
Consider a correlated Gaussian in two variables, as illustrated in Figure 11.11, having con- ditional distributions of width l and marginal distributions of width L.
The typical stepsizeisgovernedbytheconditionaldistributionsandwillbeoforderl.
Because thestateevolvesaccordingtoarandomwalk, thenumberofstepsneededtoobtain independentsamplesfromthedistributionwillbeoforder(L/l)2.
Ofcourseifthe Gaussian distribution were uncorrelated, then the Gibbs sampling procedure would beoptimallyefficient.
Forthissimpleproblem, wecouldrotatethecoordinatesys- tem in order to decorrelate the variables.
However, in practical applications it will generallybeinfeasibletofindsuchtransformations.
One approach to reducing random walk behaviour in Gibbs sampling is called over-relaxation(Adler,1981).
Initsoriginalform, thisappliestoproblemsforwhich 11.3.
Gibbs Sampling 545 Figure11.11 Illustration of Gibbs sampling by alter- z2 nate updates of two variables whose L distribution is a correlated Gaussian.
The step size is governed by the stan- dard deviation of the conditional distri- bution(greencurve), andis O(l), lead- ing to slow progress in the direction of elongation of the joint distribution (red ellipse).
The number of steps needed to obtain an independent sample from thedistributionis O((L/l)2).
l z1 theconditionaldistributionsare Gaussian, whichrepresentsamoregeneralclassof distributionsthanthemultivariate Gaussianbecause, forexample, thenon-Gaussian distribution p(z, y) ∝ exp(−z2y2) has Gaussian conditional distributions.
At each step of the Gibbs sampling algorithm, the conditional distribution for a particular componentzihassomemeanµiandsomevarianceσ i 2.
Intheover-relaxationframe- work, thevalueofzi isreplacedwith z i =µi+α(zi −µi)+σi(1−α i 2)1/2ν (11.50) where ν is a Gaussian random variable with zero mean and unit variance, and α is a parameter such that −1 < α < 1.
For α = 0, the method is equivalent to standard Gibbssampling, andforα<0thestepisbiasedtotheoppositesideofthe mean.
This step leaves the desired distribution invariant because if zi has mean µi and variance σ2, then so too does z .
The effect of over-relaxation is to encourage i i directed motion through state space when the variables are highly correlated.
The frameworkoforderedover-relaxation(Neal,1999)generalizesthisapproachtonon- Gaussiandistributions.
The practical applicability of Gibbs sampling depends on the ease with which samples can be drawn from the conditional distributions p(zk |z \k ).
In the case of probability distributions specified using graphical models, the conditional distribu- tionsforindividualnodesdependonlyonthevariablesinthecorresponding Markov blankets, asillustratedin Figure11.12.
Fordirectedgraphs, awidechoiceofcondi- tionaldistributionsfortheindividualnodesconditionedontheirparentswillleadto conditional distributions for Gibbs sampling that are log concave.
The adaptive re- jectionsamplingmethodsdiscussedin Section11.1.3thereforeprovideaframework for Monte Carlosamplingfromdirectedgraphswithbroadapplicability.
If the graph is constructed using distributions from the exponential family, and if the parent-child relationships preserve conjugacy, then the full conditional distri- butions arising in Gibbs sampling will have the same functional form as the orig- 546 11.
SAMPLINGMETHODS Figure11.12 The Gibbs sampling method requires samples tobedrawnfromtheconditionaldistributionofavariablecondi- tioned on the remaining variables.
For graphical models, this conditional distribution is a function only of the states of the nodesinthe Markovblanket.
Foranundirectedgraphthiscom- prises the set of neighbours, as shown on the left, while for a directed graph the Markov blanket comprises the parents, the children, andtheco-parents, asshownontheright.
inal conditional distributions (conditioned on the parents) defining each node, and so standard sampling techniques can be employed.
In general, the full conditional distributionswillbeofacomplexformthatdoesnotpermittheuseofstandardsam- plingalgorithms.
However, iftheseconditionalsarelogconcave, thensamplingcan be done efficiently using adaptive rejection sampling (assuming the corresponding variableisascalar).
If, at each stage of the Gibbs sampling algorithm, instead of drawing a sample from the corresponding conditional distribution, we make a point estimate of the variable given by the maximum of the conditional distribution, then we obtain the iterated conditional modes (ICM) algorithm discussed in Section 8.3.3.
Thus ICM canbeseenasagreedyapproximationto Gibbssampling.
Because the basic Gibbs sampling technique considers one variable at a time, therearestrongdependenciesbetweensuccessivesamples.
Attheoppositeextreme, if we could draw samples directly from the joint distribution (an operation that we aresupposingisintractable), thensuccessivesampleswouldbeindependent.
Wecan hopetoimproveonthesimple Gibbssamplerbyadoptinganintermediatestrategyin whichwesamplesuccessivelyfromgroupsofvariablesratherthanindividualvari- ables.
Thisisachievedintheblocking Gibbssamplingalgorithmbychoosingblocks ofvariables, notnecessarilydisjoint, andthensamplingjointlyfromthevariablesin eachblockinturn, conditionedontheremainingvariables(Jensenetal.,1995).
11.4.
Slice Sampling Wehaveseenthatoneofthedifficultieswiththe Metropolisalgorithmisthesensi- tivitytostepsize.
Ifthisistoosmall, theresultisslowdecorrelationduetorandom walkbehaviour, whereasifitistoolargetheresultisinefficiencyduetoahighrejec- tionrate.
Thetechniqueofslicesampling(Neal,2003)providesanadaptivestepsize thatisautomaticallyadjustedtomatchthecharacteristicsofthedistribution.
Again itrequiresthatweareabletoevaluatetheunnormalizeddistribution p(z).
Consider first the univariate case.
Slice sampling involves augmenting z with an additional variable u and then drawing samples from the joint (z, u) space.
We shallseeanotherexampleofthisapproachwhenwediscusshybrid Monte Carloin Section 11.5.
The goal is to sample uniformly from the area under the distribution 11.4.
Slice Sampling 547 p˜(z) p˜(z) u zmin u zmax z(τ) z z(τ) z (a) (b) Figure 11.13 Illustration of slice sampling.
(a) For a given value z(τ), a value of u is chosen uniformly in the region 0 u ep(z(τ)), which then defines a ‘slice’ through the distribution, shown by the solid horizontal lines.
(b) Because it is infeasible to sample directly from a slice, a new sample of z is drawn from a region z min z z max, whichcontainsthepreviousvaluez(τ).
givenby p(z, u)= 1/Zp if0 u p(z) (11.51) 0 otherwise where Zp = p(z)dz.
Themarginaldistributionoverz isgivenby ep(z) 1 p(z) p(z, u)du= du= =p(z) (11.52) 0 Zp Zp and so we can sample from p(z) by sampling from p(z, u) and then ignoring the u values.
Thiscanbeachievedbyalternatelysamplingz andu.
Giventhevalueofz weevaluate p(z)andthensampleuuniformlyintherange0 u p(z), whichis straightforward.
Thenwefixuandsamplez uniformlyfromthe‘slice’throughthe distributiondefinedby{z : p(z)>u}.
Thisisillustratedin Figure11.13(a).
Inpractice, itcanbedifficulttosampledirectlyfromaslicethroughthedistribu- tionandsoinsteadwedefineasamplingschemethatleavestheuniformdistribution under p(z, u) invariant, which can be achieved by ensuring that detailed balance is satisfied.
Suppose the current value of z is denoted z(τ) and that we have obtained a corresponding sample u.
The next value of z is obtained by considering a region z min z z max thatcontainsz(τ).
Itisinthechoiceofthisregionthattheadap- tationtothecharacteristiclengthscalesofthedistributiontakesplace.
Wewantthe regiontoencompassasmuchofthesliceaspossiblesoastoallowlargemovesinz spacewhilehavingaslittleaspossibleofthisregionlyingoutsidetheslice, because thismakesthesamplinglessefficient.
Oneapproachtothechoiceofregioninvolvesstartingwitharegioncontaining z(τ) having some width w and then testing each of the end points to see if they lie within the slice.
If either end point does not, then the region is extended in that direction by increments of value w until the end point lies outside the region.
A candidatevaluez isthenchosenuniformlyfromthisregion, andifitlieswithinthe slice, thenitformsz(τ+1).
Ifitliesoutsidetheslice, thentheregionisshrunksuch thatz formsanendpointandsuchthattheregionstillcontainsz(τ).
Thenanother 548 11.
SAMPLINGMETHODS candidatepointisdrawnuniformlyfromthisreducedregionandsoon, untilavalue ofz isfoundthatlieswithintheslice.
Slice sampling can be applied to multivariate distributions by repeatedly sam- pling each variable in turn, in the manner of Gibbs sampling.
This requires that we are able to compute, for each component zi, a function that is proportional to p(zi |z \i ).
11.5.
The Hybrid Monte Carlo Algorithm Aswehavealreadynoted, oneofthemajorlimitationsofthe Metropolisalgorithm isthatitcanexhibitrandomwalkbehaviourwherebythedistancetraversedthrough the state space grows only as the square root of the number of steps.
The problem cannotberesolvedsimplybytakingbiggerstepsasthisleadstoahighrejectionrate.
Inthissection, weintroduceamoresophisticatedclassoftransitionsbasedonan analogywithphysicalsystemsandthathasthepropertyofbeingabletomakelarge changes to the system state while keeping the rejection probability small.
It is ap- plicabletodistributionsovercontinuousvariablesforwhichwecanreadilyevaluate thegradientofthelogprobabilitywithrespecttothestatevariables.
Wewilldiscuss the dynamical systems framework in Section 11.5.1, and then in Section 11.5.2 we explainhowthismaybecombinedwiththe Metropolisalgorithmtoyieldthepow- erfulhybrid Monte Carloalgorithm.
Abackgroundinphysicsisnotrequiredasthis sectionisself-containedandthekeyresultsareallderivedfromfirstprinciples.
11.5.1 Dynamical systems Thedynamicalapproachtostochasticsamplinghasitsoriginsinalgorithmsfor simulating the behaviour of physical systems evolving under Hamiltonian dynam- ics.
In a Markov chain Monte Carlo simulation, the goal is to sample from a given probabilitydistributionp(z).
Theframeworkof Hamiltoniandynamicsisexploited bycastingtheprobabilisticsimulationintheformofa Hamiltoniansystem.
Inorder to remain in keeping with the literature in this area, we make use of the relevant dynamical systems terminology where appropriate, which will be defined as we go along.
Thedynamicsthatweconsidercorrespondstotheevolutionofthestatevariable z = {zi } under continuous time, which we denote byτ.
Classical dynamics is de- scribedby Newton’ssecondlawofmotioninwhichtheaccelerationofanobjectis proportionaltotheappliedforce, correspondingtoasecond-orderdifferentialequa- tion over time.
We can decompose a second-order equation into two coupled first- order equations by introducing intermediate momentum variables r, corresponding totherateofchangeofthestatevariablesz, havingcomponents dzi ri = (11.53) dτ wherethezicanberegardedaspositionvariablesinthisdynamicsperspective.
Thus 11.5.
The Hybrid Monte Carlo Algorithm 549 foreachpositionvariablethereisacorrespondingmomentumvariable, andthejoint spaceofpositionandmomentumvariablesiscalledphasespace.
Withoutlossofgenerality, wecanwritetheprobabilitydistributionp(z)inthe form 1 p(z)= exp(−E(z)) (11.54) Zp where E(z)isinterpretedasthepotentialenergyofthesystemwheninstatez.
The system acceleration is the rate of change of momentum and is given by the applied force, whichitselfisthenegativegradientofthepotentialenergy dri =− ∂E(z) .
(11.55) dτ ∂zi It is convenient to reformulate this dynamical system using the Hamiltonian framework.
Todothis, wefirstdefinethekineticenergyby 1 1 K(r)= r 2 = r2.
(11.56) i 2 2 i Thetotalenergyofthesystemisthenthesumofitspotentialandkineticenergies H(z, r)=E(z)+K(r) (11.57) we can now express the dynamics of the system in terms of the Hamiltonian equa- Exercise 11.15 tionsgivenby dzi ∂H = (11.58) dτ ∂ri dri = − ∂H .
(11.59) dτ ∂zi William Hamilton His other great achievement was the development of 1805–1865 quaternions, whichgeneralizetheconceptofcomplex numbers by introducing three distinct square roots of William Rowan Hamilton was an minus one, which satisfy i2 = j2 = k2 = ijk = −1.
Irish mathematician and physicist, It is said that these equations occurred to him while and child prodigy, who was ap- walkingalongthe Royal Canalin Dublinwithhiswife, pointed Professor of Astronomy at on 16 October 1843, and he promptly carved the Trinity College, Dublin, in1827, be- equations into the side of Broome bridge.
Although fore he had even graduated.
One thereisnolongeranyevidenceofthecarving, thereis of Hamilton’smostimportantcontributionswasanew nowastoneplaqueonthebridgecommemoratingthe formulation of dynamics, which played a significant discoveryanddisplayingthequaternionequations.
role in the later development of quantum mechanics.
550 11.
SAMPLINGMETHODS During the evolution of this dynamical system, the value of the Hamiltonian H is constant, asiseasilyseenbydifferentiation d H ∂H dzi ∂H dri = + dτ ∂zi dτ ∂ri dτ i ∂H ∂H ∂H ∂H = − =0.
(11.60) ∂zi ∂ri ∂ri ∂zi i A second important property of Hamiltonian dynamical systems, known as Li- ouville’s Theorem, is that they preserve volume in phase space.
In other words, if weconsideraregionwithinthespaceofvariables(z, r), thenasthisregionevolves undertheequationsof Hamiltoniandynamics, itsshapemaychangebutitsvolume willnot.
Thiscanbeseenbynotingthattheflowfield(rateofchangeoflocationin phasespace)isgivenby dz dr V = , (11.61) dτ dτ andthatthedivergenceofthisfieldvanishes ∂ dzi ∂ dri div V = + ∂zi dτ ∂ri dτ i ∂ ∂H ∂ ∂H = − + =0.
(11.62) ∂zi ∂ri ∂ri ∂zi i Now consider the joint distribution over phase space whose total energy is the Hamiltonian, i.
e., thedistributiongivenby 1 p(z, r)= exp(−H(z, r)).
(11.63) ZH Using the two results of conservation of volume and conservation of H, it follows that the Hamiltonian dynamics will leave p(z, r) invariant.
This can be seen by consideringasmallregionofphasespaceoverwhich H isapproximatelyconstant.
If we follow the evolution of the Hamiltonian equations for a finite time, then the volumeofthisregionwillremainunchangedaswillthevalueof Hinthisregion, and hencetheprobabilitydensity, whichisafunctiononlyof H, willalsobeunchanged.
Although H is invariant, the values of z and r will vary, and so by integrating the Hamiltonian dynamics over a finite time duration it becomes possible to make largechangestozinasystematicwaythatavoidsrandomwalkbehaviour.
Evolution under the Hamiltonian dynamics will not, however, sample ergodi- callyfromp(z, r)becausethevalueof H isconstant.
Inordertoarriveatanergodic sampling scheme, we can introduce additional moves in phase space that change the value of H while also leaving the distribution p(z, r) invariant.
The simplest waytoachievethisistoreplacethevalueofrwithonedrawnfromitsdistribution conditioned on z.
This can be regarded as a Gibbs sampling step, and hence from 11.5.
The Hybrid Monte Carlo Algorithm 551 Section 11.3 we see that this also leaves the desired distribution invariant.
Noting that z and r are independent in the distribution p(z, r), we see that the conditional Exercise 11.16 distributionp(r|z)isa Gaussianfromwhichitisstraightforwardtosample.
In a practical application of this approach, we have to address the problem of performing a numerical integration of the Hamiltonian equations.
This will neces- sarilyintroducenumericalerrorsandsoweshoulddeviseaschemethatminimizes theimpactofsucherrors.
Infact, itturnsoutthatintegrationschemescanbedevised forwhich Liouville’stheoremstillholdsexactly.
Thispropertywillbeimportantin thehybrid Monte Carloalgorithm, whichisdiscussedin Section11.5.2.
Onescheme forachievingthisiscalledtheleapfrogdiscretizationandinvolvesalternatelyupdat- ing discrete-time approximations z and r to the position and momentum variables using ∂E ri(τ + /2) = ri(τ)− ( z(τ)) (11.64) 2∂zi zi(τ + ) = zi(τ)+ ri(τ + /2) (11.65) ∂E ri(τ + ) = ri(τ + /2)− ( z(τ + )).
(11.66) 2∂zi Weseethatthistakestheformofahalf-stepupdateofthemomentumvariableswith stepsize /2, followedbyafull-stepupdateofthepositionvariableswithstepsize , followedbyasecondhalf-stepupdateofthemomentumvariables.
Ifseveralleapfrog stepsareappliedinsuccession, itcanbeseenthathalf-stepupdatestothemomentum variables can be combined into full-step updates with step size .
The successive updatestopositionandmomentumvariablesthenleapfrogovereachother.
Inorder toadvance thedynamics byatime interval τ, we needtotake τ/ steps.
The error involvedinthediscretizedapproximationtothecontinuoustimedynamicswillgoto zero, assumingasmoothfunction E(z), inthelimit → 0.
However, foranonzero asusedinpractice, someresidualerrorwillremain.
Weshallseein Section11.5.2 howtheeffectsofsucherrorscanbeeliminatedinthehybrid Monte Carloalgorithm.
Insummarythen, the Hamiltoniandynamicalapproachinvolvesalternatingbe- tweenaseriesofleapfrogupdatesandaresamplingofthemomentumvariablesfrom theirmarginaldistribution.
Notethat the Hamiltonian dynamics method, unlike the basic Metropolis algo- rithm, is able to make use of information about the gradient of the log probability distributionaswellasaboutthedistributionitself.
Ananalogoussituationisfamiliar from the domain of function optimization.
In most cases where gradient informa- tionisavailable, itishighlyadvantageoustomakeuseofit.
Informally, thisfollows from the fact that in a space of dimension D, the additional computational cost of evaluatingagradientcomparedwithevaluatingthefunctionitselfwilltypicallybea fixedfactor independent of D, whereas the D-dimensional gradient vector conveys D pieces of information compared with the one piece of information given by the functionitself.
552 11.
SAMPLINGMETHODS 11.5.2 Hybrid Monte Carlo Aswediscussedintheprevioussection, foranonzerostepsize , thediscretiza- tionoftheleapfrogalgorithmwillintroduceerrorsintotheintegrationofthe Hamil- tonian dynamical equations.
Hybrid Monte Carlo (Duane et al., 1987; Neal, 1996) combines Hamiltoniandynamicswiththe Metropolisalgorithmandtherebyremoves anybiasassociatedwiththediscretization.
Specifically, thealgorithmusesa Markovchainconsistingofalternatestochastic updates of the momentum variabler and Hamiltonian dynamical updates using the leapfrog algorithm.
After each application of the leapfrog algorithm, the resulting candidate state is accepted or rejected according to the Metropolis criterion based on the value of the Hamiltonian H.
Thus if (z, r) is the initial state and (z , r ) is the state after the leapfrog integration, then this candidate state is accepted with probability min(1, exp{H(z, r)−H(z , r )}).
(11.67) Iftheleapfrogintegrationweretosimulatethe Hamiltoniandynamicsperfectly, then every such candidate step would automatically be accepted because the value of H wouldbeunchanged.
Duetonumericalerrors, thevalueof H maysometimes decrease, andwewouldlikethe Metropoliscriteriontoremoveanybiasduetothis effectandensurethattheresultingsamplesareindeeddrawnfromtherequireddis- tribution.
Inorderforthistobethecase, weneedtoensurethattheupdateequations corresponding to the leapfrog integration satisfy detailed balance (11.40).
This is easilyachievedbymodifyingtheleapfrogschemeasfollows.
Before the start of each leapfrog integration sequence, we choose at random, with equal probability, whether to integrate forwards in time (using step size ) or backwards in time (using step size − ).
We first note that the leapfrog integration scheme(11.64),(11.65), and(11.66)istime-reversible, sothatintegrationfor Lsteps using step size − will exactly undo the effect of integration for L steps using step size .
Next we show that the leapfrog integration preserves phase-space volume exactly.
This follows from the fact that each step in the leapfrog scheme updates eitherazivariableoranri variablebyanamountthatisafunctiononlyoftheother variable.
Asshownin Figure11.14, thishastheeffectofshearingaregionofphase spacewhilenotalteringitsvolume.
Finally, we use these results to show that detailed balance holds.
Consider a small region R of phase space that, under a sequence of L leapfrog iterations of step size , maps to a region R .
Using conservation of volume under the leapfrog iteration, weseethatif RhasvolumeδV thensotoowill R .
Ifwechooseaninitial pointfromthedistribution(11.63)andthenupdateitusing Lleapfroginteractions, theprobabilityofthetransitiongoingfrom Rto R isgivenby 1 1 exp(−H(R))δV min{1, exp(−H(R)+H(R ))}.
(11.68) ZH 2 where the factor of 1/2 arises from the probability of choosing to integrate with a positivestepsizeratherthananegativeone.
Similarly, theprobabilityofstartingin 11.5.
The Hybrid Monte Carlo Algorithm 553 ri r i zi z i Figure 11.14 Each step of the leapfrog algorithm (11.64)–(11.66) modifies either a position variable z i or a momentumvariabler i.
Becausethechangetoonevariableisafunctiononlyoftheother, anyregioninphase spacewillbeshearedwithoutchangeofvolume.
region R andintegratingbackwardsintimetoendupinregion Risgivenby 1 1 exp(−H(R ))δV min{1, exp(−H(R )+H(R))}.
(11.69) ZH 2 It is easily seen that the two probabilities (11.68) and (11.69) are equal, and hence Exercise 11.17 detailedbalanceholds.
Notethatthisproofignoresanyoverlapbetweentheregions Rand R butiseasilygeneralizedtoallowforsuchoverlap.
Itisnotdifficulttoconstructexamplesforwhichtheleapfrogalgorithmreturns toitsstartingpositionafterafinitenumberofiterations.
Insuchcases, therandom replacement of the momentum values before each leapfrog integration will not be sufficienttoensureergodicitybecausethepositionvariableswillneverbeupdated.
Such phenomena are easily avoided by choosing the magnitude of the step size at randomfromsomesmallinterval, beforeeachleapfrogintegration.
We can gain some insight into the behaviour of the hybrid Monte Carlo algo- rithm by considering its application to a multivariate Gaussian.
For convenience, consider a Gaussian distribution p(z) with independent components, for which the Hamiltonianisgivenby 1 1 1 H(z, r)= z2+ r2.
(11.70) 2 σ2 i 2 i i i i Our conclusions will be equally valid for a Gaussian distribution having correlated componentsbecausethehybrid Monte Carloalgorithmexhibitsrotationalisotropy.
Duringtheleapfrogintegration, eachpairofphase-spacevariableszi, ri evolvesin- dependently.
However, the acceptance or rejection of the candidate point is based on the value of H, which depends on the values of all of the variables.
Thus, a significant integration error in any one of the variables could lead to a high prob- ability of rejection.
In order that the discrete leapfrog integration be a reasonably 554 11.
SAMPLINGMETHODS good approximation to the true continuous-time dynamics, it is necessary for the leapfrog integration scale to be smaller than the shortest length-scale over which the potential is varying significantly.
This is governed by the smallest value of σi, whichwedenotebyσ min.
Recallthatthegoaloftheleapfrogintegrationinhybrid Monte Carlo is to move a substantial distance through phase space to a new state thatisrelativelyindependentoftheinitialstateandstillachieveahighprobabilityof acceptance.
Inordertoachievethis, theleapfrogintegrationmustbecontinuedfora numberofiterationsoforderσ max /σ min.
By contrast, consider the behaviour of a simple Metropolis algorithm with an isotropic Gaussianproposaldistributionofvariances2, consideredearlier.
Inorder toavoidhighrejectionrates, thevalueofsmustbeoforderσ min.
Theexplorationof state space then proceeds by a random walk and takes of order(σ max /σ min )2 steps toarriveataroughlyindependentstate.
11.6.
Estimating the Partition Function As we have seen, most of the sampling algorithms considered in this chapter re- quire only the functional form of the probability distribution up to a multiplicative constant.
Thusifwewrite 1 p E(z)= exp(−E(z)) (11.71) ZE then the value of the normalization constant ZE, also known as the partition func- tion, isnotneededinordertodrawsamplesfromp(z).
However, knowledgeofthe value of ZE can be useful for Bayesian model comparison since it represents the model evidence (i.
e., the probability of the observed data given the model), and so it is of interest to consider how its value might be obtained.
We assume that direct evaluationbysumming, orintegrating, thefunctionexp(−E(z))overthestatespace ofzisintractable.
For model comparison, it is actually the ratio of the partition functions for two modelsthatisrequired.
Multiplicationofthisratiobytheratioofpriorprobabilities givestheratioofposteriorprobabilities, whichcanthenbeusedformodelselection ormodelaveraging.
Onewaytoestimatearatioofpartitionfunctionsistouseimportancesampling fromadistributionwithenergyfunction G(z) ZE = z exp(−E(z)) ZG z exp(−G(z)) exp(−E(z)+G(z))exp(−G(z)) = z exp(−G(z)) z = E G(z) [exp(−E+G)] exp(−E(z(l))+G(z(l))) (11.72) l 11.6.
Estimatingthe Partition Function 555 where {z(l)} are samples drawn from the distribution defined by p G(z).
If the dis- tributionp G isoneforwhichthepartitionfunctioncanbeevaluatedanalytically, for examplea Gaussian, thentheabsolutevalueof ZE canbeobtained.
Thisapproachwillonlyyieldaccurateresultsiftheimportancesamplingdistri- butionp Giscloselymatchedtothedistributionp E, sothattheratiop E/p Gdoesnot havewidevariations.
Inpractice, suitableanalyticallyspecifiedimportancesampling distributionscannotreadilybefoundforthekindsofcomplexmodelsconsideredin thisbook.
Analternativeapproachisthereforetousethesamplesobtainedfroma Markov chaintodefinetheimportance-samplingdistribution.
Ifthetransitionprobabilityfor the Markovchainisgivenby T(z, z ), andthesamplesetisgivenbyz(1),..., z(L), thenthesamplingdistributioncanbewrittenas L 1 exp(−G(z))= T(z(l), z) (11.73) ZG l=1 whichcanbeuseddirectlyin(11.72).
Methodsforestimatingtheratiooftwopartitionfunctionsrequirefortheirsuc- cessthatthetwocorrespondingdistributionsbereasonablycloselymatched.
Thisis especiallyproblematicifwewishtofindtheabsolutevalueofthepartitionfunction for a complex distribution because it is only for relatively simple distributions that the partition function can be evaluated directly, and so attempting to estimate the ratioofpartitionfunctionsdirectlyisunlikelytobesuccessful.
Thisproblemcanbe tackledusingatechniqueknownaschaining(Neal,1993; Barberand Bishop,1997), whichinvolvesintroducingasuccessionofintermediatedistributionsp 2 ,..., p M−1 that interpolate between a simple distribution p 1 (z) for which we can evaluate the normalization coefficient Z 1 and the desired complex distribution p M(z).
We then have ZM = Z 2 Z 3 ··· ZM (11.74) Z 1 Z 1 Z 2 ZM−1 in which the intermediate ratios can be determined using Monte Carlo methods as discussed above.
One way to construct such a sequence of intermediate systems is to use an energy function containing a continuous parameter 0 α 1 that interpolatesbetweenthetwodistributions Eα(z)=(1−α)E 1 (z)+αEM(z).
(11.75) If the intermediate ratios in (11.74) are to be found using Monte Carlo, it may be moreefficienttouseasingle Markovchainrunthantorestartthe Markovchainfor eachratio.
Inthiscase, the Markovchainisruninitiallyforthesystemp 1 andthen aftersomesuitablenumberofstepsmovesontothenextdistributioninthesequence.
Note, however, that the system must remain close to the equilibrium distribution at eachstage.
556 11.
SAMPLINGMETHODS Exercises 11.1 ( ) www Show that the finite sample estimator f defined by (11.2) has mean equalto E[f]andvariancegivenby(11.3).
11.2 ( ) Suppose that z is a random variable with uniform distribution over (0,1) and that we transform z using y = h−1(z) where h(y) is given by (11.6).
Show that y hasthedistributionp(y).
11.3 ( ) Givenarandomvariablezthatisuniformlydistributedover(0,1), findatrans- formationy =f(z)suchthatyhasa Cauchydistributiongivenby(11.8).
11.4 ( ) Suppose that z 1 and z 2 are uniformly distributed over the unit circle, as shown in Figure 11.3, and that we make the change of variables given by (11.10) and(11.11).
Showthat(y 1 , y 2 )willbedistributedaccordingto(11.12).
11.5 ( ) www Letzbea D-dimensionalrandomvariablehavinga Gaussiandistribu- tionwithzeromeanandunitcovariancematrix, andsupposethatthepositivedefinite symmetricmatrixΣhasthe CholeskydecompositionΣ=LLTwhere Lisalower- triangular matrix (i.
e., one with zeros above the leading diagonal).
Show that the variable y = µ+Lz has a Gaussian distribution with mean µ and covariance Σ.
Thisprovidesatechniqueforgeneratingsamplesfromageneralmultivariate Gaus- sianusingsamplesfromaunivariate Gaussianhavingzeromeanandunitvariance.
11.6 ( ) www Inthisexercise, weshowmorecarefullythatrejectionsamplingdoes indeed draw samples from the desired distribution p(z).
Suppose the proposal dis- tributionisq(z)andshowthattheprobabilityofasamplevaluezbeingacceptedis givenby p(z)/kq(z)where pisanyunnormalizeddistributionthatisproportionalto p(z), andtheconstantkissettothesmallestvaluethatensureskq(z) p(z)forall valuesofz.
Notethattheprobabilityofdrawingavaluezisgivenbytheprobability of drawing that value from q(z) times the probability of accepting that value given that it has been drawn.
Make use of this, along with the sum and product rules of probability, towritedownthenormalizedformforthedistributionoverz, andshow thatitequalsp(z).
11.7 ( ) Supposethatz hasauniformdistributionovertheinterval[0,1].
Showthatthe variabley =btanz+chasa Cauchydistributiongivenby(11.16).
11.8 ( ) Determine expressions for the coefficients ki in the envelope distribution (11.17)foradaptiverejectionsamplingusingtherequirementsofcontinuityandnor- malization.
11.9 ( ) By making use of the technique discussed in Section 11.1.1 for sampling from a single exponential distribution, devise an algorithm for sampling from the piecewiseexponentialdistributiondefinedby(11.17).
11.10 ( ) Showthatthesimplerandomwalkovertheintegersdefinedby(11.34),(11.35), and (11.36) has the property that E[(z(τ))2] = E[(z(τ−1))2] + 1/2 and hence by inductionthat E[(z(τ))2]=τ/2.
Exercises 557 Figure11.15 A probability distribution over two variables z 1 z2 and z 2 that is uniform over the shaded regions andthatiszeroeverywhereelse.
z1 11.11 ( ) www Show that the Gibbs sampling algorithm, discussed in Section 11.3, satisfiesdetailedbalanceasdefinedby(11.40).
11.12 ( ) Considerthedistributionshownin Figure11.15.
Discusswhetherthestandard Gibbs sampling procedure for this distribution is ergodic, and therefore whether it wouldsamplecorrectlyfromthisdistribution 11.13 ( ) Considerthesimple3-nodegraphshownin Figure11.16inwhichtheobserved nodexisgivenbya Gaussiandistribution N(x|µ,τ−1)withmeanµandprecision τ.
Suppose that the marginal distributions over the mean and precision are given by N(µ|µ 0 , s 0 )and Gam(τ|a, b), where Gam(·|·,·)denotesagammadistribution.
Writedownexpressionsfortheconditionaldistributionsp(µ|x,τ)andp(τ|x,µ)that would be required in order to apply Gibbs sampling to the posterior distribution p(µ,τ|x).
11.14 ( ) Verify that the over-relaxation update (11.50), in which zi has mean µi and variance σi, and where ν has zero mean and unit variance, gives a value z i with meanµi andvarianceσ i 2.
11.15 ( ) www Using(11.56)and(11.57), showthatthe Hamiltonianequation(11.58) is equivalent to (11.53).
Similarly, using (11.57) show that (11.59) is equivalent to (11.55).
11.16 ( ) Bymakinguseof(11.56), (11.57), and(11.63), showthattheconditionaldis- tributionp(r|z)isa Gaussian.
Figure11.16 A graph involving an observed Gaussian variable x with µ τ priordistributionsoveritsmeanµandprecisionτ.
x 558 11.
SAMPLINGMETHODS 11.17 ( ) www Verifythatthetwoprobabilities(11.68)and(11.69)areequal, andhence thatdetailedbalanceholdsforthehybrid Monte Carloalgorithm.
In Chapter9, wediscussedprobabilisticmodelshavingdiscretelatentvariables, such as the mixture of Gaussians.
We now explore models in which some, or all, ofthe latent variables are continuous.
An important motivation for such models is that many data sets have the property that the data points all lie close to a manifold of much lower dimensionality than that of the original data space.
To see why this might arise, consider an artificial data set constructed by taking one ofthe off-line Appendix A digits, representedbya64 x 64pixelgrey-levelimage, andembeddingitinalarger imageofsize100x100bypaddingwithpixelshavingthevaluezero(corresponding towhitepixels)inwhichthelocationandorientationofthedigitisvariedatrandom, asillustratedin Figure 12.1.
Eachoftheresultingimagesisrepresentedbyapointin the 100 x 100 = 10, OOO-dimensionaldataspace.
However, acrossadatasetofsuch images, there are only three degrees offreedom ofvariability, corresponding to the vertical and horizontal translations and the rotations.
The datapoints will therefore live on a subspace ofthe data space whose intrinsic dimensionality is three.
Note 559 560 12.
CONTINUOUS LATENT VARIABLES Figure12.1 Asyntheticdataselobtainedbytakingoneoftheoff-linedigitimagesandcreating multi plecopies in eachofwhich thedigithas undergonea randomdisplacementand rotation within some larger image field.
The resulting images each have 100 )( 100 = 10.000 pixels.
that the manifold will be nonlinear because.
for instance.
if we translate the digit past a particular pixel, that pixel value will go from zero (white)10one (black) and back to zero again.
which is clearly a nonlinear function of the digit position.
In thisexample.
!.
helranslation androtation parametersare latent variables because we observe only the image vectors and are not told which values ofthe translation or rotation variables were usedtocreate them.
Forreal digit imagedata, there will be a funherdegreeoffreedom arising from scaling.
Moreover there will be multiple addilional degrees of freedom associaled wilh more complex deformations due to the variability in an individual's wriling 3S well as lhe differences in writing slyles between individuals.
evenheless.
the numberofsuch degreesoffreedom will besmall compared tothedimensionalityof Ihedataset.
Appendi XA Anotherexample is provided by the oil flow data set.
in which (foragiven ge- ometrical configuration ofthegas, WOller, and oil phases)there areonlytwodegrees offreedom ofvariabilitycorrespondingtothefraction ofoil inthe pipeandthe frac tion of water (the fraction ofgas Ihen being determined).
Ahhough the data space comprises 12 measuremenl S, adata setofpoints will lieclose toa Iwo-dimensional manifold embedded within this space.
In this case, the manifold comprises scveral distinct segments corresponding todifferent flow regimes.
each such segment being a(noisy) continuous two-dimensional manifold.
Ifour goal is datacompression.
or density modelling, then therecan be benefits in exploiling this manifold strucl Ure.
In praclice.
the data points will not be confined precisely to a smooth low dimensional manifold, and we can interpret the departures ofdata points from the manifold as ·noise'.
This leads naturally to a generative view of such models in which we first select a poinl within the manifold according to some latent variable distribution and then generate an observed data point by : ldding noise, drawn from someconditional distribution ofthedata varillbles given the latent varillbles.
Thc simplest continuous latent variable model assumes Gaussian distributions for both thc latent and observed variables and makes use of a linear, Gaussian de- Se CTion 8.1..
J pendence of the observed variables on Ihe slate of the latent variables.
This leads to a probabilislic fonnulation ofthe well-known technique ofprincipal component analysis (Pe A), as well as 10arelated model called factor analysis.
Section 12.1 In thischapterw will begin wilh aslandard, nonprobabilistic treatment of Pe A.
and thcn we show how Pe A arises naturally as the maximum likelihood solution 10 12.1.
Principal C01n[>OM"1 Anal Jsis 561 Flgu, e12.2 P'if>cipal compooont a",,~" seeks" $pace 01 ! owe, dimensionality.
kt"(>WIl as ! he P<lno> pal sub Space "nd denoted I: Jy the magenta "1 line.
SUCh Itlet the Grthogonet [jiojecti Oh 01 ! he data points ('ed do Isl onto t P'Ns ~ """'imizesthevaria,..,., of! heproja<: tedpoints is based on m..
mizing the """,-<>I·squares of ! he projection errors.
ind'cated by the bfi.>e lines.
S'crio" 12.2 apanlcula, fonn oflinear-Gau"ian latem "ariable model.
This probabilistic refor mulation bring~ many ad\'imlag~s, su~h as tl>l: use I)f EM for parametereslimalion, rrinciple<J c~tensioos 10 Oli~turc, of Pe A model" and Ba)'~sian formulat; ons that allow tbe number ofrrincipal com[>Oncntsto be detennined a Ul Omatically from ! be data.
Finally'.
"cdi Sl; us< briefly""'eral gencrali, ation, ofthe latent Yariableconcept that g<l be~ood tbe linear-Gaussian assumption including non·Gau"i"n I.
tcnt yari ____'c2=.~1.
Principal Component Analysis Principal compooem analy,;" or rc A.; s a technique tha! is "'idely u<ed for appli.
cationssuchasdimensionality.-eduction, lossydatacomprc"ion, feature e>tracti"".
f~.
lbcrc an: t....
o commonly used definitions of Pe A that giye rise to the >arne theprojttteddata i' ma~imi, e<J(1I",.
l Iing.
1933).
Equi"alemly,; tcanbedefinedas tbe linear projectionthat minimi"'.
the averageprojttlioncost.
definedast~ mean squa.-eddistance! letween thedata [>Oint<andtbeir p<ojtttioo, (Pearson, 19(1).
The l"J'"OC"s<ofonhogonal projection i' illustraled in Figu Te 12.2.
We con, idereach of thesedefinitions in tum.
12,1.1 Mllximllm variance lormulation Euclidean variable "'ilh dimen, ionality D.
Our goal is to project If>/:: data onto a 'paceha"ingdimen, ionality M < D"hile Ill3Jli", i, illgthe "ariall Ceoftheprojttted 562 12.
CONTINUOUSLATENTVARIABLES chapter, we shall considertechniques to determine an appropriate value of IV! from thedata.
To begin with, considerthe projection onto a one-dimensional space (M = 1).
We can define the direction ofthis space using a D-dimensional vector Ul, which for convenience (and without loss ofgenerality) we shall choose to be aunit vector so that uf Ul = 1 (note that we are only interested in the direction defined by Ul, notin the magnitudeof Ul itself).
Eachdatapoint Xn is then projectedonto ascalar value uf Xn.
The meanoftheprojecteddata is ufx wherex is thesamplesetmean given by (12.1) and the varianceofthe projecteddataisgiven by (12.2) where S is thedatacovariancematrixdefinedby 1 N S = - NL " J (x n - x)(x n - x)T .
(12.3) n=l Wenow maximizetheprojectedvariance Uf SUl withrespectto Ul.
Clearly, thishas to be aconstrained maximizationtoprevent 00.
Theappropriateconstraint comes from the normalization condition uf Ul = 1.
To enforce this constraint, Appendix E we introduce a Lagrange multiplier that we shall denote by AI, and then make an unconstrained maximizationof (12.4) By setting the derivative with respect to Ul equal to zero, we see that this quantity will have astationary pointwhen (12.5) which saysthat Ul mustbeaneigenvectorof S.
Ifweleft-multiply by uf andmake use ofuf Ul = 1, we seethatthe varianceis given by (12.6) and so the variance will be a maximum when we set Ul equal to the eigenvector having the largest eigenvalue AI.
This eigenvector is known as the first principal component.
We can define additional principal components in an incremental fashion by choosing each new direction to be that which maximizes the projected variance 12.1.
Principal Component Analysis 563 amongst all possible directions orthogonal to those already considered.
Ifwe con siderthegeneralcaseofan M-dimensional projectionspace, theoptimallinearpro jectionfor which the varianceofthe projected datais maximizedis now defined by the M eigenvectors U1,...
, UM ofthedatacovariancematrix S correspondingtothe To summarize, principal component analysis involves evaluating the mean x andthecovariancematrix S ofthedatasetandthen finding the M eigenvectorsof S correspondingtothe M largesteigenvalues.
Algorithmsforfindingeigenvectorsand eigenvalues, as well as additional theorems related to eigenvector decomposition, can be found in Golub and Van Loan (1996).
Note that the computational cost of computingthefull eigenvectordecomposition for amatrixofsize D x Dis O(D3).
Ifwe plan to project our data onto the first M principal components, then we only need to find the first M eigenvalues and eigenvectors.
This can be done with more efficient techniques, such as the power method (Golub and Van Loan, 1996), that Section 12.2.2 scalelike O(MD2 ), oralternatively wecanmake use ofthe EM algorithm.
12.1.2 Minimum-error formulation We now discuss an alternative formulation of pe A based on projection error Appendix C minimization.
Todothis, weintroduceacompleteorthonormalsetof D-dimensional basis vectors {Ui} wherei = 1,...
, D thatsatisfy (12.7) Becausethis basisiscomplete, eachdatapointcanberepresentedexactlybyalinear combinationofthe basisvectors D Xn = Laniui (12.8) i=l where the coefficients ani will be different for different data points.
This simply corresponds to a rotation ofthe coordinate system to a new system defined by the {Ui}, and the original D components {Xnl' ...
, Xn D} arereplacedby an equivalent set {anl' ...
, an D}.
Taking the inner product with Uj, and making use ofthe or thonormality property, weobtainanj = x; Uj, andso withoutloss ofgenerality we canwrite D L Xn = (X~Ui) Ui· (12.9) i=l Our goal, however, is to approximate this data point using a representation in volving a restricted number M < D ofvariables corresponding to a projection onto a lower-dimensional subspace.
The M-dimensional linear subspace can be repre sented, without loss of generality, by the first M of the basis vectors, and so we approximateeachdatapoint Xn by M D x = + (12.10) n L Zni Ui L bi Ui i=l i=M+l 564 12.
CONTINUOUS LATENTVARIABLES where the {Zni} depend onthe particulardata point, whereas the {bd areconstants that are the samefor all datapoints.
We arefree to choosethe {Ui}, the {Zni}, and the {bd soastominimizethedistortionintroducedbythereduction indimensional ity.
Asourdistortionmeasure, weshallusethesquareddistancebetweentheoriginal datapoint Xn and its approximation X n, averaged overthe dataset, so thatourgoal is to minimize N ~ L J = Ilx n - x n 112 .
(12.11) n=l Considerfirst ofall the minimization with respect to the quantities {Zni}.
Sub stituting for X n, setting the derivative withrespect to Znj to zero, and making useof theorthonormality conditions, weobtain (12.12) wherej = 1,...
, M.
Similarly, settingthederivativeof J with respectto b to zero, i and again makinguseofthe orthonormalityrelations, gives b j = - X T Uj (12.13) wherej = M + 1,...
, D.
Ifwesubstitutefor Zni andb i, andmakeuseofthegeneral expansion (12.9), weobtain LD {( ud Xn - Xn = Xn - x)T Ui (12.14) i=M+l x from which we see that the displacement vector from Xn to n lies in the space orthogonal to the principal subspace, because itis a linearcombination of{ud for + x projected points must lie within the principal subspace, but we can move them n freely within that subspace, and so the minimum error is given by the orthogonal projection.
We therefore obtain an expression for the distortion measure J as a function purely ofthe {ud in theform ~ ~ J = N 1 L L ( X T n Ui - _ X T U ) i 2 = L D U T i SUi.
(12.15) n=li=M+l i=M+l There remains the task ofminimizing J with respect to the {Ui}, which must be a constrained minimization otherwise we will obtain the vacuous result Ui = O.
The constraints arise from the orthonormality conditions and, as we shall see, the solution will be expressed in terms ofthe eigenvector expansion ofthe covariance matrix.
Beforeconsideringaformal solution, letustrytoobtainsomeintuitionabout theresultbyconsideringthecaseofatwo-dimensional dataspace D = 2andaone dimensional principal subspace M = 1.
We have to choose a direction U2 so as to 12.1.
Principal Component Analysis 565 minimize J = UISU2' subject to the normalization constraint u IU2 = 1.
Using a Lagrangemultiplier A2 to enforcetheconstraint, weconsiderthe minimizationof (12.16) Setting the derivative with respect to U2 to zero, we obtain SU2 = A2U2 so that U2 is an eigenvector of S with eigenvalue A2.
Thus any eigenvector will define a sta tionary point ofthe distortion measure.
To find the value of J at the minimum, we back-substitute the solution for U2 into the distortion measure to give J = A2.
We thereforeobtaintheminimumvalueof J bychoosing U2 tobetheeigenvectorcorre spondingtothesmallerofthe two eigenvalues.
Thus we shouldchoosetheprincipal subspacetobealignedwiththeeigenvectorhavingthelargereigenvalue.
Thisresult accords with ourintuition that, in orderto minimize the average squared projection distance, we should choose the principal component subspace to pass through the mean ofthe datapoints and to be aligned with the directions ofmaximum variance.
For the case when the eigenvalues are equal, any choice ofprincipal direction will give rise to the samevalue of J.
Exercise 12.2 Thegeneralsolutiontotheminimizationof J forarbitrary D andarbitrary M < D isobtainedbychoosingthe {Ui}tobeeigenvectorsofthecovariancematrixgiven by SUi = Ai Ui (12.17) where i = 1,...
, D, and as usual the eigenvectors {Ui} are chosen to be orthonor mal.
Thecorrespondingvalue ofthe distortion measureis then givenby D L J= Ai (12.18) i=M+l whichis simplythe sumoftheeigenvaluesofthoseeigenvectorsthatareorthogonal to the principal subspace.
We therefore obtainthe minimum value of J by selecting these eigenvectors to be those having the D - M smallest eigenvalues, and hence the eigenvectors defining the principal subspace are those corresponding to the M largesteigenvalues.
Although we have considered M < D, the PCA analysis still holds if M = D, in which case there is no dimensionality reduction but simply a rotation ofthe coordinateaxes to align withprincipalcomponents.
Finally, itis worthnotingthatthereexists acloselyrelatedlineardimensionality reductiontechniquecalledcanonicalcorrelationanalysis, or CCA (Hotelling, 1936; Bach and Jordan, 2002).
Whereas PCA works with a single random variable, CCA considers two (or more) variables and tries to find a corresponding pair of linear subspacesthathavehighcross-correlation, sothateachcomponentwithinoneofthe subspacesiscorrelatedwithasinglecomponentfromtheothersubspace.
Itssolution canbeexpressedin terms ofageneralizedeigenvectorproblem.
12.1.3 Applications of pe A We can illustrate the use of PCA for data compression by considering the off Appendix A line digits data set.
Because each eigenvector ofthe covariance matrix is a vector 566 12.
COl\'TINUOli S LATf; I\'T \'ARIAIILES cligitsdataset.
t<>getl'lerwith! hecorrespondi~~.
; n the OIigi",,1 D-<limensional space.
wecan represent thoeigenw: cto<sas imago<of tho same sil O as ,1>0 data poi",,_ 11,.
first Ih'e .
ig.
n,'occ Of S.
along wich tl>o corre sponding.
igen,'slue,.
are <IIo"'n in Figure 12,3, A pl O! ofll>ocomplete spect"'m uf oigo",·alue,.
sone<! intodecreasingorder.
isshown in Figure 12.4{ai.
Thedi'tortion measure J a SSQCiated wilh choo<ing a particular value of M is gi.'en by tho sum + ofthe eig.
n", lues from M I up to 0 and is pt O! ted for different ,'aluo< of .\1 in Figure 12,4(b).
If"'e <utlslitut.
(12,12) and(12.13) into(12.10).
we can write the I'CA appro~­ imation toadata "eel'" x~ i" the fonn M " '- L.-{.
x~",)u,+ I: (xl'u,)u, (12.19) ~ ._M+l - M x+L(X , ~U,-XTU,)U; (12.20) • , , to' , 10' , , , ", "- , " ,., " ", , ~ ., ~ ; 0 ", ~ ~ " FIIIUre 12,4 (a) PIol at ! he e Jo I; nv.
loo ."., etrum lor the off·1ine digits data set (b) P10t 01 ! he sum at the <: liscarded."".
Ioos, which "'l'fesoots! hes.
um-ol·SQ"", esdistortlon J i<*~ byprojecti<Xlthedata onto ap<incipalcomponenlslll>spaee'"dimensionalitv M.
oblair...:! by 'e1aio"li! Xl , If j)<incipal~n1S 10<various val,,"01 , If.
As , II increason ! tie re<: onst, ucti Of I ~s more ao:: urate and woukl ~ portee! when .-If D ~ 28 x 28 ~ ."-1.
K where we ha"e made moe ofthe relation " L x = ,-, (x'",) u; (12.21) which follow.
from the completene" of the {u, I, Thi.
represent.
a contpre"ioo "f the data >ct.
Ilttau>e for each data poim we ha,..
repla«d the V·dimensiooal "o<: lorx" Wilh an , I[.
din>en, ional "o<: tor having componem, (x~'" _ X'",).
11Ie 'mailer the "alue of M.
the greater the degree ofcomp.-e", ion.
Example.
of Pe A ,""on't""tioosofdata points for thedigitsdataset are shown in Figure 12.5 Anolher application of priocipal compcmenl analy, i.
i' to data pre-processing.
Inthi'case, lhegoal isn O! dimensionality red UC1ion but ratherthetmn, formmion of adata sel in or<k' to standa'lli'.
e eenain ofil S pmpenies.
Thiscan be in'portanl in allowing.
ubsequent pallem ,""ognition algorithm.10 be applied successfully 10 the data>ct.
Typically.
ilisdonewilentheoriginal "ariable.
are mea, ured in "arioosdif.
ferent unil'or! la"esignificantlydif Terent ,'ariabilil}'.
Forinstance in the Old Faithful AI''''''''/;'\'A data sel.
the time betv.-een eruption.
i.
typicany an order of magni1Ude greaterthan lhe d Urali"" of.
n erupt;,,".
When W'e applied the ".
nl Cans algorill"" 10 thi< data Seer/on 9.1 set, ".-e first made a separ.
te linear re-sealing of the individual "anable' socb thm each "ariable had zero mean and unit "ariance.
ll Us is known asslll Nlardiv·., g the dota.
and thec O\'anance matrix for lhe 'lando, di/, eddala hascomponents (12,22) (",,, el,,,;,,,, where<1, isthe,'anaoceof: c,.
Thisi<known asthe matri.'oftheoriginal dota and ha' the propeny thai ift""orompooent, X; and x, ofthedata are perfee1ly 11",,'1""', using Pe A we can make a It>Of'e subst.
mial nonnalizat; oo ofthedata togi\'C it zeromean and unitco'·ariance.
sothatdifferent "anables becomederorre late<l Todo this.
we first""rile theei8Cn"cclorequation (12,17) in the form su= UL (12.23) 568 12.
CONTINUOUS LATENTVARIABLES 100 2 2 00' O 90 B 000 80 0 70 0 0 08 0 0 ,=~o ~ 0 Oc PO 0 ~0 tj 60 50 ~OOID -2 -2 40 2 4 6 -2 0 2 -2 0 2 Figure 12.6 Illustration ofthe effects of linear pre-processing applied to the Old Faithful dataset.
The ploton the leftshowsthe original data.
Thecentreplotshowsthe resultofstandardizingthe individualvariablestozero mean and unit variance.
Also shown are the principal axes of this normalized data set, plotted overthe range ±A~/2.
The ploton the rightshowsthe result ofwhitening ofthe datato give itzero mean and unitcovariance.
where L is a D x D diagonal matrix with elements Ai, and U is a D x D orthog onal matrix with columns given by Ui.
Then we define, for each data point Xn, a transformed valuegiven by (12.24) where x is the sample meandefined by (12.1).
Clearly, the set {Yn} has zero mean, and its covariance isgivenbythe identity matrix because LN 1~ T L-1/2U (X n - x)(x n - x)TUL-1/2 n=l L~1/2UTSUL-1/2 = L-1/2LL-1/2 = I.
(12.25) This operation is known as whitening or sphereing the dataand is illustratedfor the Appendix A Old Faithfuldatasetin Figure 12.6.
Itis interesting to compare PCA with the Fisher linear discriminant which was discussed in Section 4.1.4.
Both methods can be viewed as techniques for linear dimensionality reduction.
However, PCA is unsupervised and depends only on the values Xn whereas Fisherlineardiscriminantalso uses class-labelinformation.
This differenceis highlightedbythe examplein Figure 12.7.
Anothercommon application ofprincipal component analysis is to data visual ization.
Hereeachdatapointisprojectedontoatwo-dimensional (M = 2) principal subspace, so that a data point Xn is plotted at Cartesian coordinates given by x'J.
U1 and x'J.
U2, where Ul and U2 are the eigenvectors corresponding to the largest and Appendix A second largest eigenvalues.
An example ofsuch a plot, for the oil flow data set, is shown in Figure 12.8.
Fig"", 12.7 A comparison 01 pro: ipal compo Mnt analysis ....111 Fisha(s linaar discriminant 101 """", <*man""'" " .
' - ~'" ~ .•••._', .• a tw li o ty d r i & m d a u n c s li i o o n n .
s, H be e l r o e ng to in o g d t a o ta tw in o ' ~.
~ '-':''--' classes s IIOWI1 in red and blue.
is ': r-'---~+·_ ~-J to be Pf OI"Cled onto a s.
ingle di· mension.
PCA c/>xlsasthe direc· tion 01 maximum varia""e.
s IIOWI1 ~.,, trythama9""ta Co""'.
wt11chleads to strong class overlap.
whereas ." ! he Fisl>ef Ii Mardi SCf Ornillanttakes accoun1 <: A too class labels and .,':----;!---;-" leadstoaprojectionontotheg<ean _.
S 0 3 CUM! giving much t>etler class separation Fig"", 12.8 Visualilatlon01! heoill'low <latal Ietobtained tryprojoectingthe<lataontothelirsttwoprin.
cipalcompone<1ts.
The<ed, blue, and9r&en points corre-spond to ! he 'Iami NI(, 't>omo- genoous', and '8nnula~flow oonligurations ", specrive Iy.
12.1.4 pe A for high-dimensional data In someapplication.
ofpli Tl Cipalcomponent analysis.
the numberofdatapoints issmallerthan t!>c dimensionalityoftroedata 'pace.
FOI"example.
", e might want to apply Pe A to adata <el of a few hundred images, each of,,'hich rorrespo OOs to a enlour"aluesforeachofthepi.", lsintroeimage), NOIethatina D-<limen, ionalspace a set ofj Y points.
",'here N < D.
defines a linear subspa:: e ", hose dimensi"nality is at ""'st N - 1, and SO there is linle point in applying Pe A for ,'alue< of M tha I"'" greater than N - I, Indeed, if"'e pelf"",, Pe A we will find that at least D - N + I ofthe eigen".
luesart lero.
eorrespnnding t Qeigenvectorsaloog ", hose direclioosthedata <elhas 10mvarianee.
Funhem>ore.
typical alg Ol"ithm, for finding theeigen,'eet""ofa Dx D matrix ha"eacomputatiooal eoslthmscaleslike O(D~J.
a OOsofor appliealionssuch as the imagee, ample.
adirec' applicationof Pe A will becomputatiooallyinfe,,-sib Je.
W.
can resoh'ethisproblemasfo Il",",'" Fir; l.
let usdefine X tobe the (N " DJ· 570 12.
CONTINUOUSLATENTVARIABLES dimensionalcentreddatamatrix, whose nth rowisgivenby (x n - X)T.
Thecovari ance matrix (12.3) can then be written as S = N-1XTX, and the corresponding eigenvectorequationbecomes 1 T -NX XUi = Ai Ui.
(12.26) Now pre-multiplyboth sidesby X to give N 1 XX T (XUi) = Ai(XUi)' (12.27) Ifwe now define Vi = XUi, weobtain - 1 XX T Vi = Ai Vi (12.28) N whichis aneigenvectorequationfor the N x N matrix N-1XXT .
We seethatthis hasthesame N -1 eigenvalues astheoriginalcovariancematrix(whichitselfhasan additional D - N +1eigenvaluesofvaluezero).
Thus wecansolvetheeigenvector problem in spaces oflower dimensionality with computational cost O(N3 ) instead of O(D3 ).
Inorderto determine theeigenvectors, we multiplybothsides of(12.28) by XT to give N1XT X ) (X T Vi) = Ai(XT Vi) (12.29) ( from which we see that (XTVi) is an eigenvector of S with eigenvalue Ai.
Note, however, thattheseeigenvectorsneednotbenormalized.
Todeterminetheappropri ate normalization, were-scale Ui ex: X T Vi by aconstantsuchthat Ilui II = 1, which, assuming Vi has beennormalizedto unitlength, gives 1 XT (12.30) Ui = (NA i)1/2 Vi· In summary, to apply this approach we first evaluate XXT and then find its eigen vectorsandeigenvaluesandthencomputetheeigenvectorsintheoriginaldataspace using (12.30).
12.2.
Probabilistic pe A The formulation of PCA discussed in the previous section was based on a linear projectionofthe dataontoasubspaceoflowerdimensionality than the originaldata space.
We now show that PCA can also be expressed as the maximum likelihood solutionofaprobabilistic latentvariable model.
This reformulationof PCA, known asprobabilisticpe A, brings several advantages comparedwithconventional PCA: • Probabilistic PCA represents a constrained form ofthe Gaussian distribution in which the number offree parameters can be restricted while still allowing the model to capturethe dominantcorrelationsin adataset.
12.2.
Probabilisticpe A 571 • We can derive an EM algorithm for PCA that is computationally efficient in situations where only a few leading eigenvectors are required and that avoids Section 12.2.2 having to evaluatethe datacovariancematrix as anintermediate step.
• Thecombinationofaprobabilisticmodeland EMallows us to deal withmiss ing values inthe dataset.
• Mixtures ofprobabilistic PCA models can be formulated in aprincipled way and trained usingthe EM algorithm.
• Probabilistic PCA forms the basis for a Bayesian treatment of PCA in which the dimensionality ofthe principal subspace can be found automatically from Section 12.2.3 the data.
• The existence of a likelihood function allows direct comparison with other probabilisticdensity models.
Bycontrast, conventional PCAwill assignalow reconstructioncostto datapoints thatareclose to the principal subspaceeven ifthey liearbitrarilyfar from the trainingdata.
• Probabilistic PCA can be used to modelclass-conditional densities and hence be appliedto classificationproblems.
• Theprobabilistic PCAmodelcanberun generativelytoprovidesamplesfrom the distribution.
This formulation of PCA as a probabilistic model was proposed independently by Tipping and Bishop (1997, 1999b)andby Roweis (1998).
As we shall seelater, itis closelyrelatedtofactoranalysis(Basilevsky, 1994).
Section 8.1.4 Probabilistic PCA is a simple example of the linear-Gaussian framework, in whichallofthemarginalandconditionaldistributions are Gaussian.
Wecanformu lateprobabilistic PCAbyfirstintroducinganexplicitlatentvariablezcorresponding to the principal-component subspace.
Next we define a Gaussian prior distribution p(z) overthelatentvariable, togetherwitha Gaussianconditionaldistributionp(xlz) for the observed variable x conditioned on the value ofthe latent variable.
Specifi cally, the priordistributionoverz is given by azero-mean unit-covariance Gaussian p(z) = N(z IO, I).
(12.31) Similarly, theconditionaldistributionofthe observedvariable x, conditionedonthe valueofthe latentvariable z, is again Gaussian, oftheform p(xlz) = N(xl Wz + J-L, a2I) (12.32) in which the mean ofx is a general linear function of z governed by the D x M matrix Wandthe D-dimensional vector J-L.
Note thatthis factorizes withrespectto Section 8.2.2 the elements ofx, in other words this is an example ofthe naive Bayes model.
As we shall see shortly, thecolumns of W span alinearsubspace within the data space thatcorresponds to the principal subspace.
The otherparameterin this model is the scalara2 governingthe varianceoftheconditionaldistribution.
Notethatthereis no 572 11.
CONTINUOUS LAT!:: NT VANIM1LI:: S .-/ , , , , , , , , , Flgu..
12.9 I\n ~I"'tfat""o Ilt>e II"""fative vi&w o I1t>e p<ot>abi!; st", Pe A mode Ifof" two-dimensiooal <! ala space and a on&-<lirnent.
ionallat/l<1t space, An Ob&erved <! ala point x Is generated by first drawing a valuei fof1t>e Iat&n1vafiatlle/f(lm ~s priordist,~t""P(~) and Itlen drawing a val"" fof x lroman i SO/fop K: Gaussian distr~t""(iijust, al&(lbytheredcir<: ie's)havingmean wi+"andco Y8r1.
once ,,'1 Thel/f&er\ellips.&$showl! le density"""toors! ofthe marg'''''1 dis1r1bulion PIx).
loss ofge""ra Jity in assuming a zero mean.
unit co\'ariance Gau"ian for the latent distributi"n II{Z) because a more gcneral Gau"i3n di"ributi"n would gi"erisetoan equivalent probabili"ic n>odel.
Wecan viewthe probabilistic Pe A model from ageoerati"e\'iew""intin "hich a sampled '-alue of the ob""Yed ,..
riable is ob Iained by first choo, ing a ,..
Iue for the latent ,'ariahle aod then >ampling the OO",,,'e; j ,-ariable cooditioned on this lao tent \'alue, Specifically, the V-dimen'ional OO"''''ed '-ariable x isdefined by a lin· ea, tran, formati,," of the '\/·dimen, i"nal latcnt '-ariable z plu, additi'-e Gaussian 'noise',<0that ,,=\VZ+,,+~ (12.33) w!>ere z is an M-di""'nsional Gaussian lalent variable.
and ..
is a V·dimensi"nal processisillustrated in Figure 12.9.
NOIe that thisframe".-orl< isbasedon a mapping from latent , pace 10 data space.
in contrast 10 the nl()l'(: C(""'cnti,,,,"1 "iew "f I'CA dis.
cus"'d alx", e, 11Ie ",,'e= mapping, from data space tothe latent space.
,,-ill he OOlained , honly using Haycs· lhwn: m.
SUf! ll'OSC we wish 10deten"ine the "aluesofll>o parameters \V.
I' and ,,' u SIng maximum likelihuo<l, Towrite """"nlhe likeliltood function, we needan""pression for tl>o marginal distributioo p{") oftl>o~,,'ed ...
riahle_ This is exprt__sed.
fmn' the sum aod p, oduct rules"fprobability, in the form (11,34) ll""au S(: this corresponds to a linear·Gau"i, n l T1(ll Ic L thi< marginal di, tribulion is E, e,,-ise 12,7 again Gaussian.
atld isgiven by ,,(,,) _ N{xllf, C) (IUS) 12.2.
Probabilisticpe A 573 where the D x D covariance matrix C is defined by C = WWT + 0- 21.
(12.36) Thisresultcanalsobederivedmoredirectlybynotingthatthepredictivedistribution will be Gaussian and then evaluating its mean and covariance using (12.33).
This gives IE[x] IE[Wz + JL +E] = JL (12.37) cov[x] IE [(Wz+E)(WZ +E)T] IE [WZZTW T ] +IE[EE T ]= WW T+ 0- 21 (12.38) wherewehaveusedthefactthatzand Eareindependentrandomvariablesandhence are uncorrelated.
Intuitively, we can think ofthe distribution p(x) as being defined by taking an isotropic Gaussian 'spray can' and movingitacross theprincipal subspace spraying Gaussian ink with density determined by 0- 2 and weighted by the priordistribution.
Theaccumulatedinkdensity gives rise to a 'pancake' shapeddistributionrepresent ing the marginaldensityp(x).
The predictive distribution p(x) is governed by the parameters JL, W, and 0- 2 • However, there is redundancy in this parameterizationcorresponding to rotations of the latent space coordinates.
To see this, consider a matrix W = WR where R is an orthogonal matrix.
Using the orthogonality property RRT = I, we see that the quantity WWT thatappears in the covariancematrix C takes theform (12.39) and hence is independent of R.
Thus there is a whole family ofmatrices W all of whichgiverisetothesamepredictivedistribution.
Thisinvariancecanbeunderstood in terms ofrotations within the latent space.
We shall return to a discussion ofthe numberofindependentparametersin this model later.
When we evaluate the predictive distribution, we require C-1 , which involves theinversionofa D x D matrix.
Thecomputationrequiredtodothiscanbereduced bymaking use ofthe matrixinversionidentity (C.7) to give C-1 = 0--11- 0--2WM-1WT (12.40) where the M x M matrix M isdefinedby M = WTW + 0- 21.
(12.41) Becausewe invert M ratherthan inverting C directly, the costofevaluating C-1 is reducedfrom O(D3 ) to O(M3 ).
As well as the predictive distribution p(x), we will also require the posterior distributionp(zlx), whichcanagainbewrittendowndirectlyusingtheresult(2.116) Exercise 12.8 for linear-Gaussianmodels to give (12.42) Note that the posterior mean depends on x, whereas the posterior covariance is in dependentofx.
574 12.
CONTINUOUSLATENTVARIABLES Figure12.10 Theprobabilisticpe Amodelforadatasetof N obser vations ofx can be expressed as a directed graph in which eachobservation Xn isassociatedwithavalue Zn ofthelatentvariable.
..-+--w N 12.2.1 Maximum likelihood pe A We next consider the determination of the model parameters using maximum likelihood.
Given a data set X = {x n} of observed data points, the probabilistic pe A model can be expressed as a directed graph, as shown in Figure 12.10.
The correspondingloglikelihoodfunction isgiven, from (12.35), by N L Inp(XIJL, W, O'2 ) = lnp(xn IW, JL, O'2 ) n=l 1"" N -- N 2 D -ln(2n) - 2N ln Ie[ - 2L,..(x n - JL)Tc-1(x n - JL).
(12.43) n=l Setting the derivative ofthe log likelihood with respect to JL equal to zero gives the =x x expectedresult JL where isthedatameandefinedby(12.1).
Back-substituting wecanthenwritetheloglikelihoodfunction intheform N Inp(XIW, JL,0'2) = -2 {DIn(2n) +In Ie[ +Tr (C-1S)} (12.44) where S is the datacovariance matrix definedby (12.3).
Becausethe log likelihood isaquadraticfunction of JL, this solutionrepresents theuniquemaximum, ascan be confirmedbycomputing secondderivatives.
Maximization with respect to W and 0'2 is more complex but nonetheless has an exactclosed-form solution.
Itwas shownby Tipping and Bishop(1999b) that all ofthe stationarypoints ofthe loglikelihoodfunction can bewritten as (12.45) where UM is a D x M matrix whosecolumns are given by any subset(ofsize M) of the eigenvectors ofthe data covariance matrix S, the M x M diagonal matrix L has elements given by the corresponding eigenvalues ..\, and R is an arbitrary M M x M orthogonal matrix.
Furthermore, Tippingand Bishop(1999b)showedthatthemaximumofthelike lihood function is obtained when the M eigenvectors are chosen to be those whose eigenvalues are the M largest(all othersolutionsbeing saddlepoints).
Asimilarre sultwasconjectured independentlyby Roweis (1998), although no proofwasgiven.
12.2.
Probabilisticpe A 575 Again, weshallassumethattheeigenvectorshavebeenarrangedinorderofdecreas ingvaluesofthecorrespondingeigenvalues, sothatthe M principaleigenvectorsare Ul,"" UM.
In this case, the columns of W define the principal subspace ofstan dard PCA.
Thecorresponding maximumlikelihood solutionfor (J'2 is then givenby 1 D L (J'~L = D-M Ai (12.46) i=M+l so that (J'~L is the average varianceassociatedwiththe discardeddimensions.
Because R isorthogonal, itcanbeinterpretedas arotationmatrixinthe M x M latentspace.
Ifwesubstitutethesolutionfor W intotheexpressionfor C, andmake use ofthe orthogonality property RR T = I, we see that C is independent of R.
This simply says that the predictive density is unchanged by rotations in the latent spaceas discussedearlier.
Fortheparticularcaseof R = I, we seethatthe columns of W are the principal component eigenvectors scaled by the variance parameters Ai - (J'2.
The interpretation ofthese scaling factors is clear once we recognize that for aconvolutionofindependent Gaussiandistributions (inthis casethelatentspace distribution and the noise model) the variances are additive.
Thus the variance Ai in the direction ofaneigenvector Ui is composed ofthe sum ofa contribution Ai (J'2 from the projection ofthe unit-variance latent space distribution into data space through the corresponding column of W, plus an isotropic contribution ofvariance (J'2 whichis added in alldirections bythenoise model.
It is worth taking a moment to study the form of the covariance matrix given by (12.36).
Considerthe varianceofthepredictivedistributionalong somedirection specified by the unit vector v, where v Tv = 1, which is given by v TCv.
First suppose that v is orthogonal to the principal subspace, in otherwords itis given by some linear combination ofthe discarded eigenvectors.
Then v TV = 0 and hence v TCv = (J'2.
Thus the model predicts a noise variance orthogonal to the principal subspace, which, from (12.46), isjusttheaverageofthediscardedeigenvalues.
Now suppose that v = Ui where Ui is one ofthe retained eigenvectors defining the prin cipal subspace.
Then v TCv = (Ai - (J'2) + (J'2 = Ai.
In other words, this model correctlycapturesthevarianceofthedataalongtheprincipalaxes, andapproximates the varianceinall remainingdirections witha singleaveragevalue (J'2.
One way to construct the maximum likelihood density model would simply be to find the eigenvectors and eigenvalues ofthe data covariance matrix and then to evaluate Wand (J'2 using the results given above.
In this case, we would choose R = I for convenience.
However, ifthe maximum likelihood solution is found by numerical optimization ofthe likelihood function, for instance using an algorithm suchas conjugate gradients (Fletcher, 1987; Nocedal and Wright, 1999; Bishop and Section 12.2.2 Nabney, 2008) or through the EM algorithm, then the resulting value of R is es sentially arbitrary.
This implies that the columns of W need not be orthogonal.
If an orthogonal basis is required, the matrix W can be post-processed appropriately (Golub and Van Loan, 1996).
Alternatively, the EM algorithm can be modified in such a way as to yield orthonormal principal directions, sorted in descending order ofthe correspondingeigenvalues, directly (Ahn and Oh, 2003).
576 12.
CONTINUOUSLATENTVARIABLES Therotationalinvarianceinlatentspacerepresentsaformofstatisticalnoniden tifiability, analogous to that encountered for mixture models in the case ofdiscrete latent variables.
Here there is a continuum ofparameters all ofwhich lead to the samepredictivedensity, incontrasttothe discrete nonidentifiability associatedwith componentre-labellinginthemixturesetting.
Ifweconsiderthe caseof M = D, so that there is no reduction ofdimension ality, then UM = U and L M = L.
Making use of the orthogonality properties UUT = I and RRT = I, we seethatthecovariance C ofthemarginaldistribution forx becomes (12.47) and so we obtain the standard maximum likelihood solution for an unconstrained Gaussiandistributionin which thecovariancematrix isgiven by the samplecovari ance.
Conventional PCAisgenerallyformulated asaprojectionofpointsfromthe D dimensionaldataspaceontoan M -dimensionallinearsubspace.
Probabilistic PCA, however, ismostnaturallyexpressedasamappingfromthelatentspaceintothedata space via (12.33).
Forapplications such as visualization and data compression, we canreversethis mappingusing Bayes' theorem.
Anypointx indataspacecanthen be summarized by its posterior mean and covariance in latent space.
From (12.42) themeanisgivenby (12.48) where M isgivenby(12.41).
Thisprojectsto apointindataspacegivenby Wl E[zlx] + J-L.
(12.49) Section 3.3.1 Note thatthis takes the sameform as theequations for regularizedlinearregression and is a consequence ofmaximizing the likelihood function for a linear Gaussian model.
Similarly, the posterior covariance is given from (12.42) by 0-2M- 1 and is independentofx.
Ifwetakethe limit0- 2 ----t 0, thentheposteriormeanreduces to (12.50) which represents an orthogonal projection ofthe data point onto the latent space, Exercise 12.11 andsowerecoverthestandard PCAmodel.
Theposteriorcovarianceinthis limitis zero, however, and the density becomes singular.
For0- 2 > 0, the latent projection Exercise 12.12 is shiftedtowards theorigin, relativetotheorthogonalprojection.
Finally, we note that an important role for the probabilistic PCA model is in definingamultivariate Gaussiandistributioninwhichthenumberofdegreesoffree dom, inotherwordsthenumberofindependentparameters, canbecontrolledwhilst still allowing the model to capture the dominant correlations in the data.
Recall thata general Gaussiandistribution has D(D +1)/2 independentparameters in its Section 2.3 covariance matrix (plus another D parameters in its mean).
Thus the number of parameters scales quadratically with D andcan becomeexcessivein spaces ofhigh 12.2.
Probabilisticpe A 577 dimensionality.
Ifwerestrictthecovariancematrixtobediagonal, thenithasonly D independent parameters, and so the numberofparameters now grows linearly with dimensionality.
However, itnowtreats the variablesasifthey wereindependentand hencecan no longerexpress any correlationsbetweenthem.
Probabilistic Pe Apro vides an elegant compromise in which the M most significant correlations can be capturedwhilestillensuringthatthetotal numberofparametersgrowsonlylinearly with D.
We can see this by evaluating the number of degrees of freedom in the PPCA model as follows.
The covariance matrix C depends on the parameters W, whichhassize D x M, anda2 , givingatotalparametercountof DM+ 1.
However, we have seenthat there is someredundancy inthis parameterizationassociated with rotations ofthecoordinate systemin the latentspace.
Theorthogonalmatrix R that expressestheserotationshassize M x M.
Inthefirstcolumnofthismatrixthereare M - 1independent parameters, because the column vector must be normalized to unitlength.
Inthe secondcolumnthereare M - 2independentparameters, because thecolumnmustbenormalizedandalso mustbeorthogonaltothepreviouscolumn, andsoon.
Summingthisarithmeticseries, weseethat R hasatotalof M(M-1)/2 independent parameters.
Thus the number ofdegrees offreedom in the covariance matrix C is given by DM +1- M(M - 1)/2.
(12.51) The number ofindependent parameters in this model thereforeonly grows linearly with D, for fixed M.
Ifwe take M = D - 1, then we recover the standard result Exercise 12.14 for a full covariance Gaussian.
In this case, the variance along D - 1 linearly in dependentdirections is controlledby thecolumns of W, andthe variancealong the remainingdirectionisgivenbya2 .
If M = 0, themodelisequivalenttotheisotropic covariancecase.
12.2.2 EM algorithm for pe A As we have seen, the probabilistic PCA model can be expressed in terms ofa marginalization over a continuous latent space z in which for each data point Xn, there is a corresponding latent variable Zn.
We can therefore make use ofthe EM algorithm to find maximumlikelihoodestimatesofthe modelparameters.
This may seem rather pointless because we have already obtained an exact closed-form so lution for the maximum likelihood parameter values.
However, in spaces ofhigh dimensionality, there may be computational advantages in using an iterative EM procedureratherthan working directly with the samplecovariancematrix.
This EM Section 12.2.4 procedure can also be extended to the factor analysis model, for which there is no closed-form solution.
Finally, it allows missing data to be handled in a principled way.
Wecan derivethe EMalgorithmforprobabilistic PCAbyfollowing thegeneral Section 9.4 framework for EM.
Thus we write down the complete-data log likelihood and take its expectation with respect to the posterior distribution of the latent distribution evaluated using 'old' parameter values.
Maximization of this expected complete data log likelihood then yields the 'new' parameter values.
Because the datapoints 578 12.
CONTINUOUS LATENTVARIABLES areassumed independent, thecomplete-datalog likelihoodfunction takes theform N L Inp (X, ZIJL, W,(J2) = {lnp(xnlzn) + lnp(zn)} (12.52) n=l where the nth row ofthe matrix Z is given by Zn.
We already know that the exact maximumlikelihoodsolutionfor JLisgiven bythesamplemeanxdefinedby(12.1), and it is convenient to substitutefor JL at this stage.
Making use ofthe expressions (12.31)and(12.32)forthelatentandconditionaldistributions, respectively, andtak ingtheexpectationwithrespecttotheposteriordistributionoverthelatentvariables, weobtain Notethatthis depends ontheposteriordistributiononlythroughthesufficientstatis ticsofthe Gaussian.
Thus in the E step, we use theoldparametervalues toevaluate M-1WT(X n - x) (12.54) (J2M- 1 + l E[zn]l E[zn]T (12.55) which follow directly from the posteriordistribution (12.42) together with the stan dardresultl E[znz~] = cov[zn] + JE[zn]JE[zn]T.
Here M is defined by (12.41).
In the M step, we maximize with respect to Wand (J2, keeping the posterior statistics fixed.
Maximization with respect to is straightforward.
For the maxi (T2 Exercise 12.15 mization with respectto W we make useof(C.24), andobtainthe M-stepequations [t, exn -X)Il IZn]T] [t, Il[Zn Z~]]-' W new (12.56) 1 LN (Jn2ew = ND {llxn - xl12 - 2l E[zn]TW~ew(xn - x) n=l +Tr (JE[znz J]W~ew W new)}.
(12.57) The EMalgorithmforprobabilistic PCAproceeds byinitializingtheparameters and then alternately computing the sufficient statistics ofthe latent space posterior distributionusing(12.54)and(12.55)inthe Estepandrevisingtheparametervalues using (12.56) and (12.57) inthe M step.
One of the benefits of the EM algorithm for PCA is computational efficiency for large-scale applications (Roweis, 1998).
Unlike conventional PCA based on an 12.2.
Probabilisticpe A 579 eigenvector decomposition of the sample covariance matrix, the EM approach is iterative and so might appear to be less attractive.
However, each cycle ofthe EM algorithm can be computationally much more efficient than conventional PCA in spaces ofhigh dimensionality.
To see this, we note that the eigendecomposition of the covariance matrix requires O(D3 ) computation.
Often we are interested only in the first M eigenvectors and their corresponding eigenvalues, in which case we can use algorithms that are 0 (MD2 ).
However, the evaluation ofthe covariance matrix itselftakes 0 (ND2 ) computations, where N is the number of data points.
Algorithms such as the snapshot method (Sirovich, 1987), which assume that the eigenvectors are linear combinations ofthe data vectors, avoid direct evaluation of the covariance matrix butare O(N3 ) andhence unsuited to largedatasets.
The EM algorithm described here also does not construct the covariance matrix explicitly.
Instead, the most computationally demanding steps are those involving sums over « the datasetthatare 0 (NDM).
Forlarge D, and M D, this can be a significant savingcomparedto0 (ND2 ) andcanoffsettheiterativenatureofthe EMalgorithm.
Note that this EM algorithm can be implemented in an on-line form in which each D-dimensional data point is read in and processed and then discarded before the next data point is considered.
To see this, note that the quantities evaluated in the E step (an M-dimensional vector and an M x M matrix) can be computed for eachdatapoint separately, and in the M step we needto accumulate sums overdata points, which we can do incrementally.
This approach can be advantageous ifboth Nand D are large.
Because we now have a fully probabilistic model for PCA, we can deal with missing data, provided that it is missing at random, by marginalizing over the dis tribution of the unobserved variables.
Again these missing values can be treated using the EM algorithm.
We give an example ofthe use ofthis approach for data visualizationin Figure 12.11.
Anotherelegantfeatureofthe EMapproachisthatwecantakethelimita2 0, ----t correspondingto standard PCA, and still obtainavalid EM-like algorithm(Roweis, 1998).
From(12.55), we seethatthe onlyquantity weneedtocomputeinthe Estep is JE[zn].
Furthermore, the Mstepis simplifie~ because M = WTW.
Toemphasize the simplicity ofthe algorithm, letus define X to be amatrix ofsize N x D whose nth row is given by the vector Xn - x and similarly define 0 to be amatrix ofsize D x M whose nth row is given by the vector JE[zn].
The Estep (12.54) ofthe EM algorithmfor PCAthenbecomes o = (W~d Wold)-l W~d X (12.58) andthe M step (12.56) takes theform W = XTOT(OOT)-l.
(12.59) new Again these can be implementedin an on-line form.
These equations have asimple interpretationasfollows.
Fromourearlierdiscussion, weseethatthe Estepinvolves anorthogonalprojectionofthedatapointsontothecurrentestimatefortheprincipal subspace.
Correspondingly, the M step represents a re-estimation of the principal 580 12.
CONTl Nl JOl JS I"Ht; I'IT Vi\RIARl ES Fig"..12.11 Probabilistic PCAvisoo, zsbon01a portion0I1he""! lowdatasetlo<Ihe! irsl100(lata»einls, The lheonaobtainedwit"""lmiss....
val L>ll$ subspace to minimize ! he squared reoonslru Ctioo error in 'o Ihich the proje<: tion, are Ewrrise /2,/7 C., N.
We ean gh'e a , imple physical analogy for this EM algorithm.
which is easily visualized for D = 2 and M = 1.
Coo, ider a collectioo nf data point' , n t WI) dimension', aodlet tileu""'-dimensiunal principal subspacebe representedbya <ohd rod.
Nowatla Cheachdata point tothe nx I viaa , pringoo<:)"ing Hoo I;: e', law ("um J energy i, propol1ional 10 , lie square oflile spring".
length).
In ll1e E 'tel', we keep the nx I hed and allow the attachment point' tn , Iide up and <I<, wn ll1e nx I '" a, to minimizell1e e",,'ll Y, Thiscau",.
each attachment point (independently)10position Itselfat the orthogonal pmjeclion ofthe c~spondingdata point onto the nx I.
In the M'tel'.
wekeepthe attachment poi Ol' fil<ed andthen releasetilenx I and allowit to m'>,'e 10tile minimumenergyposilion.
11Ie Eand M 'teps are then repeated until 12.2.3 Bayesian pe A S<J far in OIlr di",""ioo of Pe A.
we have ",'.
nled Ihal tile '"Ine , II for , lie dl, nen, ionalit)" of tile principal .
ubspace is gi"en, In praclice.
".-e nlmt c OOose a suilable ,..
I"" according 10 the application.
For , isuali, a, ion.
we ge""", ny choose .\1 = 2.
whereas for OIher application, the approrrial C choice for ,1/ ma)" be less dea,.
One appmao: h i.
10 pi", the eigen"alue 'peclrum for lhe data set.
analog,•." 10 the example in Figure 12.4 for the off_line digits dala SCI, and look to see iflite eige",,, I....
nmurallyform twogroupscomprisingasetof, mall ,'aluesseparated by a , ign; flcant gap from a", tofrelativel)" large ,'alues, indicating a natural cholcc f<>r AI, In practice.
suchagapi, oflen ''''' seen , , , ", • ,<, • ...., ~, • 0 0 0 -, -, -, , , , -, -, 0 0 0 , , , ", o -, , , , -, -, -, o o o set X withthedata pointsshownin1JI'e«l, t"ll"t M'W'i1!1l!>e t'IMp Mdpal""""""", IS (shownaseigenveclor1 scaled by It>esquafll 'OOIS04theeige J'l\lllluel).
(b) Initialconfigurat"'"01tooprincipalsul>sl>a<:<tdefined by W, shown in md, t OO"lhf Ir withthe f K'(Ijeclions01the latll<11 points Z inlotoo<lataspace, gi Itoo by ZWT, shownin <') S! flp.
Afterl! Ie MC()<>; l Est"l' Be<: au,", th~ pm/xlhili>lic Pe A modd hasa well·defined likelillood f"fl Ction, we S, uion I.
J <w Idemploycros,-,-.1idation todelerminethe \"a Jueofdi"", nsiooa! ity by"'Iecting tit<: large, t log likelihood t>I1 a '-alidation data set Such an opprooch.
hov.·~\-er.
can become computationally ro<lly.
p3rticularl)' if we CQnsid<:, • probabilistic mi Xl Ure of Pe A modds (Tipping and Bishop.
1999a) in "hich we seek 10 <! etermi'" the appropriatedimwen., ionalily ", paraltly for tochcomponenl inlt1e mixm"" Gi'-en thai ha,-ea probabilislic formulalion of Pe A, ils«ms natural 10 s«k u Buye, ian approach 10 model seleclion.
To do thi,.
,,"'e nee<! 10 marginalize 001 the model paramele" /'.
\V.
und ,,' wilh ""peel to appropriate priordistribution'.
This Can be done by u, ing a ,-ariation.
l framework to .
pproxim'le the allulylic.
lly intractable murginali UOi; oo, (Bi, hop.
1mb).
1l Ic marginal likelihood v.
lues.
given '"Tue' by ttle ,'ari., ionallowerbour.
d, cun lhen bec<>mpun: d for ar.
ngeofdifferent "f;\I ar.
d Itie '"Iuegiving Iht largest marginal likelihood ", Iecloo_ l1ere we consider.
simpler approach introducoo by b.
ased on the rddmu"p- 582 12.
CONTINUOUSLATENTVARIABLES Figure 12.13 Probabilistic graphical model for Bayesian pe A in which the distribution overthe parameter matrix W is governed byavectora ofhyperparameters.
w N proximation, whichis appropriate whenthenumberofdatapoints is relativelylarge and the corresponding posterior distribution is tightly peaked (Bishop, 1999a).
It involves a specific choice of prior over W that allows surplus dimensions in the principal subspacetobeprunedoutofthemodel.
Thiscorrespondstoanexampleof automaticrelevancedetermination, or ARD, discussedin Section7.2.2.
Specifically, we define an independent Gaussian prior over each column of W, which represent the vectors defining theprincipal subspace.
Each such Gaussian has an independent variancegovernedbyaprecisionhyperparameter O: i sothat (12.60) where Wi is theith columnof W.
Theresulting modelcanberepresentedusingthe directedgraph shownin Figure 12.13.
The values for O: i will be found iteratively by maximizing the marginallikeli hoodfunction in which W has beenintegratedout.
As aresultofthis optimization, some of the O: i may be driven to infinity, with the corresponding parameters vec tor Wi being driven to zero (the posterior distribution becomes a delta function at the origin) giving a sparse solution.
The effective dimensionality of the principal subspace is then determined by the numberoffinite O: i values, and the correspond ing vectors Wi can be thought of as 'relevant' for modelling the data distribution.
In this way, the Bayesian approach is automatically making the trade-off between improving the fit to the data, by using a largernumberofvectors Wi with theircor responding eigenvalues Ai each tuned to the data, and reducing the complexity of the model by suppressing some ofthe Wi vectors.
The origins ofthis sparsity were Section 7.2 discussedearlierin the contextofrelevance vectormachines.
Thevaluesof O: iarere-estimatedduringtrainingbymaximizingthelogmarginal likelihoodgivenby J p(Xla, J-L,0'2) = p(XIW, J-L, O'2)p(Wla)d W (12.61) wherethelogofp(XIW, J-L,0'2) isgivenby(12.43).
Notethatforsimplicitywealso treat J-L and 0'2 as parameters to be estimated, rather than defining priors overthese parameters.
12.2.
Probabilisticpe A 583 Because this integrationis intractable, we make use ofthe Laplace approxima Section 4.4 tion.
Ifwe assume thatthe posteriordistribution is sharply peaked, as willoccurfor sufficientlylargedatasets, thenthere-estimationequations obtainedbymaximizing Section 3.5.3 the marginallikelihoodwithrespectto ai take the simpleform (12.62) which follows from (3.98), noting that the dimensionality of Wi is D.
These re estimations are interleaved with the EM algorithm updates for determining Wand a2 • The E-step equations are again given by (12.54) and (12.55).
Similarly, the M step equation for a2 is again given by (12.57).
The only change is to the M-step equationfor W, whichis modifiedto give (12.63) where A = diag(ai)' The valueof I-" is given bythe samplemean, as before.
Ifwe choose M = D - 1then, ifall ai values are finite, the model represents afull-covariance Gaussian, while ifall the ai go to infinity the model is equivalent to an isotropic Gaussian, and sothe modelcanencompass all pennissiblevalues for the effective dimensionality ofthe principal subspace.
Itis also possibleto consider smaller values of M, which will save on computational cost but which will limit the maximum dimensionality ofthe subspace.
A comparison ofthe results ofthis algorithm with standardprobabilistic PCAis shownin Figure 12.14.
Bayesian PCA provides an opportunity to illustrate the Gibbs sampling algo rithm discussed in Section 11.3.
Figure 12.15 shows an example of the samples from the hyperparameters Inai for adata setin D = 4dimensions in which the di mensionalityofthelatentspaceis M = 3butinwhichthedatasetisgeneratedfrom aprobabilistic PCAmodelhavingonedirectionofhighvariance, withtheremaining directions comprising low variance noise.
This result shows clearly the presence of threedistinctmodesintheposteriordistribution.
Ateachstepoftheiteration, oneof the hyperparameters has a small value and the remaining two have large values, so that two ofthe three latentvariables are suppressed.
Duringthe courseofthe Gibbs sampling, the solutionmakes sharptransitions betweenthe three modes.
The model described here involves a prior only over the matrix W.
A fully Bayesian treatment of PCA, including priors over 1-", a2 , and n, and solved us ing variational methods, is described in Bishop (1999b).
For a discussion ofvari ous Bayesian approaches to detennining the appropriate dimensionality for a PCA model, see Minka(2001c).
12.2.4 Factor analysis Factoranalysis is alinear-Gaussian latent variable model that is closelyrelated toprobabilistic PCA.
Itsdefinitiondiffersfromthatofprobabilistic PCAonlyinthat the conditional distribution ofthe observed variable x given the latent variable z is 584 12.
CONTINUOUS LATENT VARIABLES • • •• • • • • •• • • • • • • • • • • • • • • • • • •• • • • • • • • •• • •• • • • • • • • • • • • • • • Figure 12.14 'Hinloo' diagrams of the matrix W in which each element 01 the matrix is depicted as a square (white lor positive and black lor negative values) whose area is proportional to the magnitude of that element.
The synthetic data sel comprises 300 data points in D = 10dimensions sampled from a Gaussian distributionhaving standarddeviation 1.0 in 3directions and standard deviation 0.5 in the remaining 7directions fora data set in D = 10dimensions having AT = 3directions with largervariance than the remaining 7 directions.
Theleft-handplolshowstheresult Irommaximumlikelihoodprobabilistic PCA, and the left·handplot shows the corresponding resuft from Bayesian pe A.
We see how the Bayesianmodelisabletodiscovertheappropriatedimensionalitybysuppressingthe 6surplusdegreesoffreedom.
taken to have adiagonal ratherthan an isotropiccovariance sothat + p(xlz) = N(xl Wz 1'.\II) (12.64) where ill isa Dx Ddiagonal matrix.
Notethatthefactoranalysismodel, incommon withprobabilistic PCA.
assumesthattheobserved variables Xl, ...
, Xoare indepen dent.
given the latent variable z.
In essence.
the factor analysis model isexplaining the observed covariance structure ofthe data by representing the independent vari ance associated with each coordinate in the matrix 1J.' and capturing the covariance between variables in the matrix W.
In the factor analysis literature.
the columns of W.
which capture the correlations between observed variables.
are calledfacl Or loadings.
and the diagonal elements of 1J.'.
which represent the independent noise variances for each ofthe variables, arecalled llniqllenesses.
The origins of factor analysis are as old as those of PCA.
and discussions of factor analysis can be found in the books by Everitt (1984).
Bartholomew (1987), and Basilevsky (1994).
Links between factor analysis and PCA were investigated by Lilwley (1953) and Anderson (1963) who showed that at stationary points of the likelihood function.
for a facl Or analysis model with 1J.' = (121, the columns of W are scaled eigenvectors ofthe sample covariance matrix.
and (12 is the average of the discarded eigenvalues.
Later.
Tipping and Bishop (1999b) showed that the maximum of the log likelihood function occurs when the eigenvectors comprising Warechosen tobetheprincipal eigenvectors.
Making use of (2.115).
we see that the marginal distribution for the observed 12.2.
l'ru": ohilislk I'CA 585 Fll Iure12.15 Gillbs .,,,,>p! j"lllo< Bay<lslan PCA sh<Ming plots oj Ino, versus ~eralion number br three " values.
showing th"'" mo Ots <A ! he posterior distribution.
,-"riabl, i'gi,-, n by1'(x) ~N(Xlj', C) whe...
now C=WWT+'i'.
(12,6~) Eurr"e 12,19 Aswith probabilistic PCA, thi, mo MI is im-"ri.
rrl to'Olalionsin 11><0 latent 'pace.
Histoocally, factor anal)',; s has been lhe , ubjerl ofc Ol1tro,-ersy wroe" a! tempt< h",-e bttn "'a<k: toplace an intc'P"'t"lioo on the ind; vidual faclon (the c OOfdinates in z_space).
which h3.\ pm"en problematic due to lr.
e """i<lcmifiabilily of fact Of analysis associmed with Mation' in this 'pace.
Fromoorperspeoh-e, howe,-er.
we shall .
iew factor analysis as a form of lalent "ariable densily model.
in which the form of tl>c lalent 'pace i' of interest but n O! the particular choicc of coordinates usedtodescrit>c il.
Ifwe wishtoremove thedegeneracya'sociated with latent 'pace ro Iations.
""e mu, tcon'idernon-Gaussian latent ,-"riabledi'tribution,.
gi"irrg rise10 S"na" 12.4 independent component .
n.
lysi, (l CA)models.
muimum likelihood.
11..
solution for I' i' ag"in given by the ",,,, pie "'ean.
How· ey C'."nli~e probabili, tic l'CA.
lllcre i'nolongeraclosed-form maximum likelihood solution for \V.
",'hich mu.\ltherdorcbefound i'er.
li,'c1)'_ Becausefaclor anal)', i.
is alatentvariable mode L thi'can bedon.
usingan EM algorilhm (R.
bin and Thayer.
1982)! h"tis"nalogou, totheoneused(Ofpml>; lbili.
tie Pe A.
Specihcally.
lhe E-'lep eqn Jtioo, are g;'-en by T E[zo J = GW ",-'(xn- xl (12,66) E[z"z~J _ G +E[zo]E[z,,]T (1267) where ""e h",'e defi""d (l2,6H) NOieth"tthi'i'e.
pre,<edinafor'" thaiin,-oh'esinycrsi"nofmalrices"f Sil O ,\Ix, If rathe'lhan D x D (ex"",,,, for tbe D x D diagooal matrix o J' "'hose in,-erse i.' 'ri"ial 586 12.
CONTINUOUS LATENTVARIABLES « to compute in O(D) steps), which is convenientbecause often M D.
Similarly, Exercise 12.22 the M-stepequationstake theform - [~(x" [~Ill[Znz~I]-' wnew Xllll IZn]"] (12.69) {s- ~ ~1ll[Zn](Xn diag W.'w _ xl"} (12.70) wherethe 'diag' operatorsets allofthenondiagonal elementsofamatrixto zero.
A Bayesiantreatmentofthefactoranalysis modelcanbeobtainedbyastraightforward applicationofthe techniques discussedinthis book.
Anotherdifferencebetweenprobabilistic PCAandfactoranalysisconcernstheir Exercise 12.25 different behaviour under transformations ofthe data set.
For PCA and probabilis tic PCA, if we rotate the coordinate system in data space, then we obtain exactly the same fit to the data but with the W matrix transformed by the corresponding rotation matrix.
However, for factor analysis, the analogous property is that if we make a component-wise re-scaling ofthe data vectors, then this is absorbed into a correspondingre-scalingofthe elements of\)i.
12.3.
Kernel pe A In Chapter 6, we saw how the technique ofkernel substitution allows us to take an algorithm expressed in terms of scalar products of the form x Tx' and generalize that algorithm by replacing the scalar products with a nonlinear kernel.
Here we apply this technique ofkernel substitution to principal componentanalysis, thereby obtaining anonlineargeneralizationcalledkernelpe A (Scholkopfetal., 1998).
Consider a data set {x n} ofobservations, where n = 1,..., N, in a space of dimensionality D.
In order to keep the notation uncluttered, we shall assume that we have already subtracted the sample mean from each ofthe vectors Xn, so that Ln n X = O.
The first step is to express conventional PCA in such aform that the datavectors {x n} appearonly in theform ofthe scalarproducts x~ Xm.
Recall that theprincipalcomponentsaredefinedbytheeigenvectors Ui ofthecovariancematrix SUi = Ai Ui (12.71) where i = 1,...
, D.
Here the D x D samplecovariancematrix S is definedby (12.72) u T and theeigenvectors are normalized suchthat Ui = 1.
Now consider a nonlinear transformation ¢(x) into an M-dimensional feature space, so that each data point Xn is thereby projected onto a point ¢(x n).
We can 12.3.
K~mci l'Co'.
587 ' \ ...
Pf Ol Eled by' """'*-tranllklfmalion ~,,} 1n Io.
fa.
tur.
space (f Ight_ plot).
By I*b'~ PCA in the __ptrform)l-.
bnl Pe Aill fnlll Klop ICe.
..-Iudl, mp Iicl Ily«l Ii'Il'S• -'_ CO\-.
ull CC m Mf U , n (~ .
If*'e,~l , ""by .~.
.
L.
C - ,<>(x.
j4>(X.)T (12.73) and , l~",,,, n'"MOl"opan, ion i'«lined by Cv, = A, v, (12,74) lell'U, tha I Y, !-ali, fies " .
L.
s , <b C".) {<b(x.
l Tv,} - )"Y, (12.7~) , .
L..
v, '"' 11,.4>(".).
(12.76) 588 12.
CONTINUOUS LATENTVARIABLES Substituting thisexpansion back intotheeigenvectorequation, weobtain 1 N N N N L ¢(xn)¢(xn)T L aim¢(Xm)= Ai L ain¢(Xn), (12.77) n=l m=l n=l The key stepis now to expressthis in terms ofthe kernel function k(x n, x m) = ¢(Xn)T¢(x m), which wedobymultiplying both sides by ¢(x Z)T to give 1 N m N N Lk(XI'Xn) L aimk(Xn, xm)= Ai Laink(XI'Xn), (12.78) n=l m=l n=l This can bewritten inmatrix notationas (12.79) where ai is an N-dimensional column vector with elements ani for n = 1,...
, N.
Wecanfind solutionsfor ai by solvingthe following eigenvalueproblem (12.80) in which we have removed a factor of K from both sides of (12.79).
Note that the solutions of (12.79) and (12.80) differ only by eigenvectors of K having zero Exercise 12.26 eigenvalues thatdo not affecttheprincipal componentsprojection.
The normalizationconditionfor thecoefficients ai is obtained by requiring that theeigenvectors in feature spacebenormalized.
Using (12.76) and (12.80), wehave N N L 1= V; Vi = L ainaim¢(xn)T¢(xm) = a; K~ = Ai Na; ai' (12.81) n=lm=l Having solved theeigenvectorproblem, the resulting principal componentpro jections can then also be cast in terms ofthe kernel function so that, using (12.76), theprojectionofapointx ontoeigenvectori isgiven by N N L L Yi(X) = ¢(x)TVi = ain¢(x)T¢(xn) = aink(X, xn) (12.82) n=l n=l andso again isexpressedinterms ofthekernelfunction.
In the original D-dimensional x spacethere are D orthogonal eigenvectors and hence we can find at most D linear principal components.
The dimensionality M ofthe feature space, however, can be much larger than D (even infinite), and thus we can find a numberofnonlinear principal components that can exceed D.
Note, however, that the number of nonzero eigenvalues cannot exceed the number N of data points, because (even if M > N) the covariance matrix in feature space has rank at most equal to N.
This is reflected in the fact that kernel PCA involves the eigenvectorexpansionofthe N x N matrix K.
12.3.
Kernel PCA 589 So far we have assumed that the projected data set given by ¢(x n) has zero mean, which in general will not be the case.
We cannot simply compute and then subtractoffthe mean, since we wish to avoid working directly infeature space, and so again, we formulate the algorithm purely in-! erms ofthe kernel function.
The projecteddatapoints aftercentralizing, denoted ¢(x n), are givenby (12.83) andthe correspondingelementsofthe Grammatrix are givenby K nm = ¢(xn)T¢(x m ) 1 N ¢(xn)T¢(x N L ¢(xn)T¢(x Z) m ) - Z=l 1 N 1 N N - N L¢(XZ)T¢(X + N2 LL¢(Xj)T¢(x Z) m ) Z=l j=l Z=l 1 N k(xn, x m) - N L k(xz, x m) Z=l 1 N 1 N N - N Lk(xn, xz) + N2 LLk(Xj, Xl)' (12.84) Z=l j=l 1=1 This can beexpressedin matrix notationas (12.85) where IN denotes the N x N matrix in which every elementtakes the value l/N.
Thus wecanevaluate K~ using onlythekernelfunction andthenuse K~ to determine theeigenvaluesandeigenvectors.
Notethatthestandard PCAalgorithmisrecovered Exercise 12.27 as a special case ifwe use a linearkernel k(x, x') = x Tx/.
Figure 12.17 shows an exampleofkernel PCAappliedto asyntheticdataset(Scholkopfetal., 1998).
Here a 'Gaussian' kernel oftheform k(x, x') = exp(-llx - x/112/0.1) (12.86) is applied to a synthetic data set.
The lines correspond to contours along which the projectionontothecorrespondingprincipalcomponent, definedby N ¢(X? Vi = L aink(X, xn) (12.87) n=l isconstant.
_.
590 12.
CONTINUOUS LATENT VAl UABLES Figure 12.11 E"llmple01kernel PCA, witha Gaussiankernelaw Ii OO10asynthetic<latasatintwo<: Iirnensions, showing ! he first"l"flight eigenfunclions along w~h l!>eir e9tnvail Nls.
The oootours am lines along which ! he t"" ! he th"'" dusters.
! he ""'" ~ sp Ii I o Ilhe eluste, into ha Mo S.
and lol Iowing Ihree ~again sp I~! heduste," intohalvesalongdirectionsorthogonal10thopr EMou Ssplils, Oneobvioo'dls.
a Jmota~eof I: emel !'CA Isthafif invoh'esfinding lheelgen"e< tors ofthe N x N malri>: K ra W.
Ihan lhe D x D malri, Sofcor,..
emionallinear !'CA.
and! i OIn prac1lce for large data "'1' appro, lmation<are often u S(: d Finally.
""e OOIe that i"<tandard linear I'CA, weoften retain someredoce<lnum· ber L < Dofeigenvectors and then appro, lmale 0data vttl<:>r Xn b}' its projection i~ 0,,1"lhe L-dimensional principal subspace, defined by , i~-L:«",)"" (12.88) I" kernell'CA.
this will in gencr~1 not be fl O'slble, To see thl', OOIe Ihat the map ping 4'(x) maps the D-dimensional x space i"t" 0 D-dimensioo.
l manij Qi II in lhe M-dimemioo.
l femure space <1>.
Tl Ie: .'ector x i' koown a< lhe f'",.
imagr of lhe c","""ponding poi"l 4'(x).
However, fhe projec1ioo ofpoinl> in feature <J'3C" ""to the linear rc A , ub, p""" in that 'pachea.
w,.
ill typically"''' lie On fhe nonlinear D dimensional manifold and! i Owill nul ac"""", pondlng p",.
lmo~eind Ol Ospa<."C, Technlque<ho.-elhereforebttn proposedfor finding approximalepre-image<i B""lr Nat..
2(04).
12.4.
Nonlinear Latent Variable Models 591 12.4.
Nonlinear Latent Variable Models Inthis chapter, we havefocussed onthe simplestclass ofmodels havingcontinuous latent variables, namely those based on linear-Gaussian distributions.
As well as having great practical importance, these models are relatively easy to analyse and to fit to data and can also be used as components in more complex models.
Here weconsiderbriefly somegeneralizations ofthisframework to models thatareeither nonlinearornon-Gaussian, orboth.
In fact, the issues of nonlinearity and non-Gaussianity are related because a general probability density can be obtained from a simple fixed reference density, Exercise 12.28 such as a Gaussian, bymaking anonlinearchange ofvariables.
This ideaforms the basisofseveralpracticallatentvariable models as we shall see shortly.
12.4.1 Independent component analysis We begin by considering models in which the observed variables are related linearly to the latentvariables, butfor which the latentdistribution is non-Gaussian.
An important class of such models, known as independent component analysis, or le A, arises when we consider adistribution overthe latentvariables that factorizes, sothat M p(z) = IIp(Zj).
(12.89) j=l To understand the role of such models, consider a situation in which two people are talking at the same time, and we record their voices using two microphones.
If we ignore effects such as time delay and echoes, then the signals received by the microphones at any point in time will be given by linear combinations of the amplitudes of the two voices.
The coefficients of this linear combination will be constant, and if we can infer their values from sample data, then we can invert the mixing process (assuming it is nonsingular) and thereby obtain two clean signals eachofwhichcontainsthevoiceofjustoneperson.
Thisisanexampleofaproblem called blind source separation in which 'blind' refers to the fact that we are given only the mixeddata, andneitherthe original sources northe mixing coefficients are observed (Cardoso, 1998).
This type of problem is sometimes addressed using the following approach (Mac Kay, 2003) in which we ignorethe temporal nature ofthe signals and treat the successive samples as i.
i.
d.
We consider agenerative model in which there are two latentvariablescorrespondingtothe unobservedspeech signalamplitudes, andthere aretwoobservedvariablesgivenbythesignal values atthemicrophones.
Thelatent variableshaveajointdistributionthatfactorizes asabove, andtheobservedvariables are givenbyalinearcombinationofthelatentvariables.
Thereis no needtoinclude anoisedistribution becausethe numberoflatentvariables equalsthe numberofob served variables, and therefore the marginal distribution of the observed variables will not in general be singular, so the observed variables are simply deterministic linear combinations of the latent variables.
Given a data set of observations, the 592 12.
CONTINUOUS LATENTVARIABLES likelihoodfunction for this model isafunction ofthecoefficients in the linearcom bination.
The log likelihood can be maximized using gradient-based optimization giving rise to aparticularversion ofindependentcomponentanalysis.
Thesuccessofthisapproachrequiresthatthelatentvariableshavenon-Gaussian distributions.
To seethis, recall thatinprobabilistic PCA (andinfactor analysis) the latent-space distribution is given by a zero-mean isotropic Gaussian.
The model therefore cannot distinguish between two different choices for the latent variables where thesediffersimply by a rotation in latentspace.
This can be verifieddirectly by noting that the marginal density (12.35), and hence the likelihood function, is unchanged if we make the transformation W -) WR where R is an orthogonal matrixsatisfying RRT = I, becausethematrix C givenby(12.36)isitselfinvariant.
Extending the model to allow more general Gaussian latent distributions does not change this conclusion because, as we have seen, such a model is equivalent to the zero-meanisotropic Gaussianlatentvariable model.
Anotherway toseewhya Gaussianlatentvariabledistributioninalinearmodel is insufficient to find independent components is to note that the principal compo nentsrepresentarotationofthecoordinate systemindataspace suchas todiagonal izethe covariancematrix, sothatthe datadistributionin the newcoordinatesis then uncorrelated.
Although zero correlation is a necessary condition for independence Exercise 12.29 it is not, however, sufficient.
In practice, a common choice for the latent-variable distribution is given by 1 1 p(z) = --,.-----,- (12.90) J 7fcosh(zj) which has heavy tails compared to a Gaussian, reflecting the observation that many real-worlddistributions alsoexhibitthis property.
Theoriginal ICA model (Belland Sejnowski, 1995)was basedontheoptimiza tion ofan objective function defined by information maximization.
One advantage ofa probabilistic latent variable formulation is that it helps to motivate and formu late generalizations ofbasic ICA.
Forinstance, independentfactor analysis (Attias, 1999a)considers amodel in which the numberoflatent and observed variables can differ, the observed variablesare noisy, and the individuallatentvariables haveflex ible distributions modelled by mixtures of Gaussians.
The log likelihood for this model is maximized using EM, and the reconstruction ofthe latent variables is ap proximated using a variational approach.
Many other types of model have been considered, and there is now a huge literature on ICA and its applications (Jutten and Herault, 1991; Comon etat., 1991; Amari etat., 1996; Pearlmutter and Parra, 1997; Hyvarinen and Oja, 1997; Hinton et at., 2001; Miskin and Mac Kay, 2001; Hojen-Sorensen etat., 2002; Choudrey and Roberts, 2003; Chanetat., 2003; Stone, 2004).
12.4.2 Autoassociative neural networks In Chapter5 we considered neural networks in the contextofsupervised learn ing, where the role of the network is to predict the output variables given values 12.4.
Nonlinear Latent Variable Models 593 Figure12.18 An autoassociative m Ultilayer perceptron having twolayersofweights.
Suchanetworkistrainedto map input vectors onto themselves by minimiza tion ot a sum-ot-squares error.
Even with non linear units in the hidden layer, such a network is equivalent to linear principal component anal inputs outputs ysis.
Links representing bias parameters have been omittedforclarity.
for the input variables.
However, neural networks have also been applied to un supervised learning where they have been used for dimensionality reduction.
This is achieved by using a network having the same number ofoutputs as inputs, and optimizing the weights so as to minimize some measure ofthe reconstruction error betweeninputs andoutputs withrespectto asetoftrainingdata.
Consider first a multilayer perceptron ofthe form shown in Figure 12.18, hav ing D inputs, D output units and M hidden units, with M < D.
The targets used to train the network are simply the input vectors themselves, so that the network is attempting to map each input vector onto itself.
Such a network is said to form an autoassociative mapping.
Since the number ofhidden units is smaller than the numberofinputs, aperfectreconstruction ofall inputvectors is not in general pos sible.
We therefore determine the network parameters w by minimizing an error function whichcaptures the degreeofmismatch betweenthe inputvectors and their reconstructions.
In particular, we shallchooseasum-of-squares erroroftheform 1LN E(w) = "2 Ily(x n, w) - x n 112 • (12.91) n=l If the hidden units have linear activations functions, then it can be shown that the error function has a unique global minimum, and that at this minimum the network performsaprojectionontothe M-dimensionalsubspacewhichisspannedbythefirst M principal components ofthe data (Bourlard and Kamp, 1988; Baldi and Hornik, 1989).
Thus, the vectorsofweights whichleadintothe hidden units in Figure 12.18 form a basis set which spans the principal subspace.
Note, however, that these vec tors need not be orthogonal or normalized.
This result is unsurprising, since both principalcomponentanalysisandthe neuralnetworkareusinglineardimensionality reduction andare minimizing the same sum-of-squareserrorfunction.
Itmightbethoughtthatthelimitationsofalineardimensionalityreductioncould beovercomeby usingnonlinear(sigmoidal)activationfunctions forthehidden units in the networkin Figure 12.18.
However, evenwith nonlinearhiddenunits, the min imum error solution is again given by the projection onto the principal component subspace (Bourlardand Kamp, 1988).
Thereis thereforenoadvantageinusingtwo layerneural networks to performdimensionality reduction.
Standard techniquesfor principal component analysis (based on singular value decomposition) are guaran teed to give the correct solutionin finite time, and they also generate an ordered set ofeigenvalueswithcorrespondingorthonormaleigenvectors.
594 12.
CONTINUOUS LATENT VARIABLES Figure12.19 Addition of extra hidden lay F, • F, • ers of noolinear units gives an auloassocialive network which can performanoolineardimen siooalityreduction.
inputs outputs x, x, The situation is different, however.
if additional hidden layers are pcrmillcd in thenetwork.
Considerthefour-layerautoassociativcnetwork shown in Figure 12.19.
Again theoutput unitsare linear, and the M units in the second hidden layercan also be linear.
however, the first and third hidden layers have sigmoidal nonlinearactiva tion functions.
The network is again trained by minimization of the error function (12.91).
We can view this network as two successive functional mappings F] and F 2, as indicated in Figure 12.19.
The first mapping F] projects the original D dimensional data onto an AI-dimensional subspace S defined by the activations of the unitsin thesecond hidden layer.
Becauseofthepresenceofthe first hidden layer ofnonlinearunits.
this mapping is very general.
and in particular is not restricted to being linear.
Similarly.
thesecond halfofthe networkdefinesan arbitrary functional mapping from the M-dimensional space back intotheoriginal D-dimensional input space.
This has asimple geometrical interpretation.
as indicated for thecase D = 3 and M = 2in Figure 12.20.
Such a network effectively perfonns a nonlinear principal component analysis.
X3 " F, • x, " Figure12.20 Geometricalinterpretationofthemappingsperformedbythenetworkin Figure12.1gforthecase of 0 = 3inputs and AI = 2 units in the middle hidden layer.
The function F, maps from an M-dimensional space S intoa D-dimensiooalspaceandthereforedefinesthewayinwhichthespace S isembeddedwithinthe original x-space.
Since the mapping F, can be r"I()(llinear, the embedding 01S can be nonplanar, as indicated in the figure.
The mapping F.
then defines a projectiorl of points in the original D-dimensional space into the M-dimensional subspace S.
12.4.
Nonlinear Latent Variable Models 595 It has the advantage ofnot being limited to linear transformations, although itcon tains standard principal component analysis as a special case.
However, training the networknow involves anonlinearoptimizationproblem, sincetheerrorfunction (12.91) is no longer aquadratic function ofthe network parameters.
Computation allyintensivenonlinearoptimizationtechniquesmustbeused, andthereistheriskof finding a suboptimal local minimum ofthe errorfunction.
Also, the dimensionality ofthesubspace mustbespecifiedbeforetraining thenetwork.
12.4.3 Modelling nonlinear manifolds As we have already noted, many natural sources of data correspond to low dimensional, possibly noisy, nonlinear manifolds embedded within the higher di mensional observed data space.
Capturing this property explicitly can lead to im proved density modelling compared with more general methods.
Here we consider brieflyarange oftechniques thatattemptto do this.
One way to model the nonlinear structure is through a combination of linear models, sothatwemakeapiece-wiselinearapproximationtothemanifold.
Thiscan beobtained, for instance, by usingaclusteringtechniquesuchas K-meansbasedon Euclidean distance to partition the data set into local groups with standard PCA ap plied to each group.
Abetter approach is to use the reconstruction error for cluster assignment(Kambhatlaand Leen, 1997; Hintonetal., 1997)asthen acommoncost function is being optimized in each stage.
However, these approaches still suffer from limitations due to the absence of an overall density model.
By using prob abilistic PCA it is straightforward to define a fully probabilistic model simply by considering a mixture distribution in which the components are probabilistic PCA models (Tipping and Bishop, 1999a).
Such a model has both discrete latent vari ables, corresponding to the discrete mixture, as well as continuous latent variables, and the likelihood function can be maximized using the EM algorithm.
A fully Bayesian treatment, basedonvariationalinference(Bishop and Winn, 2000), allows the numberofcomponents in the mixture, as well as the effective dimensionalities ofthe individual models, to be inferred from the data.
There are many variants of this modelin whichparameters suchas the W matrixorthe noise variances are tied across components in the mixture, or in which the isotropic noise distributions are replacedby diagonalones, giving rise to amixture offactor analysers (Ghahramani and Hinton, 1996a; Ghahramani and Beal, 2000).
Themixture ofprobabilistic PCA models can also be extendedhierarchically to produce an interactive data visualiza tion algorithm (Bishop and Tipping, 1998).
An alternative to considering amixture oflinear models is to consider a single nonlinear model.
Recall that conventional PCA finds a linear subspace that passes close to the data in a least-squares sense.
This concept can be extended to one dimensional nonlinearsurfaces in theform ofprincipalcurves(Hastieand Stuetzle, 1989).
Wecandescribeacurveina D-dimensionaldataspaceusingavector-valued function f().), whichisavectoreachofwhoseelementsis afunction ofthescalar)..
There are many possible ways to parameterize the curve, ofwhich a natural choice x is the arc length along the curve.
For any given point in data space, we can find the pointon the curvethat is closestin Euclidean distance.
We denote this point by 596 12.
CONTINUOUSLATENTVARIABLES >..
= gf(X) because it depends on the particular curve f(>").
For a continuous data density p(x), a principal curve is defined as onefor which every pointon the curve is the meanofall thosepointsin dataspacethatprojectto it, sothat JE[Xlgf(X) = >..] = f(>").
(12.92) For a given continuous density, there can be many principal curves.
In practice, we are interested in finite data sets, and we also wish to restrict attention to smooth curves.
Hastieand Stuetzle(1989) proposea two-stageiterativeprocedurefor find ingsuch principalcurves, somewhatreminiscentofthe EM algorithmfor PCA.
The curve is initializedusing thefirst principalcomponent, and then the algorithm alter nates between a data projection step andcurve re-estimation step.
In the projection step, each data point is assigned to a value of >..
corresponding to the closest point on the curve.
Then in the re-estimation step, each point on the curve is given by a weighted average ofthose points that project to nearby points on the curve, with pointsclosestonthecurvegiventhegreatestweight.
Inthecasewherethesubspace is constrainedto be linear, the procedure converges to the first principal component and is equivalent to the powermethod for finding the largest eigenvectorofthe co variance matrix.
Principalcurvescan begeneralizedto multidimensional manifolds calledprincipalsurfaces although these have found limited use due to the difficulty ofdatasmoothingin higherdimensionsevenfor two-dimensional manifolds.
PCA is often used to project a data set onto a lower-dimensional space, for ex ample two dimensional, for the purposes ofvisualization.
Another lineartechnique withasimilaraimismultidimensionalscaling, or MDS(Coxand Cox,2000).
Itfinds alow-dimensional projection ofthe data such as to preserve, as closely as possible, the pairwise distances between data points, and involves finding the eigenvectors of thedistancematrix.
Inthecasewherethedistancesare Euclidean, itgivesequivalent results to PCA.
The MDS concept can be extended to a wide variety ofdata types specifiedin terms ofasimilarity matrix, givingnonmetric MDS.
Two other nonprobabilistic methods for dimensionality reduction and data vi sualization are worthy ofmention.
Locally linear embedding, or LLE (Roweis and Saul, 2000) first computes the set of coefficients that best reconstructs each data point from its neighbours.
These coefficients are arranged to be invariant to rota tions, translations, andscalings ofthatdatapointand its neighbours, andhencethey characterize the local geometrical properties ofthe neighbourhood.
LLEthen maps the high-dimensional datapoints down to a lowerdimensional space while preserv ing these neighbourhood coefficients.
If the local neighbourhood for a particular data point can be considered linear, then the transformation can be achieved using a combination of translation, rotation, and scaling, such as to preserve the angles formed between the data points and their neighbours.
Because the weights are in varianttothesetransformations, weexpectthesameweightvaluestoreconstructthe data points in the low-dimensional space as in the high-dimensional data space.
In spiteofthe nonlinearity, the optimizationfor LLEdoes notexhibitlocal minima.
In isometricfeature mapping, or isomap (Tenenbaum etai., 2000), the goal is to project the data to a lower-dimensional space using MDS, but where the dissim ilarities are defined in terms of the geodesic distances measured along the mani- 12.4.
Nonlinear Latent Variable Models 597 fold.
For instance, if two points lie on a circle, then the geodesic is the arc-length distance measured around the circumference of the circle not the straight line dis tance measured along the chord connecting them.
The algorithm first defines the neighbourhoodforeachdatapoint, eitherbyfinding the K nearest neighbours orby finding all points within a sphere ofradius E.
A graph is then constructed by link ing all neighbouring points and labelling them with their Euclidean distance.
The geodesic distance between any pair of points is then approximated by the sum of the arc lengths along the shortest path connecting them (which itselfis found using standardalgorithms).
Finally, metric MDSisappliedtothegeodesicdistancematrix tofind the low-dimensionalprojection.
Our focus in this chapter has been on models for which the observed vari ables are continuous.
We can also consider models having continuous latent vari ables together with discrete observed variables, giving rise to latent trait models (Bartholomew, 1987).
In this case, the marginalization over the continuous latent variables, even for alinearrelationship between latent and observed variables, can not be performed analytically, and so more sophisticated techniques are required.
Tipping (1999) uses variational inference in a model with a two-dimensional latent space, allowing a binary data set to be visualized analogously to the use of PCA to visualize continuous data.
Note that this model is the dual ofthe Bayesian logistic regression problem discussed in Section 4.5.
In the case of logistic regression we have N observations ofthe feature vector <l>n which are parameterized by a single parametervectorw, whereasinthe latentspacevisualization model thereis asingle latent space variable x (analogous to <1» and N copies ofthe latent variable Wn.
A generalization ofprobabilistic latent variable models to general exponential family distributions isdescribedin Collins etal.
(2002).
We have already noted that an arbitrary distribution can be formed by taking a Gaussian random variable and transforming it through a suitable nonlinearity.
This is exploited in a general latent variable model called a density network (Mac Kay, 1995; Mac Kay and Gibbs, 1999) in which the nonlinear function is governed by a multilayeredneural network.
Ifthe networkhasenough hidden units, itcan approx Chapter5 imate a given nonlinear function to any desired accuracy.
The downside ofhaving such aflexible modelisthatthemarginalizationoverthelatentvariables, requiredin order to obtain the likelihood function, is no longer analytically tractable.
Instead, Chapter JJ the likelihood is approximated using Monte Carlo techniques by drawing samples from the Gaussian prior.
Themarginalization overthe latentvariables then becomes a simple sum with one term for each sample.
However, because a large number ofsample points may be required in order to give an accurate representation ofthe marginal, this procedure canbecomputationally costly.
Ifweconsidermorerestrictedforms forthenonlinearfunction, andmakeanap propriatechoiceofthelatentvariabledistribution, thenwecanconstructalatentvari able model that is both nonlinear and efficient to train.
The generative topographic mapping, or GTM (Bishop et a I., 1996; Bishop et a I., 1997a; Bishop eta I., 1998b) usesalatentdistributionthatisdefinedbyafiniteregulargridofdeltafunctions over the (typically two-dimensional) latent space.
Marginalization over the latent space thensimplyinvolvessummingoverthecontributionsfromeachofthegridlocations.
598 12.
CONTINUOUS LATENT VAK1AHU': S .•.
'" Fll Iu.
e 12.21 ? lotottrleoillk Yw <: lata Wllisualiz.
ed using Pe Aontheleftand GTMon Itle ngr, t FOftile GTM model.
each<latapoin Ils plollfldattilemean otitspos M'k><dislributionin ..
tents;>ace, Tile"","ineantymlhe Ch"l'l~f j The no"liotar mapping is gi,'en by alinear regression model tha I allow, for general IIO/llinearily while beinga linear fuoction oftile adapli'-e parameler<, NOIe tha I tilt S~etioo /.4 usual limitationoflinearregression models arising from theen"", ofdimen, iooalily does1101ariseinthe Contr~1oflhe GT~Isi""'ethe "\3nifoldgenerall)' ha<t,,'odi"ltn· sions irrespecti'-e ofthe dimensionality ofthe data space, A coo""!",, nce of Illese 11"0 cooices is that the likelihood funclion can be e~pressedanalytically in dosed form and can be optimil C<.! efficiently o, ing the EM algorithm_ The resolting GTM model h Is a lwo-dimensional nonlinear manifold 10 tile dala sel.
and by e"alualing the posteriordistril J", lion (Wer latent space for the data poi"", theycan he projectt<J backtothe lalent'JI'K'"for.'isualilalionpurposes, Figure 12,21 sl"""sacomparison oftheoildata..,1"isualired wilh lincar Pe A andwilhlhe IIO/lhnear GT~I, '''If TIlt GTMcanbeseen asaprobabilistic"'rsionofanearliernl Od<lcall Mthe org"nidng""'p.
or SOM (Kohonen.
1982: Kobonen.
(995).
which also represents a Iwo-dimensiooal IIO/llincar manifoid as a regular array of disc"'le points.
The SOM i' somewhat remin;""'nt of the K·trl Can, algorithm in that data points are a., igr.
ed to nearby Prol Ol)'j>C '-eclon tha I are lhen subsc<juenlly updale<!.
Initially.
lhe pro IOI)'jl('S are distribuled at random, and during the training process they 'selr organize' soas toa Pl'ro~imalea smoolh manifold.
Unlike K-mean'.
how'e"e..
the difficult tos." the parameters ofthe model and 10assesscon'-ergence.
There i' also no guarantee that the '", If-<>rganilalion' will take place ..
this is depen""nl 00 the choice ofappropriateparanlttcr"alo C'f,,, any particulardatasel.
By OOf Itrast, GTMoptimize, the loglikelihood functioo, andthe resolting model define' a probabilily den, ity in dma , pace, In fae L il corre, ponds to a con, m, incd mi, ture of Gaussian, in which the component.' , h.
re a COnlnl On ".
riance.•nd the mean, are con'trained to lie on a 'moo Ih tw-o-di ITl Cn, iooal n1anifold.
This proba- Exercises 599 bilistic foundation also makes it very straightforward to define generalizations of GTM (Bishopetal., 1998a)suchas a Bayesiantreatment, dealing with missing val- Section 6.4 ues, a principled extension to discrete variables, the use of Gaussian processes to define the manifold, orahierarchical GTM model (Tinoand Nabney, 2002).
Because the manifoldin GTMis defined as acontinuous surface, notjustat the prototype vectors as in the SOM, it is possibleto computethe magnificationfactors corresponding to the local expansions and compressions ofthe manifold needed to fit the data set (Bishop et al., 1997b) as well as the directional curvatures of the manifold (Tino et al., 2001).
These can be visualized along with the projected data and provideadditionalinsightintothe model.
Exercises l IB 12.1 (**) In this exercise, we use proof by induction to show that the linear projectiononto an M -dimensional subspace thatmaximizes the varianceofthe pro jecteddata is defined by the M eigenvectors ofthe datacovariance matrix S, given by (12.3), corresponding to the M largest eigenvalues.
In Section 12.1, this result was proven for the case of M = 1.
Now suppose the result holds for some general value of M and show that it consequently holds for dimensionality M + 1.
To do this, first set the derivative of the variance of the projected data with respect to a vector UM+1 defining the new direction in data space equal to zero.
This should be done subject to the constraints that UM+l be orthogonal to the existing vectors U1,"" UM, and also that it be normalized to unit length.
Use Lagrange multipli- Appendix E ers to enforce these constraints.
Then make use ofthe orthonormality properties of the vectors U1,"" UM to show that the new vector UM+1 is an eigenvector of S.
Finally, show that the variance is maximized if the eigenvector is chosen to be the onecorrespondingto eigenvector AM+1 where theeigenvalues have been orderedin decreasing value.
12.2 (**) Show that the minimum value of the PCA distortion measure J given by (12.15) with respect to the Ui, subject to the orthonormality constraints (12.7), is obtained when the Ui are eigenvectors ofthe data covariance matrix S.
To do this, introduce a matrix H of Lagrange multipliers, one for each constraint, so that the modifieddistortionmeasure, in matrix notation reads ] = Tr{ UTSU}+ Tr{H(I - UTU)} (12.93) where U is a m~trix ofdimensio~D x (D - M) whose columns are gi:::..
en b~ Ui.
Now minimize J with respect to U and show that the s~ution satisfies SU = UH.
Clearly, one possible solution is that the columns of U are eigenvectors of S, in which case H is a diagonal matrix containing the corresponding eigenvalues.
To obtain the general solution, show that H can be assumed to be a symmetr~ ma~ix, andby using itseigenvect£rexpansion showthatthegeneral solutionto SU =~UH gives the same value for J as the specific solution in which the columns of U are 600 12.
CONTINUOUS LATENTVARIABLES the eigenvectors of S.
Because these solutions are all equivalent, it is convenientto choosetheeigenvectorsolution.
12.3 (*) Verify that the eigenvectors defined by (12.30) are normalized to unit length, assuming thattheeigenvectors Vi have unitlength.
Imm 12.4 (*) Suppose we replace the zero-mean, unit-covariance latent space distri bution (12.31) in the probabilistic PCAmodel by a general Gaussian distributionof theform N(zlm, ~).
Byredefiningtheparametersofthemodel, showthatthisleads to an identical model for the marginal distributionp(x) overthe observedvariables for any validchoice ofm and ~.
12.5 (**) Let x be a D-dimensional random variable having a Gaussian distribution given by N(x IJL, ~), and consider the M-dimensional random variable given by y = Ax +b where A is an M x D matrix.
Show that y also has a Gaussian distribution, and find expressions for its mean and covariance.
Discuss the form of this Gaussiandistributionfor M < D, for M = D, andfor M > D.
Imm 12.6 (*) Draw a directed probabilistic graph for the probabilistic PCA model described in Section 12.2 in which the components ofthe observed variable x are shown explicitly as separate nodes.
Hence verify that the probabilistic PCA model has the same independence structure as the naive Bayes model discussed in Sec tion 8.2.2.
12.7 (**) Bymaking use ofthe results (2.270) and(2.271) forthe mean andcovariance ofa general distribution, derive the result (12.35) for the marginal distributionp(x) inthe probabilistic PCAmodel.
Imm 12.8 (**) Bymakinguseoftheresult(2.116), showthattheposteriordistribution p(zlx) fortheprobabilistic PCAmodelisgivenby (12.42).
12.9 (*) Verify that maximizing the log likelihood (12.43) for the probabilistic PCA model with respect to the parameter JL gives the result JLML = x where x is the mean ofthe datavectors.
12.10 (**) Byevaluatingthe secondderivativesoftheloglikelihoodfunction (12.43)for theprobabilistic PCAmodelwithrespecttotheparameter JL, showthatthestationary point JLML = x represents the unique maximum.
Imm 12.11 (**) Showthatinthe limit(Y2 -.
0, theposteriormeanfortheprobabilistic PCA model becomes an orthogonal projection onto the principal subspace, as in conventional PCA.
12.12 (**) For (Y2 > 0 show that the posterior mean in the probabilistic PCA model is shiftedtowards the originrelativeto the orthogonalprojection.
12.13 (**) Showthatthe optimalreconstruction ofadatapointunderprobabilistic PCA, according to the leastsquaresprojectioncostofconventional PCA, isgivenby (12.94) Exercises 601 12.14 (*) The numberofindependent parameters in the covariance matrixfor the proba bilistic PCA model with an M -dimensional latent space and a D-dimensional data space is given by (12.51).
Verify that in the case of M = D - 1, the number of indepe°ndentparametersis the sameasinageneralcovariance Gaussian, whereasfor M = itis the same as for a Gaussian withan isotropic covariance.
IIi I! I 12.15 (**) Derivethe M-step equations (12.56) and (12.57) for the probabilistic PCA model bymaximization ofthe expectedcomplete-datalog likelihood function givenby (12.53).
12.16 (***) In Figure 12.11, weshowedanapplicationofprobabilistic PCAto adataset in which someofthe datavalues were missing atrandom.
Derivethe EMalgorithm for maximizingthe likelihoodfunction for the probabilistic PCAmodel in this situ ation.
Note thatthe {zn}, as well as the missing data values that are components of the vectors {x n}, arenow latentvariables.
Showthatinthespecialcaseinwhichall ofthe data values are observed, this reduces to the EM algorithm for probabilistic PCAderivedin Section 12.2.2.
IIi I! I 12.17 (**) Let W be a D x M matrix whose columns define a linear subspace ofdimensionality M embedded within adata space ofdimensionality D, and let J1 be a D-dimensional vector.
Given a data set {x n} where n = 1,...
, N, we can approximate the data points using a linear mapping from a set of M -dimensional vectors {zn}, so that Xn is approximated by WZn + J1.
The associated sum-of squaresreconstructioncostis givenby N L J = Ilx n - J1- Wz n 112 .
(12.95) n=l Firstshowthatminimizing J withrespectto J1leadstoananalogousexpressionwith Xnand Zn replacedbyzero-meanvariables Xn- xand Zn - Z, respectively, wherex and Zdenotesamplemeans.
Thenshowthatminimizing J withrespectto Zn, where W is kept fixed, gives rise to the PCA Estep (12.58), and that minimizing J with respectto W, where {zn} is keptfixed, gives riseto the PCAM step (12.59).
12.18 (*) Derive an expression for the number ofindependent parameters in the factor analysis modeldescribedin Section 12.2.4.
IIi I! I 12.19 (**) Show that the factor analysis model described in Section 12.2.4 is invariantunderrotations ofthe latent spacecoordinates.
12.20 (**) By considering second derivatives, show that the only stationary point of the loglikelihoodfunction for the factor analysis modeldiscussedin Section 12.2.4 with respect to the parameter J1 is given by the sample mean defined by (12.1).
Furthermore, show thatthis stationarypointis amaximum.
12.21 (**) Derive the formulae (12.66) and (12.67) for the E step ofthe EM algorithm for factor analysis.
Notethatfrom theresultof Exercise 12.20, theparameter J1 can bereplacedbythe samplemeanx.
602 12.
CONTINUOUS LATENTVARIABLES 12.22 (**) Writedownanexpressionfortheexpectedcomplete-dataloglikelihoodfunc tionfor the factor analysis model, and hencederive the corresponding Mstepequa tions (12.69) and (12.70).
III! I 12.23 (*) Draw a directed probabilistic graphical model representing a discrete mixture ofprobabilistic PCA models in which each PCA model has its own values of W, JL, and 0- 2 • Now draw a modified graph in which these parametervalues are sharedbetweenthecomponents ofthe mixture.
12.24 (***) We saw in Section 2.3.7 that Student's t-distribution can be viewed as an infinite mixture of Gaussians in which we marginalize with respect to a continu ous latent variable.
By exploiting this representation, formulate an EM algorithm formaximizing theloglikelihoodfunction for amultivariate Student'st-distribution givenan observedsetofdatapoints, andderive theforms ofthe Eand Mstepequa tions.
III! I 12.25 (**) Consideralinear-Gaussianlatent-variablemodel havingalatentspace distribution p(z) = N(x IO, I) and a conditional distribution for the observed vari able p(xlz) = N(xl Wz + IL, <p) where <P is an arbitrary symmetric, positive definite noise covariance matrix.
Now suppose that we make a nonsingular linear transformation of the data variables x ---t Ax, where A is a D x D matrix.
If JLML' WML and <PML represent the maximum likelihood solution corresponding to the originaluntransformeddata, showthat AJLML' AWML, and A<PMLA T willrep resentthecorresponding maximumlikelihood solutionfor the transformeddata set.
Finally, showthattheformofthemodelispreservedintwocases: (i)A isadiagonal matrix and <P is adiagonal matrix.
This corresponds to the case offactor analysis.
The transformed <P remains diagonal, and hence factor analysis is covariant under component-wise re-scaling ofthe data variables; (ii) A is orthogonal and <P is pro portionalto the unitmatrix sothat <P = 0- 21.
Thiscorrespondstoprobabilistic PCA.
Thetransformed <P matrixremains proportionaltothe unitmatrix, andhenceproba bilistic PCAis covariantunderarotationofthe axes ofdata space, as is the casefor conventional PCA.
\ 12.26 (**) Show that any vector ai that satisfies (12.80) will also satisfy (12.79).
Also, show thatfor any solution of(12.80) having eigenvalue A, we can add any multiple of an eigenvector of K having zero eigenvalue, and obtain a solution to (12.79) that also has eigenvalue A.
Finally, show that such modifications do not affect the principal-componentprojection given by (12.82).
12.27 (**) Showthattheconventionallinear PCAalgorithmisrecoveredasaspecialcase ofkernel PCAifwechoosethe linearkernelfunction given by k(x, x') = x Tx'.
III! I 12.28 (**) Use the transformation property (1.27) ofaprobability density under a change of variable to show that any density p(y) can be obtained from a fixed density q(x) that is everywhere nonzero by making a nonlinear change ofvariable y = f(x) in which f(x) is a monotonic function so that 0 :::; j'(x) < 00.
Write down the differential equation satisfiedby f(x) and draw adiagram illustrating the transformationofthe density.
Exercises 603 Em 12.29 (**) Supposethattwovariables Zl and Z2 areindependentsothatp(zl'Z2) = P(Zl)P(Z2)' Show that the covariance matrix between these variables is diagonal.
This shows that independence is a sufficient condition for two variables to be un correlated.
Now consider two variables Yl and Y2 in which -1 :0;; Yl :0;; 1 and Y2 = yg.
Write down the conditional distribution p(Y2IYl) and observe that this is dependent on Yb showing that the two variables are not independent.
Now show thatthecovariancematrix betweenthese two variables isagaindiagonal.
Todo this, use the relation P(Yl, Y2) = P(YI)p(Y2IYl) to show that the off-diagonal terms are zero.
This counter-example shows that zero correlation is not a sufficient condition for independence.
13 Sequential Data So far in this book, we have focussed primarily on sets of data points that were as- sumedtobeindependentandidenticallydistributed(i.
i.
d.).
Thisassumptionallowed ustoexpressthelikelihoodfunctionastheproductoveralldatapointsoftheprob- ability distribution evaluated at each data point.
For many applications, however, the i.
i.
d.
assumption will be a poor one.
Here we consider a particularly important classofsuchdatasets, namelythosethatdescribesequentialdata.
Theseoftenarise throughmeasurementoftimeseries, forexampletherainfallmeasurementsonsuc- cessivedaysataparticularlocation, orthedailyvaluesofacurrencyexchangerate, or the acoustic features at successive time frames used for speech recognition.
An example involving speech data is shown in Figure 13.1.
Sequential data can also ariseincontextsotherthantimeseries, forexamplethesequenceofnucleotidebase pairs along a strand of DNA or the sequence of characters in an English sentence.
For convenience, we shall sometimes refer to ‘past’ and ‘future’ observations in a sequence.
However, themodelsexploredinthischapterareequallyapplicabletoall 605 606 13.
SEQUENTIALDATA Figure13.1 Example of a spectro- gramofthespokenwords“Bayes’theo- rem”showingaplotoftheintensityofthe spectralcoefficientsversustimeindex.
formsofsequentialdata, notjusttemporalsequences.
It is useful to distinguish between stationary and nonstationary sequential dis- tributions.
Inthestationarycase, thedataevolvesintime, butthedistributionfrom whichitisgeneratedremainsthesame.
For themorecomplexnonstationarysitua- tion, thegenerativedistributionitselfisevolvingwithtime.
Hereweshallfocuson thestationarycase.
Formanyapplications, suchasfinancialforecasting, wewishtobeabletopre- dict the next value in a time series given observations of the previous values.
In- tuitively, we expect that recent observations are likely to be more informative than morehistoricalobservationsinpredictingfuturevalues.
Theexamplein Figure13.1 shows that successive observations of the speech spectrum are indeed highly cor- related.
Furthermore, it would be impractical to consider a general dependence of future observations on all previous observations because the complexity of such a modelwouldgrowwithoutlimitasthenumberofobservationsincreases.
Thisleads ustoconsider Markovmodelsinwhichweassumethatfuturepredictionsareinde- 13.1.
Markov Models 607 Figure13.2 The simplest approach to modelling a sequence of ob- servations is to treat them x1 x2 x3 x4 as independent, correspond- ingtoagraphwithoutlinks.
pendentofallbutthemostrecentobservations.
Althoughsuchmodelsaretractable, theyarealsoseverelylimited.
Wecanob- tainamoregeneralframework, whilestillretainingtractability, bytheintroduction oflatentvariables, leadingtostatespacemodels.
Asin Chapters9and12, weshall see that complex models can thereby be constructed from simpler components (in particular, from distributions belonging to the exponential family) and can be read- ily characterized using the framework of probabilistic graphical models.
Here we focus on the two most important examples of state space models, namely the hid- den Markov model, in which the latent variables are discrete, andlinear dynamical systems, inwhichthelatentvariablesare Gaussian.
Bothmodelsaredescribedbydi- rectedgraphshavingatreestructure(noloops)forwhichinferencecanbeperformed efficientlyusingthesum-productalgorithm.
13.1.
Markov Models The easiest way to treat sequential data would be simply to ignore the sequential Suchanapproach, however, wouldfailtoexploitthesequentialpatternsinthedata, such as correlations between observations that are close in the sequence.
Suppose, forinstance, thatweobserveabinaryvariabledenotingwhetheronaparticularday itrainedornot.
Givenatimeseriesofrecentobservationsofthisvariable, wewish topredictwhetheritwillrainonthenextday.
Ifwetreatthedataasi.
i.
d., thenthe onlyinformationwecangleanfromthedataistherelativefrequencyofrainydays.
However, weknowinpracticethattheweatheroftenexhibitstrendsthatmaylastfor severaldays.
Observingwhetherornotitrainstodayisthereforeofsignificanthelp inpredictingifitwillraintomorrow.
To express such effects in a probabilistic model, we need to relax the i.
i.
d.
as- sumption, and one of the simplest ways to do this is to consider a Markov model.
First of all we note that, without loss of generality, we can use the product rule to expressthejointdistributionforasequenceofobservationsintheform N n=1 If we now assume that each of the conditional distributions on the right-hand side is independent of all previous observations except the most recent, we obtain the first-order Markovchain, whichisdepictedasagraphicalmodelin Figure13.3.
The 608 13.
SEQUENTIALDATA Figure13.3 A first-order Markov chain of ob- s tr e ib r u va tio tio n n p s (x {x n | n x } n i − n 1 w ) h o i f c a h p th a e rt d ic i u s- - x1 x2 x3 x4 lar observation x n is conditioned on the value of the previous ob- servationx n−1.
jointdistributionforasequenceof N observationsunderthismodelisgivenby N n=2 Section8.2 Fromthed-separationproperty, weseethattheconditionaldistributionforobserva- tionxn, givenalloftheobservationsuptotimen, isgivenby p(xn |x 1 ,..., xn−1 )=p(xn |xn−1 ) (13.3) whichiseasilyverifiedbydirectevaluationstartingfrom(13.2)andusingtheprod- Exercise 13.1 uctruleofprobability.
Thusifweusesuchamodeltopredictthenextobservation inasequence, thedistributionofpredictionswilldependonlyonthevalueoftheim- mediatelyprecedingobservationandwillbeindependentofallearlierobservations.
In most applications of such models, the conditional distributions p(xn |xn−1 ) thatdefinethemodelwillbeconstrainedtobeequal, correspondingtotheassump- tionofastationarytimeseries.
Themodelisthenknownasahomogeneous Markov chain.
Forinstance, iftheconditionaldistributionsdependonadjustableparameters (whose values might be inferred from a set of training data), then all of the condi- tionaldistributionsinthechainwillsharethesamevaluesofthoseparameters.
Although this is more general than the independence model, it is still very re- strictive.
Formanysequentialobservations, weanticipatethatthetrendsinthedata over several successive observations will provide important information in predict- ingthenextvalue.
Onewaytoallowearlierobservationstohaveaninfluenceisto movetohigher-order Markovchains.
Ifweallowthepredictionstodependalsoon theprevious-but-onevalue, weobtainasecond-order Markovchain, representedby thegraphin Figure13.4.
Thejointdistributionisnowgivenby N n=3 Again, usingd-separationorbydirectevaluation, weseethattheconditionaldistri- bution of xn given xn−1 and xn−2 is independent of all observations x 1 ,...
xn−3.
Figure13.4 Asecond-order Markovchain, in whichtheconditionaldistribution of a particular observation x n dependsonthevaluesofthetwo previousobservationsx n−1 and x1 x2 x3 x4 x n−2.
13.1.
Markov Models 609 Figure13.5 We can represent sequen- z1 z2 zn−1 zn zn+1 tial data using a Markov chain of latent variables, with each observation condi- tioned on the state of the corresponding latent variable.
This important graphical structureformsthefoundationbothforthe x1 x2 xn−1 xn xn+1 hidden Markov model and for linear dy- namicalsystems.
Eachobservationisnowinfluencedbytwopreviousobservations.
Wecansimilarly consider extensions to an Mth order Markov chain in which the conditional distri- butionforaparticular variabledependsontheprevious M variables.
However, we have paid a price for this increased flexibility because the number of parameters in themodelisnowmuchlarger.
Supposetheobservationsarediscretevariableshav- ing K states.
Then theconditional distributionp(xn |xn−1 ) in afirst-order Markov chainwillbespecifiedbyasetof K−1parametersforeachofthe K statesofxn−1 giving a total of K(K −1) parameters.
Now suppose we extend the model to an Mth order Markovchain, sothatthejointdistributionisbuiltupfromconditionals p(xn |xn−M,..., xn−1 ).
If the variables are discrete, and if the conditional distri- butions are represented by general conditional probability tables, then the number of parameters in such a model will have KM−1(K −1) parameters.
Because this growsexponentiallywith M, itwilloftenrenderthisapproachimpracticalforlarger valuesof M.
For continuous variables, we can use linear-Gaussian conditional distributions in which each node has a Gaussian distribution whose mean is a linear function of its parents.
This is known as an autoregressive or AR model (Box et al., 1994; Thiesson et al., 2004).
An alternative approach is to use a parametric model for p(xn |xn−M,..., xn−1 ) such as a neural network.
This technique is sometimes called a tapped delay line because it corresponds to storing (delaying) the previous M values of the observed variable in order to predict the next value.
The number ofparameterscanthenbemuchsmallerthaninacompletelygeneralmodel(forex- ample it may grow linearly with M), although this is achieved at the expense of a restrictedfamilyofconditionaldistributions.
Suppose we wish to build a model for sequences that is not limited by the Markovassumptiontoanyorderandyetthatcanbespecifiedusingalimitednumber offreeparameters.
Wecanachievethisbyintroducingadditionallatentvariablesto permitarichclassofmodelstobeconstructedoutofsimplecomponents, aswedid withmixturedistributionsin Chapter9andwithcontinuouslatentvariablemodelsin Chapter 12.
For each observation xn, we introduce a corresponding latent variable zn (whichmaybeofdifferenttypeordimensionalitytotheobservedvariable).
We nowassumethatitisthelatentvariablesthatforma Markovchain, givingrisetothe graphicalstructureknownasastatespacemodel, whichisshownin Figure13.5.
It satisfiesthekeyconditionalindependencepropertythatzn−1andzn+1areindepen- dentgivenzn, sothat zn+1 ⊥⊥zn−1 |zn.
(13.5) 610 13.
SEQUENTIALDATA Thejointdistributionforthismodelisgivenby N N n=2 n=1 Using the d-separation criterion, we see that there is always a path connecting any twoobservedvariablesxnandxmviathelatentvariables, andthatthispathisnever blocked.
Thusthepredictivedistributionp(xn+1 |x 1 ,..., xn)forobservationxn+1 givenallpreviousobservationsdoesnotexhibitanyconditionalindependenceprop- erties, and so our predictions for xn+1 depends on all previous observations.
The observed variables, however, do not satisfy the Markov property at any order.
We shalldiscusshowtoevaluatethepredictivedistributioninlatersectionsofthischap- ter.
There are two important models for sequential data that are described by this graph.
Ifthelatentvariablesarediscrete, thenweobtainthehidden Markovmodel, Section13.2 or HMM (Elliott et al., 1995).
Note that the observed variables in an HMM may bediscreteorcontinuous, andavarietyofdifferentconditionaldistributionscanbe usedtomodelthem.
Ifboththelatentandtheobservedvariablesare Gaussian(with alinear-Gaussiandependenceoftheconditionaldistributionsontheirparents), then Section13.3 weobtainthelineardynamicalsystem.
13.2.
Hidden Markov Models The hidden Markov model can be viewed as a specific instance of the state space model of Figure 13.5 in which the latent variables are discrete.
However, if we examine a single time slice of the model, we see that it corresponds to a mixture distribution, with component densities given by p(x|z).
It can therefore also be interpretedasanextensionofamixturemodelinwhichthechoiceofmixturecom- ponentforeachobservationisnotselectedindependentlybutdependsonthechoice of component for the previous observation.
The HMM is widely used in speech recognition (Jelinek, 1997; Rabiner and Juang, 1993), natural language modelling (Manning and Schu¨tze, 1999), on-line handwriting recognition (Nag et al., 1986), andfortheanalysisofbiologicalsequencessuchasproteinsand DNA(Kroghetal., 1994; Durbinetal.,1998; Baldiand Brunak,2001).
Asinthecaseofastandardmixturemodel, thelatentvariablesarethediscrete multinomialvariableszn describingwhichcomponentofthemixtureisresponsible for generating the corresponding observation xn.
Again, it is convenient to use a 1-of-K codingscheme, asusedformixturemodelsin Chapter9.
Wenowallowthe probability distribution of zn to depend on the state of the previous latent variable zn−1throughaconditionaldistributionp(zn |zn−1 ).
Becausethelatentvariablesare K-dimensionalbinaryvariables, thisconditionaldistributioncorrespondstoatable of numbers that we denote by A, the elements of which are known as transition probabilities.
They are given by Ajk ≡ p(znk = 1|zn−1, j = 1), and because they areprobabilities, theysatisfy0 Ajk 1with k Ajk = 1, sothatthematrix A 13.2.
Hidden Markov Models 611 Figure13.6 Transitiondiagramshowingamodelwhosela- A22 tentvariableshavethreepossiblestatescorre- sponding to the three boxes.
The black lines denote the elements of the transition matrix A21 A jk.
A12 k =2 A32 A23 k =1 A11 k =3 A31 A13 A33 has K(K−1)independentparameters.
Wecanthenwritetheconditionaldistribution explicitlyintheform K K p(zn |zn−1, A )= A z jk n−1, j z nk.
(13.7) k=1j=1 Theinitiallatentnodez 1 isspecialinthatitdoesnothaveaparentnode, andso it has a marginal distribution p(z 1 ) represented by a vector of probabilities π with elementsπk ≡p(z 1k =1), sothat K p(z 1 |π)= π k z 1k (13.8) k=1 where k πk =1.
Thetransitionmatrixissometimesillustrateddiagrammaticallybydrawingthe statesasnodesinastatetransitiondiagramasshownin Figure13.6forthecaseof K = 3.
Note that this does not represent a probabilistic graphical model, because thenodesarenotseparatevariablesbutratherstatesofasinglevariable, andsowe haveshownthestatesasboxesratherthancircles.
It is sometimes useful to take a state transition diagram, of the kind shown in Figure13.6, andunfold itovertime.
Thisgivesanalternativerepresentation of the Section8.4.5 transitionsbetweenlatentstates, knownasalatticeortrellisdiagram, andwhichis shownforthecaseofthehidden Markovmodelin Figure13.7.
The specification of the probabilistic model is completed by defining the con- ditionaldistributionsoftheobservedvariablesp(xn |zn,φ), whereφisasetofpa- rametersgoverningthedistribution.
Theseareknownasemissionprobabilities, and mightforexamplebegivenby Gaussiansoftheform(9.11)iftheelementsofxare continuous variables, or by conditional probability tables if x is discrete.
Because xn is observed, the distribution p(xn |zn,φ) consists, for a given value of φ, of a vectorof K numberscorrespondingtothe K possiblestatesofthebinaryvectorzn.
612 13.
SEQUENTIALDATA Figure13.7 Ifweunfoldthestatetransitiondia- gramof Figure13.6overtime, weobtainalattice, A11 A11 A11 ortrellis, representationofthelatentstates.
Each k =1 columnofthisdiagramcorrespondstooneofthe latentvariablesz n.
k =2 k =3 A33 A33 A33 n−2 n−1 n n+1 Wecanrepresenttheemissionprobabilitiesintheform K p(xn |zn,φ)= p(xn |φ k ) z nk.
(13.9) k=1 We shall focuss attention on homogeneous models for which all of the condi- tionaldistributionsgoverningthelatentvariablessharethesameparameters A, and similarlyalloftheemissiondistributionssharethesameparametersφ(theextension tomoregeneralcasesisstraightforward).
Notethatamixturemodelforani.
i.
d.
data setcorrespondstothespecialcaseinwhichtheparameters Ajk arethesameforall valuesofj, sothattheconditionaldistributionp(zn |zn−1 )isindependentofzn−1.
This corresponds to deleting the horizontal links in the graphical model shown in Figure13.5.
Thejointprobabilitydistributionoverbothlatentandobservedvariablesisthen givenby N N p(X, Z|θ)=p(z 1 |π) p(zn |zn−1 , A) p(xm |zm,φ) (13.10) n=2 m=1 where X = {x 1 ,..., x N }, Z = {z 1 ,..., z N }, andθ = {π, A,φ}denotestheset of parameters governing the model.
Most of our discussion of the hidden Markov model will be independent of the particular choice of the emission probabilities.
Indeed, the model is tractable for a wide range of emission distributions including discrete tables, Gaussians, and mixtures of Gaussians.
It is also possible to exploit Exercise 13.4 discriminative models such as neural networks.
These can be used to model the emission density p(x|z) directly, or to provide a representation for p(z|x) that can beconvertedintotherequiredemissiondensityp(x|z)using Bayes’theorem(Bishop etal.,2004).
Wecangainabetterunderstandingofthehidden Markovmodelbyconsidering itfromagenerativepointofview.
Recallthattogeneratesamplesfromamixtureof 13.2.
Hidden Markov Models 613 1 1 0.5 0.5 k=1 k=3 k=2 0 0 0 0.5 1 0 0.5 1 Figure 13.8 Illustration of sampling from a hidden Markov model having a 3-state latent variable z and a Gaussianemissionmodelp(x|z)wherexis2-dimensional.
(a)Contoursofconstantprobabilitydensityforthe emissiondistributionscorrespondingtoeachofthethreestatesofthelatentvariable.
(b)Asampleof50points drawnfromthehidden Markovmodel, colourcodedaccordingtothecomponentthatgeneratedthemandwith linesconnectingthesuccessiveobservations.
Herethetransitionmatrixwasfixedsothatinanystatethereisa 5%probabilityofmakingatransitiontoeachoftheotherstates, andconsequentlya90%probabilityofremaining inthesamestate.
Gaussians, wefirstchoseoneofthecomponentsatrandomwithprobabilitygivenby themixingcoefficientsπk andthengenerateasamplevectorxfromthecorrespond- ing Gaussiancomponent.
Thisprocessisrepeated N timestogenerateadatasetof N independentsamples.
Inthecaseofthehidden Markovmodel, thisprocedureis modified as follows.
We first choose the initial latent variablez 1 with probabilities governed by the parameters πk and then sample the corresponding observation x 1.
Now wechoose thestate of thevariable z 2 according to thetransition probabilities p(z 2 |z 1 )usingthealreadyinstantiatedvalueofz 1.
Thussupposethatthesamplefor z 1 corresponds to state j.
Then we choose the state k of z 2 with probabilities Ajk fork = 1,..., K.
Onceweknowz 2 wecandrawasampleforx 2 andalsosample the next latent variable z 3 and so on.
This is an example of ancestral sampling for Section8.1.2 a directed graphical model.
If, for instance, we have a model in which the diago- nal transition elements Akk are much larger than the off-diagonal elements, then a typicaldatasequencewillhavelongrunsofpointsgeneratedfromasinglecompo- nent, withinfrequenttransitionsfromonecomponenttoanother.
Thegenerationof samplesfromahidden Markovmodelisillustratedin Figure13.8.
Therearemanyvariantsofthestandard HMMmodel, obtainedforinstanceby imposingconstraintsontheformofthetransitionmatrix A(Rabiner,1989).
Herewe mentiononeofparticularpracticalimportancecalledtheleft-to-right HMM, which is obtained by setting the elements Ajk of A to zero if k < j, as illustrated in the 614 13.
SEQUENTIALDATA Figure13.9 Exampleofthestatetransitiondiagramfora3-state A11 A22 A33 left-to-righthidden Markovmodel.
Notethatoncea statehasbeenvacated, itcannotlaterbere-entered.
A12 A23 k =1 k =2 k =3 A13 statetransitiondiagramfora3-state HMMin Figure13.9.
Typicallyforsuchmodels theinitialstateprobabilitiesforp(z 1 )aremodifiedsothatp(z 11 )=1andp(z 1j)=0 forj = 1, inotherwordseverysequenceisconstrainedtostartinstatej = 1.
The transitionmatrixmaybefurtherconstrainedtoensurethatlargechangesinthestate indexdonotoccur, sothat Ajk = 0ifk > j +∆.
Thistypeofmodelisillustrated usingalatticediagramin Figure13.10.
Many applications of hidden Markov models, for example speech recognition, oron-linecharacterrecognition, makeuseofleft-to-rightarchitectures.
Asanillus- trationoftheleft-to-righthidden Markovmodel, weconsideranexampleinvolving handwritten digits.
This uses on-line data, meaning that each digit is represented by the trajectory of the pen as a function of time in the form of a sequence of pen coordinates, in contrast to the off-line digits data, discussed in Appendix A, which comprisesstatictwo-dimensionalpixellatedimagesoftheink.
Examplesoftheon- line digits are shown in Figure 13.11.
Here we train a hidden Markov model on a subset of data comprising 45 examples of the digit ‘2’.
There are K = 16 states, eachofwhichcangeneratealinesegmentoffixedlengthhavingoneof16possible angles, and so the emission distribution is simply a 16 × 16 table of probabilities associatedwiththeallowedanglevaluesforeachstateindexvalue.
Transitionprob- abilities are all set to zero except for those that keep the state index k the same or thatincrementitby1, andthemodelparametersareoptimizedusing25iterationsof EM.
Wecangainsomeinsightintotheresultingmodelbyrunningitgeneratively, as shownin Figure13.11.
Figure13.10 Lattice diagram for a 3-state left- to-right HMMinwhichthestateindexkisallowed A11 A11 A11 toincreasebyatmost1ateachtransition.
k =1 k =2 k =3 A33 A33 A33 n−2 n−1 n n+1 13.2.
Hidden Markov Models 615 Figure13.11 Toprow: examplesofon-linehandwritten digits.
Bottomrow: syntheticdigitssam- pled generatively from a left-to-right hid- den Markovmodelthathasbeentrained onadatasetof45handwrittendigits.
Oneofthemostpowerfulpropertiesofhidden Markovmodelsistheirabilityto exhibit some degree of invariance to local warping (compression and stretching) of thetimeaxis.
Tounderstandthis, considerthewayinwhichthedigit‘2’iswritten in the on-line handwritten digits example.
A typical digit comprises two distinct sectionsjoinedatacusp.
Thefirstpartofthedigit, whichstartsatthetopleft, hasa sweepingarcdowntothecusporloopatthebottomleft, followedbyasecondmore- or-lessstraightsweependingatthebottomright.
Naturalvariationsinwritingstyle willcausetherelativesizesofthetwosectionstovary, andhencethelocationofthe cusporloopwithinthetemporalsequencewillvary.
Fromagenerativeperspective suchvariationscanbeaccommodatedbythehidden Markovmodelthroughchanges inthenumberoftransitionstothesamestateversusthenumberoftransitionstothe successivestate.
Note, however, thatifadigit‘2’iswritteninthereverseorder, that is, startingatthebottomrightandendingatthetopleft, theneventhoughthepentip coordinatesmaybeidenticaltoanexamplefromthetrainingset, theprobabilityof theobservationsunderthemodelwillbeextremelysmall.
Inthespeechrecognition context, warpingofthetimeaxisisassociatedwithnaturalvariationsinthespeedof speech, andagainthehidden Markovmodelcanaccommodatesuchadistortionand notpenalizeittooheavily.
13.2.1 Maximum likelihood for the HMM Ifwehaveobservedadataset X={x 1 ,..., x N }, wecandeterminetheparam- eters of an HMM using maximum likelihood.
The likelihood function is obtained fromthejointdistribution(13.10)bymarginalizingoverthelatentvariables p(X|θ)= p(X, Z|θ).
(13.11) Z Becausethejointdistributionp(X, Z|θ)doesnotfactorizeovern(incontrasttothe mixture distribution considered in Chapter 9), we cannot simply treat each of the summationsoverzn independently.
Norcanweperformthesummationsexplicitly because there are N variables to be summed over, each of which has K states, re- sulting in a total of KN terms.
Thus the number of terms in the summation grows 616 13.
SEQUENTIALDATA exponentially with the length of the chain.
In fact, the summation in (13.11) cor- responds to summing over exponentially many paths through the lattice diagram in Figure13.7.
Wehavealreadyencounteredasimilardifficultywhenweconsideredtheinfer- ence problem for the simple chain of variables in Figure 8.32.
There we were able tomakeuseoftheconditionalindependencepropertiesofthegraphtore-orderthe summations in order to obtain an algorithm whose cost scales linearly, instead of exponentially, withthelengthofthechain.
Weshallapplyasimilartechniquetothe hidden Markovmodel.
Afurtherdifficultywiththeexpression(13.11)forthelikelihoodfunctionisthat, because it corresponds to a generalization of a mixture distribution, it represents a summation over the emission models for different settings of the latent variables.
Direct maximization of the likelihood function will therefore lead to complex ex- Section9.2 pressionswithnoclosed-formsolutions, aswasthecaseforsimplemixturemodels (recallthatamixturemodelfori.
i.
d.
dataisaspecialcaseofthe HMM).
Wethereforeturntotheexpectationmaximizationalgorithmtofindanefficient framework for maximizing the likelihood function in hidden Markov models.
The EMalgorithmstartswithsomeinitialselectionforthemodelparameters, whichwe denotebyθold .
Inthe Estep, wetaketheseparametervaluesandfindtheposterior distribution of the latent variables p(Z|X,θold ).
We then use this posterior distri- bution to evaluate the expectation of the logarithm of the complete-data likelihood function, as a function of the parameters θ, to give the function Q(θ,θold ) defined by Q(θ,θold )= p(Z|X,θold )lnp(X, Z|θ).
(13.12) Z At this point, it is convenient to introduce some notation.
We shall use γ(zn) to denotethemarginalposteriordistributionofalatentvariablezn, andξ(zn−1 , zn)to denotethejointposteriordistributionoftwosuccessivelatentvariables, sothat γ(zn) = p(zn |X,θold ) (13.13) ξ(zn−1 , zn) = p(zn−1 , zn |X,θold ).
(13.14) For each value of n, we can store γ(zn) using a set of K nonnegative numbers thatsumtounity, andsimilarlywecanstoreξ(zn−1 , zn)usinga K ×K matrixof nonnegativenumbersthatagainsumtounity.
Weshallalsouseγ(znk)todenotethe conditionalprobabilityofznk = 1, withasimilaruseofnotationforξ(zn−1, j, znk) and for other probabilistic variables introduced later.
Because the expectation of a binaryrandomvariableisjusttheprobabilitythatittakesthevalue1, wehave γ(znk) = E[znk]= γ(z)znk (13.15) z ξ(zn−1, j, znk) = E[zn−1, jznk]= γ(z)zn−1, jznk.
(13.16) z If we substitute the joint distribution p(X, Z|θ) given by (13.10) into (13.12), 13.2.
Hidden Markov Models 617 andmakeuseofthedefinitionsofγ andξ , weobtain K N K K Q(θ,θold ) = γ(z 1k)lnπk+ ξ(zn−1, j, znk)ln Ajk k=1 n=2j=1 k=1 N K + γ(znk)lnp(xn |φ k ).
(13.17) n=1k=1 Thegoalofthe Estepwillbetoevaluatethequantitiesγ(zn)andξ(zn−1 , zn)effi- ciently, andweshalldiscussthisindetailshortly.
In the M step, we maximize Q(θ,θold ) with respect to the parameters θ = {π, A,φ}inwhichwetreatγ(zn)andξ(zn−1 , zn)asconstant.
Maximizationwith respect to π and A is easily achieved using appropriate Lagrange multipliers with Exercise 13.5 theresults γ(z 1k) πk = (13.18) K γ(z 1j) j=1 N ξ(zn−1, j, znk) n=2 Ajk = .
(13.19) K N ξ(zn−1, j, znl) l=1 n=2 The EMalgorithmmustbeinitializedbychoosingstartingvaluesforπand A, which shouldofcourserespectthesummationconstraintsassociatedwiththeirprobabilis- ticinterpretation.
Notethatanyelementsofπ or Athataresettozeroinitiallywill Exercise 13.6 remain zero in subsequent EM updates.
A typical initialization procedure would involveselectingrandomstartingvaluesfortheseparameterssubjecttothesumma- tion and non-negativity constraints.
Note that no particular modification to the EM resultsarerequiredforthecaseofleft-to-rightmodelsbeyondchoosinginitialvalues fortheelements Ajk inwhichtheappropriateelementsaresettozero, becausethese willremainzerothroughout.
To maximize Q(θ,θold ) with respect to φ , we notice that only the final term k in (13.17) depends on φ , and furthermore this term has exactly the same form as k the data-dependent term in the corresponding function for a standard mixture dis- tribution for i.
i.
d.
data, as can be seen by comparison with (9.40) for the case of a Gaussianmixture.
Herethequantitiesγ(znk)areplayingtheroleoftheresponsibil- ities.
If the parameters φ are independent for the different components, then this k term decouples into a sum of terms one for each value of k, each of which can be maximizedindependently.
Wearethensimplymaximizingtheweightedloglikeli- hoodfunctionfortheemissiondensityp(x|φ k )withweightsγ(znk).
Hereweshall suppose that this maximization can be done efficiently.
For instance, in the case of 618 13.
SEQUENTIALDATA Gaussian emission densities we have p(x|φ k ) = N(x|µ k ,Σk), and maximization ofthefunction Q(θ,θold )thengives N γ(znk)xn µ = n=1 (13.20) k N γ(znk) n=1 N γ(znk)(xn −µ k )(xn −µ k )T n=1 Σk = .
(13.21) N γ(znk) n=1 Forthecaseofdiscretemultinomialobservedvariables, theconditionaldistribution oftheobservationstakestheform D K p(x|z)= µ x i z k (13.22) ik i=1k=1 Exercise 13.8 andthecorresponding M-stepequationsaregivenby N γ(znk)xni n=1 µik = .
(13.23) N γ(znk) n=1 Ananalogousresultholdsfor Bernoulliobservedvariables.
The EMalgorithmrequiresinitialvaluesfortheparametersoftheemissiondis- tribution.
One way to set these is first to treat the data initially as i.
i.
d.
and fit the emission density by maximum likelihood, and then use the resulting values to ini- tializetheparametersfor EM.
13.2.2 The forward-backward algorithm Next we seek an efficient procedure for evaluating the quantities γ(znk) and ξ(zn−1, j, znk), correspondingtothe Estepofthe EMalgorithm.
Thegraphforthe hidden Markov model, shown in Figure 13.5, is a tree, and so we know that the posteriordistributionofthelatentvariablescanbeobtainedefficientlyusingatwo- Section8.4 stage message passing algorithm.
In the particular context of the hidden Markov model, this is known as the forward-backward algorithm (Rabiner, 1989), or the Baum-Welchalgorithm(Baum,1972).
Thereareinfactseveralvariantsofthebasic algorithm, allofwhichleadtotheexactmarginals, accordingtothepreciseformof 13.2.
Hidden Markov Models 619 themessagesthatarepropagatedalongthechain(Jordan,2007).
Weshallfocuson themostwidelyusedofthese, knownasthealpha-betaalgorithm.
As well as being of great practical importance in its own right, the forward- backward algorithm provides us with a nice illustration of many of the concepts introduced in earlier chapters.
We shall therefore begin in this section with a ‘con- ventional’ derivation of the forward-backward equations, making use of the sum andproductrulesofprobability, andexploitingconditionalindependenceproperties which we shall obtain from the corresponding graphical model using d-separation.
Then in Section 13.2.3, we shall see how the forward-backward algorithm can be obtainedverysimplyasaspecificexampleofthesum-productalgorithmintroduced in Section8.4.4.
Itisworthemphasizingthatevaluationoftheposteriordistributionsofthelatent variables is independent of the form of the emission density p(x|z) or indeed of whether the observed variables are continuous or discrete.
All we require is the values of the quantities p(xn |zn) for each value of zn for every n.
Also, in this sectionandthenextweshallomittheexplicitdependenceonthemodelparameters θold becausethesefixedthroughout.
We therefore begin by writing down the following conditional independence properties(Jordan,2007) p(X|zn) = p(x 1 ,..., xn |zn) p(xn+1 ,..., x N |zn) (13.24) p(X|zn−1 , zn) = p(x 1 ,..., xn−1 |zn−1 ) p(xn |zn)p(xn+1 ,..., x N |zn) (13.29) p(x N+1 |X, z N+1 ) = p(x N+1 |z N+1 ) (13.30) p(z N+1 |z N, X) = p(z N+1 |z N) (13.31) Forinstanceinthefirstoftheseresults, wenotethateverypathfromanyoneofthe nodesx 1 ,..., xn−1 tothenodexn passesthroughthenodezn, whichisobserved.
Becauseallsuchpathsarehead-to-tail, itfollowsthattheconditionalindependence property must hold.
The reader should take a few moments to verify each of these properties in turn, as an exercise in the application of d-separation.
These relations can also be proved directly, though with significantly greater effort, from the joint distributionforthehidden Markovmodelusingthesumandproductrulesofproba- Exercise 13.10 bility.
Let us begin by evaluating γ(znk).
Recall that for a discrete multinomial ran- dom variable the expected value of one of its components is just the probability of that component having the value 1.
Thus we are interested in finding the posterior 620 13.
SEQUENTIALDATA represents a vector of length K whose entries correspond to the expected values of znk.
Using Bayes’theorem, wehave γ(zn)=p(zn |X)= p(X|zn)p(zn) .
(13.32) p(X) Note that the denominator p(X) is implicitly conditioned on the parameters θold of the HMM and hence represents the likelihood function.
Using the conditional independence property (13.24), together with the product rule of probability, we obtain p(x 1 γ(zn)= = (13.33) p(X) p(X) wherewehavedefined α(zn) ≡ p(x 1 ,..., xn, zn) (13.34) The quantity α(zn) represents the joint probability of observing all of the given data up to time n and the value of zn, whereas β(zn) represents the conditional probabilityofallfuturedatafromtimen+1upto N giventhevalueofzn.
Again, α(zn) and β(zn) each represent set of K numbers, one for each of the possible settingsofthe1-of-K codedbinaryvectorzn.
Weshallusethenotationα(znk)to denotethevalueofα(zn)whenznk =1, withananalogousinterpretationofβ(znk).
We now derive recursion relations that allow α(zn) and β(zn) to be evaluated efficiently.
Again, we shall make use of conditional independence properties, in particular(13.25)and(13.26), togetherwiththesumandproductrules, allowingus toexpressα(zn)intermsofα(zn−1 )asfollows α(zn) = p(x 1 ,..., xn, zn) = p(x 1 ,..., xn |zn)p(zn) = p(xn |zn)p(x 1 ,..., xn−1 |zn)p(zn) = p(xn |zn)p (x 1 ,..., xn−1 , zn) = p(xn |zn) p(x 1 ,..., xn−1 , zn−1 , zn) z n−1 = p(xn |zn) p(x 1 ,..., xn−1 , zn |zn−1 )p(zn−1 ) z n−1 = p(xn |zn) p(x 1 ,..., xn−1 |zn−1 )p(zn |zn−1 )p(zn−1 ) z n−1 = p(xn |zn) p(x 1 ,..., xn−1 , zn−1 )p(zn |zn−1 ) zn−1 Makinguseofthedefinition(13.34)forα(zn), wethenobtain α(zn)=p(xn |zn) α(zn−1 )p(zn |zn−1 ).
(13.36) zn−1 13.2.
Hidden Markov Models 621 Figure13.12 Illustration of the forward recursion (13.36) for α(zn−1,1) α(zn,1) evaluation of the α variables.
In this fragment A11 of the lattice, we see that the quantity α(z n1 ) k =1 is obtained by taking the elements α(z n−1, j ) of α w ( e z ig n h − t 1 s ) g a i t v s e t n ep by n− A j 1 1 a , n c d or s re u s m p m on in d g in t g he t m ot u h p e w va ith l- A21 p(xn |zn,1) ues of p(z n |z n−1 ), and then multiplying by the α(zn−1,2) datacontributionp(x n |z n1 ).
k =2 A31 α(zn−1,3) k =3 n−1 n It is worth taking a moment to study this recursion relation in some detail.
Note thatthereare K termsinthesummation, andtheright-handsidehastobeevaluated for each of the K values of zn so each step of the α recursion has computational costthatscaledlike O(K2).
Theforwardrecursionequationforα(zn)isillustrated usingalatticediagramin Figure13.12.
Inordertostartthisrecursion, weneedaninitialconditionthatisgivenby K α(z 1 )=p(x 1 , z 1 )=p(z 1 )p(x 1 |z 1 )= {πkp(x 1 |φ k )}z 1k (13.37) k=1 whichtellsusthatα(z 1k), fork = 1,..., K, takesthevalueπkp(x 1 |φ k ).
Starting atthefirstnodeofthechain, wecanthenworkalongthechainandevaluateα(zn) foreverylatentnode.
Becauseeachstepoftherecursioninvolvesmultiplyingbya K ×K matrix, theoverallcostofevaluatingthesequantitiesforthewholechainis of O(K2N).
We can similarly find a recursion relation for the quantities β(zn) by making useoftheconditionalindependenceproperties(13.27)and(13.28)giving β(zn) = p (xn+1 ,..., x N |zn) = p(xn+1 ,..., x N, zn+1 |zn) z n+1 = p(xn+1 ,..., x N |zn, zn+1 )p(zn+1 |zn) z n+1 = p(xn+1 ,..., x N |zn+1 )p(zn+1 |zn) z n+1 = p(xn+2 ,..., x N |zn+1 )p(xn+1 |zn+1 )p(zn+1 |zn).
zn+1 622 13.
SEQUENTIALDATA Figure13.13 Illustration of the backward recursion β(zn,1) β(zn+1,1) (13.38)forevaluationoftheβ variables.
In A11 thisfragmentofthelattice, weseethatthe k =1 quantity β(z n1 ) is obtained by taking the components β(z n+1, k ) of β(z n+1 ) at step A12 p(xn |zn+1,1) n+1 and summing them up with weights given by the products of A 1k, correspond- β(zn+1,2) ingtothevaluesofp(z n+1 |z n )andthecor- r p e ( s x p n o |z n n d + in 1 g , k ) v .
alues of the emission density k =2 A13 p(xn |zn+1,2) β(zn+1,3) k =3 n n+1 p(xn |zn+1,3) Makinguseofthedefinition(13.35)forβ(zn), wethenobtain β(zn)= β(zn+1 )p(xn+1 |zn+1 )p(zn+1 |zn).
(13.38) zn+1 Notethatinthiscasewehaveabackwardmessagepassingalgorithmthatevaluates β(zn)intermsofβ(zn+1 ).
Ateachstep, weabsorbtheeffectofobservationxn+1 through the emission probability p(xn+1 |zn+1 ), multiply by the transition matrix p(zn+1 |zn), andthenmarginalizeoutzn+1.
Thisisillustratedin Figure13.13.
Againweneedastartingconditionfortherecursion, namelyavalueforβ(z N).
This can be obtained by setting n = N in (13.33) and replacing α(z N) with its definition(13.34)togive p(z N |X)= p(X, z N)β(z N) (13.39) p(X) whichweseewillbecorrectprovidedwetakeβ(z N)=1forallsettingsofz N.
In the M step equations, the quantity p(X) will cancel out, as can be seen, for instance, inthe M-stepequationforµ givenby(13.20), whichtakestheform k n n γ(znk)xn α(znk)β(znk)xn µ = n=1 = n=1 .
(13.40) k n n γ(znk) α(znk)β(znk) n=1 n=1 However, thequantityp(X)representsthelikelihoodfunctionwhosevaluewetyp- ically wish to monitor during the EM optimization, and so it is useful to be able to evaluateit.
Ifwesumbothsidesof(13.33)overzn, andusethefactthattheleft-hand sideisanormalizeddistribution, weobtain p(X)= α(zn)β(zn).
(13.41) zn 13.2.
Hidden Markov Models 623 Thuswecanevaluatethelikelihoodfunctionbycomputingthissum, foranyconve- nientchoiceofn.
Forinstance, ifweonlywanttoevaluatethelikelihoodfunction, thenwecandothisbyrunningtheαrecursionfromthestarttotheendofthechain, andthenusethisresultforn = N, makinguseofthefactthatβ(z N)isavectorof 1s.
Inthiscasenoβ recursionisrequired, andwesimplyhave p(X)= α(z N).
(13.42) z N Let us take a moment to interpret this result for p(X).
Recall that to compute the likelihood we should take the joint distribution p(X, Z) and sum over all possible valuesof Z.
Eachsuchvaluerepresentsaparticularchoiceofhiddenstateforevery time step, in other words every term in the summation is a path through the lattice diagram, and recall that there are exponentially many such paths.
By expressing thelikelihoodfunctionintheform(13.42), wehavereducedthecomputationalcost from being exponential in the length of the chain to being linear by swapping the order of the summation and multiplications, so that at each time step n we sum the contributions from all paths passing through each of the states znk to give the intermediatequantitiesα(zn).
Nextweconsidertheevaluationofthequantitiesξ(zn−1 , zn), whichcorrespond tothevaluesoftheconditionalprobabilitiesp(zn−1 , zn |X)foreachofthe K ×K settings for (zn−1 , zn).
Using the definition of ξ(zn−1 , zn), and applying Bayes’ theorem, wehave ξ(zn−1 , zn)=p(zn−1 , zn |X) p(X|zn−1 , zn)p(zn−1 , zn) = p(X) p(x 1 = p(X) α(zn−1 )p(xn |zn)p(zn |zn−1 )β(zn) = (13.43) p(X) wherewehavemadeuseoftheconditionalindependenceproperty(13.29)together withthedefinitionsofα(zn)andβ(zn)givenby(13.34)and(13.35).
Thuswecan calculatetheξ(zn−1 , zn)directlybyusingtheresultsoftheαandβ recursions.
Let us summarize the steps required to train a hidden Markov model using the EM algorithm.
We first make an initial selection of the parameters θold where θ ≡ (π, A,φ).
The A and π parameters are often initialized either uniformly or randomly from a uniform distribution (respecting their non-negativity and summa- tion constraints).
Initialization of the parameters φ will depend on the form of the distribution.
Forinstanceinthecaseof Gaussians, theparametersµ mightbeini- k tializedbyapplyingthe K-meansalgorithmtothedata, andΣk mightbeinitialized to the covariance matrix of the corresponding K means cluster.
Then we run both theforwardαrecursionandthebackwardβrecursionandusetheresultstoevaluate γ(zn) and ξ(zn−1 , zn).
At this stage, we can also evaluate the likelihood function.
624 13.
SEQUENTIALDATA Thiscompletesthe Estep, andweusetheresultstofindarevisedsetofparameters θnew usingthe M-stepequationsfrom Section13.2.1.
Wethencontinuetoalternate between E and M steps until some convergence criterion is satisfied, for instance whenthechangeinthelikelihoodfunctionisbelowsomethreshold.
Notethatintheserecursionrelationstheobservationsenterthroughconditional distributions of the form p(xn |zn).
The recursions are therefore independent of the type or dimensionality of the observed variables or the form of this conditional distribution, so long as its value can be computed for each of the K possible states of zn.
Since the observed variables {xn } are fixed, the quantities p(xn |zn) can be pre-computedasfunctionsofzn atthestartofthe EMalgorithm, andremainfixed throughout.
Wehaveseeninearlierchaptersthatthemaximumlikelihoodapproachismost effectivewhenthenumberofdatapointsislargeinrelationtothenumberofparame- ters.
Herewenotethatahidden Markovmodelcanbetrainedeffectively, usingmax- imum likelihood, provided the training sequence is sufficiently long.
Alternatively, we can make use of multiple shorter sequences, which requires a straightforward Exercise 13.12 modificationofthehidden Markovmodel EMalgorithm.
Inthecaseofleft-to-right models, this is particularly important because, in a given observation sequence, a givenstatetransitioncorrespondingtoanondiagonalelementof Awillseenatmost once.
Anotherquantityofinterestisthepredictivedistribution, inwhichtheobserved datais X={x 1 ,..., x N }andwewishtopredictx N+1, whichwouldbeimportant for real-time applications such as financial forecasting.
Again we make use of the sumandproductrulestogetherwiththeconditionalindependenceproperties(13.29) and(13.31)giving p(x N+1 |X) = p(x N+1 , z N+1 |X) z N+1 = p(x N+1 |z N+1 )p(z N+1 |X) z N+1 = p(x N+1 |z N+1 ) p(z N+1 , z N |X) z N+1 z N = p(x N+1 |z N+1 ) p(z N+1 |z N)p(z N |X) z N+1 z N = p(x N+1 |z N+1 ) p(z N+1 |z N) p(z N, X) p(X) z N+1 z N 1 = p(x N+1 |z N+1 ) p(z N+1 |z N)α(z N) (13.44) p(X) z N+1 z N which can be evaluated by first running a forward α recursion and then computing thefinalsummationsoverz N andz N+1.
Theresultofthefirstsummationoverz N can be stored and used once the value of x N+1 is observed in order to run the α recursion forward to the next step in order to predict the subsequent value x N+2.
13.2.
Hidden Markov Models 625 Figure13.14 A fragment of the fac- χ z1 zn−1 ψn zn tor graph representation for the hidden Markovmodel.
g1 gn−1 gn x1 xn−1 xn Notethatin(13.44), theinfluenceofalldatafromx 1 tox N issummarizedinthe K valuesofα(z N).
Thusthepredictivedistributioncanbecarriedforwardindefinitely usingafixedamountofstorage, asmayberequiredforreal-timeapplications.
Herewehavediscussedtheestimationoftheparametersofan HMMusingmax- imumlikelihood.
Thisframeworkiseasilyextendedtoregularizedmaximumlikeli- hoodbyintroducingpriorsoverthemodelparametersπ, Aandφwhosevaluesare thenestimatedbymaximizingtheirposteriorprobability.
Thiscanagainbedoneus- ingthe EMalgorithminwhichthe Estepisthesameasdiscussedabove, andthe M stepinvolvesaddingthelogofthepriordistributionp(θ)tothefunction Q(θ,θold ) before maximization and represents a straightforward application of the techniques developedatvariouspointsinthisbook.
Furthermore, wecanusevariationalmeth- Section10.1 odstogiveafully Bayesiantreatmentofthe HMMinwhichwemarginalizeoverthe parameterdistributions(Mac Kay,1997).
Aswithmaximumlikelihood, thisleadsto atwo-passforward-backwardrecursiontocomputeposteriorprobabilities.
13.2.3 The sum-product algorithm for the HMM The directed graph that represents the hidden Markov model, shown in Fig- ure13.5, isatreeandsowecansolvetheproblemoffindinglocalmarginalsforthe Section8.4.4 hiddenvariablesusingthesum-productalgorithm.
Notsurprisingly, thisturnsoutto beequivalenttotheforward-backwardalgorithmconsideredintheprevioussection, andsothesum-productalgorithmthereforeprovidesuswithasimplewaytoderive thealpha-betarecursionformulae.
Webeginbytransformingthedirectedgraphof Figure13.5intoafactorgraph, of which a representative fragment is shown in Figure 13.14.
This form of the fac- tor graph shows all variables, both latent and observed, explicitly.
However, for the purpose of solving the inference problem, we shall always be conditioning on thevariablesx 1 ,..., x N, andsowecansimplifythefactorgraphbyabsorbingthe emission probabilities into the transition probability factors.
This leads to the sim- plified factor graph representation in Figure 13.15, in which the factors are given by h(z 1 ) = p(z 1 )p(x 1 |z 1 ) (13.45) fn(zn−1 , zn) = p(zn |zn−1 )p(xn |zn).
(13.46) 626 13.
SEQUENTIALDATA Figure13.15 A simplified form of fac- h fn torgraphtodescribethehidden Markov model.
z1 zn−1 zn To derive the alpha-beta algorithm, we denote the final hidden variable z N as the root node, and first pass messages from the leaf node h to the root.
From the generalresults(8.66)and(8.69)formessagepropagation, weseethatthemessages whicharepropagatedinthehidden Markovmodeltaketheform µ zn−1 →f n (zn−1 ) = µ f n−1 →zn−1 (zn−1 ) (13.47) µf n →zn (zn) = fn(zn−1 , zn)µ zn−1 →f n (zn−1 ) (13.48) zn−1 Theseequationsrepresentthepropagationofmessagesforwardalongthechainand are equivalent to the alpha recursions derived in the previous section, as we shall nowshow.
Notethatbecausethevariablenodeszn haveonlytwoneighbours, they performnocomputation.
We can eliminate µ zn−1 →f n (zn−1 ) from (13.48) using (13.47) to give a recur- sionforthef →zmessagesoftheform µf n →zn (zn)= fn(zn−1 , zn)µf n−1 →zn−1 (zn−1 ).
(13.49) zn−1 Ifwenowrecallthedefinition(13.46), andifwedefine α(zn)=µf n →zn (zn) (13.50) then we obtain the alpha recursion given by (13.36).
We also need to verify that the quantities α(zn) are themselves equivalent to those defined previously.
This is easily done by using the initial condition (8.71) and noting that α(z 1 ) is given by h(z 1 ) = p(z 1 )p(x 1 |z 1 ) which is identical to (13.37).
Because the initial α is the same, and because they are iteratively computed using the same equation, all subsequentαquantitiesmustbethesame.
Next we consider the messages that are propagated from the root node back to theleafnode.
Thesetaketheform µf n+1 →f n (zn)= fn+1 (zn, zn+1 )µf n+2 →f n+1 (zn+1 ) (13.51) zn+1 where, as before, we have eliminated the messages of the type z → f since the variable nodes perform no computation.
Using the definition (13.46) to substitute forfn+1 (zn, zn+1 ), anddefining β(zn)=µf n+1 →zn (zn) (13.52) 13.2.
Hidden Markov Models 627 we obtain the beta recursion given by (13.38).
Again, we can verify that the beta variablesthemselvesareequivalentbynotingthat(8.70)impliesthattheinitialmes- sage send by the root variable node is µ z N →f N (z N) = 1, which is identical to the initializationofβ(z N)givenin Section13.2.2.
Thesum-productalgorithmalsospecifieshowtoevaluatethemarginalsonceall themessageshavebeenevaluated.
Inparticular, theresult(8.63)showsthatthelocal marginalatthenodezn isgivenbytheproductoftheincomingmessages.
Because we have conditioned on the variables X = {x 1 ,..., x N }, we are computing the jointdistribution p(zn, X)=µf n →zn (zn)µf n+1 →zn (zn)=α(zn)β(zn).
(13.53) Dividingbothsidesbyp(X), wethenobtain p(zn, X) α(zn)β(zn) γ(zn)= = (13.54) p(X) p(X) 13.2.4 Scaling factors Thereisanimportantissuethatmustbeaddressedbeforewecanmakeuseofthe forwardbackwardalgorithminpractice.
Fromtherecursionrelation(13.36), wenote that at each step the new value α(zn) is obtained from the previous value α(zn−1 ) bymultiplyingbyquantitiesp(zn |zn−1 )andp(xn |zn).
Becausetheseprobabilities areoftensignificantlylessthanunity, asweworkourwayforwardalongthechain, the values of α(zn) can go to zero exponentially quickly.
For moderate lengths of chain (say 100 or so), the calculation of the α(zn) will soon exceed the dynamic rangeofthecomputer, evenifdoubleprecisionfloatingpointisused.
Inthecaseofi.
i.
d.
data, weimplicitlycircumventedthisproblemwiththeeval- uationoflikelihoodfunctionsbytakinglogarithms.
Unfortunately, thiswillnothelp herebecauseweareformingsumsofproductsofsmallnumbers(weareinfactim- plicitlysummingoverallpossiblepathsthroughthelatticediagramof Figure13.7).
Wethereforeworkwithre-scaledversionsofα(zn)andβ(zn)whosevaluesremain of order unity.
As we shall see, the corresponding scaling factors cancel out when weusethesere-scaledquantitiesinthe EMalgorithm.
In(13.34), wedefinedα(zn) = p(x 1 ,..., xn, zn)representingthejointdistri- butionofalltheobservationsuptoxn andthelatentvariablezn.
Nowwedefinea normalizedversionofαgivenby α (zn)=p(zn |x 1 ,..., xn)= α(zn) (13.55) p(x 1 ,..., xn) whichweexpecttobewellbehavednumericallybecauseitisaprobabilitydistribu- tionover K variablesforanyvalueofn.
Inordertorelatethescaledandoriginalal- phavariables, weintroducescalingfactorsdefinedbyconditionaldistributionsover theobservedvariables 628 13.
SEQUENTIALDATA Fromtheproductrule, wethenhave n p(x 1 ,..., xn)= cm (13.57) m=1 andso n m=1 Wecanthenturntherecursionequation(13.36)forαintooneforα givenby cnα (zn)=p(xn |zn) α (zn−1 )p(zn |zn−1 ).
(13.59) zn−1 Notethatateachstageoftheforwardmessagepassingphase, usedtoevaluateα (zn), we have to evaluate and store cn, which is easily done because it is the coefficient thatnormalizestheright-handsideof(13.59)togiveα (zn).
Wecansimilarlydefinere-scaledvariablesβ(zn)using N β(zn)= cm β(zn) (13.60) m=n+1 whichwillagainremainwithinmachineprecisionbecause, from(13.35), thequan- titiesβ(zn)aresimplytheratiooftwoconditionalprobabilities p(xn+1 ,..., x N |zn) β(zn)= p(xn+1 ,..., x N |x 1 ,..., xn) .
(13.61) Therecursionresult(13.38)forβthengivesthefollowingrecursionforthere-scaled variables cn+1 β(zn)= β(zn+1 )p(xn+1 |zn+1 )p(zn+1 |zn).
(13.62) zn+1 In applying this recursion relation, we make use of the scaling factorscn that were previouslycomputedintheαphase.
From(13.57), weseethatthelikelihoodfunctioncanbefoundusing N p(X)= cn.
(13.63) n=1 Similarly, using(13.33)and(13.43), togetherwith(13.63), weseethattherequired Exercise 13.15 marginalsaregivenby γ(zn) = α (zn)β (zn) (13.64) ξ(zn−1 , zn) = cnα (zn−1 )p(xn |zn)p(zn |z−1 )β (zn).
(13.65) 13.2.
Hidden Markov Models 629 Finally, wenotethatthereisanalternativeformulationoftheforward-backward algorithm(Jordan,2007)inwhichthebackwardpassisdefinedbyarecursionbased the quantities γ(zn) = α (zn)β (zn) instead of using β (zn).
This α–γ recursion requires that the forward pass be completed first so that all the quantities α (zn) are available for the backward pass, whereas the forward and backward passes of theα–β algorithmcanbedoneindependently.
Althoughthesetwoalgorithmshave comparablecomputationalcost, theα–β versionisthemostcommonlyencountered Section13.3 one in the case of hidden Markov models, whereas for linear dynamical systems a recursionanalogoustotheα–γ formismoreusual.
13.2.5 The Viterbi algorithm Inmanyapplicationsofhidden Markovmodels, thelatentvariableshavesome meaningful interpretation, and so it is often of interest to find the most probable sequence of hidden states for a given observation sequence.
For instance in speech recognition, wemightwishtofindthemostprobablephonemesequenceforagiven series of acoustic observations.
Because the graph for the hidden Markov model is a directed tree, this problem can be solved exactly using the max-sum algorithm.
Werecallfromourdiscussionin Section8.4.5thattheproblemoffindingthemost probablesequenceoflatentstatesisnotthesameasthatoffindingthesetofstates that are individually the most probable.
The latter problem can be solved by first running the forward-backward (sum-product) algorithm to find the latent variable marginalsγ(zn)andthenmaximizingeachoftheseindividually(Dudaetal.,2001).
However, thesetofsuchstateswillnot, ingeneral, correspondtothemostprobable sequenceofstates.
Infact, thissetofstatesmightevenrepresentasequencehaving zero probability, if it so happens that two successive states, which in isolation are individuallythemostprobable, aresuchthatthetransitionmatrixelementconnecting themiszero.
In practice, we are usually interested in finding the most probable sequence of states, andthiscanbesolvedefficientlyusingthemax-sumalgorithm, whichinthe contextofhidden Markovmodelsisknownasthe Viterbialgorithm(Viterbi,1967).
Note that the max-sum algorithm works with log probabilities and so there is no need to use re-scaled variables as was done with the forward-backward algorithm.
Figure 13.16 shows a fragment of the hidden Markov model expanded as lattice diagram.
Aswehavealreadynoted, thenumberofpossiblepathsthroughthelattice growsexponentiallywiththelengthofthechain.
The Viterbialgorithmsearchesthis space of paths efficiently to find the most probable path with a computational cost thatgrowsonlylinearlywiththelengthofthechain.
Aswiththesum-productalgorithm, wefirstrepresentthehidden Markovmodel as a factor graph, as shown in Figure 13.15.
Again, we treat the variable node z N as the root, and pass messages to the root starting with the leaf nodes.
Using the results(8.93)and(8.94), weseethatthemessagespassedinthemax-sumalgorithm aregivenby µ zn →f n+1 (zn) = µf n → zn (zn) (13.66) µf n+1 →zn+1 (zn+1 ) = m z a n x lnfn+1 (zn, zn+1 )+µ zn →f n+1 (zn) .
(13.67) 630 13.
SEQUENTIALDATA Figure13.16 A fragment of the HMM lattice showingtwopossiblepaths.
The Viterbialgorithm efficientlydeterminesthemostprobablepathfrom k =1 amongsttheexponentiallymanypossibilities.
For any given path, the corresponding probability is given by the product of the elements of the tran- sition matrix A jk, corresponding to the probabil- ities p(z n+1 |z n ) for each segment of the path, along with the emission densities p(x n |k) asso- k =2 ciatedwitheachnodeonthepath.
k =3 n−2 n−1 n n+1 Ifweeliminateµ zn →f n+1 (zn)betweenthesetwoequations, andmakeuseof(13.46), weobtainarecursionforthef →zmessagesoftheform ω(zn+1 )=lnp(xn+1 |zn+1 )+max{lnp(x +1 |zn)+ω(zn)} (13.68) zn wherewehaveintroducedthenotationω(zn)≡µf n →zn (zn).
From(8.95)and(8.96), thesemessagesareinitializedusing ω(z 1 )=lnp(z 1 )+lnp(x 1 |z 1 ).
(13.69) where we have used (13.45).
Note that to keep the notation uncluttered, we omit thedependenceonthemodelparametersθthatareheldfixedwhenfindingthemost probablesequence.
The Viterbialgorithmcanalsobederiveddirectlyfromthedefinition(13.6)of the joint distribution by taking the logarithm and then exchanging maximizations Exercise 13.16 and summations.
It is easily seen that the quantities ω(zn) have the probabilistic interpretation z 1 ,..., zn−1 Once we have completed the final maximization over z N, we will obtain the valueofthejointdistributionp(X, Z)correspondingtothemostprobablepath.
We alsowishtofindthesequenceoflatentvariablevaluesthatcorrespondstothispath.
To do this, we simply make use of the back-tracking procedure discussed in Sec- tion 8.4.5.
Specifically, we note that the maximization over zn must be performed foreachofthe K possiblevaluesofzn+1.
Supposewekeeparecordofthevalues ofzn thatcorrespondtothemaximaforeachvalueofthe K valuesofzn+1.
Letus denote this function by ψ(kn) where k ∈ {1,..., K}.
Once we have passed mes- sagestotheendofthechainandfoundthemostprobablestateofz N, wecanthen usethisfunctiontobacktrackalongthechainbyapplyingitrecursively kmax =ψ(kmax).
(13.71) n n+1 13.2.
Hidden Markov Models 631 Intuitively, we can understand the Viterbi algorithm as follows.
Naively, we could consider explicitly all of the exponentially many paths through the lattice, evaluatetheprobabilityforeach, andthenselectthepathhavingthehighestproba- bility.
However, wenoticethatwecanmakeadramaticsavingincomputationalcost as follows.
Suppose that for each path we evaluate its probability by summing up productsoftransitionandemissionprobabilitiesasweworkourwayforwardalong eachpaththroughthelattice.
Consideraparticulartimestepnandaparticularstate katthattimestep.
Therewillbemanypossiblepathsconvergingonthecorrespond- ing node in the lattice diagram.
However, we need only retain that particular path thatsofarhasthehighestprobability.
Becausethereare K statesattimestepn, we need to keep track of K such paths.
At time step n+1, there will be K2 possible pathstoconsider, comprising K possiblepathsleadingoutofeachofthe K current states, but again we need only retain K of these corresponding to the best path for eachstateattimen+1.
Whenwereachthefinaltimestep N wewilldiscoverwhich statecorrespondstotheoverallmostprobablepath.
Becausethereisauniquepath coming intothat statewecan tracethepath backto step N −1tosee whatstate it occupiedatthattime, andsoonbackthroughthelatticetothestaten=1.
13.2.6 Extensions of the hidden Markov model The basic hidden Markov model, along with the standard training algorithm based on maximum likelihood, has been extended in numerous ways to meet the requirementsofparticularapplications.
Herewediscussafewofthemoreimportant examples.
Weseefromthedigitsexamplein Figure13.11thathidden Markovmodelscan be quite poor generative models for the data, because many of the synthetic digits look quite unrepresentative of the training data.
If the goal is sequence classifica- tion, therecanbesignificantbenefitindeterminingtheparametersofhidden Markov models using discriminative rather than maximum likelihood techniques.
Suppose wehaveatrainingsetof Robservationsequences Xr, wherer = 1,..., R, eachof whichislabelledaccordingtoitsclassm, wherem=1,..., M.
Foreachclass, we haveaseparatehidden Markovmodelwithitsownparametersθ m, andwetreatthe problemofdeterminingtheparametervaluesasastandardclassificationproblemin whichweoptimizethecross-entropy R lnp(mr |Xr).
(13.72) r=1 Using Bayes’ theorem this can be expressed in terms of the sequence probabilities associatedwiththehidden Markovmodels R p(Xr |θ r)p(mr) ln (13.73) r=1 M l=1 p(Xr |θ l)p(lr) where p(m) is the prior probability of class m.
Optimization of this cost function is more complex than for maximum likelihood (Kapadia, 1998), and in particular 632 13.
SEQUENTIALDATA Figure13.17 Section of an autoregressive hidden zn−1 zn zn+1 Markovmodel, inwhichthedistribution of the observation x n depends on a subsetofthepreviousobservationsas well as on the hidden state z n.
In this example, thedistributionofx ndepends xn−1 xn xn+1 onthetwopreviousobservationsx n−1 andx n−2.
requires that every training sequence be evaluated under each of the models in or- der to compute the denominator in (13.73).
Hidden Markov models, coupled with discriminative training methods, are widely used in speech recognition (Kapadia, 1998).
Asignificantweaknessofthehidden Markovmodelisthewayinwhichitrep- resentsthedistributionoftimesforwhichthesystemremainsinagivenstate.
Tosee theproblem, notethattheprobabilitythatasequencesampledfromagivenhidden Markovmodelwillspendprecisely T stepsinstatekandthenmakeatransitiontoa differentstateisgivenby p(T)=(Akk) T (1−Akk)∝exp(−T ln Akk) (13.74) andsoisanexponentiallydecayingfunctionof T.
Formanyapplications, thiswill beaveryunrealisticmodelofstateduration.
Theproblemcanberesolvedbymod- ellingstatedurationdirectlyinwhichthediagonalcoefficients Akkareallsettozero, andeachstatekisexplicitlyassociatedwithaprobabilitydistributionp(T|k)ofpos- sible duration times.
From a generative point of view, when a state k is entered, a value T representingthenumberoftimestepsthatthesystemwillremaininstatek isthendrawnfromp(T|k).
Themodelthenemits T valuesoftheobservedvariable xt, whicharegenerally assumedtobeindependentsothatthecorrespondingemis- sion density is simply T t=1 p(xt |k).
This approach requires some straightforward modificationstothe EMoptimizationprocedure(Rabiner,1989).
Another limitation of the standard HMM is that it is poor at capturing long- range correlations between the observed variables (i.
e., between variables that are separated by many time steps) because these must be mediated via the first-order Markovchainofhiddenstates.
Longer-rangeeffectscouldinprinciplebeincluded byaddingextralinkstothegraphicalmodelof Figure13.5.
Onewaytoaddressthis istogeneralizethe HMMtogivetheautoregressivehidden Markovmodel(Ephraim et al., 1989), an example of which is shown in Figure 13.17.
For discrete observa- tions, this corresponds to expanded tables of conditional probabilities for the emis- siondistributions.
Inthecaseofa Gaussianemissiondensity, wecanusethelinear- Gaussian framework in which the conditional distribution for xn given the values of the previous observations, and the value of zn, is a Gaussian whose mean is a linear combination of the values of the conditioning variables.
Clearly the number ofadditionallinksinthegraphmustbelimitedtoavoidanexcessivethenumberof freeparameters.
Intheexampleshownin Figure13.17, eachobservationdependson 13.2.
Hidden Markov Models 633 Figure13.18 Example of an input-output hidden un−1 un un+1 Markov model.
In this case, both the emission probabilities and the transition probabilities depend on the values of a sequenceofobservationsu 1 ,..., u N.
zn−1 zn zn+1 xn−1 xn xn+1 the two preceding observed variables as well as on the hidden state.
Although this graphlooksmessy, wecanagainappealtod-separationtoseethatinfactitstillhas asimpleprobabilisticstructure.
Inparticular, ifweimagineconditioningonzn we see that, as with the standard HMM, the values of zn−1 and zn+1 are independent, corresponding to the conditional independence property (13.5).
This is easily veri- fiedbynotingthateverypathfromnodezn−1 tonodezn+1 passesthroughatleast one observed node that is head-to-tail with respect to that path.
As a consequence, wecanagainuseaforward-backwardrecursioninthe Estepofthe EMalgorithmto determine the posterior distributions of the latent variables in a computational time thatislinearinthelengthofthechain.
Similarly, the Mstepinvolvesonlyaminor modification of the standard M-step equations.
In the case of Gaussian emission densitiesthisinvolvesestimatingtheparametersusingthestandardlinearregression equations, discussedin Chapter3.
Wehaveseenthattheautoregressive HMMappearsasanaturalextensionofthe standard HMMwhenviewedasagraphicalmodel.
Infacttheprobabilisticgraphical modellingviewpointmotivatesaplethoraofdifferentgraphicalstructuresbasedon the HMM.
Anotherexampleistheinput-output hidden Markovmodel(Bengioand Frasconi,1995), inwhichwehaveasequenceofobservedvariablesu 1 ,..., u N, in addition to the output variables x 1 ,..., x N, whose values influence either the dis- tribution of latent variables or output variables, or both.
An example is shown in Figure13.18.
Thisextendsthe HMMframeworktothedomainofsupervisedlearn- ingforsequentialdata.
Itisagaineasytoshow, throughtheuseofthed-separation criterion, thatthe Markovproperty(13.5)forthechainoflatentvariablesstillholds.
Toverifythis, simplynotethatthereisonlyonepathfromnodezn−1 tonodezn+1 andthisishead-to-tailwithrespecttotheobservednodezn.
Thisconditionalinde- pendencepropertyagainallowstheformulationofacomputationallyefficientlearn- ing algorithm.
In particular, we can determine the parameters θ of the model by maximizingthelikelihoodfunction L(θ) = p(X|U,θ)where Uisamatrixwhose rows are given by u T.
As a consequence of the conditional independence property n (13.5)thislikelihoodfunctioncanbemaximizedefficientlyusingan EMalgorithm Exercise 13.18 inwhichthe Estepinvolvesforwardandbackwardrecursions.
Anothervariantofthe HMMworthyofmentionisthefactorialhidden Markov model (Ghahramani and Jordan, 1997), in which there are multiple independent 634 13.
SEQUENTIALDATA Figure13.19 A pris fa in c g to t r w ia o l M hid a d rk e o n v M ch a a r i k n o s v o m f l o a d te e n l t c v o a m ri- - z ( n 2 − ) 1 z ( n 2) z ( n 2 + ) 1 ables.
Forcontinuousobservedvariables x, onepossiblechoiceofemissionmodel is a linear-Gaussiandensity in which the m na e t a io n n o o f f t t h h e e G st a a u te s s sia o n ft i h s e a co lin rr e e a s r po co n m din b g i- z ( n 1 − ) 1 z ( n 1) z ( n 1 + ) 1 latentvariables.
xn−1 xn xn+1 Markov chains of latent variables, and the distribution of the observed variable at agiventimestepisconditionalonthestatesofallofthecorrespondinglatentvari- ablesatthatsametimestep.
Figure13.19showsthecorrespondinggraphicalmodel.
Themotivationforconsideringfactorial HMMcanbeseenbynotingthatinorderto represent, say, 10 bits of information at a given time step, a standard HMM would need K =210 =1024latentstates, whereasafactorial HMMcouldmakeuseof10 binarylatentchains.
Theprimarydisadvantageoffactorial HMMs, however, liesin theadditionalcomplexityoftrainingthem.
The Mstepforthefactorial HMMmodel isstraightforward.
However, observationofthexvariablesintroducesdependencies between the latent chains, leading to difficulties with the E step.
This can be seen (1) (2) by noting that in Figure 13.19, the variables zn and zn are connected by a path which is head-to-head at node xn and hence they are not d-separated.
The exact E stepforthismodeldoesnotcorrespondtorunningforwardandbackwardrecursions alongthe M Markovchainsindependently.
Thisisconfirmedbynotingthatthekey conditional independence property (13.5) is not satisfied for the individual Markov chainsinthefactorial HMMmodel, asisshownusingd-separationin Figure13.20.
Now suppose that there are M chains of hidden nodes and for simplicity suppose thatalllatentvariableshavethesamenumber K ofstates.
Thenoneapproachwould be to note that there are KM combinations of latent variables at a given time step Figure13.20 E w x h a ic m h p i l s e o h f ea a d p -t a o t - h h , e h a i d gh a lig t h th te e d o in bs g e r r e v e e n d , z ( n 2 − ) 1 z ( n 2) z ( n 2 + ) 1 nodes x n−1 and x n+1, and head-to-tail at the unobserved nodes z( n 2 − ) 1 , z( n 2) and z(2) .
Thus the path is not blocked and so n+ th 1 econditionalindependenceproperty z ( n 1 − ) 1 z ( n 1) z ( n 1 + ) 1 (13.5)doesnotholdfortheindividualla- tent chains of the factorial HMM model.
As a consequence, there is no efficient exact Estepforthismodel.
xn−1 xn xn+1 13.3.
Linear Dynamical Systems 635 andsowecantransformthemodelintoanequivalentstandard HMMhavingasingle chain of latent variables each of which has KM latent states.
We can then run the standard forward-backward recursions in the E step.
This has computational com- plexity O(NK2M)thatisexponentialinthenumber M oflatentchainsandsowill be intractable for anything other than small values of M.
One solution would be tousesamplingmethods(discussedin Chapter11).
Asanelegantdeterministical- Section10.1 ternative, Ghahramaniand Jordan(1997)exploitedvariationalinferencetechniques to obtain a tractable algorithm for approximate inference.
This can be done using asimplevariationalposteriordistributionthatisfullyfactorizedwithrespecttothe latent variables, or alternatively by using a more powerful approach in which the variationaldistributionisdescribedbyindependent Markovchainscorrespondingto thechainsoflatentvariablesintheoriginalmodel.
Inthelattercase, thevariational inferencealgorithmsinvolvesrunningindependentforwardandbackwardrecursions along each chain, which is computationally efficient and yet is also able to capture correlationsbetweenvariableswithinthesamechain.
Clearly, therearemanypossibleprobabilisticstructuresthatcanbeconstructed accordingtotheneedsofparticularapplications.
Graphicalmodelsprovideageneral technique for motivating, describing, andanalysing suchstructures, and variational methodsprovideapowerfulframeworkforperforminginferenceinthosemodelsfor whichexactsolutionisintractable.
13.3.
Linear Dynamical Systems In order to motivate the concept of linear dynamical systems, let us consider the followingsimpleproblem, whichoftenarisesinpracticalsettings.
Supposewewish to measure the value of an unknown quantity z using a noisy sensor that returns a observation x representing the value of z plus zero-mean Gaussian noise.
Given a single measurement, our best guess for z is to assume that z = x.
However, we canimproveourestimateforzbytakinglotsofmeasurementsandaveragingthem, becausetherandomnoisetermswilltendtocanceleachother.
Nowlet’smakethe situation more complicated by assuming that we wish to measure a quantity z that ischangingovertime.
Wecantakeregularmeasurementsofxsothatatsomepoint intimewehaveobtainedx 1 ,..., x N andwewishtofindthecorrespondingvalues z 1 ,..., x N.
Ifwesimplyaveragethemeasurements, theerrorduetorandomnoise willbereduced, butunfortunatelywewilljustobtainasingleaveragedestimate, in which we have averaged over the changing value of z, thereby introducing a new sourceoferror.
Intuitively, wecouldimaginedoingabitbetterasfollows.
Toestimatethevalue ofz N, wetakeonlythemostrecentfewmeasurements, sayx N−L,..., x N andjust average these.
If z is changing slowly, and the random noise level in the sensor is high, it would make sense to choose a relatively long window of observations to average.
Conversely, ifthesignalischangingquickly, andthenoiselevelsaresmall, wemightbebetterjusttousex N directlyasourestimateofz N.
Perhapswecould do even better if we take a weighted average, in which more recent measurements 636 13.
SEQUENTIALDATA makeagreatercontributionthanlessrecentones.
Althoughthissortofintuitiveargumentseemsplausible, itdoesnottellushow to form a weighted average, and any sort of hand-crafted weighing is hardly likely to be optimal.
Fortunately, we can address problems such as this much more sys- tematically by defining a probabilistic model that captures the time evolution and measurementprocessesandthenapplyingtheinferenceandlearningmethodsdevel- oped in earlier chapters.
Here we shall focus on a widely used model known as a lineardynamicalsystem.
As we have seen, the HMM corresponds to the state space model shown in Figure 13.5 in which the latent variables are discrete but with arbitrary emission probability distributions.
This graph of course describes a much broader class of probabilitydistributions, allofwhichfactorizeaccordingto(13.6).
Wenowconsider extensions to other distributions for the latent variables.
In particular, we consider continuous latent variables in which the summations of the sum-product algorithm become integrals.
The general form of the inference algorithms will, however, be thesameasforthehidden Markovmodel.
Itisinterestingtonotethat, historically, hidden Markovmodelsandlineardynamicalsystemsweredevelopedindependently.
Once they are both expressed as graphical models, however, the deep relationship betweenthemimmediatelybecomesapparent.
Onekeyrequirementisthatweretainanefficientalgorithmforinferencewhich is linear in the length of the chain.
This requires that, for instance, when we take a quantity α (zn−1 ), representing the posterior probability ofzn given observations x 1 ,..., xn, andmultiplybythetransitionprobabilityp(zn |zn−1 )andtheemission probabilityp(xn |zn)andthenmarginalizeoverzn−1, weobtainadistributionover zn that is of the same functional form as that over α (zn−1 ).
That is to say, the distributionmustnotbecomemorecomplexateachstage, butmustonlychangein itsparametervalues.
Notsurprisingly, theonlydistributionsthathavethisproperty ofbeingclosedundermultiplicationarethosebelongingtotheexponentialfamily.
Here we consider the most important example from a practical perspective, whichisthe Gaussian.
Inparticular, weconsideralinear-Gaussianstatespacemodel sothatthelatentvariables{zn }, aswellastheobservedvariables{xn }, aremulti- variate Gaussiandistributionswhosemeansarelinearfunctionsofthestatesoftheir parents in the graph.
We have seen that a directed graph of linear-Gaussian units isequivalent toa joint Gaussian distribution over allof thevariables.
Furthermore, marginals such as α (zn) are also Gaussian, so that the functional form of the mes- sagesispreservedandwewillobtainanefficientinferencealgorithm.
Bycontrast, suppose that the emission densities p(xn |zn) comprise a mixture of K Gaussians each of which has a mean that is linear in zn.
Then even if α (z 1 ) is Gaussian, the quantity α (z 2 ) will be a mixture of K Gaussians, α (z 3 ) will be a mixture of K2 Gaussians, andsoon, andexactinferencewillnotbeofpracticalvalue.
We have seen that the hidden Markov model can be viewed as an extension of the mixture models of Chapter 9 to allow for sequential correlations in the data.
Inasimilarway, wecanviewthelineardynamicalsystemasageneralizationofthe continuouslatentvariablemodelsof Chapter12suchasprobabilistic PCAandfactor analysis.
Each pair of nodes {zn, xn } represents a linear-Gaussian latent variable 13.3.
Linear Dynamical Systems 637 model for that particular observation.
However, the latent variables {zn } are no longertreatedasindependentbutnowforma Markovchain.
Becausethemodelisrepresentedbyatree-structureddirectedgraph, inference problemscanbesolvedefficientlyusingthesum-productalgorithm.
Theforwardre- cursions, analogoustotheαmessagesofthehidden Markovmodel, areknownasthe Kalman filter equations (Kalman, 1960; Zarchan and Musoff, 2005), and the back- ward recursions, analogous to the β messages, are known as the Kalman smoother equations, or the Rauch-Tung-Striebel (RTS) equations (Rauch et al., 1965).
The Kalmanfilteriswidelyusedinmanyreal-timetrackingapplications.
Becausethelineardynamicalsystemisalinear-Gaussianmodel, thejointdistri- butionoverallvariables, aswellasallmarginalsandconditionals, willbe Gaussian.
It follows that the sequence of individually most probable latent variable values is Exercise 13.19 thesameasthemostprobablelatentsequence.
Thereisthusnoneedtoconsiderthe analogueofthe Viterbialgorithmforthelineardynamicalsystem.
Because the model has linear-Gaussian conditional distributions, we can write thetransitionandemissiondistributionsinthegeneralform p(zn |zn−1 ) = N(zn |Azn−1 ,Γ) (13.75) p(xn |zn) = N(xn |Czn,Σ).
(13.76) Theinitiallatentvariablealsohasa Gaussiandistributionwhichwewriteas p(z 1 )=N(z 1 |µ 0 , V 0 ).
(13.77) Notethatinordertosimplifythenotation, wehaveomittedadditiveconstantterms from the means of the Gaussians.
In fact, it is straightforward to include them if Exercise 13.24 desired.
Traditionally, thesedistributionsaremorecommonlyexpressedinanequiv- alentformintermsofnoisylinearequationsgivenby zn = Azn−1 +wn (13.78) xn = Czn+vn (13.79) z 1 = µ 0 +u (13.80) wherethenoisetermshavethedistributions w ∼ N(w|0,Γ) (13.81) v ∼ N(v|0,Σ) (13.82) u ∼ N(u|0, V 0 ).
(13.83) The parameters of the model, denoted by θ = {A,Γ, C,Σ,µ 0 , V 0 }, can be determinedusingmaximumlikelihoodthroughthe EMalgorithm.
Inthe Estep, we needtosolvetheinferenceproblemofdeterminingthelocalposteriormarginalsfor thelatentvariables, whichcanbesolvedefficientlyusingthesum-productalgorithm, aswediscussinthenextsection.
638 13.
SEQUENTIALDATA 13.3.1 Inference in LDS We now turn to the problem of finding the marginal distributions for the latent variablesconditionalontheobservationsequence.
Forgivenparametersettings, we alsowishtomakepredictionsofthenextlatentstatezn andofthenextobservation xn conditionedontheobserveddatax 1 ,..., xn−1 foruseinreal-timeapplications.
Theseinferenceproblemscanbesolvedefficientlyusingthesum-productalgorithm, which in the context of the linear dynamical system gives rise to the Kalman filter and Kalmansmootherequations.
It is worth emphasizing that because the linear dynamical system is a linear- Gaussianmodel, thejointdistributionoveralllatentandobservedvariablesissimply a Gaussian, and so in principle we could solve inference problems by using the standardresultsderivedinpreviouschaptersforthemarginalsandconditionalsofa multivariate Gaussian.
The role of the sum-product algorithm is to provide a more efficientwaytoperformsuchcomputations.
Linear dynamical systems have the identical factorization, given by (13.6), to hidden Markovmodels, andareagaindescribedbythefactorgraphsin Figures13.14 and 13.15.
Inference algorithms therefore take precisely the same form except that summationsoverlatentvariablesarereplacedbyintegrations.
Webeginbyconsid- ering the forward equations in which we treat z N as the root node, and propagate messagesfromtheleafnodeh(z 1 )totheroot.
From(13.77), theinitialmessagewill be Gaussian, and because each of the factors is Gaussian, all subsequent messages will also be Gaussian.
By convention, we shall propagate messages that are nor- malizedmarginaldistributionscorrespondingtop(zn |x 1 ,..., xn), whichwedenote by α (zn)=N(zn |µ n , Vn).
(13.84) This is precisely analogous to the propagation of scaled variables α (zn) given by (13.59)inthediscretecaseofthehidden Markovmodel, andsotherecursionequa- tionnowtakestheform cnα (zn)=p(xn |zn) α (zn−1 )p(zn |zn−1 )dzn−1 .
(13.85) Substitutingfortheconditionalsp(zn |zn−1 )andp(xn |zn), using(13.75)and(13.76), respectively, andmakinguseof(13.84), weseethat(13.85)becomes cn N (zn |µ n , Vn)=N(xn |Czn,Σ) N(zn |Azn−1 ,Γ)N(zn−1 |µ n−1 , Vn−1 )dzn−1 .
(13.86) Herewearesupposingthatµ n−1 and Vn−1 areknown, andbyevaluatingtheinte- gral in (13.86), we wish to determine values for µ n and Vn.
The integral is easily evaluatedbymakinguseoftheresult(2.115), fromwhichitfollowsthat N(zn |Azn−1 ,Γ)N(zn−1 |µ n−1 , Vn−1 )dzn−1 =N(zn |Aµ n−1 , Pn−1 ) (13.87) 13.3.
Linear Dynamical Systems 639 wherewehavedefined Pn−1 =AVn−1 AT+Γ.
(13.88) Wecannowcombinethisresultwiththefirstfactorontheright-handsideof(13.86) bymakinguseof(2.115)and(2.116)togive µ n = Aµ n−1 +Kn(xn −CAµ n−1 ) (13.89) Vn = (I−Kn C)Pn−1 (13.90) cn = N(xn |CAµ n−1 , CPn−1 CT+Σ).
(13.91) Here we have made use of the matrix inverse identities (C.5) and (C.7) and also definedthe Kalmangainmatrix Kn =Pn−1 CT CPn−1 CT+Σ −1 .
(13.92) Thus, given the values of µ n−1 and Vn−1, together with the new observation xn, wecanevaluatethe Gaussianmarginalforzn havingmeanµ n andcovariance Vn, aswellasthenormalizationcoefficientcn.
Theinitialconditionsfortheserecursionequationsareobtainedfrom c 1 α (z 1 )=p(z 1 )p(x 1 |z 1 ).
(13.93) Because p(z 1 ) is given by (13.77), and p(x 1 |z 1 ) is given by (13.76), we can again makeuseof(2.115)tocalculatec 1 and(2.116)tocalculateµ 1 and V 1 giving µ 1 = µ 0 +K 1 (x 1 −Cµ 0 ) (13.94) V 1 = (I−K 1 C)V 0 (13.95) c 1 = N(x 1 |Cµ 0 , CV 0 CT+Σ) (13.96) where K 1 =V 0 CT CV 0 CT+Σ −1 .
(13.97) Similarly, thelikelihoodfunctionforthelineardynamicalsystemisgivenby(13.63) inwhichthefactorscn arefoundusingthe Kalmanfilteringequations.
We can interpret the steps involved in going from the posterior marginal over zn−1 to the posterior marginal over zn as follows.
In (13.89), we can view the quantity Aµ n−1 asthepredictionofthemeanoverznobtainedbysimplytakingthe mean over zn−1 and projecting it forward one step using the transition probability matrix A.
Thispredictedmeanwouldgiveapredictedobservationforxn givenby CAzn−1 obtained by applying the emission probability matrix C to the predicted hidden state mean.
We can view the update equation (13.89) for the mean of the hidden variable distribution as taking the predicted mean Aµ and then adding n−1 a correction that is proportional to the error xn −CAzn−1 between the predicted observationandtheactualobservation.
Thecoefficientofthiscorrectionisgivenby the Kalmangainmatrix.
Thuswecanviewthe Kalmanfilterasaprocessofmaking successive predictions and then correcting these predictions in the light of the new observations.
Thisisillustratedgraphicallyin Figure13.21.
640 13.
SEQUENTIALDATA zn−1 zn zn Figure 13.21 The linear dynamical system can be viewed as a sequence of steps in which increasing un- certainty in the state variable due to diffusion is compensated by the arrival of new data.
In the left-hand plot, the blue curve shows the distribution p(z n−1 |x 1 ,..., x n−1 ), which incorporates all the data up to step n−1.
The diffusion arising from the nonzero variance of the transition probability p(z n |z n−1 ) gives the distribution p(z n |x 1 ,..., x n−1 ), showninredinthecentreplot.
Notethatthisisbroaderandshiftedrelativetothebluecurve (whichisshowndashedinthecentreplotforcomparison).
Thenextdataobservationx ncontributesthroughthe emissiondensityp(x n |z n ), whichisshownasafunctionofz ningreenontheright-handplot.
Notethatthisisnot adensitywithrespecttoz n andsoisnotnormalizedtoone.
Inclusionofthisnewdatapointleadstoarevised distributionp(z n |x 1 ,..., x n )forthestatedensityshowninblue.
Weseethatobservationofthedatahasshifted andnarrowedthedistributioncomparedtop(z n |x 1 ,..., x n−1 )(whichisshownindashedintheright-handplot forcomparison).
If we consider a situation in which the measurement noise is small compared to the rate at which the latent variable is evolving, then we find that the posterior Exercise 13.27 distributionforzndependsonlyonthecurrentmeasurementxn, inaccordancewith the intuition from our simple example at the start of the section.
Similarly, if the latentvariableisevolvingslowlyrelativetotheobservationnoiselevel, wefindthat theposteriormeanforzn isobtainedbyaveragingallofthemeasurementsobtained Exercise 13.28 uptothattime.
One of the most important applications of the Kalman filter is to tracking, and thisisillustratedusingasimpleexampleofanobjectmovingintwodimensionsin Figure13.22.
So far, we have solved the inference problem of finding the posterior marginal foranodezn givenobservationsfromx 1 uptoxn.
Nextweturntotheproblemof finding the marginal for a node zn given all observations x 1 to x N.
For temporal data, this corresponds to the inclusion of future as well as past observations.
Al- thoughthiscannotbeusedforreal-timeprediction, itplaysakeyroleinlearningthe parameters of the model.
By analogy with the hidden Markov model, this problem can be solved by propagating messages from node x N back to node x 1 and com- biningthisinformationwiththatobtainedduringtheforwardmessagepassingstage usedtocomputetheα (zn).
In the LDS literature, it is usual to formulate this backward recursion in terms ofγ(zn) = α (zn)β (zn)ratherthanintermsofβ (zn).
Becauseγ(zn)mustalsobe Gaussian, wewriteitintheform γ(zn)=α (zn)β (zn)=N(zn |µ n , V n).
(13.98) To derive the required recursion, we start from the backward recursion (13.62) for 13.3.
Linear Dynamical Systems 641 Figure13.22 An illustration of a linear dy- namical system being used to track a moving object.
The blue points indicate the true positions oftheobjectinatwo-dimensional space at successive time steps, the green points denote noisy measurements of the positions, and the red crosses indicate the means of the inferred posterior distributions of the positions ob- tainedbyrunningthe Kalmanfil- tering equations.
The covari- ances of the inferred positions are indicated by the red ellipses, which correspond to contours havingonestandarddeviation.
β(zn), which, forcontinuouslatentvariables, canbewrittenintheform cn+1 β(zn)= β(zn+1 )p(xn+1 |zn+1 )p(zn+1 |zn)dzn+1 .
(13.99) We now multiply both sides of (13.99) by α (zn) and substitute for p(xn+1 |zn+1 ) Exercise 13.29 and(13.91), togetherwith(13.98), andaftersomemanipulationweobtain µ n = µ n +Jn µ n+1 −Aµ N (13.100) V n = Vn+Jn V n+1 −Pn JT n (13.101) wherewehavedefined Jn =Vn AT(Pn) −1 (13.102) andwehavemadeuseof AVn =Pn JT n .
Notethattheserecursionsrequirethatthe forward pass be completed first so that the quantities µ n and Vn will be available forthebackwardpass.
For the EM algorithm, we also require the pairwise posterior marginals, which canbeobtainedfrom(13.65)intheform ξ(zn−1 , zn)=(cn) −1 α (zn−1 )p(xn |zn)p(zn |z−1 )β (zn) N(zn−1 |µ n−1 , Vn−1 )N(zn |Azn−1 ,Γ)N(xn |Czn,Σ)N(zn |µ n , V n) = .
cnα (zn) (13.103) Substituting for α (zn) using (13.84) and rearranging, we see that ξ(zn−1 , zn) is a Gaussian with mean given with components γ(zn−1 ) and γ(zn), and a covariance Exercise 13.31 betweenzn andzn−1 givenby cov[zn, zn−1 ]=Jn−1 Vn.
(13.104) 642 13.
SEQUENTIALDATA 13.3.2 Learning in LDS Sofar, wehaveconsideredtheinferenceproblemforlineardynamicalsystems, assumingthatthemodelparametersθ ={A,Γ, C,Σ,µ 0 , V 0 }areknown.
Next, we considerthedeterminationoftheseparametersusingmaximumlikelihood(Ghahra- mani and Hinton, 1996b).
Because the model has latent variables, this can be ad- dressedusingthe EMalgorithm, whichwasdiscussedingeneraltermsin Chapter9.
Wecanderivethe EMalgorithmforthelineardynamicalsystemasfollows.
Let us denote the estimated parameter values at some particular cycle of the algorithm byθold .
Fortheseparametervalues, wecanruntheinferencealgorithmtodetermine theposteriordistributionofthelatentvariablesp(Z|X,θold ), ormorepreciselythose local posterior marginals that are required in the M step.
In particular, we shall requirethefollowingexpectations E[zn ] = µ n (13.105) E znz T n−1 = Jn−1 V n+µ n µ T n−1 (13.106) E znz T n = V n+µ n µ T n (13.107) wherewehaveused(13.104).
Now we consider the complete-data log likelihood function, which is obtained bytakingthelogarithmof(13.6)andisthereforegivenby N lnp(X, Z|θ) = lnp(z 1 |µ 0 , V 0 )+ lnp(zn |zn−1 , A,Γ) n=2 N + lnp(xn |zn, C,Σ) (13.108) n=1 inwhichwehavemadethedependenceontheparametersexplicit.
Wenowtakethe expectation of the complete-data log likelihood with respect to the posterior distri- butionp(Z|X,θold )whichdefinesthefunction Q(θ,θold )=E [lnp(X, Z|θ)].
(13.109) Z|θold Inthe Mstep, thisfunctionismaximizedwithrespecttothecomponentsofθ.
Consider first the parameters µ 0 and V 0.
If we substitute for p(z 1 |µ 0 , V 0 ) in (13.108)using(13.77), andthentaketheexpectationwithrespectto Z, weobtain 1 1 Q(θ,θold )=− ln|V |−E (z −µ )TV −1(z −µ ) +const 2 0 Z|θold 2 1 0 0 1 0 where all terms not dependent on µ 0 or V 0 have been absorbed into the additive constant.
Maximization with respect to µ 0 and V 0 is easily performed by making use of the maximum likelihood solution for a Gaussian distribution discussed in Exercise 13.32 Section2.3.4, giving 13.3.
Linear Dynamical Systems 643 µn 0 ew = E[z 1 ] (13.110) V 0 new = E[z 1 z T 1 ]−E[z 1 ]E[z T 1 ].
(13.111) Similarly, tooptimize AandΓ, wesubstituteforp(zn |zn−1 , A,Γ)in(13.108) using(13.75)giving N −1 Q(θ,θold )=− ln|Γ| 2 N 1 −E Z|θold (zn −Azn−1 )TΓ −1(zn −Azn−1 ) +const (13.112) 2 n=2 inwhichtheconstantcomprisestermsthatareindependentof AandΓ.
Maximizing Exercise 13.33 withrespecttotheseparametersthengives N N −1 Anew = E znz T n−1 E zn−1 z T n−1 (13.113) n=2 n=2 N 1 Γnew = N −1 E znz T n −Anew E zn−1 z T n n=2 −E znz T n−1 Anew+Anew E zn−1 z T n−1 (Anew) T .
(13.114) Notethat Anew mustbeevaluatedfirst, andtheresultcanthenbeusedtodetermine Γnew .
Finally, in order to determine the new values of C and Σ, we substitute for p(xn |zn, C,Σ)in(13.108)using(13.76)giving N Q(θ,θold ) = − ln|Σ| 2 N 1 −E Z|θold (xn −Czn)TΣ −1(xn −Czn) +const.
2 n=1 Exercise 13.34 Maximizingwithrespectto CandΣthengives N N −1 Cnew = xn E z T n E znz T n (13.115) n=1 n=1 N 1 Σnew = xnx T n −Cnew E[zn]x T n N n=1 −xn E z T n Cnew+Cnew E znz T n Cnew .
(13.116) 644 13.
SEQUENTIALDATA We have approached parameter learning in the linear dynamical system using maximumlikelihood.
Inclusionofpriorstogivea MAPestimateisstraightforward, and a fully Bayesian treatment can be found by applying the analytical approxima- tion techniques discussed in Chapter 10, though a detailed treatment is precluded hereduetolackofspace.
13.3.3 Extensions of LDS As with the hidden Markov model, there is considerable interest in extending thebasiclineardynamicalsysteminordertoincreaseitscapabilities.
Althoughthe assumption of a linear-Gaussian model leads to efficient algorithms for inference andlearning, italsoimpliesthatthemarginaldistributionoftheobservedvariables issimplya Gaussian, whichrepresentsasignificantlimitation.
Onesimpleextension ofthelineardynamicalsystemistousea Gaussianmixtureastheinitialdistribution for z 1.
If this mixture has K components, then the forward recursion equations (13.85)willleadtoamixtureof K Gaussiansovereachhiddenvariablezn, andso themodelisagaintractable.
Formanyapplications, the Gaussianemissiondensityisapoorapproximation.
If instead we try to use a mixture of K Gaussians as the emission density, then the posterior α (z 1 ) will also be a mixture of K Gaussians.
However, from (13.85) the posterior α (z 2 ) will comprise a mixture of K2 Gaussians, and so on, with α (zn) beinggivenbyamixtureof Kn Gaussians.
Thusthenumberofcomponentsgrows exponentiallywiththelengthofthechain, andsothismodelisimpractical.
More generally, introducing transition or emission models that depart from the linear-Gaussian (or other exponential family) model leads to an intractable infer- ence problem.
We can make deterministic approximations such as assumed den- Chapter10 sity filtering or expectation propagation, or we can make use of sampling methods, as discussed in Section 13.3.4.
One widely used approach is to make a Gaussian approximation by linearizing around the mean of the predicted distribution, which givesrisetotheextended Kalmanfilter(Zarchanand Musoff,2005).
Aswithhidden Markovmodels, wecandevelopinterestingextensionsoftheba- siclineardynamicalsystembyexpandingitsgraphicalrepresentation.
Forexample, the switching state space model (Ghahramani and Hinton, 1998) can be viewed as acombinationofthehidden Markovmodelwithasetoflineardynamicalsystems.
The model has multiple Markov chains of continuous linear-Gaussian latent vari- ables, eachofwhichisanalogoustothelatentchainofthelineardynamicalsystem discussedearlier, togetherwitha Markovchainofdiscretevariablesoftheformused in a hidden Markov model.
The output at each time step is determined by stochas- tically choosing one of the continuous latent chains, using the state of the discrete latentvariableasaswitch, andthenemittinganobservationfromthecorresponding conditionaloutputdistribution.
Exactinferenceinthismodelisintractable, butvari- ational methods lead to an efficient inference scheme involving forward-backward recursionsalongeachofthecontinuousanddiscrete Markovchainsindependently.
Notethat, ifweconsidermultiplechainsofdiscretelatentvariables, anduseoneas theswitchtoselectfromtheremainder, weobtainananalogousmodelhavingonly discretelatentvariablesknownastheswitchinghidden Markovmodel.
13.3.
Linear Dynamical Systems 645 13.3.4 Particle filters For dynamical systems which do not have a linear-Gaussian, for example, if Chapter11 theyuseanon-Gaussianemissiondensity, wecanturntosamplingmethodsinorder to find a tractable inference algorithm.
In particular, we can apply the sampling- importance-resampling formalism of Section 11.1.5 to obtain a sequential Monte Carloalgorithmknownastheparticlefilter.
Consider the class of distributions represented by the graphical model in Fig- ure 13.5, and suppose we are given the observed values Xn = (x 1 ,..., xn) and wewishtodraw Lsamplesfromtheposteriordistributionp(zn |Xn).
Using Bayes’ theorem, wehave E[f(zn)] = f(zn)p(zn |Xn)dzn = f(zn)p(zn |xn, Xn−1 )dzn f(zn)p(xn |zn)p(zn |Xn−1 )dzn = p(xn |zn)p(zn |Xn−1 )dzn L w(l)f(z(l)) (13.117) n n l=1 where{z ( n l)}isasetofsamplesdrawnfromp(zn |Xn−1 )andwehavemadeuseof theconditionalindependencepropertyp(xn |zn, Xn−1 ) = p(xn |zn), whichfollows fromthegraphin Figure13.5.
Thesamplingweights{wn (l)}aredefinedby w(l) = p(xn |z ( n l) ) (13.118) n L m=1 p(xn |z ( n m) ) wherethe samesamples areusedin thenumerator asinthe denominator.
Thus the posterior distribution p(zn |xn) is represented by the set of samples {z ( n l)} together with the corresponding weights {wn (l)}.
Note that these weights satisfy 0 wn (l) 1 (l) and l wn =1.
Because we wish to find a sequential sampling scheme, we shall suppose that a set of samples and weights have been obtained at time step n, and that we have subsequentlyobservedthevalueofxn+1, andwewishtofindtheweightsandsam- ples at time step n+1.
We first sample from the distribution p(zn+1 |Xn).
This is 646 13.
SEQUENTIALDATA straightforwardsince, againusing Bayes’theorem p(zn+1 |Xn) = p(zn+1 |zn, Xn)p(zn |Xn)dzn = p(zn+1 |zn)p(zn |Xn)dzn = p(zn+1 |zn)p(zn |xn, Xn−1 )dzn p(zn+1 |zn)p(xn |zn)p(zn |Xn−1 )dzn = p(xn |zn)p(zn |Xn−1 )dzn = w n (l)p(zn+1 |z( n l)) (13.119) l wherewehavemadeuseoftheconditionalindependenceproperties p(zn+1 |zn, Xn) = p(zn+1 |zn) (13.120) p(xn |zn, Xn−1 ) = p(xn |zn) (13.121) which follow from the application of the d-separation criterion to the graph in Fig- ure 13.5.
The distribution given by (13.119) is a mixture distribution, and samples canbedrawnbychoosingacomponentlwithprobabilitygivenbythemixingcoef- ficientsw(l) andthendrawingasamplefromthecorrespondingcomponent.
Insummary, wecanvieweachstepoftheparticlefilteralgorithmascomprising two stages.
At time step n, we have a sample representation of the posterior dis- tributionp(zn |Xn)expressedassamples{z ( n l)}withcorrespondingweights{wn (l)}.
Thiscanbeviewedasamixturerepresentationoftheform(13.119).
Toobtainthe corresponding representation for the next time step, we first draw L samples from the mixture distribution (13.119), and then for each sample we use the new obser- vation xn+1 to evaluate the corresponding weights w n (l + ) 1 ∝ p(xn+1 |z ( n l + ) 1 ).
This is illustrated, forthecaseofasinglevariablez, in Figure13.23.
The particle filtering, or sequential Monte Carlo, approach has appeared in the literature under various names including the bootstrap filter (Gordon et al., 1993), survivalofthefittest(Kanazawaetal.,1995), andthecondensationalgorithm(Isard and Blake,1998).
Exercises 13.1 ( ) www Use the technique of d-separation, discussed in Section 8.2, to verify that the Markov model shown in Figure 13.3 having N nodes in total satisfies the a model described by the graph in Figure 13.4 in which there are N nodes in total Exercises 647 p(zn |Xn) p(zn+1 |Xn) p(xn+1 |zn+1) p(zn+1 |Xn+1) z Figure 13.23 Schematic illustration of the operation of the particle filter for a one-dimensional latent space.
At time step n, the posterior p(z n |x n ) is represented as a mixture distribution, shownschematicallyascircleswhosesizesareproportionaltotheweightsw n (l) .
Asetof Lsamplesisthendrawnfromthisdistributionandthenewweightsw(l) evaluatedusing n+1 p(x n+1 |z( n l + ) 1 ).
satisfiestheconditionalindependenceproperties p(xn |x 1 ,..., xn−1 )=p(xn |xn−1 , xn−2 ) (13.122) forn=3,..., N.
13.2 ( ) Considerthejointprobabilitydistribution(13.2)correspondingtothedirected graph of Figure 13.3.
Using the sum and product rules of probability, verify that thisjointdistributionsatisfiestheconditionalindependenceproperty(13.3)forn = 2,..., N.
Similarly, show that the second-order Markov model described by the jointdistribution(13.4)satisfiestheconditionalindependenceproperty p(xn |x 1 ,..., xn−1 )=p(xn |xn−1 , xn−2 ) (13.123) forn=3,..., N.
13.3 ( ) Byusingd-separation, showthatthedistributionp(x 1 ,..., x N)oftheobserved dataforthestatespacemodelrepresentedbythedirectedgraphin Figure13.5does not satisfy any conditional independence properties and hence does not exhibit the Markovpropertyatanyfiniteorder.
13.4 ( ) www Considerahidden Markovmodelinwhichtheemissiondensitiesare represented by a parametric model p(x|z, w), such as a linear regression model or aneuralnetwork, inwhichw isavectorof adaptiveparameters.
Describehowthe parameterswcanbelearnedfromdatausingmaximumlikelihood.
648 13.
SEQUENTIALDATA 13.5 ( ) Verifythe M-stepequations(13.18)and(13.19)fortheinitialstateprobabili- tiesandtransitionprobabilityparametersofthehidden Markovmodelbymaximiza- tionoftheexpectedcomplete-dataloglikelihoodfunction(13.17), usingappropriate Lagrangemultiplierstoenforcethesummationconstraintsonthecomponentsofπ and A.
13.6 ( ) Show that if any elements of the parameters π or A for a hidden Markov modelareinitiallysettozero, thenthoseelementswillremainzeroinallsubsequent updatesofthe EMalgorithm.
13.7 ( ) Considerahidden Markovmodelwith Gaussianemissiondensities.
Showthat maximization of the function Q(θ,θold ) with respect to the mean and covariance parametersofthe Gaussiansgivesrisetothe M-stepequations(13.20)and(13.21).
13.8 ( ) www Forahidden Markovmodelhavingdiscreteobservationsgovernedby amultinomialdistribution, showthattheconditionaldistributionoftheobservations given the hidden variables is given by (13.22) and the corresponding M step equa- tionsaregivenby(13.23).
Writedown theanalogous equationsfor the conditional distributionandthe Mstepequationsforthecaseofahidden Markovwithmultiple binary output variables each of which is governed by a Bernoulli conditional dis- tribution.
Hint: refer to Sections 2.1 and 2.2 for a discussion of the corresponding maximumlikelihoodsolutionsfori.
i.
d.
dataifrequired.
13.9 ( ) www Use the d-separation criterion to verify that the conditional indepen- denceproperties(13.24)–(13.31)aresatisfiedbythejointdistributionforthehidden Markovmodeldefinedby(13.6).
13.10 ( ) Byapplyingthesumandproductrulesofprobability, verifythatthecondi- tionalindependenceproperties(13.24)–(13.31)aresatisfiedbythejointdistribution forthehidden Markovmodeldefinedby(13.6).
13.11 ( ) Startingfromtheexpression(8.72)forthemarginaldistributionoverthevari- ables of a factor in a factor graph, together with the results for the messages in the sum-product algorithm obtained in Section 13.2.3, derive the result (13.43) for the joint posterior distribution over two successive latent variables in a hidden Markov model.
13.12 ( ) Suppose we wish to train a hidden Markov model by maximum likelihood using data that comprises R independent sequences of observations, which we de- note by X(r) where r = 1,..., R.
Show that in the E step of the EM algorithm, we simply evaluate posterior probabilities for the latent variables by running the α and β recursions independently for each of the sequences.
Also show that in the Mstep, theinitialprobabilityandtransitionprobabilityparametersarere-estimated Exercises 649 usingmodifiedformsof(13.18)and(13.19)givenby R (r) γ(z ) 1k r=1 πk = (13.124) R K (r) γ(z ) 1j r=1 j=1 R N (r) (r) ξ(z , z ) n−1, j n, k r=1n=2 Ajk = (13.125) R K N (r) (r) ξ(z , z ) n−1, j n, l r=1 l=1 n=2 where, for notational convenience, we have assumed that the sequences are of the samelength(thegeneralizationtosequencesofdifferentlengthsisstraightforward).
Similarly, showthatthe M-stepequationforre-estimationofthemeansof Gaussian emissionmodelsisgivenby R N γ(z (r) )x(r) nk n µ = r=1n=1 .
(13.126) k R N (r) γ(z ) nk r=1n=1 Notethatthe M-stepequationsforotheremissionmodelparametersanddistributions takeananalogousform.
13.13 ( ) www Use the definition (8.64) of the messages passed from a factor node toavariablenodeinafactorgraph, togetherwiththeexpression(13.6)forthejoint distribution in a hidden Markov model, to show that the definition (13.50) of the alphamessageisthesameasthedefinition(13.34).
13.14 ( ) Use the definition (8.67) of the messages passed from a factor node to a variable node in a factor graph, together with the expression (13.6) for the joint distribution in a hidden Markov model, to show that the definition (13.52) of the betamessageisthesameasthedefinition(13.35).
13.15 ( ) Usetheexpressions(13.33)and(13.43)forthemarginalsinahidden Markov modeltoderivethecorrespondingresults(13.64)and(13.65)expressedintermsof re-scaledvariables.
13.16 ( ) In this exercise, we derive the forward message passing equation for the Viterbialgorithmdirectlyfromtheexpression(13.6)forthejointdistribution.
This involvesmaximizingoverallofthehiddenvariablesz 1 ,..., z N.
Bytakingthelog- arithm and then exchanging maximizations and summations, derive the recursion 650 13.
SEQUENTIALDATA (13.68) where the quantities ω(zn) are defined by (13.70).
Show that the initial conditionforthisrecursionisgivenby(13.69).
13.17 ( ) www Showthatthedirectedgraphfortheinput-outputhidden Markovmodel, givenin Figure13.18, canbeexpressedasatree-structuredfactorgraphoftheform shown in Figure 13.15 and write down expressions for the initial factor h(z 1 ) and forthegeneralfactorfn(zn−1 , zn)where2 n N.
13.18 ( ) Using the result of Exercise 13.17, derive the recursion equations, includ- ing the initial conditions, for the forward-backward algorithm for the input-output hidden Markovmodelshownin Figure13.18.
13.19 ( ) www The Kalmanfilterandsmootherequationsallowtheposteriordistribu- tions over individual latent variables, conditioned on all of the observed variables, to be found efficiently for linear dynamical systems.
Show that the sequence of latent variable values obtained by maximizing each of these posterior distributions individuallyisthesameasthemostprobablesequenceoflatentvalues.
Todothis, simplynotethatthejointdistributionofalllatentandobservedvariablesinalinear dynamicalsystemis Gaussian, andhenceallconditionalsandmarginalswillalsobe Gaussian, andthenmakeuseoftheresult(2.98).
13.20 ( ) www Usetheresult(2.115)toprove(13.87).
13.21 ( ) Use the results (2.115) and (2.116), together with the matrix identities (C.5) and(C.7), toderivetheresults(13.89),(13.90), and(13.91), wherethe Kalmangain matrix Kn isdefinedby(13.92).
13.22 ( ) www Using (13.93), together with the definitions (13.76) and (13.77) and theresult(2.115), derive(13.96).
13.23 ( ) Using(13.93), togetherwiththedefinitions(13.76)and(13.77)andtheresult 13.24 ( ) www Considerageneralizationof(13.75)and(13.76)inwhichweinclude constanttermsaandcinthe Gaussianmeans, sothat p(zn |zn−1 )=N(zn |Azn−1 +a,Γ) (13.127) p(xn |zn)=N(xn |Czn+c,Σ).
(13.128) Showthatthisextensioncanbere-caseintheframeworkdiscussedinthischapterby definingastatevectorzwithanadditionalcomponentfixedatunity, andthenaug- mentingthematrices Aand Cusingextracolumnscorrespondingtotheparameters aandc.
13.25 ( ) In this exercise, we show that when the Kalman filter equations are applied to independent observations, they reduce to the results given in Section 2.3 for the maximumlikelihoodsolutionforasingle Gaussiandistribution.
Considertheprob- lemoffindingthemeanµofasingle Gaussianrandomvariablex, inwhichweare given a set of independent observations {x 1 ,..., x N }.
To model this we can use Exercises 651 a linear dynamical system governed by (13.75) and (13.76), with latent variables {z 1 ,..., z N }inwhich Cbecomestheidentitymatrixandwherethetransitionprob- ability A = 0 because the observations are independent.
Let the parameters m 0 and V 0 of the initial state be denoted by µ 0 and σ 0 2, respectively, and suppose that Σbecomesσ2.
Writedownthecorresponding Kalmanfilterequationsstartingfrom theseareequivalenttotheresults(2.141)and(2.142)obtaineddirectlybyconsider- ingindependentdata.
13.26 ( ) Consideraspecialcaseofthelineardynamicalsystemof Section13.3thatis equivalenttoprobabilistic PCA, sothatthetransitionmatrix A = 0, thecovariance Γ = I, and the noise covariance Σ = σ2I.
By making use of the matrix inversion identity (C.7) show that, if the emission density matrix C is denoted W, then the posterior distribution over the hidden states defined by (13.89) and (13.90) reduces totheresult(12.42)forprobabilistic PCA.
13.27 ( ) www Consider a linear dynamical system of the form discussed in Sec- tion13.3inwhichtheamplitudeoftheobservationnoisegoestozero, sothatΣ=0.
Show that the posterior distribution for zn has mean xn and zero variance.
This accords with our intuition that if there is no noise, we should just use the current observationxntoestimatethestatevariableznandignoreallpreviousobservations.
13.28 ( ) Consider a special case of the linear dynamical system of Section 13.3 in whichthestatevariablezn isconstrainedtobeequaltothepreviousstatevariable, whichcorrespondsto A = IandΓ = 0.
Forsimplicity, assumealsothat V 0 → ∞ sothattheinitialconditionsforzareunimportant, andthepredictionsaredetermined purelybythedata.
Useproofbyinductiontoshowthattheposteriormeanforstate zn is determined by the average of x 1 ,..., xn.
This corresponds to the intuitive resultthatifthestatevariableisconstant, ourbestestimateisobtainedbyaveraging theobservations.
13.29 ( ) Starting from the backwards recursion equation (13.99), derive the RTS smoothing equations (13.100) and (13.101) for the Gaussian linear dynamical sys- tem.
13.30 ( ) Starting from the result (13.65) for the pairwise posterior marginal in a state space model, derive the specific form (13.103) for the case of the Gaussian linear dynamicalsystem.
13.31 ( ) Startingfromtheresult(13.103)andbysubstitutingforα (zn)using(13.84), verifytheresult(13.104)forthecovariancebetweenzn andzn−1.
13.32 ( ) www Verifytheresults(13.110)and(13.111)forthe M-stepequationsfor µ 0 and V 0 inthelineardynamicalsystem.
13.33 ( ) Verifytheresults(13.113)and(13.114)forthe M-stepequationsfor AandΓ inthelineardynamicalsystem.
652 13.
SEQUENTIALDATA 13.34 ( ) Verifytheresults(13.115)and(13.116)forthe M-stepequationsfor CandΣ inthelineardynamicalsystem.
14 Combining Models Inearlierchapters, wehaveexploredarangeofdifferentmodelsforsolvingclassifi- cationandregressionproblems.
Itisoftenfoundthatimprovedperformancecanbe obtainedbycombiningmultiplemodelstogetherinsomeway, insteadofjustusing asinglemodelinisolation.
Forinstance, wemighttrain Ldifferentmodelsandthen make predictions using the average of the predictions made by each model.
Such combinations of models are sometimes calledcommittees.
In Section 14.2, we dis- cusswaystoapplythecommitteeconceptinpractice, andwealsogivesomeinsight intowhyitcansometimesbeaneffectiveprocedure.
One important variant of the committee method, known as boosting, involves trainingmultiplemodelsinsequenceinwhichtheerrorfunctionusedtotrainapar- ticularmodeldependsontheperformanceofthepreviousmodels.
Thiscanproduce substantialimprovementsinperformancecomparedtotheuseofasinglemodeland isdiscussedin Section14.3.
Instead of averaging the predictions of a set of models, an alternative form of 653 654 14.
COMBININGMODELS model combination is to select one of the models to make the prediction, in which the choice of model is a function of the input variables.
Thus different models be- come responsible for making predictions in different regions of input space.
One widely used framework of this kind is known as adecision tree in which the selec- tion process can be described as a sequence of binary selections corresponding to the traversal of a tree structure and is discussed in Section 14.4.
In this case, the individualmodelsaregenerallychosentobeverysimple, andtheoverallflexibility of the model arises from the input-dependent selection process.
Decision trees can beappliedtobothclassificationandregressionproblems.
One limitation of decision trees is that the division of input space is based on hard splits in which only one model is responsible for making predictions for any givenvalueoftheinputvariables.
Thedecisionprocesscanbesoftenedbymoving toaprobabilisticframeworkforcombiningmodels, asdiscussedin Section14.5.
For example, ifwehaveasetof K modelsforaconditionaldistributionp(t|x, k)where xistheinputvariable, tisthetargetvariable, andk = 1,..., K indexesthemodel, thenwecanformaprobabilisticmixtureoftheform K p(t|x)= πk(x)p(t|x, k) (14.1) k=1 in which πk(x) = p(k|x) represent the input-dependent mixing coefficients.
Such modelscanbeviewedasmixturedistributionsinwhichthecomponentdensities, as wellasthemixingcoefficients, areconditionedontheinputvariablesandareknown asmixturesofexperts.
Theyarecloselyrelatedtothemixturedensitynetworkmodel discussedin Section5.6.
14.1.
Bayesian Model Averaging It is important to distinguish between model combination methods and Bayesian modelaveraging, asthetwoareoftenconfused.
Tounderstandthedifference, con- Section9.2 sidertheexampleofdensityestimationusingamixtureof Gaussiansinwhichseveral Gaussian components are combined probabilistically.
The model contains a binary latent variable z that indicates which component of the mixture is responsible for generating the corresponding data point.
Thus the model is specified in terms of a jointdistribution p(x, z) (14.2) andthecorrespondingdensityovertheobservedvariablexisobtainedbymarginal- izingoverthelatentvariable p(x)= p(x, z).
(14.3) z 14.2.
Committees 655 Inthecaseofour Gaussianmixtureexample, thisleadstoadistributionoftheform K p(x)= πk N(x|µ k ,Σk) (14.4) k=1 with the usual interpretation of the symbols.
This is an example of model combi- nation.
Forindependent, identicallydistributeddata, wecanuse(14.3)towritethe marginalprobabilityofadataset X={x 1 ,..., x N }intheform N N p(X)= p(xn)= p(xn, zn) .
(14.5) n=1 n=1 zn Thusweseethateachobserveddatapointxnhasacorrespondinglatentvariablezn.
Now suppose we have several different models indexed by h = 1,..., H with priorprobabilitiesp(h).
Forinstanceonemodelmightbeamixtureof Gaussiansand anothermodelmightbeamixtureof Cauchydistributions.
Themarginaldistribution overthedatasetisgivenby H p(X)= p(X|h)p(h).
(14.6) h=1 Thisisanexampleof Bayesianmodelaveraging.
Theinterpretationofthissumma- tion over h is that just one model is responsible for generating the whole data set, and the probability distribution over h simply reflects our uncertainty as to which model that is.
As the size of the data set increases, this uncertainty reduces, and the posterior probabilities p(h|X) become increasingly focussed on just one of the models.
Thishighlightsthekeydifferencebetween Bayesianmodelaveragingandmodel combination, because in Bayesian model averaging the whole data set is generated byasinglemodel.
Bycontrast, whenwecombinemultiplemodels, asin(14.5), we see that different data points within the data set can potentially be generated from differentvaluesofthelatentvariablezandhencebydifferentcomponents.
Although we have considered the marginal probability p(X), the same consid- erationsapplyforthepredictivedensityp(x|X)orforconditionaldistributionssuch Exercise 14.1 asp(t|x, X, T).
14.2.
Committees The simplest way to construct a committee is to average the predictions of a set of individualmodels.
Suchaprocedurecanbemotivatedfromafrequentistperspective Section3.2 by considering the trade-off between bias and variance, which decomposes the er- rorduetoamodelintothebiascomponentthatarisesfromdifferencesbetweenthe modelandthetruefunctiontobepredicted, andthevariancecomponentthatrepre- sentsthesensitivityofthemodeltotheindividualdatapoints.
Recallfrom Figure3.5 656 14.
COMBININGMODELS thatwhenwetrainedmultiplepolynomialsusingthesinusoidaldata, andthenaver- agedtheresultingfunctions, thecontributionarisingfromthevariancetermtendedto cancel, leadingtoimprovedpredictions.
Whenweaveragedasetoflow-biasmod- els (corresponding to higher order polynomials), we obtained accurate predictions fortheunderlyingsinusoidalfunctionfromwhichthedataweregenerated.
In practice, of course, we have only a single data set, and so we have to find a way to introduce variability between the different models within the committee.
One approach is to use bootstrap data sets, discussed in Section 1.2.3.
Consider a regressionprobleminwhichwearetryingtopredictthevalueofasinglecontinuous variable, andsupposewegenerate M bootstrapdatasetsandthenuseeachtotrain aseparatecopyym(x)ofapredictivemodelwherem = 1,..., M.
Thecommittee predictionisgivenby M 1 y COM (x)= ym(x).
(14.7) M m=1 Thisprocedureisknownasbootstrapaggregationorbagging(Breiman,1996).
Suppose the true regression function that we are trying to predict is given by h(x), so that the output of each of the models can be written as the true value plus anerrorintheform ym(x)=h(x)+ m(x).
(14.8) Theaveragesum-of-squareserrorthentakestheform E x {ym(x)−h(x)}2 =E x m(x)2 (14.9) where E x [·] denotes a frequentist expectation with respect to the distribution of the inputvectorx.
Theaverageerrormadebythemodelsactingindividuallyistherefore M 1 E AV = E x m(x)2 .
(14.10) M m=1 Similarly, theexpectederrorfromthecommittee(14.7)isgivenby ⎡ ⎤ M 2 E COM = E x ⎣ 1 ym(x)−h(x) ⎦ M m=1 ⎡ ⎤ M 2 = E x ⎣ 1 m(x) ⎦ (14.11) M m=1 Ifweassumethattheerrorshavezeromeanandareuncorrelated, sothat E x [ m(x)] = 0 (14.12) E x [ m(x) l(x)] = 0, m =l (14.13) 14.3.
Boosting 657 Exercise 14.2 thenweobtain 1 E COM = E AV .
(14.14) M This apparently dramatic result suggests that the average error of a model can be reduced by a factor of M simply by averaging M versions of the model.
Unfortu- nately, itdependsonthekeyassumptionthattheerrorsduetotheindividualmodels areuncorrelated.
Inpractice, theerrorsaretypicallyhighlycorrelated, andthereduc- tioninoverallerrorisgenerallysmall.
Itcan, however, beshownthattheexpected committeeerrorwillnotexceedtheexpectederroroftheconstituentmodels, sothat Exercise 14.3 E COM E AV.
Inordertoachievemoresignificantimprovements, weturntoamore sophisticatedtechniqueforbuildingcommittees, knownasboosting.
14.3.
Boosting Boostingisapowerfultechniqueforcombiningmultiple‘base’classifierstoproduce aformofcommitteewhoseperformancecanbesignificantlybetterthanthatofany of the base classifiers.
Here we describe the most widely used form of boosting algorithm called Ada Boost, short for ‘adaptive boosting’, developed by Freund and Schapire (1996).
Boosting can give good results even if the base classifiers have a performancethatisonlyslightlybetterthanrandom, andhencesometimesthebase classifiersareknownasweaklearners.
Originallydesignedforsolvingclassification problems, boostingcanalsobeextendedtoregression(Friedman,2001).
The principal difference between boosting and the committee methods such as bagging discussed above, is that the base classifiers are trained in sequence, and each base classifier is trained using a weighted form of the data set in which the weighting coefficient associated with each data point depends on the performance of the previous classifiers.
In particular, points that are misclassified by one of the base classifiers are given greater weight when used to train the next classifier in the sequence.
Once all the classifiers have been trained, their predictions are then combined through a weighted majority voting scheme, as illustrated schematically in Figure14.1.
Consideratwo-classclassificationproblem, inwhichthetrainingdatacomprises inputvectorsx 1 ,..., x N alongwithcorrespondingbinarytargetvariablest 1 ,..., t N where tn ∈ {−1,1}.
Each data point is given an associated weighting parameter wn, which is initially set 1/N for all data points.
We shall suppose that we have a procedure available for training a base classifier using weighted data to give a function y(x) ∈ {−1,1}.
At each stage of the algorithm, Ada Boost trains a new classifierusingadatasetinwhichtheweightingcoefficientsareadjustedaccording to the performance of the previously trained classifier so as to give greater weight tothemisclassifieddatapoints.
Finally, whenthedesirednumberofbaseclassifiers have been trained, they are combined to form a committee using coefficients that givedifferentweighttodifferentbaseclassifiers.
Thepreciseformofthe Ada Boost algorithmisgivenbelow.
658 14.
COMBININGMODELS Figure14.1 Schematic illustration of the boosting framework.
Each {wn (1)} {wn (2)} {wn (M)} baseclassifiery m (x)istrained onaweightedformofthetrain- ing set (blue arrows) in which the weights w n (m) depend on the performance of the pre- vious base classifier y m−1 (x) y1(x) y2(x) y M(x) (greenarrows).
Onceallbase classifiers have been trained, they are combined to give the final classifier Y M (x) (red arrows).
M YM(x)=sign αmym(x) m Ada Boost 1.
Initializethedataweightingcoefficients{wn }bysettingwn (1) = 1/N for n=1,..., N.
2.
Form=1,..., M: (a) Fitaclassifierym(x)tothetrainingdatabyminimizingtheweighted errorfunction N Jm = w n (m)I(ym(xn) =tn) (14.15) n=1 where I(ym(xn) = tn) is the indicator function and equals 1 when ym(xn) =tn and0otherwise.
(b) Evaluatethequantities N w n (m)I(ym(xn) =tn) n=1 m = (14.16) N w(m) n n=1 andthenusethesetoevaluate 1− m αm =ln .
(14.17) m (c) Updatethedataweightingcoefficients w n (m+1) =w n (m)exp{αm I(ym(xn) =tn)} (14.18) 14.3.
Boosting 659 3.
Makepredictionsusingthefinalmodel, whichisgivenby M YM(x)=sign αmym(x) .
(14.19) m=1 We see that the first base classifier y 1 (x) is trained using weighting coeffi- (1) cients wn that are all equal, which therefore corresponds to the usual procedure for training a single classifier.
From (14.18), we see that in subsequent iterations (m) the weighting coefficients wn are increased for data points that are misclassified anddecreasedfordatapointsthatarecorrectlyclassified.
Successiveclassifiersare thereforeforcedtoplacegreateremphasisonpointsthathavebeenmisclassifiedby previous classifiers, and data points that continue to be misclassified by successive classifiers receive ever greater weight.
The quantities m represent weighted mea- sures of the error rates of each of the base classifiers on the data set.
We therefore seethattheweightingcoefficientsαm definedby(14.17)givegreaterweighttothe moreaccurateclassifierswhencomputingtheoveralloutputgivenby(14.19).
The Ada Boost algorithmisillustrated in Figure14.2, usingasubsetof30data pointstakenfromthetoyclassificationdatasetshownin Figure A.7.
Hereeachbase learnersconsistsofathresholdononeoftheinputvariables.
Thissimpleclassifier Section14.4 corresponds to a form of decision tree known as a ‘decision stumps’, i.
e., a deci- siontreewithasinglenode.
Thuseachbaselearnerclassifiesaninputaccordingto whetheroneoftheinputfeaturesexceedssomethresholdandthereforesimplyparti- tionsthespaceintotworegionsseparatedbyalineardecisionsurfacethatisparallel tooneoftheaxes.
14.3.1 Minimizing exponential error Boosting was originally motivated using statistical learning theory, leading to upperboundsonthegeneralizationerror.
However, theseboundsturnouttobetoo loosetohavepracticalvalue, andtheactualperformanceofboostingismuchbetter than the bounds alone would suggest.
Friedman et al.
(2000) gave a different and verysimpleinterpretationofboostingintermsofthesequentialminimizationofan exponentialerrorfunction.
Considertheexponentialerrorfunctiondefinedby N E = exp{−tnfm(xn)} (14.20) n=1 wherefm(x)isaclassifierdefinedintermsofalinearcombinationofbaseclassifiers yl(x)oftheform m 1 fm(x)= αlyl(x) (14.21) 2 l=1 andtn ∈ {−1,1}arethetrainingsettargetvalues.
Ourgoalistominimize E with respecttoboththeweightingcoefficientsαlandtheparametersofthebaseclassifiers yl(x).
660 14.
COMBININGMODELS 2 2 2 m=1 m=2 m=3 0 0 0 −2 −2 −2 −1 0 1 2 −1 0 1 2 −1 0 1 2 2 2 2 m=6 m=10 m=150 0 0 0 −2 −2 −2 −1 0 1 2 −1 0 1 2 −1 0 1 2 Figure 14.2 Illustration of boosting in which the base learners consist of simple thresholds applied to one or other of the axes.
Each figure shows the number m of base learners trained so far, along with the decision boundary of the most recent base learner (dashed black line) and the combined decision boundary of the en- semble(solidgreenline).
Eachdatapointisdepictedbyacirclewhoseradiusindicatestheweightassignedto thatdatapointwhentrainingthemostrecentlyaddedbaselearner.
Thus, forinstance, weseethatpointsthat aremisclassifiedbythem=1baselearneraregivengreaterweightwhentrainingthem=2baselearner.
Instead of doing a global error function minimization, however, we shall sup- pose that the base classifiers y 1 (x),..., ym−1 (x) are fixed, as are their coefficients α 1 ,...,αm−1, andsoweareminimizingonlywithrespecttoαm andym(x).
Sep- arating off the contribution from base classifier ym(x), we can then write the error functionintheform N 1 E = exp −tnfm−1 (xn)− tnαmym(xn) 2 n=1 N 1 = w n (m)exp − tnαmym(xn) (14.22) 2 n=1 where the coefficients wn (m) = exp{−tnfm−1 (xn)} can be viewed as constants because we are optimizing only αm and ym(x).
If we denote by T m the set of data points that are correctly classified by ym(x), and if we denote the remaining misclassified points by M m, then we can in turn rewrite the error function in the 14.3.
Boosting 661 form E = e −α m /2 w(m)+e α m /2 w(m) n n n∈T n∈M m m N N = (e α m /2−e −α m /2) w n (m)I(ym(xn) =tn)+e −α m /2 w n (m).
n=1 n=1 (14.23) Whenweminimizethiswithrespecttoym(x), weseethatthesecondtermiscon- stant, andsothisisequivalenttominimizing(14.15)becausetheoverallmultiplica- tive factor in front of the summation does not affect the location of the minimum.
Similarly, minimizingwithrespecttoαm, weobtain(14.17)inwhich m isdefined Exercise 14.6 by(14.16).
From(14.22)weseethat, havingfoundαm andym(x), theweightsonthedata pointsareupdatedusing 1 w n (m+1) =w n (m)exp − tnαmym(xn) .
(14.24) 2 Makinguseofthefactthat tnym(xn)=1−2I(ym(xn) =tn) (14.25) (m) weseethattheweightswn areupdatedatthenextiterationusing w n (m+1) =w n (m)exp(−αm/2)exp{αm I(ym(xn) =tn)}.
(14.26) Because the term exp(−αm/2) is independent of n, we see that it weights all data pointsbythesamefactorandsocanbediscarded.
Thusweobtain(14.18).
Finally, onceallthebaseclassifiersaretrained, newdatapointsareclassifiedby evaluatingthesignofthecombinedfunctiondefinedaccordingto(14.21).
Because thefactorof1/2doesnotaffectthesignitcanbeomitted, giving(14.19).
14.3.2 Error functions for boosting The exponential error function that is minimized by the Ada Boost algorithm differs from those considered in previous chapters.
To gain some insight into the nature of the exponential error function, we first consider the expected error given by E x, t[exp{−ty(x)}]= exp{−ty(x)}p(t|x)p(x)dx.
(14.27) t Ifweperformavariationalminimizationwithrespecttoallpossiblefunctionsy(x), Exercise 14.7 weobtain 1 p(t=1|x) y(x)= ln (14.28) 2 p(t=−1|x) 662 14.
COMBININGMODELS Figure14.3 Plot of the exponential (green) and E(z) rescaled cross-entropy (red) error functions along with the hinge er- ror (blue) used in support vector machines, and the misclassification error (black).
Note that for large negative values of z = ty(x), the cross-entropy gives a linearly in- creasing penalty, whereas the expo- nentiallossgivesanexponentiallyin- creasingpenalty.
z −2 −1 0 1 2 whichishalfthelog-odds.
Thusthe Ada Boostalgorithmisseekingthebestapprox- imationtothelogoddsratio, withinthespaceoffunctionsrepresentedbythelinear combination of base classifiers, subject to the constrained minimization resulting from the sequential optimization strategy.
This result motivates the use of the sign functionin(14.19)toarriveatthefinalclassificationdecision.
Wehavealreadyseenthattheminimizery(x)ofthecross-entropyerror(4.90) for two-class classification is given by the posterior class probability.
In the case Section7.1.2 of a target variable t ∈ {−1,1}, we have seen that the error function is given by ln(1 + exp(−yt)).
This is compared with the exponential error function in Fig- ure 14.3, where we have divided the cross-entropy error by a constant factor ln(2) so that it passes through the point (0,1) for ease of comparison.
We see that both can be seen as continuous approximations to the ideal misclassification error func- tion.
Anadvantageoftheexponentialerroristhatitssequentialminimizationleads to the simple Ada Boost scheme.
One drawback, however, is that it penalizes large negative values of ty(x) much more strongly than cross-entropy.
In particular, we see that for large negative values of ty, the cross-entropy grows linearly with |ty|, whereas the exponential error function grows exponentially with|ty|.
Thus the ex- ponential error function will be much less robust to outliers or misclassified data points.
Anotherimportantdifferencebetweencross-entropyandtheexponentialer- ror function is that the latter cannot be interpreted as the log likelihood function of Exercise 14.8 any well-defined probabilistic model.
Furthermore, the exponential error does not generalizetoclassificationproblemshaving K > 2classes, againincontrasttothe Section4.3.4 cross-entropyforaprobabilisticmodel, whichiseasilygeneralizedtogive(4.108).
Theinterpretationofboostingasthesequentialoptimizationofanadditivemodel under an exponential error (Friedman et al., 2000) opens the door to a wide range of boosting-like algorithms, including multiclass extensions, by altering the choice oferrorfunction.
Italsomotivatestheextensiontoregressionproblems(Friedman, 2001).
Ifweconsiderasum-of-squareserrorfunctionforregression, thensequential minimization of an additive model of the form (14.21) simply involves fitting each Exercise 14.9 newbaseclassifiertotheresidualerrorstn −fm−1 (xn)fromthepreviousmodel.
As we have noted, however, the sum-of-squares error is not robust to outliers, and this 14.4.
Tree-based Models 663 Figure14.4 Comparison of the squared error E(z) (green) with the absolute error (red) showing how the latter places much less emphasis on large errors and hence is more robust to outliers and mislabelleddatapoints.
−1 0 1 z canbeaddressedbybasingtheboostingalgorithmontheabsolutedeviation|y−t| instead.
Thesetwoerrorfunctionsarecomparedin Figure14.4.
14.4.
Tree-based Models There are various simple, but widely used, models that work by partitioning the input space into cuboid regions, whose edges are aligned with the axes, and then assigning a simple model (for example, a constant) to each region.
They can be viewed as a model combination method in which only one model is responsible for making predictions at any given point in input space.
The process of selecting a specific model, given a new input x, can be described by a sequential decision making process corresponding to the traversal of a binary tree (one that splits into two branches at each node).
Here we focus on a particular tree-based framework calledclassificationandregressiontrees, or CART (Breimanetal.,1984), although therearemanyothervariantsgoingbysuchnamesas ID3and C4.5(Quinlan,1986; Quinlan,1993).
Figure 14.5 shows an illustration of a recursive binary partitioning of the input space, along with the corresponding tree structure.
In this example, the first step Figure14.5 Illustration of a two-dimensional in- x2 put space that has been partitioned into five regions using axis-aligned E boundaries.
θ3 B C D θ2 A θ1 θ4 x1 664 14.
COMBININGMODELS Figure14.6 Binary tree corresponding to the par- titioning of input space shown in Fig- x1 >θ1 ure14.5.
x2 θ2 x2 >θ3 x1 θ4 A B C D E dividesthewholeoftheinputspaceintotworegionsaccordingtowhetherx 1 θ 1 orx 1 > θ 1 whereθ 1 isaparameterofthemodel.
Thiscreatestwosubregions, each of which can then be subdivided independently.
For instance, the region x 1 θ 1 is further subdivided according to whether x 2 θ 2 or x 2 > θ 2, giving rise to the regionsdenoted Aand B.
Therecursivesubdivisioncanbedescribedbythetraversal of the binary tree shown in Figure 14.6.
For any new inputx, we determine which region it falls into by starting at the top of the tree at the root node and following a path down to a specific leaf node according to the decision criteria at each node.
Notethatsuchdecisiontreesarenotprobabilisticgraphicalmodels.
Withineachregion, thereisaseparatemodeltopredictthetargetvariable.
For instance, in regression we might simply predict a constant over each region, or in classificationwemightassigneachregiontoaspecificclass.
Akeypropertyoftree- based models, which makes them popular in fields such as medical diagnosis, for example, is that they are readily interpretable by humans because they correspond to a sequence of binary decisions applied to the individual input variables.
For in- stance, topredictapatient’sdisease, wemightfirstask“istheirtemperaturegreater thansomethreshold?”.
Iftheanswerisyes, thenwemightnextask“istheirblood pressure less than some threshold?”.
Each leaf of the tree is then associated with a specificdiagnosis.
In order to learn such a model from a training set, we have to determine the structure of the tree, including which input variable is chosen at each node to form thesplitcriterionaswellasthevalueofthethresholdparameterθi forthesplit.
We alsohavetodeterminethevaluesofthepredictivevariablewithineachregion.
Considerfirstaregressionprobleminwhichthegoalistopredictasingletarget variabletfroma D-dimensionalvectorx = (x 1 ,..., x D)T ofinputvariables.
The training data consists of input vectors {x 1 ,..., x N } along with the corresponding continuouslabels{t 1 ,..., t N }.
Ifthepartitioningoftheinputspaceisgiven, andwe minimizethesum-of-squareserrorfunction, thentheoptimalvalueofthepredictive variablewithinanygivenregionisjustgivenbytheaverageof thevaluesoftn for Exercise 14.10 thosedatapointsthatfallinthatregion.
Now consider how to determine the structure of the decision tree.
Even for a fixednumberofnodesinthetree, theproblemofdeterminingtheoptimalstructure (includingchoiceofinputvariableforeachsplitaswellasthecorrespondingthresh- 14.4.
Tree-based Models 665 olds)tominimizethesum-of-squareserrorisusuallycomputationallyinfeasibledue to the combinatorially large number of possible solutions.
Instead, a greedy opti- mizationisgenerallydonebystartingwithasinglerootnode, correspondingtothe wholeinputspace, andthengrowingthetreebyaddingnodesoneatatime.
Ateach steptherewillbesomenumberofcandidateregionsininputspacethatcanbesplit, corresponding to the addition of a pair of leaf nodes to the existing tree.
For each of these, there is a choice of which of the D input variables to split, as well as the valueofthethreshold.
Thejointoptimizationofthechoiceofregiontosplit, andthe choiceofinputvariableandthreshold, canbedoneefficientlybyexhaustivesearch notingthat, foragivenchoiceofsplitvariableandthreshold, theoptimalchoiceof predictive variable is given by the local average of the data, as noted earlier.
This isrepeatedforallpossiblechoicesofvariabletobesplit, andtheonethatgivesthe smallestresidualsum-of-squareserrorisretained.
Given a greedy strategy for growing the tree, there remains the issue of when to stop adding nodes.
A simple approach would be to stop when the reduction in residualerrorfallsbelowsomethreshold.
However, itisfoundempiricallythatoften none of the available splits produces a significant reduction in error, and yet after severalmoresplitsasubstantialerrorreductionisfound.
Forthisreason, itiscom- mon practice to grow a large tree, using a stopping criterion based on the number ofdatapointsassociatedwiththeleafnodes, andthenprunebacktheresultingtree.
Thepruningisbasedonacriterionthatbalancesresidualerroragainstameasureof model complexity.
If we denote the starting tree for pruning by T 0, then we define T ⊂ T 0 to be a subtree of T 0 if it can be obtained by pruning nodes from T 0 (in otherwords, bycollapsinginternalnodesbycombiningthecorrespondingregions).
Supposetheleafnodesareindexedbyτ =1,...,|T|, withleafnodeτ representing aregion R τ ofinputspacehaving Nτ datapoints, and|T|denotingthetotalnumber ofleafnodes.
Theoptimalpredictionforregion R τ isthengivenby 1 yτ = tn (14.29) Nτ xn ∈R τ andthecorrespondingcontributiontotheresidualsum-of-squaresisthen Qτ(T)= {tn −yτ }2 .
(14.30) xn ∈R τ Thepruningcriterionisthengivenby |T| C(T)= Qτ(T)+λ|T| (14.31) τ=1 Theregularizationparameterλdeterminesthetrade-offbetweentheoverallresidual sum-of-squares error and the complexity of the model as measured by the number |T|ofleafnodes, anditsvalueischosenbycross-validation.
Forclassificationproblems, theprocessofgrowingandpruningthetreeissim- ilar, exceptthatthesum-of-squareserrorisreplacedbyamoreappropriatemeasure 666 14.
COMBININGMODELS of performance.
If we define pτk to be the proportion of data points in region R τ assigned to classk, where k = 1,..., K, then two commonly used choices are the cross-entropy K Qτ(T)= pτklnpτk (14.32) k=1 andthe Giniindex K Qτ(T)= pτk(1−pτk).
(14.33) k=1 Thesebothvanishforpτk =0andpτk =1andhaveamaximumatpτk =0.5.
They encouragetheformationofregionsinwhichahighproportionofthedatapointsare assignedtooneclass.
Thecrossentropyandthe Giniindexarebettermeasuresthan themisclassificationrateforgrowingthetreebecausetheyaremoresensitivetothe Exercise 14.11 node probabilities.
Also, unlike misclassification rate, they are differentiable and hencebettersuitedtogradientbasedoptimizationmethods.
Forsubsequentpruning ofthetree, themisclassificationrateisgenerallyused.
The human interpretability of a tree model such as CART is often seen as its majorstrength.
However, inpracticeitisfoundthattheparticulartreestructurethat islearnedisverysensitivetothedetailsofthedataset, sothatasmallchangetothe trainingdatacanresultinaverydifferentsetofsplits(Hastieetal.,2001).
There are other problems with tree-based methods of the kind considered in this section.
One is that the splits are aligned with the axes of the feature space, whichmaybeverysuboptimal.
Forinstance, toseparatetwoclasseswhoseoptimal decision boundary runs at 45 degrees to the axes would need a large number of axis-parallelsplitsoftheinputspaceascomparedtoasinglenon-axis-alignedsplit.
Furthermore, thesplitsinadecisiontreearehard, sothateachregionofinputspace isassociatedwithone, andonlyone, leafnodemodel.
Thelastissueisparticularly problematicinregressionwherewearetypicallyaimingtomodelsmoothfunctions, andyetthetreemodelproducespiecewise-constantpredictionswithdiscontinuities atthesplitboundaries.
14.5.
Conditional Mixture Models Wehaveseenthatstandarddecisiontreesarerestrictedbyhard, axis-alignedsplitsof theinputspace.
Theseconstraintscanberelaxed, attheexpenseofinterpretability, byallowingsoft, probabilisticsplitsthatcanbefunctionsofalloftheinputvariables, not just one of them at a time.
If we also give the leaf models a probabilistic inter- pretation, wearriveatafullyprobabilistictree-basedmodelcalledthehierarchical mixtureofexperts, whichweconsiderin Section14.5.3.
An alternative way to motivate the hierarchical mixture of experts model is to startwithastandardprobabilisticmixturesofunconditionaldensitymodelssuchas Chapter9 Gaussiansandreplacethecomponentdensitieswithconditionaldistributions.
Here we consider mixtures of linear regression models (Section 14.5.1) and mixtures of 14.5.
Conditional Mixture Models 667 logisticregressionmodels(Section14.5.2).
Inthesimplestcase, themixingcoeffi- cientsareindependentoftheinputvariables.
Ifwemakeafurthergeneralizationto allowthemixingcoefficientsalsotodependontheinputsthenweobtainamixture of experts model.
Finally, if we allow each component in the mixture model to be itselfamixtureofexpertsmodel, thenweobtainahierarchicalmixtureofexperts.
14.5.1 Mixtures of linear regression models One of the many advantages of giving a probabilistic interpretation to the lin- ear regression model is that it can then be used as a component in more complex probabilistic models.
This can be done, for instance, by viewing the conditional distribution representing the linear regression model as a node in a directed prob- abilistic graph.
Here we consider a simple example corresponding to a mixture of linearregressionmodels, whichrepresentsastraightforwardextensionofthe Gaus- sian mixture model discussed in Section 9.2 to the case of conditional Gaussian distributions.
We therefore consider K linear regression models, each governed by its own weightparameterwk.
Inmanyapplications, itwillbeappropriatetouseacommon noisevariance, governedbyaprecisionparameterβ, forall K components, andthis isthecaseweconsiderhere.
Wewillonceagainrestrictattentiontoasingletarget Exercise 14.12 variablet, thoughtheextensiontomultipleoutputsisstraightforward.
Ifwedenote themixingcoefficientsbyπk, thenthemixturedistributioncanbewritten K p(t|θ)= πk N(t|w k Tφ,β −1) (14.34) k=1 whereθdenotesthesetofalladaptiveparametersinthemodel, namely W ={wk }, π = {πk }, and β.
The log likelihood function for this model, given a data set of observations{φ n , tn }, thentakestheform N K lnp(t|θ)= ln πk N(tn |w k Tφ n ,β −1) (14.35) n=1 k=1 wheret=(t 1 ,..., t N)T denotesthevectoroftargetvariables.
In order to maximize this likelihood function, we can once again appeal to the EMalgorithm, whichwillturnouttobeasimpleextensionofthe EMalgorithmfor unconditional Gaussianmixturesof Section9.2.
Wecanthereforebuildonourexpe- riencewiththeunconditionalmixtureandintroduceaset Z={zn }ofbinarylatent variables where znk ∈ {0,1} in which, for each data point n, all of the elements k = 1,..., K are zero except for a single value of 1 indicating which component ofthemixturewasresponsibleforgeneratingthatdatapoint.
Thejointdistribution overlatentandobservedvariablescanberepresentedbythegraphicalmodelshown in Figure14.7.
Exercise 14.13 Thecomplete-dataloglikelihoodfunctionthentakestheform N K lnp(t, Z|θ)= znkln πk N(tn |w k Tφ n ,β −1) .
(14.36) n=1k=1 668 14.
COMBININGMODELS Figure14.7 Probabilistic directed graph representing a mixture of zn φ linearregressionmodels, definedby(14.35).
π n β tn W N The EMalgorithmbeginsbyfirstchoosinganinitialvalueθold forthemodelparam- eters.
In the E step, these parameter values are then used to evaluate the posterior probabilities, or responsibilities, of each component k for every data point n given by γnk =E[znk]=p(k|φ n ,θold )= π j k π N j N (tn (t | n w |w k Tφ j T n φ , n β , − β 1 − ) 1) .
(14.37) The responsibilities are then used to determine the expectation, with respect to the posterior distribution p(Z|t,θold ), of the complete-data log likelihood, which takes theform N K Q(θ,θold )=E Z [lnp(t, Z|θ)]= γnk lnπk+ln N(tn |w k Tφ n ,β −1) .
n=1k=1 In the M step, we maximize the function Q(θ,θold ) with respect to θ, keeping the γnk fixed.
For the optimization w ith respect to the mixing coefficients πk we need to take account of the constraint k πk = 1, which can be done with the aid of a Exercise 14.14 Lagrangemultiplier, leadingtoan M-stepre-estimationequationforπk intheform N 1 πk = γnk.
(14.38) N n=1 Note that this has exactly the same form as the corresponding result for a simple mixtureofunconditional Gaussiansgivenby(9.22).
Nextconsiderthemaximizationwithrespecttotheparametervectorwk ofthe kth linear regression model.
Substituting for the Gaussian distribution, we see that thefunction Q(θ,θold ), asafunctionoftheparametervectorwk, takestheform N Q(θ,θold )= γnk − β tn −w k Tφ n 2 +const (14.39) 2 n=1 wheretheconstanttermincludesthecontributionsfromotherweightvectorswj for j = k.
Note that the quantity we are maximizing is similar to the (negative of the) standard sum-of-squares error (3.12) for a single linear regression model, but with the inclusion of the responsibilities γnk.
This represents a weighted least squares 14.5.
Conditional Mixture Models 669 problem, in which the term corresponding to thenth data point carries a weighting coefficient given by βγnk, which could be interpreted as an effective precision for eachdatapoint.
Weseethateachcomponentlinearregressionmodelinthemixture, governedbyitsownparametervectorwk, isfittedseparatelytothewholedatasetin the Mstep, butwitheachdatapointnweightedbytheresponsibilityγnk thatmodel ktakesforthatdatapoint.
Settingthederivativeof(14.39)withrespecttowk equal tozerogives N 0= γnk tn −w k Tφ n φ n (14.40) n=1 whichwecanwriteinmatrixnotationas 0=ΦTRk(t−Φwk) (14.41) where Rk = diag(γnk) is a diagonal matrix of size N ×N.
Solving for wk, we obtain wk = ΦTRkΦ −1 ΦTRkt.
(14.42) This represents a set of modified normal equations corresponding to the weighted least squares problem, of the same form as (4.99) found in the context of logistic regression.
Note that after each E step, the matrix Rk will change and so we will havetosolvethenormalequationsafreshinthesubsequent Mstep.
Finally, we maximize Q(θ,θold ) with respect to β.
Keeping only terms that dependonβ, thefunction Q(θ,θold )canbewritten N K Q(θ,θold )= γnk 1 lnβ− β tn −w k Tφ n 2 .
(14.43) 2 2 n=1k=1 Settingthederivativewithrespecttoβ equaltozero, andrearranging, weobtainthe M-stepequationforβ intheform N K 1 = 1 γnk tn −w k Tφ n 2 .
(14.44) β N n=1k=1 In Figure 14.8, we illustrate this EM algorithm using the simple example of fitting a mixture of two straight lines to a data set having one input variable x and one target variable t.
The predictive density (14.34) is plotted in Figure 14.9 using the converged parameter values obtained from the EM algorithm, corresponding to theright-handplotin Figure14.8.
Alsoshowninthisfigureistheresultoffitting asinglelinearregressionmodel, whichgivesaunimodalpredictivedensity.
Wesee that the mixture model gives a much better representation of the data distribution, and this is reflected in the higher likelihood value.
However, the mixture model alsoassignssignificantprobabilitymasstoregionswherethereisnodatabecauseits predictivedistributionisbimodalforallvaluesofx.
Thisproblemcanberesolvedby extendingthemodeltoallowthemixturecoefficientsthemselvestobefunctionsof x, leadingtomodelssuchasthemixturedensitynetworksdiscussedin Section5.6, andhierarchicalmixtureofexpertsdiscussedin Section14.5.3.
670 14.
COMBININGMODELS 1.5 1.5 1.5 1 1 1 0.5 0.5 0.5 0 0 0 −0.5 −0.5 −0.5 −1 −1 −1 −1.5 −1.5 −1.5 1 1 1 0.8 0.8 0.8 0.6 0.6 0.6 0.4 0.4 0.4 0.2 0.2 0.2 0 0 0 Figure14.8 Exampleofasyntheticdataset, shownbythegreenpoints, havingoneinputvariablexandone targetvariablet, togetherwithamixtureoftwolinearregressionmodelswhosemeanfunctionsy(x, w k ), where k ∈ {1,2}, are shown by the blue and red lines.
The upper three plots show the initial configuration (left), the resultofrunning30iterationsof EM(centre), andtheresultafter50iterationsof EM(right).
Hereβwasinitialized tothereciprocalofthetruevarianceofthesetoftargetvalues.
Thelowerthreeplotsshowthecorresponding responsibilities plotted as a vertical line for each data point in which the length of the blue segment gives the posteriorprobabilityofthebluelineforthatdatapoint(andsimilarlyfortheredsegment).
14.5.2 Mixtures of logistic models Because the logistic regression model defines a conditional distribution for the targetvariable, giventheinputvector, itisstraightforwardtouseitasthecomponent distributioninamixturemodel, therebygivingrisetoaricherfamilyofconditional distributionscomparedtoasinglelogisticregressionmodel.
Thisexampleinvolves a straightforward combination of ideas encountered in earlier sections of the book andwillhelpconsolidatetheseforthereader.
Theconditionaldistributionofthetargetvariable, foraprobabilisticmixtureof K logisticregressionmodels, isgivenby K p(t|φ,θ)= πky k t [1−yk] 1−t (14.45) k=1 where φ is the feature vector, yk = σ w k Tφ is the output of component k, and θ denotestheadjustableparametersnamely{πk }and{wk }.
Now suppose we are given a data set {φ n , tn }.
The corresponding likelihood 14.5.
Conditional Mixture Models 671 Figure14.9 Theleft plot shows the predictiveconditional density correspondingto theconverged solution in Figure14.8.
Thisgivesaloglikelihoodvalueof−3.0.
Averticalslicethroughoneoftheseplotsataparticular valueofxrepresentsthecorrespondingconditionaldistributionp(t|x), whichweseeisbimodal.
Theplotonthe rightshowsthepredictivedensityforasinglelinearregressionmodelfittedtothesamedatasetusingmaximum likelihood.
Thismodelhasasmallerloglikelihoodof−27.6.
functionisthengivenby N K p(t|θ)= πky n t n k [1−ynk] 1−t n (14.46) n=1 k=1 where ynk = σ(w k Tφ n ) and t = (t 1 ,..., t N)T.
We can maximize this likelihood function iteratively by making use of the EM algorithm.
This involves introducing latentvariablesznk thatcorrespondtoa1-of-K codedbinaryindicatorvariablefor eachdatapointn.
Thecomplete-datalikelihoodfunctionisthengivenby N K p(t, Z|θ)= πky n t n k [1−ynk] 1−t n z nk (14.47) n=1k=1 where Z is the matrix of latent variables with elements znk.
We initialize the EM algorithmbychoosinganinitialvalueθold forthemodelparameters.
Inthe Estep, wethenusetheseparametervaluestoevaluatetheposteriorprobabilitiesofthecom- ponentskforeachdatapointn, whicharegivenby γnk =E[znk]=p(k|φ n ,θold )= π j k π y j n t n y k n t [ n j 1 [ − 1− yn y k n ] 1 j − ] 1 t − n t n .
(14.48) Theseresponsibilitiesarethenusedtofindtheexpectedcomplete-dataloglikelihood asafunctionofθ, givenby Q(θ,θold )=E Z [lnp(t, Z|θ)] N K = γnk {lnπk+tnlnynk +(1−tn)ln(1−ynk)}.
(14.49) n=1k=1 672 14.
COMBININGMODELS The M step involves maximization of this function with respect toθ, keeping θold , andhenceγnk, fixed.
Maximizationwithrespecttoπk canbedon eintheusualway, witha Lagrangemultipliertoenforcethesummationconstraint k πk = 1, giving thefamiliarresult N 1 πk = γnk.
(14.50) N n=1 To determine the {wk }, we note that the Q(θ,θold ) function comprises a sum over terms indexed by k each of which depends only on one of the vectors wk, so thatthedifferentvectorsaredecoupledinthe Mstepofthe EMalgorithm.
Inother words, thedifferentcomponentsinteractonlyviatheresponsibilities, whicharefixed during the M step.
Note that the M step does not have a closed-form solution and must be solved iteratively using, for instance, the iterative reweighted least squares Section4.3.3 (IRLS)algorithm.
Thegradientandthe Hessianforthevectorwk aregivenby N ∇ k Q = γnk(tn −ynk)φ n (14.51) n=1 N Hk = −∇ k ∇ k Q= γnkynk(1−ynk)φ n φT n (14.52) n=1 where∇ k denotesthegradientwithrespecttowk.
Forfixedγnk, theseareindepen- dentof{wj }forj = k andsowecansolveforeachwk separatelyusingthe IRLS Section4.3.3 algorithm.
Thusthe M-stepequationsforcomponentk correspondsimplytofitting asinglelogisticregressionmodeltoaweighteddatasetinwhichdatapointncarries a weight γnk.
Figure 14.10 shows an example of the mixture of logistic regression modelsappliedtoasimpleclassificationproblem.
Theextensionofthismodeltoa Exercise 14.16 mixtureofsoftmaxmodelsformorethantwoclassesisstraightforward.
14.5.3 Mixtures of experts In Section 14.5.1, we considered a mixture of linear regression models, and in Section 14.5.2 we discussed the analogous mixture of linear classifiers.
Although these simple mixtures extend the flexibility of linear models to include more com- plex (e.
g., multimodal) predictive distributions, they are still very limited.
We can further increase the capability of such models by allowing the mixing coefficients themselvestobefunctionsoftheinputvariable, sothat K p(t|x)= πk(x)pk(t|x).
(14.53) k=1 Thisisknownasamixtureofexpertsmodel(Jacobsetal.,1991)inwhichthemix- ingcoefficientsπk(x)areknownasgatingfunctionsandtheindividualcomponent densitiespk(t|x)arecalledexperts.
Thenotionbehindtheterminologyisthatdiffer- ent components can model the distribution in different regions of input space (they 14.5.
Conditional Mixture Models 673 Figure 14.10 Illustration of a mixture of logistic regression models.
The left plot shows data points drawn fromtwoclassesdenotedredandblue, inwhichthebackgroundcolour(whichvariesfrompureredtopureblue) denotesthetrueprobabilityoftheclasslabel.
Thecentreplotshowstheresultoffittingasinglelogisticregression modelusingmaximumlikelihood, inwhichthebackgroundcolourdenotesthecorrespondingprobabilityofthe classlabel.
Becausethecolourisanear-uniformpurple, weseethatthemodelassignsaprobabilityofaround 0.5 to each of the classes over most of input space.
The right plot shows the result of fitting a mixture of two logisticregressionmodels, whichnowgivesmuchhigherprobabilitytothecorrectlabelsformanyofthepoints intheblueclass.
are ‘experts’ at making predictions in their own regions), and the gating functions determinewhichcomponentsaredominantinwhichregion.
The gating functions πk(x) must sati sfy the usual constraints for mixing co- efficients, namely 0 πk(x) 1 and k πk(x) = 1.
They can therefore be represented, forexample, bylinearsoftmaxmodelsoftheform(4.104)and(4.105).
If the experts are also linear (regression or classification) models, then the whole model can be fitted efficiently using the EM algorithm, with iterative reweighted leastsquaresbeingemployedinthe Mstep(Jordanand Jacobs,1994).
Such a model still has significant limitations due to the use of linear models for the gating and expert functions.
A much more flexible model is obtained by using a multilevel gating function to give the hierarchical mixture of experts, or HME model (Jordan and Jacobs, 1994).
To understand the structure of this model, imagine a mixture distribution in which each component in the mixture is itself a mixturedistribution.
Forsimpleunconditionalmixtures, thishierarchicalmixtureis Exercise 14.17 trivially equivalent to a single flat mixture distribution.
However, when the mixing coefficients are input dependent, this hierarchical model becomes nontrivial.
The HMEmodelcanalsobeviewedasaprobabilisticversionofdecisiontreesdiscussed in Section14.4andcanagainbetrainedefficientlybymaximumlikelihoodusingan Section4.3.3 EMalgorithmwith IRLSinthe Mstep.
ABayesiantreatmentofthe HMEhasbeen givenby Bishopand Svense´n(2003)basedonvariationalinference.
Weshallnotdiscussthe HMEindetailhere.
However, itisworthpointingout thecloseconnectionwiththemixturedensitynetworkdiscussedin Section5.6.
The principal advantage of the mixtures of experts model is that it can be optimized by EM in which the M step for each mixture component and gating model involves a convex optimization (although the overall optimization is nonconvex).
By con- trast, the advantage of the mixture density network approach is that the component 674 14.
COMBININGMODELS densities and the mixing coefficients share the hidden units of the neural network.
Furthermore, inthemixturedensitynetwork, thesplitsoftheinputspacearefurther relaxedcomparedtothehierarchicalmixtureofexpertsinthattheyarenotonlysoft, andnotconstrainedtobeaxisaligned, buttheycanalsobenonlinear.
Exercises 14.1 ( ) www Considerasetmodelsoftheformp(t|x, zh,θ h, h)inwhichxisthe inputvector, tisthetargetvector, hindexesthedifferentmodels, zh isalatentvari- ableformodelh, andθ h isthesetofparametersformodelh.
Supposethemodels havepriorprobabilitiesp(h)andthatwearegivenatrainingset X={x 1 ,..., x N } and T = {t 1 ,..., t N }.
Write down the formulae needed to evaluate the predic- tive distribution p(t|x, X, T) in which the latent variables and the model index are marginalizedout.
Usetheseformulaetohighlightthedifferencebetween Bayesian averagingofdifferentmodelsandtheuseoflatentvariableswithinasinglemodel.
14.2 ( ) The expected sum-of-squares error E AV for a simple committee model can be defined by (14.10), and the expected error of the committee itself is given by (14.11).
Assuming that the individual errors satisfy (14.12) and (14.13), derive the result(14.14).
14.3 ( ) www By making use of Jensen’s inequality (1.115), for the special case of the convex function f(x) = x2, show that the average expected sum-of-squares error E AV of the members of a simple committee model, given by (14.10), and the expectederror E COM ofthecommitteeitself, givenby(14.11), satisfy E COM E AV .
(14.54) 14.4 ( ) By making use of Jensen’s in equality (1.115), show that the result (14.54) derived in the previous exercise hods for any error function E(y), not just sum-of- squares, provideditisaconvexfunctionofy.
14.5 ( ) www Consider a committee in which we allow unequal weighting of the constituentmodels, sothat M y COM (x)= αmym(x).
(14.55) m=1 In order to ensure that the predictions y COM (x) remain within sensible limits, sup- pose that we require that they be bounded at each value of x by the minimum and maximumvaluesgivenbyanyofthemembersofthecommittee, sothat y min (x) y COM (x) y max (x).
(14.56) Show that a necessary and sufficient condition for this constraint is that the coeffi- cientsαm satisfy M αm 0, αm =1.
(14.57) m=1 Exercises 675 14.6 ( ) www By differentiating the error function (14.23) with respect toαm, show that the parameters αm in the Ada Boost algorithm are updated using (14.17) in which m isdefinedby(14.16).
14.7 ( ) Bymakingavariationalminimizationoftheexpectedexponentialerrorfunction givenby(14.27)withrespecttoallpossiblefunctionsy(x), showthattheminimizing functionisgivenby(14.28).
14.8 ( ) Show that the exponential error function (14.20), which is minimized by the Ada Boostalgorithm, doesnotcorrespondtotheloglikelihoodofanywell-behaved probabilisticmodel.
Thiscanbedonebyshowingthatthecorrespondingconditional distributionp(t|x)cannotbecorrectlynormalized.
14.9 ( ) www Showthatthesequentialminimizationofthesum-of-squareserrorfunc- tionforanadditivemodeloftheform(14.21)inthestyleofboostingsimplyinvolves fittingeachnewbaseclassifiertotheresidualerrorstn −fm−1 (xn)fromtheprevious model.
14.10 ( ) Verify that if we minimize the sum-of-squares error between a set of training values {tn } and a single predictive value t, then the optimal solution for t is given bythemeanofthe{tn }.
14.11 ( ) Consider a data set comprising 400 data points from class C 1 and 400 data points from class C 2.
Suppose that a tree model A splits these into (300,100) at thefirstleafnodeand(100,300)atthesecondleafnode, where(n, m)denotesthat n points are assigned to C 1 and m points are assigned to C 2.
Similarly, suppose that a second tree model B splits them into (200,400) and (200,0).
Evaluate the misclassificationratesforthetwotreesandhenceshowthattheyareequal.
Similarly, evaluatethecross-entropy(14.32)and Giniindex(14.33)forthetwotreesandshow thattheyarebothlowerfortree Bthanfortree A.
14.12 ( ) Extendtheresultsof Section14.5.1foramixtureoflinearregressionmodels tothecaseofmultipletargetvaluesdescribedbyavectort.
Todothis, makeuseof theresultsof Section3.1.5.
14.13 ( ) www Verifythatthecomplete-dataloglikelihoodfunctionforthemixtureof linearregressionmodelsisgivenby(14.36).
14.14 ( ) Usethetechniqueof Lagrangemultipliers(Appendix E)toshowthatthe M-step re-estimationequationforthemixingcoefficientsinthemixtureoflinearregression modelstrainedbymaximumlikelihood EMisgivenby(14.38).
14.15 ( ) www Wehavealreadynotedthatifweuseasquaredlossfunctioninaregres- sion problem, the corresponding optimal prediction of the target variable for a new input vector is given by the conditional mean of the predictive distribution.
Show that the conditional mean for the mixture of linear regression models discussed in Section14.5.1isgivenbyalinearcombinationofthemeansofeachcomponentdis- tribution.
Note that if the conditional distribution of the target data is multimodal, theconditionalmeancangivepoorpredictions.
676 14.
COMBININGMODELS 14.16 ( ) Extendthelogisticregressionmixturemodelof Section14.5.2toamixture ofsoftmaxclassifiersrepresenting C 2classes.
Writedownthe EMalgorithmfor determiningtheparametersofthismodelthroughmaximumlikelihood.
14.17 ( ) www Consideramixturemodelforaconditionaldistributionp(t|x)ofthe form K p(t|x)= πkψk(t|x) (14.58) k=1 inwhicheachmixturecomponentψk(t|x)isitselfamixturemodel.
Showthatthis two-level hierarchical mixture is equivalent to a conventional single-level mixture model.
Now suppose that the mixing coefficients in both levels of such a hierar- chical model are arbitrary functions of x.
Again, show that this hierarchical model is again equivalent to a single-level model with x-dependent mixing coefficients.
Finally, consider the case in which the mixing coefficients at both levels of the hi- erarchical mixture are constrained to be linear classification (logistic or softmax) models.
Show that the hierarchical mixture cannot in general be represented by a single-level mixture having linear classification models for the mixing coefficients.
Hint: to do this it is sufficient to construct a single counter-example, so consider a mixtureoftwocomponentsinwhichoneofthosecomponentsisitselfamixtureof twocomponents, withmixingcoefficientsgivenbylinear-logisticmodels.
Showthat thiscannotberepresentedbyasingle-levelmixtureof3componentshavingmixing coefficientsdeterminedbyalinear-softmaxmodel.
Appendix A.
Data Sets Inthisappendix, wegiveabriefintroductiontothedatasetsusedtoillustratesome of the algorithms described in this book.
Detailed information on file formats for these data sets, as well as the data files themselves, can be obtained from the book website: http://research.
microsoft.
com/∼cmbishop/PRML Handwritten Digits The digits data used in this book is taken from the MNIST data set (Le Cun et al., 1998), which itself was constructed by modifying a subset of the much larger data setproducedby NIST(the National Instituteof Standardsand Technology).
Itcom- prises a training set of 60,000 examples and a test set of 10,000 examples.
Some ofthedatawascollectedfrom Census Bureauemployeesandtherestwascollected fromhigh-schoolchildren, andcarewastakentoensurethatthetestexampleswere writtenbydifferentindividualstothetrainingexamples.
The original NIST data had binary (black or white) pixels.
To create MNIST, theseimagesweresizenormalizedtofitina20×20pixelboxwhilepreservingtheir aspectratio.
Asaconsequenceoftheanti-aliasingusedtochangetheresolutionof the images, the resulting MNIST digits are grey scale.
These images were then centredina28×28box.
Examplesofthe MNISTdigitsareshownin Figure A.1.
Error rates for classifying the digits range from 12% for a simple linear classi- fier, through 0.56% for a carefully designed support vector machine, to 0.4% for a convolutionalneuralnetwork(Le Cunetal.,1998).
677 678 A.
DATASETS Figure A.1 One hundred examples of the MNIST digits chosen at ran- domfromthetrainingset.
Oil Flow This is a synthetic data set that arose out of a project aimed at measuring nonin- vasively the proportions of oil, water, and gas in North Sea oil transfer pipelines (Bishopand James,1993).
Itisbasedontheprincipleofdual-energygammadensit- ometry.
Theideasisthatifanarrowbeamofgammaraysispassedthroughthepipe, theattenuationintheintensityofthebeamprovidesinformationaboutthedensityof materialalongitspath.
Thus, forinstance, thebeamwillbeattenuatedmorestrongly byoilthanbygas.
A single attenuation measurement alone is not sufficient because there are two degreesoffreedomcorrespondingtothefractionofoilandthefractionofwater(the fractionofgasisredundantbecausethethreefractionsmustaddtoone).
Toaddress this, twogammabeamsofdifferentenergies(inotherwordsdifferentfrequenciesor wavelengths)arepassedthroughthepipealongthesamepath, andtheattenuationof eachismeasured.
Becausetheabsorbtionpropertiesofdifferentmaterialsvarydif- ferentlyasafunctionofenergy, measurementoftheattenuationsatthetwoenergies providestwoindependentpiecesofinformation.
Giventheknownabsorbtionprop- ertiesofoil, water, andgasatthetwoenergies, itisthenasimplemattertocalculate theaveragefractionsofoilandwater(andhenceofgas)measuredalongthepathof thegammabeams.
Thereisafurthercomplication, however, associatedwiththemotionofthema- terials along the pipe.
If the flow velocity is small, then the oil floats on top of the water with the gas sitting above the oil.
This is known as a laminar or stratified A.
DATASETS 679 Figure A.2 Thethreegeometricalconfigurationsoftheoil, water, andgasphasesusedtogeneratetheoil- flow data set.
For each configuration, the pro- portionsofthethreephasescanvary.
Stratified Annular Oil Water Gas Mix Homogeneous flowconfigurationandisillustratedin Figure A.2.
Astheflowvelocityisincreased, morecomplexgeometricalconfigurationsoftheoil, water, andgascanarise.
Forthe purposes of this data set, two specific idealizations are considered.
In the annular configurationtheoil, water, andgasformconcentriccylinderswiththewateraround theoutsideandthegasinthecentre, whereasinthehomogeneousconfigurationthe oil, water and gas are assumed to be intimately mixed as might occur at high flow velocities under turbulent conditions.
These configurations are also illustrated in Figure A.2.
We have seen that a single dual-energy beam gives the oil and water fractions measuredalongthepathlength, whereasweareinterestedinthevolumefractionsof oil and water.
This can be addressed by using multiple dual-energy gamma densit- ometerswhosebeamspassthroughdifferentregionsofthepipe.
Forthisparticular data set, there are six such beams, and their spatial arrangement is shown in Fig- ure A.3.
A single observation is therefore represented by a 12-dimensional vector comprising the fractions of oil and water measured along the paths of each of the beams.
Weare, however, interestedinobtainingtheoverallvolumefractionsofthe threephasesinthepipe.
Thisismuchliketheclassicalproblemoftomographicre- construction, usedinmedicalimagingforexample, inwhichatwo-dimensionaldis- Figure A.3 Crosssectionofthepipeshowingthearrangementofthe six beam lines, each of which comprises a single dual- energygammadensitometer.
Notethattheverticalbeams are asymmetrically arranged relative to the central axis (shownbythedottedline).
680 A.
DATASETS tributionistobereconstructedfromannumberofone-dimensionalaverages.
Here therearefarfewerlinemeasurementsthaninatypicaltomographyapplication.
On theotherhandtherangeofgeometricalconfigurationsismuchmorelimited, andso the configuration, as well as the phase fractions, can be predicted with reasonable accuracyfromthedensitometerdata.
Forsafetyreasons, theintensityofthegammabeamsiskeptrelativelyweakand sotoobtainanaccuratemeasurementoftheattenuation, themeasuredbeamintensity is integrated over a specific time interval.
For a finite integration time, there are randomfluctuationsinthemeasuredintensityduetothefactthatthegammabeams comprisediscretepacketsofenergycalledphotons.
Inpractice, theintegrationtime ischosenasacompromisebetweenreducingthenoiselevel(whichrequiresalong integrationtime)anddetectingtemporalvariationsintheflow(whichrequiresashort integrationtime).
Theoilflowdatasetisgeneratedusingrealisticknownvaluesfor theabsorptionpropertiesofoil, water, andgasatthetwogammaenergiesused, and withaspecificchoiceofintegrationtime(10seconds)chosenascharacteristicofa typicalpracticalsetup.
Eachpointinthedatasetisgeneratedindependentlyusingthefollowingsteps: 1.
Chooseoneofthethreephaseconfigurationsatrandomwithequalprobability.
2.
Choosethreerandomnumbersf 1, f 2andf 3fromtheuniformdistributionover (0,1)anddefine f f 1 2 f oil = , f water = .
(A.1) f +f +f f +f +f 1 2 3 1 2 3 This treats the three phases on an equal footing and ensures that the volume fractionsaddtoone.
3.
Foreachofthesixbeamlines, calculatetheeffectivepathlengthsthroughoil andwaterforthegivenphaseconfiguration.
4.
Perturb the path lengths using the Poisson distribution based on the known beamintensitiesandintegrationtimetoallowfortheeffectofphotonstatistics.
Eachpointinthedatasetcomprisesthe12pathlengthmeasurements, together with the fractions of oil and water and a binary label describing the phase configu- ration.
Thedatasetisdividedintotraining, validation, andtestsets, eachofwhich comprises 1,000 independent data points.
Details of the data format are available fromthebookwebsite.
In Bishopand James(1993), statistical machinelearningtechniqueswereused topredictthevolumefractionsandalsothegeometricalconfigurationofthephases shown in Figure A.2, from the 12-dimensional vector of measurements.
The 12- dimensional observation vectors can also be used to test data visualization algo- rithms.
This data set has a rich and interesting structure, as follows.
For any given configuration there are two degrees of freedom corresponding to the fractions of A.
DATASETS 681 oilandwater, andsoforinfiniteintegrationtimethedatawilllocallyliveonatwo- dimensionalmanifold.
Forafiniteintegrationtime, theindividualdatapointswillbe perturbed away from the manifold by the photon noise.
In the homogeneous phase configuration, thepathlengthsinoilandwaterarelinearlyrelatedtothefractionsof oil and water, and so the data points lie close to a linear manifold.
For the annular configuration, the relationship between phase fraction and path length is nonlinear and so the manifold will be nonlinear.
In the case of the laminar configuration the situation is even more complex because small variations in the phase fractions can cause one of the horizontal phase boundaries to move across one of the horizontal beamlinesleadingtoadiscontinuousjumpinthe12-dimensionalobservationspace.
Inthisway, thetwo-dimensionalnonlinearmanifoldforthelaminarconfigurationis brokenintosixdistinctsegments.
Notealsothatsomeofthemanifoldsfordifferent phaseconfigurationsmeetatspecificpoints, forexampleifthepipeisfilledentirely with oil, it corresponds to specific instances of the laminar, annular, and homoge- neousconfigurations.
Old Faithful Old Faithful, shownin Figure A.4, isahydrothermalgeyserin Yellowstone National Park in the state of Wyoming, U.
S.
A., and is a popular tourist attraction.
Its name stemsfromthesupposedregularityofitseruptions.
Thedatasetcomprises272observations, eachofwhichrepresentsasingleerup- tionandcontainstwovariablescorrespondingtothedurationinminutesoftheerup- tion, andthetimeuntilthenexteruption, alsoinminutes.
Figure A.5showsaplotof thetimetothenexteruptionversusthedurationoftheeruptions.
Itcanbeseenthat thetimetothenexteruptionvariesconsiderably, althoughknowledgeoftheduration ofthecurrenteruptionallowsittobepredictedmoreaccurately.
Notethatthereexist severalotherdatasetsrelatingtotheeruptionsof Old Faithful.
Figure A.4 The Old Faithful geyser in Yellowstone National Park.
c Bruce T.
Gourley www.
brucegourley.
com.
682 A.
DATASETS Figure A.5 Plotofthetimetothenexteruption 100 inminutes(verticalaxis)versusthe duration of the eruption in minutes (horizontalaxis)forthe Old Faithful 90 dataset.
80 70 60 50 40 1 2 3 4 5 6 Synthetic Data Throughoutthebook, weusetwosimplesyntheticdatasetstoillustratemanyofthe algorithms.
Thefirstoftheseisaregressionproblem, basedonthesinusoidalfunc- tion, shownin Figure A.6.
Theinputvalues{xn }aregenerateduniformlyinrange (0,1), andthecorrespondingtargetvalues{tn }areobtainedbyfirstcomputingthe correspondingvaluesofthefunctionsin(2πx), andthenaddingrandomnoisewith a Gaussiandistributionhavingstandarddeviation0.3.
Variousformsofthisdataset, havingdifferentnumbersofdatapoints, areusedinthebook.
The second data set is a classification problem having two classes, with equal prior probabilities, and is shown in Figure A.7.
The blue class is generated from a single Gaussian while the red class comes from a mixture of two Gaussians.
Be- cause we know the class priors and the class-conditional densities, it is straightfor- ward to evaluate and plot the true posterior probabilities as well as the minimum misclassification-ratedecisionboundary, asshownin Figure A.7.
A.
DATASETS 683 1 1 t t 0 0 −1 −1 0 1 0 1 x x Figure A.6 The left-hand plot shows the synthetic regression data set along with the underlying sinusoidal functionfromwhichthedatapointsweregenerated.
Theright-handplotshowsthetrueconditionaldistribution p(t|x)fromwhichthelabelsaregenerated, inwhichthegreencurvedenotesthemean, andtheshadedregion spansonestandarddeviationoneachsideofthemean.
2 0 −2 −2 0 2 Figure A.7 The left plot shows the synthetic classification data set with data from the two classes shown in redandblue.
Ontherightisaplotofthetrueposteriorprobabilities, shownonacolourscalegoingfrompure red denoting probability of the red class is 1 to pure blue denoting probability of the red class is 0.
Because these probabilities are known, the optimal decision boundary for minimizing the misclassification rate (which corresponds to the contour along which the posterior probabilities for each class equal 0.5) can be evaluated andisshownbythegreencurve.
Thisdecisionboundaryisalsoplottedontheleft-handfigure.
Appendix B.
Probability Distributions Inthisappendix, wesummarizethemainpropertiesofsomeofthemostwidelyused probabilitydistributions, andforeachdistributionwelistsomekeystatisticssuchas theexpectation E[x], thevariance (or covariance), themode, and theentropy H[x].
Allofthesedistributionsaremembersoftheexponentialfamilyandarewidelyused asbuildingblocksformoresophisticatedprobabilisticmodels.
Bernoulli This is the distribution for a single binary variable x ∈ {0,1} representing, for example, theresultofflippingacoin.
Itisgovernedbyasinglecontinuousparameter µ∈[0,1]thatrepresentstheprobabilityofx=1.
Bern(x|µ) = µ x (1−µ)1−x (B.1) E[x] = µ (B.2) var[x] = µ(1−µ) (B.3) 1 ifµ 0.5, mode[x] = (B.4) 0 otherwise H[x] = −µlnµ−(1−µ)ln(1−µ).
(B.5) The Bernoulli is a special case of the binomial distribution for the case of a single observation.
Itsconjugatepriorforµisthebetadistribution.
685 686 B.
PROBABILITYDISTRIBUTIONS Beta This is a distribution over a continuous variable µ ∈ [0,1], which is often used to representtheprobabilityforsomebinaryevent.
Itisgovernedbytwoparametersa andbthatareconstrainedbya > 0andb > 0toensurethatthedistributioncanbe normalized.
Γ(a+b) Beta(µ|a, b) = µ a−1(1−µ) b−1 (B.6) Γ(a)Γ(b) a E[µ] = (B.7) a+b ab var[µ] = (B.8) (a+b)2(a+b+1) a−1 mode[µ] = .
(B.9) a+b−2 Thebetaistheconjugatepriorforthe Bernoullidistribution, forwhichaandbcan be interpreted as the effective prior number of observations of x = 1 and x = 0, respectively.
Itsdensityisfiniteifa 1andb 1, otherwisethereisasingularity atµ=0and/orµ=1.
Fora=b=1, itreducestoauniformdistribution.
Thebeta distributionisaspecialcaseofthe K-state Dirichletdistributionfor K =2.
Binomial Thebinomialdistributiongivestheprobabilityofobservingmoccurrencesofx=1 inasetof N samplesfroma Bernoullidistribution, wheretheprobabilityofobserv- ingx=1isµ∈[0,1].
N Bin(m|N,µ) = µ m (1−µ) N−m (B.10) m E[m] = Nµ (B.11) var[m] = Nµ(1−µ) (B.12) mode[m] = (N +1)µ (B.13) where (N +1)µ denotesthelargestintegerthatislessthanorequalto(N +1)µ, andthequantity N N! = (B.14) m m!(N −m)! denotes the number of ways of choosing m objects out of a total of N identical objects.
Here m!, pronounced ‘factorial m’, denotes the product m×(m−1)× ...,×2×1.
Theparticularcaseofthebinomialdistributionfor N =1isknownas the Bernoullidistribution, andforlarge N thebinomialdistributionisapproximately Gaussian.
Theconjugatepriorforµisthebetadistribution.
B.
PROBABILITYDISTRIBUTIONS 687 Dirichlet The Dirichlet is a multivariate distribution over K random variables 0 µ k 1, wherek =1,..., K, subjecttotheconstraints K 0 µ k 1, µ k =1.
(B.15) k=1 Denotingµ=(µ 1 ,...,µ K)T andα=(α 1 ,...,α K)T, wehave K Dir(µ|α) = C(α) µα k −1 (B.16) k k=1 α E[µ k] = α k (B.17) var[µ k] = α α k 2 ( ( α α − + α 1 k ) ) (B.18) α α cov[µ j µ k] = − α 2(α j + k 1) (B.19) mode[µ k] = α α k − − K 1 (B.20) E[lnµ k] = ψ(α k)−ψ(α ) (B.21) K H[µ] = − (α k −1){ψ(α k)−ψ(α )}−ln C(α) (B.22) k=1 where Γ(α ) C(α)= (B.23) Γ(α 1 )···Γ(α K) and K α = α k .
(B.24) k=1 Here d ψ(a)≡ lnΓ(a) (B.25) da isknownasthedigammafunction(Abramowitzand Stegun,1965).
Theparameters α k aresubjecttotheconstraintα k >0inordertoensurethatthedistributioncanbe normalized.
The Dirichletformstheconjugatepriorforthemultinomialdistributionandrep- resentsageneralizationofthebetadistribution.
Inthiscase, theparametersα k can be interpreted as effective numbers of observations of the corresponding values of the K-dimensional binary observation vector x.
As with the beta distribution, the Dirichlethasfinitedensityeverywhereprovidedα k 1forallk.
688 B.
PROBABILITYDISTRIBUTIONS Gamma The Gamma is a probability distribution over a positive random variable τ > 0 governedbyparametersaandbthataresubjecttotheconstraintsa > 0andb > 0 toensurethatthedistributioncanbenormalized.
1 Gam(τ|a, b) = b a τ a−1e −bτ (B.26) Γ(a) a E[τ] = (B.27) b a var[τ] = (B.28) b2 a−1 mode[τ] = for α 1 (B.29) b E[lnτ] = ψ(a)−lnb (B.30) H[τ] = lnΓ(a)−(a−1)ψ(a)−lnb+a (B.31) where ψ(·) is the digamma function defined by (B.25).
The gamma distribution is theconjugatepriorfortheprecision(inversevariance)ofaunivariate Gaussian.
For a 1thedensityiseverywherefinite, andthespecialcaseofa=1isknownasthe exponentialdistribution.
Gaussian The Gaussianisthemostwidelyuseddistributionforcontinuousvariables.
Itisalso knownasthenormaldistribution.
Inthecaseofasinglevariablex∈(−∞,∞)itis governedbytwoparameters, themeanµ∈(−∞,∞)andthevarianceσ2 >0.
1 1 N(x|µ,σ2) = exp − (x−µ)2 (B.32) (2πσ2) 1/2 2σ2 E[x] = µ (B.33) var[x] = σ2 (B.34) mode[x] = µ (B.35) 1 1 H[x] = lnσ2+ (1+ln(2π)).
(B.36) 2 2 The inverse of the variance τ = 1/σ2 is called the precision, and the square root of the variance σ is called the standard deviation.
The conjugate prior for µ is the Gaussian, and the conjugate prior for τ is the gamma distribution.
If both µ and τ areunknown, theirjointconjugateprioristhe Gaussian-gammadistribution.
For a D-dimensional vector x, the Gaussian is governed by a D-dimensional mean vector µ and a D × D covariance matrix Σ that must be symmetric and B.
PROBABILITYDISTRIBUTIONS 689 positive-definite.
1 1 1 N(x|µ,Σ) = exp − (x−µ)TΣ −1(x−µ) (B.37) (2π) D/2|Σ|1/2 2 E[x] = µ (B.38) cov[x] = Σ (B.39) mode[x] = µ (B.40) 1 D H[x] = ln|Σ|+ (1+ln(2π)).
(B.41) 2 2 TheinverseofthecovariancematrixΛ=Σ −1 istheprecisionmatrix, whichisalso symmetricandpositivedefinite.
Averagesofrandomvariablestendtoa Gaussian, by thecentrallimittheorem, andthesumoftwo Gaussianvariablesisagain Gaussian.
The Gaussian is the distribution that maximizes the entropy for a given variance (or covariance).
Any linear transformation of a Gaussian random variable is again Gaussian.
The marginal distribution of a multivariate Gaussian with respect to a subsetofthevariablesisitself Gaussian, andsimilarlytheconditionaldistributionis also Gaussian.
Theconjugatepriorforµisthe Gaussian, theconjugatepriorforΛ isthe Wishart, andtheconjugatepriorfor(µ,Λ)isthe Gaussian-Wishart.
If we have a marginal Gaussian distribution for x and a conditional Gaussian distributionforygivenxintheform p(x) = N(x|µ,Λ −1) (B.42) p(y|x) = N(y|Ax+b, L −1) (B.43) thenthemarginaldistributionofy, andtheconditionaldistributionofxgiveny, are givenby p(y) = N(y|Aµ+b, L −1+AΛ −1AT) (B.44) p(x|y) = N(x|Σ{ATL(y−b)+Λµ},Σ) (B.45) where Σ=(Λ+ATLA) −1.
(B.46) If we have a joint Gaussian distribution N(x|µ,Σ) with Λ ≡ Σ −1 and we definethefollowingpartitions x= xa , µ= µ a (B.47) xb µ b Σaa Σab Λaa Λab Σ= , Λ= (B.48) Σba Σbb Λba Λbb thentheconditionaldistributionp(xa |xb)isgivenby p(xa |xb) = N(x|µ a|b ,Λ − aa 1) (B.49) µ a|b = µ a −Λ − aa 1Λab(xb −µ b ) (B.50) 690 B.
PROBABILITYDISTRIBUTIONS andthemarginaldistributionp(xa)isgivenby p(xa)=N(xa |µ a ,Σaa).
(B.51) Gaussian-Gamma This is the conjugate prior distribution for a univariate Gaussian N(x|µ,λ−1) in which the mean µ and the precision λ are both unknown and is also called the normal-gammadistribution.
Itcomprisestheproductofa Gaussiandistributionfor µ, whoseprecisionisproportionaltoλ, andagammadistributionoverλ.
p(µ,λ|µ 0 ,β, a, b)=N µ|µo,(βλ) −1 Gam(λ|a, b).
(B.52) Gaussian-Wishart This is the conjugate prior distribution for a multivariate Gaussian N(x|µ,Λ) in which both the mean µ and the precision Λ are unknown, and is also called the normal-Wishartdistribution.
Itcomprisestheproductofa Gaussiandistributionfor µ, whoseprecisionisproportionaltoΛ, anda WishartdistributionoverΛ.
p(µ,Λ|µ ,β, W,ν)=N µ|µ ,(βΛ) −1 W(Λ|W,ν).
(B.53) 0 0 Fortheparticularcaseofascalarx, thisisequivalenttothe Gaussian-gammadistri- bution.
Multinomial If we generalize the Bernoulli distributio n to an K-dimensional binary variable x with components xk ∈ {0,1} such that k xk = 1, then we obtain the following discretedistribution K x p(x) = µ k (B.54) k k=1 E[xk] = µk (B.55) var[xk] = µk(1−µk) (B.56) cov[xjxk] = Ijkµk (B.57) M H[x] = − µklnµk (B.58) k=1 B.
PROBABILITYDISTRIBUTIONS 691 where Ijk is the j, k element of the ident ity matrix.
Because p(xk = 1) = µk, the parametersmustsatisfy0 µk 1and k µk =1.
Themultinomialdistributionisamultivariategeneralizationofthebinomialand givesthedistributionovercountsmk fora K-statediscretevariabletobeinstatek givenatotalnumberofobservations N.
M N Mult(m 1 , m 2 ,..., m K |µ, N) = m 1 m 2 ...
m M µ m k k (B.59) k=1 E[mk] = Nµk (B.60) var[mk] = Nµk(1−µk) (B.61) cov[mjmk] = −Nµjµk (B.62) whereµ=(µ 1 ,...,µK)T, andthequantity N N! = (B.63) m 1 m 2 ...
m K m 1 !...
m K! givesthenumberofwaysoftaking N identicalobjectsandassigningmk ofthemto binkfork =1,..., K.
Thevalueofµkgivestheprobabilityoftherandomvariable takin g state k, and so these parameters are subject to the constraints 0 µk 1 and k µk = 1.
The conjugate prior distribution for the parameters {µk } is the Dirichlet.
Normal The normal distribution is simply another name for the Gaussian.
In this book, we use the term Gaussian throughout, although we retain the conventional use of the symbol N todenotethisdistribution.
Forconsistency, weshallrefertothenormal- gamma distribution as the Gaussian-gamma distribution, and similarly the normal- Wishartiscalledthe Gaussian-Wishart.
Student’s t Thisdistribution waspublishedby William Gossetin1908, buthisemployer, Gui- ness Breweries, requiredhimtopublishunderapseudonym, sohechose‘Student’.
In the univariate form, Student’s t-distribution is obtained by placing a conjugate gamma prior over the precision of a univariate Gaussian distribution and then inte- grating out the precision variable.
It can therefore be viewed as an infinite mixture 692 B.
PROBABILITYDISTRIBUTIONS of Gaussianshavingthesamemeanbutdifferentvariances.
Γ(ν/2+1/2) λ 1/2 λ(x−µ)2 −ν/2−1/2 St(x|µ,λ,ν) = 1+ (B.64) Γ(ν/2) πν ν E[x] = µ for ν >1 (B.65) 1 ν var[x] = for ν >2 (B.66) λν−2 mode[x] = µ.
(B.67) Here ν > 0 is called the number of degrees of freedom of the distribution.
The particularcaseofν =1iscalledthe Cauchydistribution.
Fora D-dimensionalvariablex, Student’st-distributioncorrespondstomarginal- izing the precision matrix of a multivariate Gaussian with respect to a conjugate Wishartpriorandtakestheform Γ(ν/2+D/2) |Λ|1/2 ∆2 −ν/2−D/2 St(x|µ,Λ,ν) = 1+ (B.68) Γ(ν/2) (νπ)D/2 ν E[x] = µ for ν >1 (B.69) ν cov[x] = Λ −1 for ν >2 (B.70) ν−2 mode[x] = µ (B.71) where∆2 isthesquared Mahalanobisdistancedefinedby ∆2 =(x−µ)TΛ(x−µ).
(B.72) In the limit ν → ∞, the t-distribution reduces to a Gaussian with mean µ and pre- cision Λ.
Student’s t-distribution provides a generalization of the Gaussian whose maximumlikelihoodparametervaluesarerobusttooutliers.
Uniform Thisisasimpledistributionforacontinuousvariablexdefinedoverafiniteinterval x∈[a, b]whereb>a.
1 U(x|a, b) = (B.73) b−a (b+a) E[x] = (B.74) 2 (b−a)2 var[x] = (B.75) 12 H[x] = ln(b−a).
(B.76) Ifxhasdistribution U(x|0,1), thena+(b−a)xwillhavedistribution U(x|a, b).
B.
PROBABILITYDISTRIBUTIONS 693 Von Mises Thevon Misesdistribution, alsoknownasthecircularnormalorthecircular Gaus- sian, isaunivariate Gaussian-likeperiodicdistributionforavariableθ ∈[0,2π).
1 p(θ|θ 0 , m) = exp{mcos(θ−θ 0 )} (B.77) 2πI (m) 0 where I 0 (m) is the zeroth-order Bessel function of the first kind.
The distribution has period 2π so that p(θ +2π) = p(θ) for all θ.
Care must be taken in interpret- ingthisdistributionbecausesimpleexpectationswillbedependentonthe(arbitrary) choice of origin for the variable θ.
The parameter θ 0 is analogous to the mean of a univariate Gaussian, andtheparameterm > 0, knownastheconcentrationparam- eter, is analogous to the precision (inverse variance).
For large m, the von Mises distributionisapproximatelya Gaussiancentredonθ 0.
Wishart The Wishart distribution is the conjugate prior for the precision matrix of a multi- variate Gaussian.
1 W(Λ|W,ν)=B(W,ν)|Λ|(ν−D−1)/2exp − Tr(W −1Λ) (B.78) 2 where D −1 ν+1−i B(W,ν) ≡ |W|−ν/2 2 νD/2π D(D−1)/4 Γ (B.79) 2 i=1 E[Λ] = νW (B.80) D ν+1−i E[ln|Λ|] = ψ +Dln2+ln|W| (B.81) 2 i=1 (ν−D−1) νD H[Λ] = −ln B(W,ν)− E[ln|Λ|]+ (B.82) 2 2 where W isa D×D symmetric, positivedefinitematrix, andψ(·)isthedigamma function defined by (B.25).
The parameter ν is called the number of degrees of freedomofthedistributionandisrestrictedtoν >D−1toensurethatthe Gamma function in the normalization factor is well-defined.
In one dimension, the Wishart reduces to the gamma distribution Gam(λ|a, b) given by (B.26) with parameters a=ν/2andb=1/2W.
Appendix C.
Properties of Matrices Inthisappendix, wegathertogethersomeusefulpropertiesandidentitiesinvolving matrices and determinants.
This is not intended to be an introductory tutorial, and itisassumedthatthereaderisalreadyfamiliarwithbasiclinearalgebra.
Forsome results, we indicate how to prove them, whereas in more complex cases we leave the interested reader to refer to standard textbooks on the subject.
In all cases, we assume that inverses exist and that matrix dimensions are such that the formulae arecorrectlydefined.
Acomprehensivediscussionoflinearalgebracanbefoundin Goluband Van Loan(1996), andanextensivecollectionofmatrixpropertiesisgiven by Lu¨tkepohl (1996).
Matrix derivatives are discussed in Magnus and Neudecker (1999).
Basic Matrix Identities Amatrix Ahaselements Aij whereiindexestherows, andj indexesthecolumns.
We use IN to denote the N ×N identity matrix (also called the unit matrix), and where there is no ambiguity over dimensionality we simply use I.
The transpose matrix AT haselements(AT)ij =Aji.
Fromthedefinitionoftranspose, wehave (AB)T =BTAT (C.1) which can be verified by writing out the indices.
The inverse of A, denoted A−1, satisfies AA −1 =A −1A=I.
(C.2) Because ABB−1A−1 =I, wehave (AB) −1 =B −1A −1.
(C.3) Alsowehave AT −1 = A −1 T (C.4) 695 696 C.
PROPERTIESOFMATRICES whichiseasilyprovenbytakingthetransposeof(C.2)andapplying(C.1).
Ausefulidentityinvolvingmatrixinversesisthefollowing (P −1+BTR −1B) −1BTR −1 =PBT(BPBT+R) −1.
(C.5) whichiseasilyverifiedbyrightmultiplyingbothsidesby(BPBT +R).
Suppose that Phasdimensionality N ×N while Rhasdimensionality M ×M, sothat Bis M ×N.
Thenif M N, itwillbemuchcheapertoevaluatetheright-handsideof (C.5)thantheleft-handside.
Aspecialcasethatsometimesarisesis (I+AB) −1A=A(I+BA) −1.
(C.6) Anotherusefulidentityinvolvinginversesisthefollowing: (A+BD −1C) −1 =A −1−A −1B(D+CA −1B) −1CA −1 (C.7) whichisknownasthe Woodburyidentityandwhichcanbeverifiedbymultiplying both sides by (A + BD−1C).
This is useful, for instance, when A is large and diagonal, and hence easy to invert, while B has many rows but few columns (and conversely for C) so that the right-hand side is much cheaper to evaluate than the left-handside.
A set of vectors {a 1 ,..., a N } is said to be linearly independent if the relation n αnan = 0 holds only if all αn = 0.
This implies that none of the vectors can be expressed as a linear combination of the remainder.
The rank of a matrix is the maximum number of linearly independent rows (or equivalently the maximum numberoflinearlyindependentcolumns).
Traces and Determinants Trace and determinant apply to square matrices.
The trace Tr(A) of a matrix A is defined as the sum of the elements on the leading diagonal.
By writing out the indices, weseethat Tr(AB)=Tr(BA).
(C.8) Byapplyingthisformulamultipletimestotheproductofthreematrices, weseethat Tr(ABC)=Tr(CAB)=Tr(BCA) (C.9) whichisknownasthecyclicpropertyofthetraceoperatorandwhichclearlyextends totheproductofanynumberofmatrices.
Thedeterminant|A|ofan N ×N matrix Aisdefinedby |A|= (±1)A 1i 1 A 2i 2 ···ANi N (C.10) inwhichthesumistakenoverallproductsconsistingofpreciselyoneelementfrom eachrowandoneelementfromeachcolumn, withacoefficient+1or−1according C.
PROPERTIESOFMATRICES 697 Thus, fora2×2matrix, thedeterminanttakestheform |A|= a a 11 a a 12 =a 11 a 22 −a 12 a 21 .
(C.11) 21 22 Thedeterminantofaproductoftwomatricesisgivenby |AB|=|A||B| (C.12) ascanbeshownfrom(C.10).
Also, thedeterminantofaninversematrixisgivenby A −1 = 1 (C.13) |A| whichcanbeshownbytakingthedeterminantof(C.2)andapplying(C.12).
If Aand Barematricesofsize N ×M, then IN +ABT = IM +ATB .
(C.14) Ausefulspecialcaseis IN +ab T =1+a Tb (C.15) whereaandbare N-dimensionalcolumnvectors.
Matrix Derivatives Sometimes we need to consider derivatives of vectors and matrices with respect to scalars.
Thederivativeofavectorawithrespecttoascalarxisitselfavectorwhose componentsaregivenby ∂a ∂ai = (C.16) ∂x ∂x i withananalogousdefinitionforthederivativeofamatrix.
Derivativeswithrespect tovectorsandmatricescanalsobedefined, forinstance ∂x ∂x = (C.17) ∂a i ∂ai andsimilarly ∂a ∂ai = .
(C.18) ∂b ij ∂bj Thefollowingiseasilyprovenbywritingoutthecomponents ∂ ∂ x Ta = a Tx =a.
(C.19) ∂x ∂x 698 C.
PROPERTIESOFMATRICES Similarly ∂ ∂A ∂B (AB)= B+A .
(C.20) ∂x ∂x ∂x Thederivativeoftheinverseofamatrixcanbeexpressedas ∂ ∂A A −1 =−A −1 A −1 (C.21) ∂x ∂x as can be shown by differentiating the equation A−1A = I using (C.20) and then rightmultiplyingby A−1.
Also ∂ ∂A ln|A|=Tr A −1 (C.22) ∂x ∂x whichweshallprovelater.
Ifwechoosextobeoneoftheelementsof A, wehave ∂ Tr(AB)=Bji (C.23) ∂Aij as can be seen by writing out the matrices using index notation.
We can write this resultmorecompactlyintheform ∂ Tr(AB)=BT.
(C.24) ∂A Withthisnotation, wehavethefollowingproperties ∂ Tr ATB = B (C.25) ∂A ∂ Tr(A) = I (C.26) ∂A ∂ Tr(ABAT) = A(B+BT) (C.27) ∂A whichcanagainbeprovenbywritingoutthematrixindices.
Wealsohave ∂ ln|A|= A −1 T (C.28) ∂A whichfollowsfrom(C.22)and(C.26).
Eigenvector Equation Forasquarematrix Aofsize M ×M, theeigenvectorequationisdefinedby Aui =λiui (C.29) C.
PROPERTIESOFMATRICES 699 fori=1,..., M, whereuiisaneigenvectorandλiisthecorrespondingeigenvalue.
Thiscanbeviewedasasetof M simultaneoushomogeneouslinearequations, and theconditionforasolutionisthat |A−λi I|=0 (C.30) whichisknownasthecharacteristicequation.
Becausethisisapolynomialoforder M inλi, itmusthave M solutions(thoughtheseneednotallbedistinct).
Therank of Aisequaltothenumberofnonzeroeigenvalues.
Of particular interest are symmetric matrices, which arise as covariance ma- trices, kernel matrices, and Hessians.
Symmetric matrices have the property that Aij =Aji, orequivalently AT =A.
Theinverseofasymmetricmatrixisalsosym- metric, ascanbeseenbytakingthetransposeof A−1A = Iandusing AA−1 = I togetherwiththesymmetryof I.
Ingeneral, theeigenvaluesofamatrixarecomplexnumbers, butforsymmetric matricestheeigenvaluesλiarereal.
Thiscanbeseenbyfirstleftmultiplying(C.29) by(u )T, where denotesthecomplexconjugate, togive i T T (u i ) Aui =λi(u i ) ui.
(C.31) Nextwetakethecomplexconjugateof(C.29)andleftmultiplybyu T togive i u TAu =λ u Tu .
(C.32) i i i i i where we have used A = A because we consider only real matrices A.
Taking thetransposeofthesecondoftheseequations, andusing AT = A, weseethatthe left-hand sides of the two equations are equal, and hence that λ i = λi and so λi mustbereal.
Theeigenvectorsuiofarealsymmetricmatrixcanbechosentobeorthonormal (i.
e., orthogonalandofunitlength)sothat u T i uj =Iij (C.33) where Iij aretheelementsoftheidentitymatrix I.
Toshowthis, wefirstleftmultiply (C.29)byu T togive j u T j Aui =λiu T j ui (C.34) andhence, byexchangeofindices, wehave u T i Auj =λju T i uj.
(C.35) We now take the transpose of the second equation and make use of the symmetry property AT =A, andthensubtractthetwoequationstogive (λi −λj)u T i uj =0.
(C.36) Hence, forλi = λj, wehaveu T i uj = 0, andhenceui anduj areorthogonal.
Ifthe twoeigenvaluesareequal, thenanylinearcombinationαui+βuj isalsoaneigen- vectorwiththesameeigenvalue, sowecanselectonelinearcombinationarbitrarily, 700 C.
PROPERTIESOFMATRICES andthenchoosethesecondtobeorthogonaltothefirst(itcanbeshownthatthede- generateeigenvectorsareneverlinearlydependent).
Hencetheeigenvectorscanbe chosentobeorthogonal, andbynormalizingcanbesettounitlength.
Becausethere are M eigenvalues, the corresponding M orthogonal eigenvectors form a complete set and so any M-dimensional vector can be expressed as a linear combination of theeigenvectors.
We can take the eigenvectors ui to be the columns of an M × M matrix U, whichfromorthonormalitysatisfies UTU=I.
(C.37) Suchamatrixissaidtobeorthogonal.
Interestingly, therowsofthismatrixarealso orthogonal, sothat UUT = I.
Toshowthis, notethat(C.37)implies UTUU−1 = U−1 =UT andso UU−1 =UUT =I.
Using(C.12), italsofollowsthat|U|=1.
Theeigenvectorequation(C.29)canbeexpressedintermsof Uintheform AU=UΛ (C.38) where Λ is an M ×M diagonal matrix whose diagonal elements are given by the eigenvaluesλi.
Ifweconsideracolumnvectorxthatistransformedbyanorthogonalmatrix U togiveanewvector x =Ux (C.39) thenthelengthofthevectorispreservedbecause x Tx =x TUTUx=x Tx (C.40) andsimilarlytheanglebetweenanytwosuchvectorsispreservedbecause x Ty =x TUTUy =x Ty.
(C.41) Thus, multiplication by U can be interpreted as a rigid rotation of the coordinate system.
From(C.38), itfollowsthat UTAU=Λ (C.42) andbecauseΛisadiagonalmatrix, wesaythatthematrix Aisdiagonalizedbythe matrix U.
Ifweleftmultiplyby Uandrightmultiplyby UT, weobtain A=UΛUT (C.43) Taking the inverse of this equation, and using (C.3) together with U−1 = UT, we have A −1 =UΛ −1UT.
(C.44) C.
PROPERTIESOFMATRICES 701 Theselasttwoequationscanalsobewrittenintheform M A = λiuiu T i (C.45) i=1 M 1 A −1 = uiu T i .
(C.46) λi i=1 Ifwetakethedeterminantof(C.43), anduse(C.12), weobtain M |A|= λi.
(C.47) i=1 Similarly, takingthetraceof(C.43), andusingthecyclicproperty(C.8)ofthetrace operatortogetherwith UTU=I, wehave M Tr(A)= λi.
(C.48) i=1 Weleaveitasanexerciseforthereadertoverify(C.22)bymakinguseoftheresults Amatrix Aissaidtobepositivedefinite, denotedby A 0, ifw TAw >0for allvaluesofthevectorw.
Equivalently, apositivedefinitematrixhasλi > 0forall of its eigenvalues (as can be seen by setting w to each of the eigenvectors in turn, andbynotingthatanarbitraryvectorcanbeexpandedasalinearcombinationofthe eigenvectors).
Note that positive definite is not the same as all the elements being positive.
Forexample, thematrix 1 2 (C.49) 3 4 haseigenvaluesλ 1 5.37andλ 2 −0.37.
Amatrixissaidtobepositivesemidef- inite if w TAw 0 holds for all values of w, which is denoted A 0, and is equivalenttoλi 0.
Appendix D.
Calculus of Variations We can think of a function y(x) as being an operator that, for any input value x, returns an output value y.
In the same way, we can define a functional F[y] to be anoperatorthattakesafunctiony(x)andreturnsanoutputvalue F.
Anexampleof afunctionalisthelengthofacurvedrawninatwo-dimensionalplaneinwhichthe pathofthecurveisdefinedintermsofafunction.
Inthecontextofmachinelearning, awidelyusedfunctionalistheentropy H[x]foracontinuousvariablexbecause, for anychoiceofprobabilitydensityfunctionp(x), itreturnsascalarvaluerepresenting theentropyofxunderthatdensity.
Thustheentropyofp(x)couldequallywellhave beenwrittenas H[p].
A common problem in conventional calculus is to find a value of x that max- imizes (or minimizes) a function y(x).
Similarly, in the calculus of variations we seekafunctiony(x)thatmaximizes(orminimizes)afunctional F[y].
Thatis, ofall possiblefunctionsy(x), wewishtofindtheparticularfunctionforwhichthefunc- tional F[y]isamaximum(orminimum).
Thecalculusofvariationscanbeused, for instance, to show that the shortest path between two points is a straight line or that themaximumentropydistributionisa Gaussian.
If we weren’t familiar with the rules of ordinary calculus, we could evaluate a conventional derivative dy/dx by making a small change to the variable x and thenexpandinginpowersof , sothat dy y(x+ )=y(x)+ +O( 2) (D.1) dx and finally taking the limit → 0.
Similarly, for a function of several variables y(x 1 ,..., x D), thecorrespondingpartialderivativesaredefinedby D ∂y ∂xi i=1 The analogous definition of a functional derivative arises when we consider how muchafunctional F[y]changeswhenwemakeasmallchange η(x)tothefunction 703 704 D.
CALCULUSOFVARIATIONS Figure D.1 A functional derivative can be defined by considering how the value of a functional F[y] changes when the function y(x) is changed to y(x)+ η(x) where η(x) is an arbitraryfunctionofx.
y(x) y(x)+ η(x) x y(x), whereη(x)isanarbitraryfunctionofx, asillustratedin Figure D.1.
Wedenote thefunctionalderivativeof E[f]withrespecttof(x)byδF/δf(x), anddefineitby thefollowingrelation: δF F[y(x)+ η(x)]=F[y(x)]+ η(x)dx+O( 2).
(D.3) δy(x) This can be seen as a natural extension of (D.2) in which F[y] now depends on a continuoussetofvariables, namelythevaluesofyatallpointsx.
Requiringthatthe functionalbestationarywithrespecttosmallvariationsinthefunctiony(x)gives δE η(x)dx=0.
(D.4) δy(x) Becausethismustholdforanarbitrarychoiceofη(x), itfollowsthatthefunctional derivativemustvanish.
Toseethis, imaginechoosingaperturbationη(x)thatiszero everywhere except in the neighbourhood of a point x, in which case the functional derivative must be zero at x = x.
However, because this must be true for every choiceof x, thefunctionalderivativemustvanishforallvaluesofx.
Consider a functional that is defined by an integral over a function G(y, y , x) thatdependsonbothy(x)anditsderivativey (x)aswellashavingadirectdepen- denceonx F[y]= G(y(x), y (x), x) dx (D.5) where the value of y(x) is assumed to be fixed at the boundary of the region of integration(whichmightbeatinfinity).
Ifwenowconsidervariationsinthefunction y(x), weobtain ∂G ∂G F[y(x)+ η(x)]=F[y(x)]+ η(x)+ η (x) dx+O( 2).
(D.6) ∂y ∂y Wenowhavetocastthisintheform(D.3).
Todoso, weintegratethesecondtermby partsandmakeuseofthefactthatη(x)mustvanishattheboundaryoftheintegral (becausey(x)isfixedattheboundary).
Thisgives ∂G d ∂G F[y(x)+ η(x)]=F[y(x)]+ − η(x)dx+O( 2) (D.7) ∂y dx ∂y D.
CALCULUSOFVARIATIONS 705 from which we can read off the functional derivative by comparison with (D.3).
Requiringthatthefunctionalderivativevanishesthengives ∂G d ∂G − =0 (D.8) ∂y dx ∂y whichareknownasthe Euler-Lagrangeequations.
Forexample, if G=y(x)2+(y (x)) 2 (D.9) thenthe Euler-Lagrangeequationstaketheform d2y y(x)− =0.
(D.10) dx2 Thissecondorderdifferentialequationcanbesolvedfory(x)bymakinguseofthe boundaryconditionsony(x).
Often, we consider functionals defined by integrals whose integrands take the form G(y, x)andthatdonotdependonthederivativesofy(x).
Inthiscase, station- aritysimplyrequiresthat∂G/∂y(x)=0forallvaluesofx.
Ifweareoptimizingafunctionalwithrespecttoaprobabilitydistribution, then weneedtomaintainthenormalizationconstraintontheprobabilities.
Thisisoften Appendix E most conveniently done using a Lagrange multiplier, which then allows an uncon- strainedoptimizationtobeperformed.
Theextensionoftheaboveresultstoamultidimensionalvariablexisstraight- forward.
For a more comprehensive discussion of the calculus of variations, see Sagan(1969).
Appendix E.
Lagrange Multipliers Lagrange multipliers, also sometimes called undetermined multipliers, are used to find the stationary points of a function of several variables subject to one or more constraints.
Considertheproblemoffindingthemaximumofafunctionf(x 1 , x 2 )subjectto aconstraintrelatingx 1 andx 2, whichwewriteintheform g(x 1 , x 2 )=0.
(E.1) Oneapproachwouldbetosolvetheconstraintequation(E.1)andthusexpressx 2 as afunctionofx 1 intheformx 2 =h(x 1 ).
Thiscanthenbesubstitutedintof(x 1 , x 2 ) togiveafunctionofx 1 aloneoftheformf(x 1 , h(x 1 )).
Themaximumwithrespect tox 1 couldthenbefoundbydifferentiationintheusualway, togivethestationary valuex 1 , withthecorrespondingvalueofx 2 givenbyx 2 =h(x 1 ).
One problem with this approach is that it may be difficult to find an analytic solutionoftheconstraintequationthatallowsx 2tobeexpressedasanexplicitfunc- tionofx 1.
Also, thisapproachtreatsx 1 andx 2 differentlyandsospoilsthenatural symmetrybetweenthesevariables.
A more elegant, and often simpler, approach is based on the introduction of a parameter λ called a Lagrange multiplier.
We shall motivate this technique from a geometrical perspective.
Consider a D-dimensional variable x with components x 1 ,..., x D.
Theconstraintequationg(x)=0thenrepresentsa(D−1)-dimensional surfaceinx-spaceasindicatedin Figure E.1.
We first note that at any point on the constraint surface the gradient ∇g(x) of theconstraintfunctionwillbeorthogonaltothesurface.
Toseethis, considerapoint xthatliesontheconstraintsurface, andconsideranearbypointx+ thatalsolies onthesurface.
Ifwemakea Taylorexpansionaroundx, wehave g(x+ ) g(x)+ T∇g(x).
(E.2) Becausebothxandx+ lieontheconstraintsurface, wehaveg(x)=g(x+ )and hence T∇g(x) 0.
Inthelimit → 0wehave T∇g(x) = 0, andbecause is 707 708 E.
LAGRANGEMULTIPLIERS Figure E.1 A geometrical picture of the technique of La- ∇f(x) grangemultipliersinwhichweseektomaximizea function f(x), subject to theconstraint g(x) = 0.
Ifxis Ddimensional, theconstraintg(x)=0cor- x A respondstoasubspaceofdimensionality D−1, indicated by the red curve.
The problem can be solved by optimizing the Lagrangian function ∇g(x) L(x,λ)=f(x)+λg(x).
g(x)=0 thenparalleltotheconstraintsurfaceg(x)=0, weseethatthevector∇g isnormal tothesurface.
Nextweseekapointx ontheconstraintsurfacesuchthatf(x)ismaximized.
Suchapointmusthavethepropertythatthevector∇f(x)isalsoorthogonaltothe constraintsurface, asillustratedin Figure E.1, becauseotherwisewecouldincrease thevalueoff(x)bymovingashortdistancealongtheconstraintsurface.
Thus∇f and∇g areparallel(oranti-parallel)vectors, andsotheremustexistaparameterλ suchthat ∇f +λ∇g =0 (E.3) whereλ =0isknownasa Lagrangemultiplier.
Notethatλcanhaveeithersign.
Atthispoint, itisconvenienttointroducethe Lagrangianfunctiondefinedby L(x,λ)≡f(x)+λg(x).
(E.4) The constrained stationarity condition (E.3) is obtained by setting ∇ x L = 0.
Fur- thermore, thecondition∂L/∂λ=0leadstotheconstraintequationg(x)=0.
Thustofindthemaximumofafunctionf(x)subjecttotheconstraintg(x)=0, we define the Lagrangian function given by (E.4) and we then find the stationary point of L(x,λ) with respect to both x and λ.
For a D-dimensional vector x, this gives D+1equationsthatdetermineboththestationarypointx andthevalueofλ.
Ifweareonlyinterestedinx , thenwecaneliminateλfromthestationarityequa- tionswithoutneedingtofinditsvalue(hencetheterm‘undeterminedmultiplier’).
Asasimpleexample, supposewewishtofindthestationarypointofthefunction f(x 1 , x 2 ) = 1−x2 1 −x2 2 subjecttotheconstraintg(x 1 , x 2 ) = x 1 +x 2 −1 = 0, as illustratedin Figure E.2.
Thecorresponding Lagrangianfunctionisgivenby L(x,λ)=1−x2 1 −x2 2 +λ(x 1 +x 2 −1).
(E.5) Theconditionsforthis Lagrangiantobestationarywithrespecttox 1, x 2, andλgive thefollowingcoupledequations: −2x 1 +λ = 0 (E.6) −2x 2 +λ = 0 (E.7) x 1 +x 2 −1 = 0.
(E.8) E.
LAGRANGEMULTIPLIERS 709 Figure E.2 A ers sim in p w le h e ic x h am th p e le a o im f th is e t u o se m o a f xi L m a i g ze ran f g ( e x 1 m , x u 2 lt ) ip = li- x2 1−x2 1 −x2 2 subjecttotheconstraintg(x 1 , x 2 ) = 0 where g(x 1 , x 2 ) = x 1 +x 2 −1.
The circles show contoursofthefunctionf(x 1 , x 2 ), andthediagonal (x 1 , x 2 ) lineshowstheconstraintsurfaceg(x 1 , x 2 )=0.
x1 g(x1, x2)=0 Solutionoftheseequationsthengivesthestationarypointas(x , x ) = (1,1), and 1 2 2 2 thecorrespondingvalueforthe Lagrangemultiplierisλ=1.
Sofar, wehaveconsideredtheproblemofmaximizingafunctionsubjecttoan equality constraint of the form g(x) = 0.
We now consider the problem of maxi- mizingf(x)subjecttoaninequalityconstraint oftheformg(x) 0, asillustrated in Figure E.3.
There are now two kinds of solution possible, according to whether the con- strained stationary point lies in the region where g(x) > 0, in which case the con- straint is inactive, or whether it lies on the boundary g(x) = 0, in which case the constraint is said to be active.
In the former case, the function g(x) plays no role and so the stationary condition is simply ∇f(x) = 0.
This again corresponds to a stationary point of the Lagrange function (E.4) but this time with λ = 0.
The lattercase, wherethesolutionliesontheboundary, isanalogoustotheequalitycon- straint discussed previously and corresponds to a stationary point of the Lagrange function (E.4) with λ = 0.
Now, however, the sign of the Lagrange multiplier is crucial, because the function f(x) will only be at a maximum if its gradient is ori- entedawayfromtheregiong(x)>0, asillustratedin Figure E.3.
Wethereforehave ∇f(x)=−λ∇g(x)forsomevalueofλ>0.
For either of these two cases, the product λg(x) = 0.
Thus the solution to the Figure E.3 Illustration of the problem of maximizing ∇f(x) f(x) subject to the inequality constraint g(x) 0.
x A ∇g(x) x B g(x)=0 g(x)>0 710 E.
LAGRANGEMULTIPLIERS problem of maximizing f(x) subject to g(x) 0 is obtained by optimizing the Lagrangefunction(E.4)withrespecttoxandλsubjecttotheconditions g(x) 0 (E.9) λ 0 (E.10) λg(x) = 0 (E.11) Theseareknownasthe Karush-Kuhn-Tucker(KKT)conditions(Karush,1939; Kuhn and Tucker,1951).
Notethatifwewishtominimize(ratherthanmaximize)thefunctionf(x)sub- jecttoaninequalityconstraintg(x) 0, thenweminimizethe Lagrangianfunction L(x,λ)=f(x)−λg(x)withrespecttox, againsubjecttoλ 0.
Finally, it is straightforward to extend the technique of Lagrange multipliers to the case of multiple equality and inequality constraints.
Suppose we wish to maxi- We then introduce Lagrange multipliers {λj } and {µk }, and then optimize the La- grangianfunctiongivenby J K L(x,{λj },{µk })=f(x)+ λjgj(x)+ µkhk(x) (E.12) j=1 k=1 subject to µk 0 and µkhk(x) = 0 for k = 1,..., K.
Extensions to constrained Appendix D functional derivatives are similarly straightforward.
For a more detailed discussion ofthetechniqueof Lagrangemultipliers, see Nocedaland Wright(1999).
REFERENCES 711 References of Mathematical Functions.
Dover.
new learning algorithm for blind signal separa- Adler, S.
L.
(1981).
Over-relaxation method for the Hasselmo (Eds.), Advances in Neural Informa- Monte Carlo evaluation of the partition func- tion Processing Systems, Volume8, pp.757–763.
tion for multiquadratic actions.
Physical Review MITPress.
D 23,2901–2904.
algorithmforprincipalcomponentanalysis.
Neu- ciently in learning.
Neural Computation 10, ral Computation15(1),57–65.
251–276.
noer (1964).
The probability problem of pattern Neurocomputing: Foundationsof Research.
MIT recognitionlearningandthemethodofpotential Press.
functions.
Automation and Remote Control 25, Anderson, T.
W.(1963).
Asymptotictheoryforprin- 1175–1190.
cipalcomponentanalysis.
Annalsof Mathemati- Akaike, H.
(1974).
A new look at statistical model cal Statistics 34,122–148.
identification.
IEEE Transactions on Automatic Control 19,716–723.
dan (2003).
An introduction to MCMC for ma- chinelearning.
Machine Learning 50,5–43.
of coefficients of divergence of one distribution Anthony, M.
and N.
Biggs (1992).
An Introduction fromanother.
Journalofthe Royal Statistical So- to Computational Learning Theory.
Cambridge ciety, B28(1),131–142.
University Press.
Reducing multiclass to binary: a unifying ap- Attias, H.(1999a).
Independentfactoranalysis.
Neu- proachformarginclassifiers.
Journalof Machine ral Computation11(4),803–851.
Learning Research 1,113–141.
Attias, H.
(1999b).
Inferring parameters and struc- Amari, S.(1985).
Differential-Geometrical Methods ture of latent variable models by variational 712 REFERENCES Uncertainty in Artificial Intelligence: Proceed- Baum, L.
E.
(1972).
An inequality and associated ingsofthe Fifth Conference, pp.21–30.
Morgan maximization technique in statistical estimation Kaufmann.
of probabilistic functions of Markov processes.
Inequalities 3,1–8.
pendentcomponentanalysis.
Journalof Machine Becker, S.
and Y.
Le Cun(1989).
Improvingthecon- Learning Research 3,1–48.
vergenceofback-propagationlearningwithsec- Saul, and B.
Scho¨lkopf(Eds.), Advancesin Neu- the 1988 Connectionist Models Summer School, ral Information Processing Systems, Volume16, pp.29–37.
Morgan Kaufmann.
Baldi, P.
and S.
Brunak(2001).
Bioinformatics: The mation maximization approach to blind separa- Machine Learning Approach (Second ed.).
MIT tion and blind deconvolution.
Neural Computa- Press.
tion7(6),1129–1159.
andprincipalcomponentanalysis: learningfrom Guided Tour.
Princeton University Press.
examples without local minima.
Neural Net- Bengio, Y.
and P.
Frasconi (1995).
An input output model comparison by Monte Carlo chaining.
In Information Processing Systems, Volume 7, pp.
vances in Neural Information Processing Sys- Bennett, K.
P.
(1992).
Robust linear programming tems, Volume9, pp.333–339.
MITPress.
discriminationoftwolinearlyseparablesets.
Op- Barber, D.
and C.
M.
Bishop (1998a).
Ensemble timization Methodsand Software 1,23–34.
tems, Volume10, pp.395–401.
Theory.
Wiley.
learning in Bayesian neural networks.
In C.
M.
(1993).
Near Shannonlimiterror-correctingcod- Bishop(Ed.), Generalizationin Neural Networks ing and decoding: Turbo-codes (1).
In Proceed- and Factor Analysis.
Charles Griffin.
Markovfields.
In Transactionsofthe7th Prague Basilevsky, A.
(1994).
Statistical Factor Analysis Conference on Information Theory, Statistical and Related Methods: Theoryand Applications.
Decision Functions and Random Processes, pp.
Wiley.
47–75.
Academia.
Bather, J.(2000).
Decision Theory: An Introduction Besag, J.
(1986).
On the statistical analysis of dirty to Dynamic Programming and Sequential Deci- pictures.
Journal of the Royal Statistical Soci- sions.
Wiley.
ety B-48,259–302.
criminantanalysisusingakernelapproach.
Neu- (1995).
Bayesian computation and stochastic ral Computation12(10),2385–2404.
systems.
Statistical Science10(1),3–66.
REFERENCES 713 Bishop, C.
M.(1991).
Afastprocedureforretraining Information Processing Systems, Volume15, pp.
the multilayer perceptron.
International Journal 793–800.
MITPress.
of Neural Systems2(3),229–236.
Bishop, C.
M.
and M.
Svense´n(2003).
Bayesianhi- sianmatrixforthemultilayerperceptron.
Neural and C.
Meek (Eds.), Proceedings Nineteenth Computation4(4),494–501.
Conference on Uncertainty in Artificial Intelli- a learning algorithm for feedforward networks.
IEEE Transactions on Neural Networks 4(5), (2004).
Distinguishing text from graphics in on- 882–884.
line handwritten ink.
In F.
Kimura and H.
Fu- Bishop, C.
M.
(1994).
Novelty detection and neu- jisawa (Eds.), Proceedings Ninth International ralnetworkvalidation.
IEEProceedings: Vision, Workshopon Frontiersin Handwriting Recogni- Image and Signal Processing 141(4), 217–222.
tion, IWFHR-9, Tokyo, Japan, pp.142–147.
Specialissueonapplicationsofneuralnetworks.
Bishop, C.
M.(1995a).
Neural Networksfor Pattern (1996).
EM optimization of latent variable den- and M.
E.
Hasselmo (Eds.), Advances in Neural Bishop, C.
M.(1995b).
Trainingwithnoiseisequiv- Information Processing Systems, Volume 8, pp.
alentto Tikhonovregularization.
Neural Compu- 465–471.
MITPress.
tation7(1),108–116.
Information Processing Systems, Volume 9, pp.
Bishop, C.
M.
(1999b).
Variational principal 354–360.
MITPress.
components.
In Proceedings Ninth Interna- ICANN’99, Volume1, pp.509–514.
IEE.
(1997b).
Magnification factors for the GTM al- gorithm.
In Proceedings IEE Fifth International Conferenceon Artificial Neural Networks, Cam- multiphaseflowsusingdual-energygammaden- bridge, U.
K., pp.
64–69.
Institute of Electrical sitometry and neural networks.
Nuclear Instru- Engineers.
ments and Methods in Physics Research A327, (1998a).
Developments of the Generative To- pographic Mapping.
Neurocomputing 21, 203– conditional probability distributions for periodic variables.
Neural Computation8(5),1123–1133.
224.
Recognition and Machine Learning: A Matlab (1998b).
GTM: the Generative Topographic (2003).
VIBES: A variational inference engine archicallatentvariablemodelfordatavisualiza- for Bayesian networks.
In S.
Becker, S.
Thrun, tion.
IEEETransactionson Pattern Analysisand and K.
Obermeyer (Eds.), Advances in Neural Machine Intelligence20(3),281–293.
714 REFERENCES Bayesianimagemodelling.
In Proceedings Sixth Learning 26,123–140.
European Conference on Computer Vision, Dublin, Volume1, pp.3–17.
Springer.
P.
J.
Stone(1984).
Classificationand Regression erarchical Bayesian models for applications in Brooks, S.
P.
(1998).
Markov chain Monte Bayesian Statistics, 7, pp.
25–43.
Oxford Uni- cian47(1),69–100.
versity Press.
Broomhead, D.
S.
and D.
Lowe (1988).
Multivari- Block, H.
D.
(1962).
The perceptron: a model able functional interpolation and adaptive net- for brain functioning.
Reviews of Modern works.
Complex Systems 2,321–355.
Physics 34(1), 123–135.
Reprinted in Anderson Buntine, W.
and A.
Weigend(1991).
Bayesianback- and Rosenfeld(1988).
propagation.
Complex Systems 5,603–643.
Blum, J.
A.(1965).
Multidimensionalstochasticap- proximation methods.
Annals of Mathematical puting second derivatives in feed-forward net- Statistics 25,737–744.
works: a review.
IEEE Transactions on Neural Bodlaender, H.
(1993).
A tourist guide through Networks5(3),480–488.
treewidth.
Acta Cybernetica 11,1–21.
Burges, C.
J.
C.
(1998).
A tutorial on support vec- A training algorithm for optimal margin classi- Discoveryand Data Mining2(2),121–167.
fiers.
In D.
Haussler(Ed.), Proceedings Fifth An- Cardoso, J.-F.(1998).
Blindsignalseparation: statis- nual Workshopon Computational Learning The- tical principles.
Proceedings of the IEEE 9(10), ory(COLT), pp.144–152.
ACM.
2009–2025.
Bourlard, H.
and Y.
Kamp(1988).
Auto-association Casella, G.
and R.
L.
Berger (2002).
Statistical In- bymultilayerperceptronsandsingularvaluede- ference(Seconded.).
Duxbury.
composition.
Biological Cybernetics 59, 291– 294.
Expert Systemsand Probabilistic Network Mod- (1994).
Time Series Analysis.
Prentice Hall.
encein Statistical Analysis.
Wiley.
data.
Neural Computation15(8),1991–2011.
mization.
Cambridge University Press.
On the geometry of feedforward neural network Boyen, X.
and D.
Koller(1998).
Tractableinference error surfaces.
Neural Computation 5(6), 910– forcomplexstochasticprocesses.
In G.
F.
Cooper 927.
Conference on Uncertainty in Artificial Intelli- (2001).
Monte Carlo Methodsfor Bayesian Com- approximateenergyminimizationviagraphcuts.
Orthogonal least squares learning algorithm for IEEETransactionson Pattern Analysisand Ma- radial basis function networks.
IEEE Transac- chine Intelligence23(11),1222–1239.
tionson Neural Networks2(2),302–309.
REFERENCES 715 mixtureof Bayesianindependentcomponentan- sional Scaling(Seconded.).
Chapmanand Hall.
alyzers.
Neural Computation15(1),213–252.
Cressie, N.(1993).
Statisticsfor Spatial Data.
Wiley.
Disorderin Physical Systems.
AVolumein Hon- methods.
Cambridge University Press.
our of John M.
Hammersley, pp.
19–32.
Oxford Csato´, L.
and M.
Opper(2002).
Sparseon-line Gaus- University Press.
sianprocesses.
Neural Computation14(3),641– A generalization of principal component analy- Csisza`r, I.
and G.
Tusna`dy (1984).
Information ge- sistotheexponentialfamily.
In T.
G.
Dietterich, ometryandalternatingminimizationprocedures.
S.
Becker, and Z.
Ghahramani (Eds.), Advances Statisticsand Decisions1(1),205–237.
in Neural Information Processing Systems, Vol- Cybenko, G.
(1989).
Approximation by superposi- ume14, pp.617–624.
MITPress.
tions of a sigmoidal function.
Mathematics of Comon, P., C.
Jutten, and J.
Herault (1991).
Blind Control, Signalsand Systems 2,304–314.
sourceseparation,2: problemsstatement.
Signal Dawid, A.
P.
(1979).
Conditional independence in Processing24(1),11–20.
statisticaltheory(withdiscussion).
Journalofthe ational Bayesian model selection for mixture Dawid, A.
P.
(1980).
Conditional independence for distributions.
In T.
Richardson and T.
Jaakkola statisticaloperations.
Annalsof Statistics 8,598– (Eds.), Proceedings Eighth International Confer- 617.
ence on Artificial Intelligence and Statistics, pp.
and Sons.
C.
Stein(2001).
Introductionto Algorithms(Sec- (1977).
Maximum likelihood from incomplete onded.).
MITPress.
datavia the EM algorithm.
Journalof the Royal Cortes, C.
and V.
N.
Vapnik (1995).
Support vector Statistical Society, B39(1),1–38.
networks.
Machine Learning 20,273–297.
Cotter, N.
E.
(1990).
The Stone-Weierstrass theo- and A.
F.
M.
Smith(2002).
Bayesian Methodsfor remanditsapplicationtoneuralnetworks.
IEEE Nonlinear Classificationand Regression.
Wiley.
Transactionson Neural Networks1(4),290–295.
Diaconis, P.
and L.
Saloff-Coste(1998).
Whatdowe Cover, T.
and P.
Hart (1967).
Nearest neighbor pat- know about the Metropolis algorithm? Journal tern classification.
IEEE Transactions on Infor- of Computerand System Sciences 57,20–36.
mation Theory IT-11,21–27.
Dietterich, T.
G.
and G.
Bakiri (1995).
Solving Information Theory.
Wiley.
output codes.
Journal of Artificial Intelligence Letters B195(2),216–222.
Cox, R.
T.
(1946).
Probability, frequency and Physics14(1),1–13.
cationand Scene Analysis.
Wiley.
716 REFERENCES son(1998).
Biological Sequence Analysis.
Cam- sion: AModern Approach.
Prentice Hall.
of probabilistic models for medical informatics.
Thirteenth International Conferenceon Machine (Eds.), Probabilistic Modelingin Bioinformatics Frey, B.
J.
(1998).
Graphical Models for Ma- and Medical Informatics, pp.297–349.
Springer.
chine Learning and Digital Communication.
MITPress.
Efron, B.
(1979).
Bootstrap methods: another look tion: Beliefpropagationingraphswithcycles.
In Elkan, C.(2003).
Usingthetriangleinequalitytoac- celerate k-means.
In Proceedings of the Twelfth Advancesin Neural Information Processing Sys- International Conference on Machine Learning, tems, Volume10.
MITPress.
pp.147–153.
AAAI.
Friedman, J.
H.
(2001).
Greedy function approxi- mation: a gradient boosting machine.
Annals of Hidden Markov Models: Estimation and Con- Statistics29(5),1189–1232.
trol.
Springer.
Additivelogisticregression: astatisticalviewof Ontheapplicationofhidden Markovmodelsfor boosting.
Annalsof Statistics 28,337–407.
enhancing noisy speech.
IEEE Transactions on Friedman, N.
and D.
Koller(2003).
Being Bayesian Acoustics, Speechand Signal Processing37(12), about network structure: A Bayesian approach 1846–1856.
tostructurediscoveryin Bayesiannetworks.
Ma- Self-organizing maps: ordering, convergence Frydenberg, M.
(1990).
The chain graph Markov properties and energy functions.
Biological Cy- property.
Scandinavian Journalof Statistics 17, bernetics 67,47–55.
333–353.
Everitt, B.
S.(1984).
An Introductionto Latent Vari- Fukunaga, K.(1990).
Introductionto Statistical Pat- able Models.
Chapmanand Hall.
tern Recognition(Seconded.).
Academic Press.
sparse Bayesian learning.
In T.
G.
Dietterich, tionofcontinuousmappingsbyneuralnetworks.
S.
Becker, and Z.
Ghahramani (Eds.), Advances Neural Networks2(3),183–192.
in Neural Information Processing Systems, Vol- Fung, R.
and K.
C.
Chang (1990).
Weighting and ume14, pp.383–389.
MITPress.
integrating evidence for stochastic simulation in ume2.
Wiley.
certaintyin Artificial Intelligence, Volume5, pp.
208–219.
Elsevier.
(1964).
The Feynman Lectures of Physics, Vol- Gallager, R.
G.
(1963).
Low-Density Parity-Check REFERENCES 717 Stochastic Simulation for Bayesian Inference.
rejection sampling for Gibbs sampling.
In Smith(Eds.), Bayesian Statistics, Volume4.
Ox- ford University Press.
bin(2004).
Bayesian Data Analysis(Seconded.).
Adaptiverejection Metropolissampling.
Applied Geman, S.
and D.
Geman (1984).
Stochastic re- Statistics 44,455–472.
laxation, Gibbs distributions, and the Bayesian tern Analysis and Machine Intelligence 6(1), ter(Eds.)(1996).
Markov Chain Monte Carloin 721–741.
Practice.
Chapmanand Hall.
inference for Bayesian mixtures of factor ana- sampling for Gibbs sampling.
Applied Statis- Processing Systems, Volume 12, pp.
449–455.
Practical Optimization.
Academic Press.
MITPress.
EM algorithm for mixtures of factor analyzers.
noise: A Gaussian process treatment.
In Ad- Technical Report CRG-TR-96-1, University of vances in Neural Information Processing Sys- Toronto.
tems, Volume10, pp.493–499.
MITPress.
eter estimation for linear dynamical systems.
Computations(Thirded.).
John Hopkins Univer- Technical Report CRG-TR-96-2, University of sity Press.
Toronto.
Good, I.(1950).
Probabilityandthe Weighingof Ev- learning for switching state-space models.
Neu- ral Computation12(4),963–996.
(1993).
Novel approach to nonlinear/non- vised learning from incomplete data via an EM Proceedings-F140(2),107–113.
Graepel, T.
(2003).
Solving noisy linear operator J.
Alspector(Eds.), Advancesin Neural Informa- equations by Gaussian processes: Application tion Processing Systems, Volume6, pp.120–127.
to ordinary and partial differential equations.
In Morgan Kaufmann.
Proceedingsofthe Twentieth International Con- hidden Markov models.
Machine Learning 29, Greig, D., B.
Porteous, and A.
Seheult (1989).
Ex- 245–275.
act maximum a-posteriori estimation for binary Gibbs, M.
N.
(1997).
Bayesian Gaussian processes images.
Journal of the Royal Statistical Society, forregressionandclassification.
Phdthesis, Uni- Series B51(2),271–279.
versityof Cambridge.
Gull, S.
F.
(1989).
Developments in maximum en- tional Gaussian process classifiers.
IEEE Trans- mum Entropyand Bayesian Methods, pp.53–71.
actionson Neural Networks 11,1458–1464.
Kluwer.
718 REFERENCES Hassibi, B.
and D.
G.
Stork (1993).
Second order of the International Conference on Independent derivatives for network pruning: optimal brain Component Analysis and Blind Signal Separa- C.
L.
Giles(Eds.), Advancesin Neural Informa- Hodgson, M.
E.(1998).
Reducingcomputationalre- tion Processing Systems, Volume5, pp.164–171.
quirements of the minimum-distance classifier.
Morgan Kaufmann.
Remote Sensingof Environments 25,117–128.
Hastie, T.
and W.
Stuetzle (1989).
Principal curves.
Hoerl, A.
E.
and R.
Kennard (1970).
Ridge regres- Journal of the American Statistical Associa- sion: biased estimation for nonorthogonal prob- tion84(106),502–516.
lems.
Technometrics 12,55–67.
Hastie, T., R.
Tibshirani, and J.
Friedman (2001).
Hofmann, T.(2000).
Learningthesimilarityofdoc- The Elementsof Statistical Learning.
Springer.
uments: an information-geometric approach to Hastings, W.
K.
(1970).
Monte Carlo sampling document retrieval and classification.
In S.
A.
methodsusing Markovchainsandtheirapplica- tions.
Biometrika 57,97–109.
vances in Neural Information Processing Sys- EMalgorithmformixturedistributions.
Statistics and Probability Letters 4,53–56.
(2002).
Mean field approaches to independent Haussler, D.(1999).
Convolutionkernelsondiscrete componentanalysis.
Neural Computation14(4), structures.
Technical Report UCSC-CRL-99-10, 889–918.
University of California, Santa Cruz, Computer Hornik, K.
(1991).
Approximation capabilities of Science Department.
multilayer feedforward networks.
Neural Net- Henrion, M.
(1988).
Propagation of uncertainty by works4(2),251–257.
logicsamplingin Bayes’networks.
In J.
F.
Lem- Hornik, K., M.
Stinchcombe, and H.
White (1989).
merand L.
N.
Kanal(Eds.), Uncertaintyin Arti- Multilayer feedforward networks are universal ficial Intelligence, Volume2, pp.149–164.
North approximators.
Neural Networks2(5),359–366.
Holland.
Hotelling, H.(1933).
Analysisofacomplexofstatis- Herbrich, R.
(2002).
Learning Kernel Classifiers.
tical variables into principal components.
Jour- MITPress.
nalof Educational Psychology 24,417–441.
Hotelling, H.
(1936).
Relations between two sets of troductiontothe Theoryof Neural Computation.
variables.
Biometrika 28,321–377.
Addison Wesley.
Modellingthemanifoldsofimagesofhandwrit- algorithm for independent component analysis.
ten digits.
IEEE Transactions on Neural Net- Neural Computation9(7),1483–1492.
works8(1),65–74.
Isard, M.
and A.
Blake (1998).
CONDENSATION Hinton, G.
E.
and D.
van Camp (1993).
Keeping – conditional density propagation for visual neural networks simple by minimizing the de- tracking.
International Journal of Computer Vi- scriptionlengthoftheweights.
In Proceedingsof sion29(1),5–18.
the Sixth Annual Conference on Computational Ito, Y.
(1991).
Representation of functions by su- Learning Theory, pp.5–13.
ACM.
perpositions of a step or sigmoid function and dero(2001).
Anewviewof ICA.
In Proceedings ral Networks4(3),385–394.
REFERENCES 719 parameter estimation via variational methods.
tic Graphical Models.
Inpreparation.
Statisticsand Computing 10,25–37.
(Eds.), Advances in Mean Field Methods, pp.
(Ed.), Learning in Graphical Models, pp.
105– 129–159.
MITPress.
162.
MITPress.
generativemodelsindiscriminativeclassifiers.
In mixtures of expertsand the EM algorithm.
Neu- Advancesin Neural Information Processing Sys- Jutten, C.
and J.
Herault(1991).
Blindseparationof tems, Volume11.
MITPress.
sources,1: Anadaptivealgorithmbasedonneu- Hinton (1991).
Adaptive mixtures of local ex- 1–10.
perts.
Neural Computation3(1),79–87.
Kalman, R.
E.(1960).
Anewapproachtolinearfil- Jaynes, E.
T.(2003).
Probability Theory: The Logic tering and prediction problems.
Transactions of of Science.
Cambridge University Press.
the American Society for Mechanical Engineer- Jebara, T.
(2004).
Machine Learning: Discrimina- ing, Series D, Journalof Basic Engineering 82, tiveand Generative.
Kluwer.
35–45.
probability in estimation problems.
Pro.
Roy.
reductionbylocalprincipalcomponentanalysis.
Soc.
AA 186,453–461.
Neural Computation9(7),1493–1516.
Recognition.
MITPress.
Stochastic simulation algorithms for dynamic probabilistic networks.
In Uncertainty in Artifi- Jensen, C., A.
Kong, and U.
Kjaerulff(1995).
Block- cial Intelligence, Volume11.
Morgan Kaufmann.
ing gibbs sampling in very large probabilistic expert systems.
International Journal of Human Kapadia, S.(1998).
Discriminative Trainingof Hid- Computer Studies.
Special Issue on Real-World den Markov Models.
Phd thesis, University of Applications of Uncertain Reasoning.
42, 647– Cambridge, U.
K.
666.
Kapur, J.
(1989).
Maximum entropy methods in sci- Networks.
UCLPress.
Karush, W.
(1939).
Minima of functions of several Jerrum, M.
and A.
Sinclair (1996).
The Markov variables with inequalities as side constraints.
chain Monte Carlo method: an approach to ap- Master’s thesis, Department of Mathematics, proximate counting and integration.
In D.
S.
Universityof Chicago.
Hochbaum (Ed.), Approximation Algorithms for NP-Hard Problems.
PWSPublishing.
tors.
Journal of the American Statistical Associ- Jolliffe, I.
T.
(2002).
Principal Component Analysis ation 90,377–395.
(Seconded.).
Springer.
MITPress.
Press.
720 REFERENCES Kindermann, R.
and J.
L.
Snell(1980).
Markov Ran- In Advances in Neural Information Processing dom Fields and Their Applications.
American Systems, Number18.
MITPress.
inpress.
Mathematical Society.
Kittler, J.
and J.
Fo¨glein(1984).
Contextualclassifi- Principledhybridsofgenerativeanddiscrimina- cationofmultispectralpixeldata.
Imageand Vi- tive models.
In Proceedings 2006 IEEE Confer- sion Computing 2,13–29.
ence on Computer Vision and Pattern Recogni- tion, New York.
Kohonen, T.
(1982).
Self-organized formation of Lauritzen, S.
and N.
Wermuth (1989).
Graphical topologically correct feature maps.
Biological models for association between variables, some Cybernetics 43,59–69.
of which are qualitative some quantitative.
An- Kohonen, T.
(1995).
Self-Organizing Maps.
nalsof Statistics 17,31–57.
Springer.
Lauritzen, S.
L.(1992).
Propagationofprobabilities, Kolmogorov, V.
and R.
Zabih (2004).
What en- meansandvariancesinmixedgraphicalassocia- ergyfunctionscanbeminimizedviagraphcuts? tion models.
Journal of the American Statistical IEEETransactionson Pattern Analysisand Ma- Association 87,1098–1108.
chine Intelligence26(2),147–159.
Lauritzen, S.
L.
(1996).
Graphical Models.
Oxford Kreinovich, V.
Y.
(1991).
Arbitrary nonlinearity is University Press.
works: a theorem.
Neural Networks 4(3), 381– calcomputationswithprobabailitiesongraphical 383.
structuresandtheirapplicationtoexpertsystems.
D.
Haussler (1994).
Hidden Markov models in 224.
computational biology: Applications to protein Lawley, D.
N.
(1953).
A modified method of esti- modelling.
Journal of Molecular Biology 235, mation in factor analysis and some large sam- 1501–1531.
ple results.
In Uppsala Symposium on Psycho- logical Factor Analysis, Number 3 in Nordisk Psykologi Monograph Series, pp.
35–42.
Upp- (2001).
Factorgraphsandthesum-productalgo- sala: Almqvistand Wiksell.
rithm.
IEEE Transactions on Information The- and M.
J.
Taylor (2002).
Optimising synchro- nisation times for mobile devices.
In T.
G.
Di- programming.
In Proceedings of the 2nd Berke- etterich, S.
Becker, and Z.
Ghahramani (Eds.), ley Symposium on Mathematical Statistics and Advancesin Neural Information Processing Sys- Probabilities, pp.
481–492.
University of Cali- tems, Volume14, pp.1401–1408.
MITPress.
fornia Press.
Kullback, S.
and R.
A.
Leibler (1951).
On infor- Structure Analysis.
Houghton Mifflin.
mation and sufficiency.
Annals of Mathematical Statistics22(1),79–86.
equivalentfeed-forwardneuralnetworks.
Neural zip code recognition.
Neural Computation 1(4), Computation6(3),543–558.
541–551.
REFERENCES 721 Advancesin Neural Information Processing Sys- Department of Physics, University of Cam- tems, Volume 2, pp.
598–605.
Morgan Kauf- bridge.
mann.
Mac Kay, D.
J.
C.
(1998).
Introduction to Gaus- (1998).
Gradient-based learning applied to doc- Networks and Machine Learning, pp.
133–166.
umentrecognition.
Proceedingsofthe IEEE 86, Springer.
2278–2324.
Mac Kay, D.
J.
C.
(1999).
Comparison of approx- supportvectormachines.
Technical Report1040, Neural Computation11(5),1035–1068.
Departmentof Statistics, Universityof Madison, Mac Kay, D.
J.
C.(2003).
Information Theory, Infer- Wisconsin.
ence and Learning Algorithms.
Cambridge Uni- Leen, T.
K.(1995).
Fromdatadistributionstoregu- versity Press.
larizationininvariantlearning.
Neural Computa- tion 7,974–981.
Lindley, D.
V.
(1982).
Scoring rules and the in- ton (Eds.), Statistics and Neural Networks: Ad- evitability of probability.
International Statisti- vancesatthe Interface, Chapter5, pp.129–145.
cal Review 50,1–26.
Oxford University Press.
Scientific Computing.
Springer.
correcting codes based on very sparse matrices.
Lloyd, S.
P.
(1982).
Least squares quantization in IEEE Transactions on Information Theory 45, PCM.
IEEE Transactions on Information The- 399–431.
ory28(2),129–137.
Mac Queen, J.
(1967).
Some methods for classifica- L.
M.
Le Cam and J.
Neyman (Eds.), Proceed- ingsofthe Fifth Berkeley Symposiumon Mathe- Neural Computation4(3),415–447.
matical Statisticsand Probability, Volume I, pp.
applied to classification networks.
Neural Com- Magnus, J.
R.
and H.
Neudecker(1999).
Matrix Dif- putation4(5),720–736.
ferential Calculuswith Applicationsin Statistics frameworkforback-propagationnetworks.
Neu- Mallat, S.(1999).
AWavelet Tourof Signal Process- ral Computation4(3),448–472.
ing(Seconded.).
Academic Press.
Mac Kay, D.
J.
C.
(1994).
Bayesian methods for Manning, C.
D.
and H.
Schu¨tze(1999).
Foundations backprop networks.
In E.
Domany, J.
L.
van of Statistical Natural Language Processing.
MIT Hemmen, and K.
Schulten (Eds.), Models of Press.
Neural Networks, III, Chapter 6, pp.
211–254.
Statistics.
Wiley.
Mac Kay, D.
J.
C.(1995).
Bayesianneuralnetworks and density networks.
Nuclear Instruments and Maybeck, P.
S.
(1982).
Stochastic models, estima- Methodsin Physics Research, A354(1),73–80.
tionandcontrol.
Academic Press.
den Markov models.
Unpublished manuscript, modelselection.
Machine Learning51(1),5–21.
722 REFERENCES Linear Models(Seconded.).
Chapmanand Hall.
MSR-TR-2004-149, Microsoft Research Cam- calculus of the ideas immanent in nervous ac- Minka, T.
(2005).
Divergence measures and mes- tivity.
Bulletin of Mathematical Biophysics 5, sage passing.
Technical Report MSR-TR-2005- 115–133.
Reprinted in Anderson and Rosenfeld 173, Microsoft Research Cambridge.
(1988).
Minka, T.
P.
(2001c).
Automatic choice of dimen- (1998).
Turbodecodingasaninstanceof Pearl’s terich, and V.
Tresp (Eds.), Advances in Neural ‘Belief Ppropagation’ algorithm.
IEEE Journal Information Processing Systems, Volume13, pp.
on Selected Areas in Communications 16, 140– 598–604.
MITPress.
152.
MITPress.
Expandededition1990.
Models: Inference and Applications to Cluster- ble learning for blind source separation.
In S.
J.
Mc Lachlan, G.
J.
and T.
Krishnan (1997).
The EM Roberts and R.
M.
Everson (Eds.), Independent Algorithmandits Extensions.
Wiley.
Component Analysis: Principles and Practice.
Mc Lachlan, G.
J.
and D.
Peel(2000).
Finite Mixture Cambridge University Press.
Models.
Wiley.
Møller, M.
(1993).
Efficient Training of Feed- Forward Neural Networks.
Ph.
D.
thesis, Aarhus lihoodestimationviathe ECMalgorithm: agen- University, Denmark.
eralframework.
Biometrika 80,267–278.
tion of state calculations by fast computing ral Computation1(2),281–294.
machines.
Journal of Chemical Physics 21(6), Moore, A.
W.
(2000).
The anchors hierarch: us- 1087–1092.
ingthetriangleinequalitytosurvivehighdimen- Metropolis, N.
and S.
Ulam (1949).
The Monte sional data.
In Proceedings of the Twelfth Con- Carlo method.
Journal of the American Statisti- ference on Uncertainty in Artificial Intelligence, cal Association44(247),335–341.
pp.397–405.
(1999).
Fisher discriminant analysis with ker- B.
Scho¨lkopf (2001).
An introduction to kernel- S.
Douglas (Eds.), Neural Networks for Signal Neural Networks12(2),181–202.
Processing IX, pp.41–48.
IEEE.
Mu¨ller, P.
and F.
A.
Quintana(2004).
Nonparametric Minka, T.
(2001a).
Expectation propagation for ap- Bayesiandataanalysis.
Statistical Science19(1), proximate Bayesian inference.
In J.
Breese and 95–110.
D.
Koller(Eds.), Proceedingsofthe Seventeenth Conference on Uncertainty in Artificial Intelli- Nabney, I.
T.(2002).
Netlab: Algorithmsfor Pattern gorithms for Bayesian inference.
Ph.
D.
thesis, Theory of Probability and its Applications 9(1), MIT.
141–142.
REFERENCES 723 recognition using hidden markov models.
In cal Applicationsand Data Analysis.
Birkha¨user.
ICASSP86, pp.2071–2074.
IEEE.
Opper, M.
and O.
Winther (1999).
A Bayesian ap- Markov chain Monte Carlo methods.
Technical Line Learningin Neural Networks, pp.363–378.
Report CRG-TR-93-1, Departmentof Computer Cambridge University Press.
Science, Universityof Toronto, Canada.
Opper, M.
and O.
Winther (2000a).
Gaussian processes and SVM: mean field theory and Neal, R.
M.
(1996).
Bayesian Learning for Neural Networks.
Springer.
Lecture Notes in Statistics B.
Scho¨lkopf, and D.
Shuurmans (Eds.), Ad- 118.
vancesin Large Margin Classifiers, pp.311–326.
Neal, R.
M.(1997).
Monte Carloimplementationof MITPress.
Gaussianprocessmodelsfor Bayesianregression Opper, M.
and O.
Winther (2000b).
Gaussian and classification.
Technical Report 9702, De- processes for classification.
Neural Computa- partment of Computer Statistics, University of tion12(11),2655–2684.
Toronto.
Osuna, E., R.
Freund, and F.
Girosi(1996).
Support Neal, R.
M.
(1999).
Suppressing random walks in vector machines: training and applications.
A.
I.
Markov chain Monte Carlo using ordered over- Memo AIM-1602, MIT.
relaxation.
In M.
I.
Jordan (Ed.), Learning in Papoulis, A.(1984).
Probability, Random Variables, Graphical Models, pp.205–228.
MITPress.
and Stochastic Processes(Seconded.).
Mc Graw- Neal, R.
M.
(2000).
Markov chain sampling for Hill.
Dirichlet process mixture models.
Journal of Parisi, G.(1988).
Statistical Field Theory.
Addison- Computationaland Graphical Statistics 9,249– Wesley.
265.
Pearl, J.
(1988).
Probabilistic Reasoning in Intelli- tics 31,705–767.
Pearlmutter, B.
A.
(1994).
Fast exact multiplication the EM algorithm that justifies incremental and 160.
Graphical Models, pp.355–368.
MITPress.
likelihood source separation: a context-sensitive eralizedlinearmodels.
Journalofthe Royal Sta- dan, and T.
Petsche (Eds.), Advances in Neural tistical Society, A 135,370–384.
Information Processing Systems, Volume 9, pp.
613–619.
MITPress.
Nilsson, N.
J.(1965).
Learning Machines.
Mc Graw- Pearson, K.(1901).
Onlinesandplanesofclosestfit Hill.
Reprinted as The Mathematical Founda- tosystemsofpointsinspace.
The London, Edin- tionsof Learning Machines, Morgan Kaufmann, burgh and Dublin Philosophical Magazine and (1990).
Journalof Science, Sixth Series 2,559–572.
Nocedal, J.
and S.
J.
Wright(1999).
Numerical Op- Platt, J.
C.
(1999).
Fast training of support vector timization.
Springer.
machinesusingsequentialminimaloptimization.
neural networks by soft weight sharing.
Neural (Eds.), Advances in Kernel Methods – Support Computation4(4),473–493.
Vector Learning, pp.185–208.
MITPress.
724 REFERENCES D.
Shuurmans(Eds.), Advancesin Large Margin ematics and other Logical Essays.
Humanities Classifiers, pp.61–73.
MITPress.
Press.
(2000).
Large margin DAGs for multiclass clas- verseof Matricesand Its Applications.
Wiley.
Rasmussen, C.
E.
(1996).
Evaluation of Gaussian Mu¨ller (Eds.), Advances in Neural Information Processes and Other Methods for Non-Linear Processing Systems, Volume 12, pp.
547–553.
Regression.
Ph.
D.
thesis, Universityof Toronto.
MITPress.
proximation and learning.
Proceedings of the Healing the relevance vector machine by aug- Proceedings of the 22nd International Confer- Powell, M.
J.
D.
(1987).
Radial basis functions for enceon Machine Learning, pp.689–696.
multivariable interpolation: a review.
In J.
C.
Approximation, pp.
143–167.
Oxford University Gaussian Processesfor Machine Learning.
MIT Press.
Press.
B.
P.
Flannery (1992).
Numerical Recipes in C: Maximumlikelihoodestimatesoflineardynam- The Art of Scientific Computing (Second ed.).
icalsystems.
AIAAJournal 3,1445–1450.
Cambridge University Press.
Learningofwordstressinasub-optimalsecond (1997).
An upper bound on the Bayesian error order backpropagation neural network.
In Pro- bars for generalized linear regression.
In S.
W.
ceedings of the IEEE International Conference on Neural Networks, Volume 1, pp.
355–361.
Mathematicsof Neural Networks: Models, Algo- IEEE.
rithmsand Applications, pp.295–299.
Kluwer.
Ripley, B.
D.
(1996).
Pattern Recognition and Neu- Quinlan, J.
R.
(1986).
Induction of decision trees.
ral Networks.
Cambridge University Press.
Machine Learning1(1),81–106.
Learning.
Morgan Kaufmann.
approximation method.
Annals of Mathematical Statistics 22,400–407.
Rabiner, L.
and B.
H.
Juang (1993).
Fundamentals models and selected applications in speech Rockafellar, R.
(1972).
Convex Analysis.
Princeton recognition.
Proceedings of the IEEE 77(2), University Press.
257–285.
Rosenblatt, F.
(1962).
Principles of Neurodynam- Ramasubramanian, V.
and K.
K.
Paliwal (1990).
A ics: Perceptrons and the Theory of Brain Mech- generalized optimization of the k-d tree for fast anisms.
Spartan.
nearest-neighboursearch.
In Proceedings Fourth IEEERegion10International Conference(TEN- Roth, V.
and V.
Steinhage(2000).
Nonlineardiscrim- REFERENCES 725 vances in Neural Information Processing Sys- Nonlinear component analysis as a kernel tems, Volume12.
MITPress.
eigenvalue problem.
Neural Computation 10(5), 1299–1319.
Roweis, S.
(1998).
EM algorithms for PCA and Solla (Eds.), Advances in Neural Information Bartlett (2000).
New support vector algorithms.
Processing Systems, Volume 10, pp.
626–632.
Neural Computation12(5),1207–1245.
MITPress.
Scho¨lkopf, B.
and A.
J.
Smola(2002).
Learningwith review of linear Gaussian models.
Neural Com- Schwarz, G.
(1978).
Estimating the dimension of a putation11(2),305–345.
model.
Annalsof Statistics 6,461–464.
Roweis, S.
and L.
Saul(2000, December).
Nonlinear Schwarz, H.
R.(1988).
Finiteelementmethods.
Aca- dimensionality reduction by locally linear em- demic Press.
bedding.
Science 290,2323–2326.
Seeger, M.(2003).
Bayesian Gaussian Process Mod- Rubin, D.
B.
(1983).
Iteratively reweighted least els: PAC-Bayesian Generalization Error Bounds squares.
In Encyclopedia of Statistical Sciences, and Sparse Approximations.
Ph.
D.
thesis, Uni- Volume4, pp.272–275.
Wiley.
versityof Edinburg.
gorithms for ML factor analysis.
Psychome- (2003).
Fastforwardselectiontospeedupsparse trika47(1),69–76.
Gaussianprocesses.
In C.
M.
Bishopand B.
Frey (Eds.), Proceedings Ninth International Work- (1986).
Learning internal representations by er- shopon Artificial Intelligenceand Statistics, Key West, Florida.
Clelland, and the PDP Research Group (Eds.), Parallel Distributed Processing: Explorations Shachter, R.
D.
and M.
Peot (1990).
Simulation ap- in the Microstructure of Cognition, Volume 1: proachestogeneralprobabilisticinferenceonbe- tainty in Artificial Intelligence, Volume 5.
Else- Rumelhart, D.
E., J.
L.
Mc Clelland, andthe PDPRe- vier.
search Group(Eds.)(1986).
Parallel Distributed Processing: Explorations in the Microstruc- Shannon, C.
E.
(1948).
A mathematical theory of ture of Cognition, Volume 1: Foundations.
MIT communication.
The Bell System Technical Jour- Press.
nal27(3),379–423and623–656.
Variations.
Dover.
Methods for Pattern Analysis.
Cambridge Uni- versity Press.
Savage, L.
J.
(1961).
The subjective basis of sta- Statistics, Universityof Michigan, Ann Arbor.
cialneuralnetworksthatgeneralize.
Neural Net- works4(1),67–79.
Scho¨lkopf, B., J.
Platt, J.
Shawe-Taylor, A.
Smola, port of a high-dimensional distribution.
Neural cientpatternrecognitionusinganewtransforma- 726 REFERENCES tion Processing Systems, Volume 5, pp.
50–58.
identification of masses in mamograms.
In Pro- Morgan Kaufmann.
ceedings Fourth IEE International Conference on Artificial Neural Networks, Volume 4, pp.
Simard, P., B.
Victorri, Y.
Le Cun, and J.
Denker 442–447.
IEE.
(1992).
Tangent prop – a formalism for specify- ing selected invariances in an adaptive network.
Tax, D.
and R.
Duin (1999).
Data domain descrip- (Eds.), Advancesin Neural Information Process- Proceedings European Symposium on Artificial Kaufmann.
Press.
Best practice for convolutional neural networks (2006).
Hierarchical Dirichletprocesses.
Journal applied to visual document analysis.
In Pro- ofthe Americal Statistical Association.
toappear.
ceedings International Conferenceon Document Analysis and Recognition (ICDAR), pp.
958– (2000, December).
Aglobalframeworkfornon- 962.
IEEEComputer Society.
linear dimensionality reduction.
Science 290, Sirovich, L.
(1987).
Turbulence and the dynamics 2319–2323.
of coherent structures.
Quarterly Applied Math- Tesauro, G.
(1994).
TD-Gammon, a self-teaching ematics45(3),561–590.
backgammon program, achieves master-level Thiesson, B., D.
M.
Chickering, D.
Heckerman, and Dietterich, and V.
Tresp(Eds.), Advancesin Neu- C.
Meek (2004).
ARMA time-series modelling ral Information Processing Systems, Volume13, with graphical models.
In M.
Chickering and pp.619–625.
MITPress.
J.
Halpern (Eds.), Proceedings of the Twentieth Spiegelhalter, D.
and S.
Lauritzen(1990).
Sequential Conference on Uncertainty in Artificial Intelli- updatingofconditionalprobabilitiesondirected gence, Banff, Canada, pp.552–560.
AUAIPress.
graphicalstructures.
Networks 20,579–605.
Tibshirani, R.
(1996).
Regression shrinkage and se- Stinchecombe, M.
and H.
White (1989).
Universal lectionviathelasso.
Journalofthe Royal Statis- approximationusingfeed-forwardnetworkswith tical Society, B 58,267–288.
non-sigmoidhiddenlayeractivationfunctions.
In Tierney, L.(1994).
Markovchainsforexploringpos- International Joint Conference on Neural Net- terior distributions.
Annals of Statistics 22(4), works, Volume1, pp.613–618.
IEEE.
1701–1762.
Stone, J.
V.
(2004).
Independent Component Analy- sis: ATutorial Introduction.
MITPress.
of Ill-Posed Problems.
V.
H.
Winston.
Sung, K.
K.
and T.
Poggio (1994).
Example-based Tino, P.
and I.
T.
Nabney (2002).
Hierarchical learning for view-based human face detection.
GTM: constructing localized non-linear projec- A.
I.
Memo1521, MIT.
tion manifolds in a principled way.
IEEE Trans- Learning: An Introduction.
MITPress.
gence24(5),639–656.
bust Bayesianmixturemodelling.
Neurocomput- ing directional curvatures to visualize folding ing 64,235–252.
patterns of the GTM projection manifolds.
In REFERENCES 727 Artificial Neural Networks – ICANN 2001, pp.
basedonempiricaldata.
Springer.
421–428.
Springer.
Vapnik, V.
N.(1995).
Thenatureofstatisticallearn- in Neural Information Processing Systems, Vol- Veropoulos, K., C.
Campbell, and N.
Cristianini ume11, pp.592–598.
MITPress.
(1999).
Controlling the sensitivity of support Tipping, M.
E.(2001).
Sparse Bayesianlearningand vector machines.
In Proceedings of the Interna- the relevance vector machine.
Journal of Ma- tional Joint Conferenceon Artificial Intelligence chine Learning Research 1,211–244.
(IJCAI99), Workshop ML3, pp.55–60.
tic principal component analysis.
Technical Re- Wavelets.
Wiley.
port NCRG/97/010, Neural Computing Research Viola, P.
and M.
Jones(2004).
Robustreal-timeface Group, Aston University.
detection.
International Journalof Computer Vi- sion57(2),137–154.
of probabilistic principal component analyzers.
Viterbi, A.
J.
(1967).
Error bounds for convolu- Neural Computation11(2),443–482.
tional codes and an asymptotically optimum de- coding algorithm.
IEEE Transactions on Infor- mation Theory IT-13,260–267.
abilistic principal component analysis.
Journal 611–622.
Digital Communication and Coding.
Mc Graw- Hill.
Tipping, M.
E.
and A.
Faul (2003).
Fast marginal Wahba, G.(1975).
Acomparisonof GCVand GML likelihood maximization for sparse Bayesian forchoosingthesmoothingparameterinthegen- eralized spline smoothing problem.
Numerical Proceedings Ninth International Workshop on Mathematics 24,383–393.
Artificial Intelligence and Statistics, Key West, (2005).
A new class of upper bounds on the log Tong, S.
and D.
Koller(2000).
Restricted Bayesop- partition function.
IEEE Transactions on Infor- timal classifiers.
In Proceedings 17th National mation Theory 51,2313–2335.
Conference on Artificial Intelligence, pp.
658– of posterior distributions.
Journal of the Royal Tresp, V.
(2001).
Scaling kernel-based systems to Statistical Society, B31(1),80–88.
largedatasets.
Data Miningand Knowledge Dis- covery5(3),197–211.
Smith(1999).
Bayesiannonparametricinference theoryof Brownianmotion.
Phys.
Rev.
36,823– (with discussion).
Journal of the Royal Statisti- 841.
cal Society, B61(3),485–527.
Communicationsofthe Associationfor Comput- Sankhya¯: The Indian Journalof Statistics.
Series ing Machinery 27,1134–1142.
A 26,359–372.
728 REFERENCES feed-forwardnetworks: aleast-squaresapproach Sparse Bayesian learning for efficient visual to generalisation.
IEEE Transactions on Neural tracking.
IEEETransactionson Pattern Analysis Networks5(3),363–371.
and Machine Intelligence27(8),1292–1304.
of Mathematics.
Chapmanand Hall, and CRC.
model conditional multivariate densities.
Neural Computation8(4),843–854.
Weston, J.
and C.
Watkins (1999).
Multi-class sup- ceedings ESANN’99, Brussels.
D-Facto Publica- sage passing.
Journal of Machine Learning Re- tions.
search 6,661–694.
Multivariate Statistics.
Wiley.
Kalman Filtering: A Practical Approach (Sec- onded.).
AIAA.
Widrow, B.
and M.
E.
Hoff (1960).
Adaptive switchingcircuits.
In IREWESCONConvention Record, Volume4, pp.96–104.
Reprintedin An- dersonand Rosenfeld(1988).
Widrow, B.
and M.
A.
Lehr(1990).30yearsofadap- tive neural networks: perceptron, madeline, and backpropagation.
Proceedingsofthe IEEE78(9), 1415–1442.
Wiegerinck, W.
and T.
Heskes (2003).
Fractional belief propagation.
In S.
Becker, S.
Thrun, and K.
Obermayer(Eds.), Advancesin Neural Infor- mation Processing Systems, Volume15, pp.455– 462.
MITPress.
Williams, C.
K.
I.
(1998).
Computation with infi- niteneuralnetworks.
Neural Computation10(5), 1203–1216.
Williams, C.
K.
I.
(1999).
Prediction with Gaussian processes: from linear regression to linear pre- dictionandbeyond.
In M.
I.
Jordan(Ed.), Learn- ing in Graphical Models, pp.
599–621.
MIT Press.
classification with Gaussian processes.
IEEE Transactions on Pattern Analysis and Machine Intelligence 20,1342–1351.
Nystrommethodtospeedupkernelmachines.
In Advancesin Neural Information Processing Sys- tems, Volume13, pp.682–688.
MITPress.
INDEX 729 Index Pagenumbersinboldindicatetheprimarysourceofinformationforthecorrespondingtopic.
1-of-K codingscheme,424 backgammon,3 backpropagation,241 acceptancecriterion,538,541,544 bagging,656 activationfunction,180,213,227 basisfunction,138,172,204,227 activeconstraint,328,709 batchtraining,240 Ada Boost,657,658 Baum-Welchalgorithm,618 adaline,196 Bayes’theorem,15 adaptiverejectionsampling,530 Bayes, Thomas,21 ADF, seeassumeddensityfiltering Bayesiananalysis, vii,9,21 AIC, see Akaikeinformationcriterion hierarchical,372 Akaikeinformationcriterion,33,217 modelaveraging,654 αfamilyofdivergences,469 Bayesianinformationcriterion,33,216 αrecursion,620 Bayesianmodelcomparison,161,473,483 ancestralsampling,365,525,613 Bayesiannetwork,360 annularflow,679 Bayesianprobability,21 ARmodel, seeautoregressivemodel beliefpropagation,403 arc,360 Bernoullidistribution,69,113,685 ARD, seeautomaticrelevancedetermination mixturemodel,444 ARMA, seeautoregressivemovingaverage Bernoulli, Jacob,69 assumeddensityfiltering,510 betadistribution,71,686 autoassociativenetworks,592 betarecursion,621 automatic relevance determination, 259, 312, 349, between-classcovariance,189 485,582 bias,27,149 autoregressivehidden Markovmodel,632 biasparameter,138,181,227,346 autoregressivemodel,609 bias-variancetrade-off,147 autoregressivemovingaverage,304 BIC, see Bayesianinformationcriterion binaryentropy,495 back-tracking,415,630 binomialdistribution,70,686 730 INDEX biologicalsequence,610 concentrationparameter,108,693 bipartitegraph,401 condensationalgorithm,646 bits,49 conditionalentropy,55 blindsourceseparation,591 conditionalexpectation,20 blockedpath,374,378,384 conditionalindependence,46,372,383 Boltzmanndistribution,387 conditionalmixturemodel, seemixturemodel Boltzmann, Ludwig Eduard,53 conditionalprobability,14 Booleanlogic,21 conjugateprior,68,98,117,490 boosting,657 convexduality,494 bootstrap,23,656 convexfunction,55,493 bootstrapfilter,646 convolutionalneuralnetwork,267 boxconstraints,333,342 correlationmatrix,567 Box-Mullermethod,527 costfunction,41 covariance,20 C4.5,663 between-class,189 calculusofvariations,462 within-class,189 canonicalcorrelationanalysis,565 covariancematrix canonicallinkfunction,212 diagonal,84 CART, seeclassificationandregressiontrees isotropic,84 Cauchydistribution,527,529,692 partitioned,85,307 causality,366 positivedefinite,308 CCA, seecanonicalcorrelationanalysis Cox’saxioms,21 centraldifferences,246 creditassignment,3 centrallimittheorem,78 cross-entropy error function, 206, 209, 235, 631, chaingraph,393 666 chaining,555 cross-validation,32,161 Chapman-Kolmogorovequations,397 cumulativedistributionfunction,18 childnode,361 curseofdimensionality,33,36 Choleskydecomposition,528 curvefitting,4 chunking,335 circularnormal, seevon Misesdistribution Dmap, seedependencymap classicalprobability,21 d-separation,373,378,443 classification,3 DAG, seedirectedacyclicgraph classificationandregressiontrees,663 DAGSVM,339 clique,385 dataaugmentation,537 clustering,3 datacompression,429 clutterproblem,511 decisionboundary,39,179 co-parents,383,492 decisionregion,39,179 code-bookvectors,429 decisionsurface, seedecisionboundary combiningmodels,45,653 decisiontheory,38 committee,655 decisiontree,654,663,673 completedataset,440 decompositionmethods,335 completingthesquare,86 degreesoffreedom,559 computationallearningtheory,326,344 degrees-of-freedomparameter,102,693 concavefunction,56 densityestimation,3,67 INDEX 731 densitynetwork,597 error-correctingoutputcodes,339 dependencymap,392 Euler, Leonhard,465 descendantnode,376 Euler-Lagrangeequations,705 designmatrix,142,347 evidenceapproximation,165,347,581 differentialentropy,53 evidencefunction,161 digammafunction,687 expectation,19 directedacyclicgraph,362 expectationconditionalmaximization,454 directedcycle,362 expectationmaximization,113,423,440 directedfactorization,381 Gaussianmixture,435 Dirichletdistribution,76,687 generalized,454 Dirichlet, Lejeune,77 samplingmethods,536 discriminantfunction,43,180,181 expectationpropagation,315,468,505 discriminativemodel,43,203 expectationstep,437 distortionmeasure,424 explainingaway,378 distributivelawofmultiplication,396 exploitation,3 DNA,610 exploration,3 documentretrieval,299 exponentialdistribution,526,688 dualrepresentation,293,329 exponentialfamily,68,113,202,490 dual-energygammadensitometry,678 extensivevariables,490 dynamicprogramming,411 dynamicalsystem,548 facedetection,2 facetracking,355 Estep, seeexpectationstep factoranalysis,583 earlystopping,259 mixturemodel,595 ECM, seeexpectationconditionalmaximization factorgraph,360,399,625 edge,360 factorloading,584 effectivenumberofobservations,72,101 factorialhidden Markovmodel,633 effectivenumberofparameters,9,170,281 factorizeddistribution,464,476 elliptical K-means,444 featureextraction,2 EM, seeexpectationmaximization featuremap,268 emissionprobability,611 featurespace,292,586 empirical Bayes, seeevidenceapproximation Fisherinformationmatrix,298 energyfunction,387 Fisherkernel,298 entropy,49 Fisher’slineardiscriminant,186 conditional,55 floodingschedule,417 differential,53 forwardkinematics,272 relative,55 forwardproblem,272 EP, seeexpectationpropagation forwardpropagation,228,243 -tube,341 forward-backwardalgorithm,618 -insensitiveerrorfunction,340 fractionalbeliefpropagation,517 equalityconstraint,709 frequentistprobability,21 equivalentkernel,159,301 fuelsystem,376 erffunction,211 functioninterpolation,299 errorbackpropagation, seebackpropagation functional,462,703 errorfunction,5,23 derivative,463 732 INDEX gammadensitometry,678 undirected,360 gammadistribution,529,688 Green’sfunction,299 gammafunction,71 GTM, seegenerativetopographicmapping gatingfunction,672 Gauss, Carl Friedrich,79 Hamilton, William Rowan,549 Gaussian,24,78,688 Hamiltoniandynamics,548 conditional,85,93 Hamiltonianfunction,549 marginal,88,93 Hammersley-Cliffordtheorem,387 maximumlikelihood,93 handwritingrecognition,1,610,614 mixture,110,270,273,430 handwrittendigit,565,614,677 sequentialestimation,94 head-to-headpath,376 sufficientstatistics,93 head-to-tailpath,375 wrapped,110 Heavisidestepfunction,206 Gaussiankernel,296 Hellingerdistance,470 Gaussianprocess,160,303 Hessianmatrix,167,215,217,238,249 Gaussianrandomfield,305 diagonalapproximation,250 Gaussian-gammadistribution,101,690 exactevaluation,253 Gaussian-Wishartdistribution,102,475,478,690 fastmultiplication,254 GEM, seeexpectationmaximization, generalized finitedifferences,252 generalization,2 inverse,252 generalizedlinearmodel,180,213 outerproductapproximation,251 generalized maximum likelihood, see evidence ap- heteroscedastic,273,311 proximation hidden Markovmodel,297,610 generativemodel,43,196,297,365,572,631 autoregressive,632 generativetopographicmapping,597 factorial,633 directionalcurvature,599 forward-backwardalgorithm,618 magnificationfactor,599 input-output,633 geodesicdistance,596 left-to-right,613 Gibbssampling,542 maximumlikelihood,615 blocking,546 scalingfactor,627 Gibbs, Josiah Willard,543 sum-productalgorithm,625 Giniindex,666 switching,644 globalminimum,237 variationalinference,625 gradientdescent,240 hiddenunit,227 Grammatrix,293 hiddenvariable,84,364,430,559 graph-cutalgorithm,390 hierarchical Bayesianmodel,372 graphicalmodel,359 hierarchicalmixtureofexperts,673 bipartite,401 hingeerrorfunction,337 directed,360 Hintondiagram,584 factorization,362,384 histogramdensityestimation,120 fullyconnected,361 HME, seehierarchicalmixtureofexperts inference,393 hold-outset,11 tree,398 homogeneousflow,679 treewidth,417 homogeneouskernel,292 triangulated,416 homogeneous Markovchain,540,608 INDEX 733 Hooke’slaw,580 iterative reweighted least squares, 207, 210, 316, hybrid Monte Carlo,548 354,672 hyperparameter,71,280,311,346,372,502 hyperprior,372 Jacobianmatrix,247,264 Jensen’sinequality,56 Imap, seeindependencemap jointree,416 i.
i.
d., seeindependentidenticallydistributed junctiontreealgorithm,392,416 ICA, seeindependentcomponentanalysis K nearestneighbours,125 ICM, seeiteratedconditionalmodes K-meansclusteringalgorithm,424,443 ID3,663 K-medoidsalgorithm,428 identifiability,435 Kalmanfilter,304,637 imagede-noising,387 extended,644 importancesampling,525,532 Kalmangainmatrix,639 importanceweights,533 Kalmansmoother,637 improperprior,118,259,472 Karhunen-Loe`vetransform,561 imputationstep,537 Karush-Kuhn-Tucker conditions, 330, 333, 342, imputation-posterioralgorithm,537 710 inactiveconstraint,328,709 kerneldensityestimator,122,326 incompletedataset,440 kernelfunction,123,292,294 independencemap,392 Fisher,298 independentcomponentanalysis,591 Gaussian,296 independentfactoranalysis,592 homogeneous,292 independentidenticallydistributed,26,379 nonvectorialinputs,297 independentvariables,17 stationary,292 independent, identicallydistributed,605 kernel PCA,586 inducedfactorization,485 kernelregression,300,302 inequalityconstraint,709 kernelsubstitution,292 inference,38,42 kerneltrick,292 informationcriterion,33 kineticenergy,549 informationgeometry,298 KKT, see Karush-Kuhn-Tuckerconditions informationtheory,48 KLdivergence, see Kullback-Leiblerdivergence input-outputhidden Markovmodel,633 kriging, see Gaussianprocess intensivevariables,490 Kullback-Leiblerdivergence,55,451,468,505 intrinsicdimensionality,559 invariance,261 Lagrangemultiplier,707 inversegammadistribution,101 Lagrange, Joseph-Louis,329 inversekinematics,272 Lagrangian,328,332,341,708 inverseproblem,272 laminarflow,678 inverse Wishartdistribution,102 Laplaceapproximation,213,217,278,315,354 IPalgorithm, seeimputation-posterioralgorithm Laplace, Pierre-Simon,24 IRLS, seeiterativereweightedleastsquares largemargin, seemargin Isingmodel,389 lasso,145 isomap,596 latentclassanalysis,444 isometricfeaturemap,596 latenttraitmodel,597 iteratedconditionalmodes,389,415 latentvariable,84,364,430,559 734 INDEX latticediagram,414,611,621,629 machinelearning, vii LDS, seelineardynamicalsystem macrostate,51 leapfrogdiscretization,551 Mahalanobisdistance,80 learning,2 manifold,38,590,595,681 learningrateparameter,240 MAP, seemaximumposterior least-mean-squaresalgorithm,144 margin,326,327,502 leave-one-out,33 error,334 likelihoodfunction,22 soft,332 likelihoodweightedsampling,534 marginallikelihood,162,165 lineardiscriminant,181 marginalprobability,14 Fisher,186 Markovblanket,382,384,545 lineardynamicalsystem,84,635 Markovboundary, see Markovblanket inference,638 Markovchain,397,539 linearindependence,696 firstorder,607 linearregression,138 homogeneous,540,608 EM,448 secondorder,608 mixturemodel,667 Markovchain Monte Carlo,537 variational,486 Markovmodel,607 linearsmoother,159 homogeneous,612 linear-Gaussianmodel,87,370 Markovnetwork, see Markovrandomfield linearlyseparable,179 Markovrandomfield,84,360,383 link,360 max-sumalgorithm,411,629 linkfunction,180,213 maximalclique,385 Liouville’s Theorem,550 maximalspanningtree,416 LLE, seelocallylinearembedding maximizationstep,437 LMSalgorithm, seeleast-mean-squaresalgorithm maximumlikelihood,9,23,26,116 localminimum,237 Gaussianmixture,432 localreceptivefield,268 singularities,480 locallylinearembedding,596 type2, seeevidenceapproximation locationparameter,118 maximummargin, seemargin logodds,197 maximumposterior,30,441 logicsampling,525 MCMC, see Markovchain Monte Carlo logisticregression,205,336 MDN, seemixturedensitynetwork Bayesian,217,498 MDS, seemultidimensionalscaling mixturemodel,670 mean,24 multiclass,209 meanfieldtheory,465 logisticsigmoid,114,139,197,205,220,227,495 meanvaluetheorem,52 logitfunction,197 measuretheory,19 loopybeliefpropagation,417 memory-basedmethods,292 lossfunction,41 messagepassing,396 lossmatrix,41 pendingmessage,417 losslessdatacompression,429 schedule,417 lossydatacompression,429 variational,491 lowerbound,484 Metropolisalgorithm,538 Mstep, seemaximizationstep Metropolis-Hastingsalgorithm,541 INDEX 735 microstate,51 Newton-Raphson,207,317 minimumrisk,44 node,360 Minkowskiloss,48 noiselesscodingtheorem,50 missingatrandom,441,579 nonidentifiability,585 missingdata,579 noninformativeprior,23,117 mixingcoefficient,111 nonparametricmethods,68,120 mixturecomponent,111 normaldistribution, see Gaussian mixturedensitynetwork,272,673 normalequations,142 mixturedistribution, seemixturemodel normal-gammadistribution,101,691 mixturemodel,162,423 normal-Wishartdistribution,102,475,478,691 conditional,273,666 normalizedexponential, seesoftmaxfunction linearregression,667 noveltydetection,44 logisticregression,670 ν-SVM,334 symmetries,483 objectrecognition,366 mixtureofexperts,672 observedvariable,364 mixtureof Gaussians,110,270,273,430 Occamfactor,217 MLP, seemultilayerperceptron oilflowdata,34,560,568,678 MNISTdata,677 Old Faithfuldata,110,479,484,681 modelcomparison,6,32,161,473,483 on-linelearning, seesequentiallearning modelevidence,161 one-versus-oneclassifier,183,339 modelselection,162 one-versus-the-restclassifier,182,338 momentmatching,506,510 orderedover-relaxation,545 momentumvariable,548 Ornstein-Uhlenbeckprocess,305 Monte Carlo EMalgorithm,536 orthogonalleastsquares,301 Monte Carlosampling,24,523 outlier,44,185,212 Moore-Penrosepseudo-inverse, seepseudo-inverse outliers,103 moralization,391,401 over-fitting,6,147,434,464 MRF, see Markovrandomfield over-relaxation,544 multidimensionalscaling,596 multilayerperceptron,226,229 PAClearning, seeprobablyapproximatelycorrect multimodality,272 PAC-Bayesianframework,345 multinomialdistribution,76,114,690 parametershrinkage,144 multiplicity,51 parentnode,361 mutualinformation,55,57 particlefilter,645 partitionfunction,386,554 Nadaraya-Watson, seekernelregression Parzenestimator, seekerneldensityestimator naive Bayesmodel,46,380 Parzenwindow,123 nats,50 patternrecognition, vii naturallanguagemodelling,610 PCA, seeprincipalcomponentanalysis naturalparameters,113 pendingmessage,417 nearest-neighbourmethods,124 perceptron,192 neuralnetwork,225 convergencetheorem,194 convolutional,267 hardware,196 regularization,256 perceptroncriterion,193 relationto Gaussianprocess,319 perfectmap,392 736 INDEX periodicvariable,105 sumrule,13,14,359 phasespace,549 theory,12 photonnoise,680 probablyapproximatelycorrect,344 plate,363 probitfunction,211,219 polynomialcurvefitting,4,362 probitregression,210 polytree,399 productruleofprobability,13,14,359 positionvariable,548 proposaldistribution,528,532,538 positivedefinitecovariance,81 protectedconjugategradients,335 positivedefinitematrix,701 proteinsequence,610 positivesemidefinitecovariance,81 pseudo-inverse,142,185 positivesemidefinitematrix,701 pseudo-randomnumbers,526 posteriorprobability,17 quadraticdiscriminant,199 posteriorstep,537 qualityparameter,351 potentialenergy,549 potentialfunction,386 radialbasisfunction,292,299 power EP,517 Rauch-Tung-Striebelequations,637 powermethod,563 regression,3 precisionmatrix,85 regressionfunction,47,95 precisionparameter,24 regularization,10 predictivedistribution,30,156 Tikhonov,267 preprocessing,2 regularizedleastsquares,144 principalcomponentanalysis,561,572,593 reinforcementlearning,3 Bayesian,580 rejectoption,42,45 EMalgorithm,577 rejectionsampling,528 Gibbssampling,583 relativeentropy,55 mixturedistribution,595 relevancevector,348 physicalanalogy,580 relevancevectormachine,161,345 principalcurve,595 responsibility,112,432,477 principalsubspace,561 ridgeregression,10 principalsurface,596 RMSerror, seeroot-mean-squareerror prior,17 Robbins-Monroalgorithm,95 conjugate,68,98,117,490 robotarm,272 consistent,257 robustness,103,185 improper,118,259,472 rootnode,399 noninformative,23,117 root-mean-squareerror,6 probabilisticgraphicalmodel, seegraphicalmodel Rosenblatt, Frank,193 probabilistic PCA,570 rotationinvariance,573,585 probability,12 RTSequations, see Rauch-Tung-Striebelequations Bayesian,21 runningintersectionproperty,416 classical,21 RVM, seerelevancevectormachine density,17 frequentist,21 samplemean,27 massfunction,19 samplevariance,27 prior,45 sampling-importance-resampling,534 productrule,13,14,359 scaleinvariance,119,261 INDEX 737 scaleparameter,119 statistical learning theory, see computational learn- scalingfactor,627 ingtheory,326,344 Schwarz criterion, see Bayesian information crite- steepestdescent,240 rion Stirling’sapproximation,51 self-organizingmap,598 stochastic,5 sequentialdata,605 stochastic EM,536 sequentialestimation,94 stochasticgradientdescent,144,240 sequentialgradientdescent,144,240 stochasticprocess,305 sequentiallearning,73,143 stratifiedflow,678 sequentialminimaloptimization,335 Student’st-distribution,102,483,691 serialmessagepassingschedule,417 subsampling,268 Shannon, Claude,55 sufficientstatistics,69,75,116 sharedparameters,368 sumruleofprobability,13,14,359 sum-of-squareserror,5,29,184,232,662 shrinkage,10 sum-productalgorithm,399,402 Shurcomplement,87 forhidden Markovmodel,625 sigmoid, seelogisticsigmoid supervisedlearning,3 simplex,76 supportvector,330 single-classsupportvectormachine,339 supportvectormachine,225 singularvaluedecomposition,143 forregression,339 sinusoidaldata,682 multiclass,338 SIR, seesampling-importance-resampling survivalofthefittest,646 skip-layerconnection,229 SVD, seesingularvaluedecomposition slackvariable,331 SVM, seesupportvectormachine slicesampling,546 switchinghidden Markovmodel,644 SMO, seesequentialminimaloptimization switchingstatespacemodel,644 smoothermatrix,159 syntheticdatasets,682 smoothingparameter,122 softmargin,332 tail-to-tailpath,374 softweightsharing,269 tangentdistance,265 softmaxfunction,115,198,236,274,356,497 tangentpropagation,262,263 SOM, seeself-organizingmap tappeddelayline,609 sparsity,145,347,349,582 targetvector,2 sparsityparameter,351 testset,2,32 spectrogram,606 thresholdparameter,181 speechrecognition,605,610 tiedparameters,368 sphereing,568 Tikhonovregularization,267 splinefunctions,139 timewarping,615 standarddeviation,24 tomography,679 standardizing,425,567 training,2 statespacemodel,609 trainingset,2 switching,644 transitionprobability,540,610 stationarykernel,292 translationinvariance,118,261 statisticalbias, seebias tree-reweightedmessagepassing,517 statisticalindependence, seeindependentvariables treewidth,417 738 INDEX trellisdiagram, seelatticediagram triangulatedgraph,416 type 2 maximum likelihood, see evidence approxi- mation undeterminedmultiplier, see Lagrangemultiplier undirectedgraph, see Markovrandomfield uniformdistribution,692 uniformsampling,534 uniquenesses,584 unobservedvariable, seelatentvariable unsupervisedlearning,3 utilityfunction,41 validationset,11,32 Vapnik-Chervonenkisdimension,344 variance,20,24,149 variationalinference,315,462,635 for Gaussianmixture,474 forhidden Markovmodel,625 local,493 VC dimension, see Vapnik-Chervonenkis dimen- sion vectorquantization,429 vertex, seenode visualization,3 Viterbialgorithm,415,629 von Misesdistribution,108,693 wavelets,139 weaklearner,657 weightdecay,10,144,257 weightparameter,227 weightsharing,268 soft,269 weightvector,181 weight-spacesymmetry,231,281 weightedleastsquares,668 well-determinedparameters,170 whitening,299,568 Wishartdistribution,102,693 within-classcovariance,189 Woodburyidentity,696 wrappeddistribution,110 Yellowstone National Park,110,681
Machine Learning Basics:

1. Machine Learning (ML):
   ML is a subset of Artificial Intelligence that enables systems to learn patterns from data and improve performance on tasks without explicit programming.

2. Types of Machine Learning:
   - Supervised Learning: Uses labeled data to predict outcomes. Examples: Regression, Classification.
   - Unsupervised Learning: Uses unlabeled data to find patterns. Examples: Clustering, Dimensionality Reduction.
   - Reinforcement Learning: Agents learn by interacting with an environment and receiving rewards or penalties.

3. Bias in Machine Learning:
   Types of bias include:
   - Sampling Bias: Data does not represent the entire population.
   - Selection Bias: Certain groups or features are overrepresented or underrepresented.
   - Measurement Bias: Inaccurate data due to flawed collection or sensors.
   - Algorithmic Bias: Model assumptions introduce systematic errors.
   - Confirmation Bias: Human biases during labeling or feature engineering.

4. Variance:
   Variance measures how much a model’s predictions change with different training data. High variance leads to overfitting.

5. Overfitting:
   When a model learns noise or details specific to the training data, reducing performance on unseen data.

6. Underfitting:
   When a model is too simple to capture the underlying patterns in the data, leading to poor training and test performance.

7. Bias-Variance Tradeoff:
   A balance between underfitting (high bias) and overfitting (high variance). Good models find an optimal tradeoff.

8. Regression:
   Predicting continuous numeric values. Examples: Linear Regression, Polynomial Regression.

9. Classification:
   Predicting discrete labels or categories. Examples: Logistic Regression, Decision Trees.

10. Clustering:
    Grouping similar data points without predefined labels. Examples: K-Means, Hierarchical Clustering.

11. Feature Engineering:
    Selecting, transforming, or creating input features to improve model performance.

12. Dimensionality Reduction:
    Reducing the number of features while preserving important information. Examples: PCA, t-SNE.

13. Cross-Validation:
    A technique for assessing model generalization by splitting data into training and validation sets multiple times.

14. Regularization:
    Methods like L1 (Lasso) and L2 (Ridge) that reduce model complexity and prevent overfitting.

15. Evaluation Metrics:
    - Accuracy: Percentage of correct predictions.
    - Precision: Correct positive predictions out of all positive predictions made.
    - Recall: Correct positive predictions out of all actual positives.
    - F1-Score: Harmonic mean of precision and recall.

16. Neural Networks:
    Computational models inspired by the human brain, consisting of layers of interconnected nodes (neurons) to learn patterns from data.

17. Deep Learning:
    A subset of ML using multi-layered neural networks for tasks like image recognition, NLP, and speech processing.

18. Decision Trees:
    A tree-structured model used for classification and regression tasks.

19. Ensemble Learning:
    Combines multiple models (e.g., Random Forest, Gradient Boosting) to improve predictive performance.

20. Support Vector Machines (SVM):
    A supervised algorithm that finds the optimal boundary (hyperplane) to separate different classes in data.
